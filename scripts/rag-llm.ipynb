{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "from pdfminer.high_level import extract_text\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! Yes, I am here. How can I assist you today?'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "chain.invoke(\"hi, are you there gpt?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plug and Play Context üö©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF to Txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_text(pdf_path, output_txt_path):\n",
    "    text = extract_text(pdf_path)\n",
    "    with open(output_txt_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = '../files/Computer Age Statistical Inference Book.pdf'\n",
    "output_txt_path = '../files/Computer Age Statistical Inference Book.txt'  \n",
    "# pdf_to_text(pdf_path, output_txt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Context for LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The Work, Computer Age Statistical Inference, was Ô¨Årst published by Cambridge University Press.\\nc(cid:13) in the Work, Bradley Efron and Trevor Hastie, 2016.\\nCambridge University Press‚Äôs catalogue entry for the Work can be found at http: // www. cambridge. org/\\n9781107149892\\nNB: The copy of the Work, as displayed on this website, can be purchased through Cambridge University\\nPress and other standard distribution channels. This copy is made available for personal use only and must\\nnot be adapted, sold or re-distributed.\\nCorrected November 10, 2017.\\n\\nThe twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. ‚ÄúBig data,‚Äù ‚Äúdata science,‚Äù and ‚Äúmachine learning‚Äù have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going?This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories ‚Äì Bayesian, frequentist, Fisherian ‚Äì individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.Efron & hastiEComputEr agE  statistiCal infErEnCE‚ÄúHow and why is computational statistics taking over the world? In this serious work of synthesis that is also fun to read, Efron and Hastie give their take on the unreasonable effectiveness of statistics and machine learning in the context of a series of clear, historically informed examples.‚Äù‚Äî Andrew Gelman, Columbia University ‚ÄúComputer Age Statistical Inference is written especially for those who want to hear the big ideas, and see them instantiated through the essential mathematics that defines statistical analysis. It makes a great supplement to the traditional curricula for beginning graduate students.‚Äù‚Äî Rob Kass, Carnegie Mellon University ‚ÄúThis is a terrific book. It gives a clear, accessible, and entertaining account of the interplay between theory and methodological development that has driven statistics in the computer age. The authors succeed brilliantly in locating contemporary algorithmic methodologies for analysis of ‚Äòbig data‚Äô within the framework of established statistical theory.‚Äù‚Äî Alastair Young, Imperial College London ‚ÄúThis is a guided tour of modern statistics that emphasizes the conceptual and computational advances of the last century. Authored by two masters of the field, it offers just the right mix of mathematical analysis and insightful commentary.‚Äù‚Äî Hal Varian, Google ‚ÄúEfron and Hastie guide us through the maze of breakthrough statistical methodologies following the computing evolution: why they were developed, their properties, and how they are used. Highlighting their origins, the book helps us understand each method‚Äôs roles in inference and/or prediction.‚Äù‚Äî Galit Shmueli, National Tsing Hua University ‚ÄúA masterful guide to how the inferential bases of classical statistics can provide a principled disciplinary frame for the data science of the twenty-first century.‚Äù ‚Äî Stephen Stigler, University of Chicago, author of Seven Pillars of Statistical Wisdom ‚ÄúA refreshing view of modern statistics. Algorithmics are put on equal footing with intuition, properties, and the abstract arguments behind them. The methods covered are indispensable to practicing statistical analysts in today‚Äôs big data and big computing landscape.‚Äù‚Äî Robert Gramacy, The University of Chicago Booth School of BusinessBradley Efron is Max H. Stein Professor, Professor of Statistics, and Professor of Biomedical Data Science at Stanford University. He has held visiting faculty appointments at Harvard, UC Berkeley, and Imperial College London. Efron has worked extensively on theories of statistical inference, and is the inventor of the bootstrap sampling technique. He received the National Medal of Science in 2005 and the Guy Medal in Gold of the Royal Statistical Society in 2014. Trevor Hastie is John A. Overdeck Professor, Professor of Statistics, and Professor of Biomedical Data Science at Stanford University. He is coauthor of Elements of Statistical Learning, a key text in the field of modern data analysis. He is also known for his work on generalized additive models and principal curves, and for his contributions to the R computing environment. Hastie was awarded the Emmanuel and Carol Parzen prize for Statistical Innovation in 2014. Institute of Mathematical Statistics MonographsEditorial Board:D. R. Cox (University of Oxford)B. Hambly (University of Oxford)S. Holmes (Stanford University)J. Wellner (University of Washington)Cover illustration: Pacific Ocean wave, North Shore, Oahu, Hawaii. ¬© Brian Sytnyk / Getty Images.Cover designed by Zoe Naylor.PRINTED IN THE UNITED KINGDOMComputEr agE statistiCal  infErEnCEalgorithms, EvidEnCE, and data sCiEnCEBradlEy Efron trEvor hastiE9781107149892 Efron & Hastie JKT C M Y K\\x0cComputer Age Statistical Inference\\n\\nAlgorithms, Evidence, and Data Science\\n\\nBradley Efron\\n\\nTrevor Hastie\\n\\nStanford University\\n\\n\\x0c\\x0c\\x0cTo Donna and Lynda\\n\\n\\x0cviii\\n\\n\\x0cContents\\n\\nPreface\\nAcknowledgments\\nNotation\\n\\nPart I Classic Statistical Inference\\n\\nAlgorithms and Inference\\nA Regression Example\\nHypothesis Testing\\nNotes\\n\\nFrequentist Inference\\nFrequentism in Practice\\nFrequentist Optimality\\nNotes and Details\\n\\nBayesian Inference\\nTwo Examples\\nUninformative Prior Distributions\\nFlaws in Frequentist Inference\\nA Bayesian/Frequentist Comparison List\\nNotes and Details\\n\\nFisherian Inference and Maximum Likelihood Estimation\\nLikelihood and Maximum Likelihood\\nFisher Information and the MLE\\nConditional Inference\\nPermutation and Randomization\\nNotes and Details\\n\\n1\\n1.1\\n1.2\\n1.3\\n\\n2\\n2.1\\n2.2\\n2.3\\n\\n3\\n3.1\\n3.2\\n3.3\\n3.4\\n3.5\\n\\n4\\n4.1\\n4.2\\n4.3\\n4.4\\n4.5\\n\\n5\\n\\nParametric Models and Exponential Families\\n\\nix\\n\\nxv\\nxviii\\nxix\\n\\n1\\n\\n3\\n4\\n8\\n11\\n\\n12\\n14\\n18\\n20\\n\\n22\\n24\\n28\\n30\\n33\\n36\\n\\n38\\n38\\n41\\n45\\n49\\n51\\n\\n53\\n\\n\\x0cx\\n\\n5.1\\n5.2\\n5.3\\n5.4\\n5.5\\n5.6\\n\\n6\\n6.1\\n6.2\\n6.3\\n6.4\\n6.5\\n\\n7\\n7.1\\n7.2\\n7.3\\n7.4\\n7.5\\n\\n8\\n8.1\\n8.2\\n8.3\\n8.4\\n8.5\\n\\nContents\\n\\nUnivariate Families\\nThe Multivariate Normal Distribution\\nFisher‚Äôs Information Bound for Multiparameter Families\\nThe Multinomial Distribution\\nExponential Families\\nNotes and Details\\n\\nPart II Early Computer-Age Methods\\n\\nEmpirical Bayes\\nRobbins‚Äô Formula\\nThe Missing-Species Problem\\nA Medical Example\\nIndirect Evidence 1\\nNotes and Details\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\nThe James‚ÄìStein Estimator\\nThe Baseball Players\\nRidge Regression\\nIndirect Evidence 2\\nNotes and Details\\n\\nGeneralized Linear Models and Regression Trees\\nLogistic Regression\\nGeneralized Linear Models\\nPoisson Regression\\nRegression Trees\\nNotes and Details\\n\\nSurvival Analysis and the EM Algorithm\\nLife Tables and Hazard Rates\\nCensored Data and the Kaplan‚ÄìMeier Estimate\\nThe Log-Rank Test\\nThe Proportional Hazards Model\\n\\n9\\n9.1\\n9.2\\n9.3\\n9.4\\n9.5 Missing Data and the EM Algorithm\\n9.6\\n\\nNotes and Details\\n\\n10\\nThe Jackknife and the Bootstrap\\n10.1 The Jackknife Estimate of Standard Error\\n10.2 The Nonparametric Bootstrap\\n10.3 Resampling Plans\\n\\n54\\n55\\n59\\n61\\n64\\n69\\n\\n73\\n\\n75\\n75\\n78\\n84\\n88\\n88\\n\\n91\\n91\\n94\\n97\\n102\\n104\\n\\n108\\n109\\n116\\n120\\n124\\n128\\n\\n131\\n131\\n134\\n139\\n143\\n146\\n150\\n\\n155\\n156\\n159\\n163\\n\\n\\x0cContents\\n\\n10.4 The Parametric Bootstrap\\n10.5\\n10.6 Notes and Details\\n\\nInÔ¨Çuence Functions and Robust Estimation\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\n11\\n11.1 Neyman‚Äôs Construction for One-Parameter Problems\\n11.2 The Percentile Method\\n11.3 Bias-Corrected ConÔ¨Ådence Intervals\\n11.4 Second-Order Accuracy\\n11.5 Bootstrap-t Intervals\\n11.6 Objective Bayes Intervals and the ConÔ¨Ådence Distribution\\n11.7 Notes and Details\\n\\nCross-Validation and Cp Estimates of Prediction Error\\n\\n12\\n12.1 Prediction Rules\\n12.2 Cross-Validation\\n12.3 Covariance Penalties\\n12.4 Training, Validation, and Ephemeral Predictors\\n12.5 Notes and Details\\n\\nObjective Bayes Inference and MCMC\\n\\n13\\n13.1 Objective Prior Distributions\\n13.2 Conjugate Prior Distributions\\n13.3 Model Selection and the Bayesian Information Criterion\\n13.4 Gibbs Sampling and MCMC\\n13.5 Example: Modeling Population Admixture\\n13.6 Notes and Details\\n\\n14\\n\\nPostwar Statistical Inference and Methodology\\n\\nPart III Twenty-First-Century Topics\\n\\nLarge-Scale Hypothesis Testing and FDRs\\n\\n15\\n15.1 Large-Scale Testing\\n15.2 False-Discovery Rates\\n15.3 Empirical Bayes Large-Scale Testing\\n15.4 Local False-Discovery Rates\\n15.5 Choice of the Null Distribution\\n15.6 Relevance\\n15.7 Notes and Details\\n\\n16\\n\\nSparse Modeling and the Lasso\\n\\nxi\\n\\n169\\n174\\n177\\n\\n181\\n181\\n185\\n190\\n192\\n195\\n198\\n204\\n\\n208\\n208\\n213\\n218\\n227\\n230\\n\\n233\\n234\\n237\\n243\\n251\\n256\\n261\\n\\n264\\n\\n269\\n\\n271\\n272\\n275\\n278\\n282\\n286\\n290\\n294\\n\\n298\\n\\n\\x0cxii\\n\\nContents\\n\\n16.1 Forward Stepwise Regression\\n16.2 The Lasso\\n16.3 Fitting Lasso Models\\n16.4 Least-Angle Regression\\n16.5 Fitting Generalized Lasso Models\\n16.6 Post-Selection Inference for the Lasso\\n16.7 Connections and Extensions\\n16.8 Notes and Details\\n\\nRandom Forests and Boosting\\n\\n17\\n17.1 Random Forests\\n17.2 Boosting with Squared-Error Loss\\n17.3 Gradient Boosting\\n17.4 Adaboost: the Original Boosting Algorithm\\n17.5 Connections and Extensions\\n17.6 Notes and Details\\n\\nNeural Networks and Deep Learning\\n\\n18\\n18.1 Neural Networks and the Handwritten Digit Problem\\n18.2 Fitting a Neural Network\\n18.3 Autoencoders\\n18.4 Deep Learning\\n18.5 Learning a Deep Network\\n18.6 Notes and Details\\n\\nSupport-Vector Machines and Kernel Methods\\n\\n19\\n19.1 Optimal Separating Hyperplane\\n19.2 Soft-Margin ClassiÔ¨Åer\\n19.3 SVM Criterion as Loss Plus Penalty\\n19.4 Computations and the Kernel Trick\\n19.5 Function Fitting Using Kernels\\n19.6 Example: String Kernels for Protein ClassiÔ¨Åcation\\n19.7 SVMs: Concluding Remarks\\n19.8 Kernel Smoothing and Local Regression\\n19.9 Notes and Details\\n\\nInference After Model Selection\\n\\n20\\n20.1 Simultaneous ConÔ¨Ådence Intervals\\n20.2 Accuracy After Model Selection\\n20.3 Selection Bias\\n20.4 Combined Bayes‚ÄìFrequentist Estimation\\n20.5 Notes and Details\\n\\n299\\n303\\n308\\n309\\n313\\n317\\n319\\n321\\n\\n324\\n325\\n333\\n338\\n341\\n345\\n347\\n\\n351\\n353\\n356\\n362\\n364\\n368\\n371\\n\\n375\\n376\\n378\\n379\\n381\\n384\\n385\\n387\\n387\\n390\\n\\n394\\n395\\n402\\n408\\n412\\n417\\n\\n\\x0cContents\\n\\nEmpirical Bayes Estimation Strategies\\n\\n21\\n21.1 Bayes Deconvolution\\n21.2 g-Modeling and Estimation\\n21.3 Likelihood, Regularization, and Accuracy\\n21.4 Two Examples\\n21.5 Generalized Linear Mixed Models\\n21.6 Deconvolution and f -Modeling\\n21.7 Notes and Details\\n\\nEpilogue\\nReferences\\nAuthor Index\\nSubject Index\\n\\nxiii\\n\\n421\\n421\\n424\\n427\\n432\\n437\\n440\\n444\\n\\n446\\n453\\n463\\n467\\n\\n\\x0cxiv\\n\\n\\x0cPreface\\n\\nStatistical inference is an unusually wide-ranging discipline, located as it is\\nat the triple-point of mathematics, empirical science, and philosophy. The\\ndiscipline can be said to date from 1763, with the publication of Bayes‚Äô\\nrule (representing the philosophical side of the subject; the rule‚Äôs early ad-\\nvocates considered it an argument for the existence of God). The most re-\\ncent quarter of this 250-year history‚Äîfrom the 1950s to the present‚Äîis\\nthe ‚Äúcomputer age‚Äù of our book‚Äôs title, the time when computation, the tra-\\nditional bottleneck of statistical applications, became faster and easier by a\\nfactor of a million.\\n\\nThe book is an examination of how statistics has evolved over the past\\nsixty years‚Äîan aerial view of a vast subject, but seen from the height of a\\nsmall plane, not a jetliner or satellite. The individual chapters take up a se-\\nries of inÔ¨Çuential topics‚Äîgeneralized linear models, survival analysis, the\\njackknife and bootstrap, false-discovery rates, empirical Bayes, MCMC,\\nneural nets, and a dozen more‚Äîdescribing for each the key methodologi-\\ncal developments and their inferential justiÔ¨Åcation.\\n\\nNeedless to say, the role of electronic computation is central to our\\nstory. This doesn‚Äôt mean that every advance was computer-related. A land\\nbridge had opened to a new continent but not all were eager to cross.\\nTopics such as empirical Bayes and James‚ÄìStein estimation could have\\nemerged just as well under the constraints of mechanical computation. Oth-\\ners, like the bootstrap and proportional hazards, were pureborn children of\\nthe computer age. Almost all topics in twenty-Ô¨Årst-century statistics are\\nnow computer-dependent, but it will take our small plane a while to reach\\nthe new millennium.\\n\\nDictionary deÔ¨Ånitions of statistical inference tend to equate it with the\\nentire discipline. This has become less satisfactory in the ‚Äúbig data‚Äù era of\\nimmense computer-based processing algorithms. Here we will attempt, not\\nalways consistently, to separate the two aspects of the statistical enterprise:\\nalgorithmic developments aimed at speciÔ¨Åc problem areas, for instance\\n\\nxv\\n\\n\\x0cxvi\\n\\nPreface\\n\\nrandom forests for prediction, as distinct from the inferential arguments\\noffered in their support.\\n\\nVery broadly speaking, algorithms are what statisticians do while infer-\\nence says why they do them. A particularly energetic brand of the statisti-\\ncal enterprise has Ô¨Çourished in the new century, data science, emphasizing\\nalgorithmic thinking rather than its inferential justiÔ¨Åcation. The later chap-\\nters of our book, where large-scale prediction algorithms such as boosting\\nand deep learning are examined, illustrate the data-science point of view.\\n(See the epilogue for a little more on the sometimes fraught statistics/data\\nscience marriage.)\\n\\nThere are no such subjects as Biological Inference or Astronomical In-\\nference or Geological Inference. Why do we need ‚ÄúStatistical Inference‚Äù?\\nThe answer is simple: the natural sciences have nature to judge the ac-\\ncuracy of their ideas. Statistics operates one step back from Nature, most\\noften interpreting the observations of natural scientists. Without Nature to\\nserve as a disinterested referee, we need a system of mathematical logic for\\nguidance and correction. Statistical inference is that system, distilled from\\ntwo and a half centuries of data-analytic experience.\\n\\nThe book proceeds historically, in three parts. The great themes of clas-\\nsical inference, Bayesian, frequentist, and Fisherian, reviewed in Part I,\\nwere set in place before the age of electronic computation. Modern practice\\nhas vastly extended their reach without changing the basic outlines. (An\\nanalogy with classical and modern literature might be made.) Part II con-\\ncerns early computer-age developments, from the 1950s through the 1990s.\\nAs a transitional period, this is the time when it is easiest to see the ef-\\nfects, or noneffects, of fast computation on the progress of statistical meth-\\nodology, both in its theory and practice. Part III, ‚ÄúTwenty-First-Century\\ntopics,‚Äù brings the story up to the present. Ours is a time of enormously\\nambitious algorithms (‚Äúmachine learning‚Äù being the somewhat disquieting\\ncatchphrase). Their justiÔ¨Åcation is the ongoing task of modern statistical\\ninference.\\n\\nNeither a catalog nor an encyclopedia, the book‚Äôs topics were chosen as\\napt illustrations of the interplay between computational methodology and\\ninferential theory. Some missing topics that might have served just as well\\ninclude time series, general estimating equations, causal inference, graph-\\nical models, and experimental design. In any case, there is no implication\\nthat the topics presented here are the only ones worthy of discussion.\\n\\nAlso underrepresented are asymptotics and decision theory, the ‚Äúmath\\nstat‚Äù side of the Ô¨Åeld. Our intention was to maintain a technical level of\\ndiscussion appropriate to Masters‚Äô-level statisticians or Ô¨Årst-year PhD stu-\\n\\n\\x0cPreface\\n\\nxvii\\n\\ndents. Inevitably, some of the presentation drifts into more difÔ¨Åcult waters,\\nmore from the nature of the statistical ideas than the mathematics. Readers\\nwho Ô¨Ånd our aerial view circling too long over some topic shouldn‚Äôt hesi-\\ntate to move ahead in the book. For the most part, the chapters can be read\\nindependently of each other (though there is a connecting overall theme).\\nThis comment applies especially to nonstatisticians who have picked up\\nthe book because of interest in some particular topic, say survival analysis\\nor boosting.\\n\\nUseful disciplines that serve a wide variety of demanding clients run\\nthe risk of losing their center. Statistics has managed, for the most part,\\nto maintain its philosophical cohesion despite a rising curve of outside de-\\nmand. The center of the Ô¨Åeld has in fact moved in the past sixty years, from\\nits traditional home in mathematics and logic toward a more computational\\nfocus. Our book traces that movement on a topic-by-topic basis. An answer\\nto the intriguing question ‚ÄúWhat happens next?‚Äù won‚Äôt be attempted here,\\nexcept for a few words in the epilogue, where the rise of data science is\\ndiscussed.\\n\\n\\x0cAcknowledgments\\n\\nWe are indebted to Cindy Kirby for her skillful work in the preparation of\\nthis book, and Galit Shmueli for her helpful comments on an earlier draft.\\nAt Cambridge University Press, a huge thank you to Steven Holt for his ex-\\ncellent copy editing, Clare Dennison for guiding us through the production\\nphase, and to Diana Gillooly, our editor, for her unfailing support.\\n\\nBradley Efron\\nTrevor Hastie\\nDepartment of Statistics\\nStanford University\\nMay 2016\\n\\nxviii\\n\\n\\x0cNotation\\n\\nxix\\n\\nThroughout the book the numbered (cid:142) sign indicates a technical note or\\nreference element which is elaborated on at the end of the chapter. There,\\nnext to the number, the page number of the referenced location is given in\\nparenthesis. For example, lowess in the notes on page 11 was referenced\\nvia a (cid:142)1 on page 6. Matrices such as ‚Ä† are represented in bold font, as\\nare certain vectors such as y, a data vector with n elements. Most other\\nvectors, such as coefÔ¨Åcient vectors, are typically not bold. We use a dark\\ngreen typewriter font to indicate data set names such as prostate,\\nvariable names such as prog from data sets, and R commands such as\\nglmnet or locfdr. No bibliographic references are given in the body of\\nthe text; important references are given in the endnotes of each chapter.\\n\\n\\x0c\\x0cPart I\\n\\nClassic Statistical Inference\\n\\n\\x0c\\x0c1\\n\\nAlgorithms and Inference\\n\\nStatistics is the science of learning from experience, particularly experi-\\nence that arrives a little bit at a time: the successes and failures of a new\\nexperimental drug, the uncertain measurements of an asteroid‚Äôs path to-\\nward Earth. It may seem surprising that any one theory can cover such an\\namorphous target as ‚Äúlearning from experience.‚Äù In fact, there are two main\\nstatistical theories, Bayesianism and frequentism, whose connections and\\ndisagreements animate many of the succeeding chapters.\\n\\nFirst, however, we want to discuss a less philosophical, more operational\\ndivision of labor that applies to both theories: between the algorithmic and\\ninferential aspects of statistical analysis. The distinction begins with the\\nmost basic, and most popular, statistical method, averaging. Suppose we\\nhave observed numbers x1; x2; : : : ; xn applying to some phenomenon of\\ninterest, perhaps the automobile accident rates in the n D 50 states. The\\nmean\\n\\nNx D\\n\\nn\\nX\\n\\niD1\\n\\nxi =n\\n\\n(1.1)\\n\\nsummarizes the results in a single number.\\n\\nHow accurate is that number? The textbook answer is given in terms of\\n\\nthe standard error,\\n\\nbse D\\n\\n\" n\\nX\\n\\niD1\\n\\n.xi (cid:0) Nx/2ƒ± .n.n (cid:0) 1//\\n\\n#1=2\\n\\n:\\n\\n(1.2)\\n\\nHere averaging (1.1) is the algorithm, while the standard error provides an\\ninference of the algorithm‚Äôs accuracy. It is a surprising, and crucial, aspect\\nof statistical theory that the same data that supplies an estimate can also\\nassess its accuracy.1\\n\\n1 ‚ÄúInference‚Äù concerns more than accuracy: speaking broadly, algorithms say what the\\n\\nstatistician does while inference says why he or she does it.\\n\\n3\\n\\n\\x0c4\\n\\nAlgorithms and Inference\\n\\nOf course, bse (1.2) is itself an algorithm, which could be (and is) subject\\nto further inferential analysis concerning its accuracy. The point is that\\nthe algorithm comes Ô¨Årst and the inference follows at a second level of\\nstatistical consideration. In practice this means that algorithmic invention\\nis a more free-wheeling and adventurous enterprise, with inference playing\\ncatch-up as it strives to assess the accuracy, good or bad, of some hot new\\nalgorithmic methodology.\\n\\nIf the inference/algorithm race is a tortoise-and-hare affair, then modern\\nelectronic computation has bred a bionic hare. There are two effects at work\\nhere: computer-based technology allows scientists to collect enormous data\\nsets, orders of magnitude larger than those that classic statistical theory\\nwas designed to deal with; huge data demands new methodology, and the\\ndemand is being met by a burst of innovative computer-based statistical\\nalgorithms. When one reads of ‚Äúbig data‚Äù in the news, it is usually these\\nalgorithms playing the starring roles.\\n\\nOur book‚Äôs title, Computer Age Statistical Inference, emphasizes the tor-\\ntoise‚Äôs side of the story. The past few decades have been a golden age of\\nstatistical methodology. It hasn‚Äôt been, quite, a golden age for statistical\\ninference, but it has not been a dark age either. The efÔ¨Çorescence of am-\\nbitious new algorithms has forced an evolution (though not a revolution)\\nin inference, the theories by which statisticians choose among competing\\nmethods. The book traces the interplay between methodology and infer-\\nence as it has developed since the 1950s, the beginning of our discipline‚Äôs\\ncomputer age. As a preview, we end this chapter with two examples illus-\\ntrating the transition from classic to computer-age practice.\\n\\n1.1 A Regression Example\\n\\nFigure 1.1 concerns a study of kidney function. Data points .xi ; yi / have\\nbeen observed for n D 157 healthy volunteers, with xi the ith volunteer‚Äôs\\nage in years, and yi a composite measure ‚Äútot‚Äù of overall function. Kid-\\nney function generally declines with age, as evident in the downward scat-\\nter of the points. The rate of decline is an important question in kidney\\ntransplantation: in the past, potential donors past age 60 were prohibited,\\nthough, given a shortage of donors, this is no longer enforced.\\n\\nThe solid line in Figure 1.1 is a linear regression\\n\\ny D OÀá0 C OÀá1x\\n\\n(1.3)\\n\\nÔ¨Åt to the data by least squares, that is by minimizing the sum of squared\\n\\n\\x0c1.1 A Regression Example\\n\\n5\\n\\nFigure 1.1 Kidney Ô¨Åtness tot vs age for 157 volunteers. The\\nline is a linear regression Ô¨Åt, showing Àô2 standard errors at\\nselected values of age.\\n\\ndeviations\\n\\nn\\nX\\n\\n.yi (cid:0) Àá0 (cid:0) Àá1xi /2\\n\\ni D1\\n\\n(1.4)\\n\\nover all choices of .Àá0; Àá1/. The least squares algorithm, which dates back\\nto Gauss and Legendre in the early 1800s, gives OÀá0 D 2:86 and OÀá1 D\\n(cid:0)0:079 as the least squares estimates. We can read off of the Ô¨Åtted line\\nan estimated value of kidney Ô¨Åtness for any chosen age. The top line of\\nTable 1.1 shows estimate 1.29 at age 20, down to (cid:0)3:43 at age 80.\\n\\nHow accurate are these estimates? This is where inference comes in:\\nan extended version of formula (1.2), also going back to the 1800s, pro-\\nvides the standard errors, shown in line 2 of the table. The vertical bars in\\nFigure 1.1 are Àô two standard errors, giving them about 95% chance of\\ncontaining the true expected value of tot at each age.\\n\\nThat 95% coverage depends on the validity of the linear regression model\\n(1.3). We might instead try a quadratic regression y D OÀá0 C OÀá1x C OÀá2x2,\\nor a cubic, etc., all of this being well within the reach of pre-computer\\nstatistical theory.\\n\\n*************************************************************************************************************************************************************2030405060708090‚àí6‚àí4‚àí2024agetot\\x0c6\\n\\nAlgorithms and Inference\\n\\nTable 1.1 Regression analysis of the kidney data; (1) linear regression\\nestimates; (2) their standard errors; (3) lowess estimates; (4) their\\nbootstrap standard errors.\\n\\nage\\n\\n20\\n\\n30\\n\\n40\\n\\n50\\n\\n60\\n\\n70\\n\\n80\\n\\n1. linear regression\\n2. std error\\n\\n3. lowess\\n4. bootstrap std error\\n\\n1.29\\n.21\\n\\n1.66\\n.71\\n\\n.50 (cid:0).28 (cid:0)1.07 (cid:0)1.86 (cid:0)2.64 (cid:0)3.43\\n.42\\n.15\\n\\n.15\\n\\n.26\\n\\n.34\\n\\n.19\\n\\n.65 (cid:0).59 (cid:0)1.27 (cid:0)1.91 (cid:0)2.68 (cid:0)3.50\\n.70\\n.23\\n\\n.31\\n\\n.37\\n\\n.32\\n\\n.47\\n\\nFigure 1.2 Local polynomial lowess(x,y,1/3) Ô¨Åt to the\\nkidney-Ô¨Åtness data, with Àô2 bootstrap standard deviations.\\n\\n(cid:142)1\\n\\nA modern computer-based algorithm lowess produced the somewhat\\nbumpy regression curve in Figure 1.2. The lowess (cid:142) 2 algorithm moves\\nits attention along the x-axis, Ô¨Åtting local polynomial curves of differing\\ndegrees to nearby .x; y/ points. (The 1/3 in the call3 lowess(x,y,1/3)\\n\\n2 Here and throughout the book, the numbered (cid:142) sign indicates a technical note or\\n\\nreference element which is elaborated on at the end of the chapter.\\n\\n3 Here and in all our examples we are employing the language R, itself one of the key\\n\\ndevelopments in computer-based statistical methodology.\\n\\n*************************************************************************************************************************************************************2030405060708090‚àí6‚àí4‚àí2024agetot\\x0c1.1 A Regression Example\\n\\n7\\n\\ndetermines the deÔ¨Ånition of local.) Repeated passes over the x-axis reÔ¨Åne\\nthe Ô¨Åt, reducing the effects of occasional anomalous points. The Ô¨Åtted curve\\nin Figure 1.2 is nearly linear at the right, but more complicated at the left\\nwhere points are more densely packed. It is Ô¨Çat between ages 25 and 35,\\na potentially important difference from the uniform decline portrayed in\\nFigure 1.1.\\n\\nThere is no formula such as (1.2) to infer the accuracy of the lowess\\ncurve. Instead, a computer-intensive inferential engine, the bootstrap, was\\nused to calculate the error bars in Figure 1.2. A bootstrap data set is pro-\\nduced by resampling 157 pairs .xi ; yi / from the original 157 with replace-\\nment, so perhaps .x1; y1/ might show up twice in the bootstrap sample,\\n.x2; y2/ might be missing, .x3; y3/ present once, etc. Applying lowess\\nto the bootstrap sample generates a bootstrap replication of the original\\ncalculation.\\n\\nFigure 1.3 25 bootstrap replications of lowess(x,y,1/3).\\n\\nFigure 1.3 shows the Ô¨Årst 25 (of 250) bootstrap lowess replications\\nbouncing around the original curve from Figure 1.2. The variability of the\\nreplications at any one age, the bootstrap standard deviation, determined\\nthe original curve‚Äôs accuracy. How and why the bootstrap works is dis-\\ncussed in Chapter 10. It has the great virtue of assessing estimation accu-\\n\\n2030405060708090‚àí4‚àí2024agetot\\x0c8\\n\\nAlgorithms and Inference\\n\\nracy for any algorithm, no matter how complicated. The price is a hundred-\\nor thousand-fold increase in computation, unthinkable in 1930, but routine\\nnow.\\n\\nThe bottom two lines of Table 1.1 show the lowess estimates and\\ntheir standard errors. We have paid a price for the increased Ô¨Çexibility of\\nlowess, its standard errors roughly doubling those for linear regression.\\n\\n1.2 Hypothesis Testing\\n\\nOur second example concerns the march of methodology and inference\\nfor hypothesis testing rather than estimation: 72 leukemia patients, 47 with\\nALL (acute lymphoblastic leukemia) and 25 with AML (acute myeloid leuk-\\nemia, a worse prognosis) have each had genetic activity measured for a\\npanel of 7,128 genes. The histograms in Figure 1.4 compare the genetic\\nactivities in the two groups for gene 136.\\n\\nFigure 1.4 Scores for gene 136, leukemia data. Top ALL\\n(n D 47), bottom AML (n D 25). A two-sample t -statistic D 3:01\\nwith p-value D :0036.\\n\\nThe AML group appears to show greater activity, the mean values being\\n\\nALL D 0:752 and AML D 0:950:\\n\\n(1.5)\\n\\n ALL scores ‚àí mean .752  0.20.40.60.81.01.21.41.60246810AML scores ‚àí mean .950 0.20.40.60.81.01.21.41.60246810\\x0c1.2 Hypothesis Testing\\n\\n9\\n\\nIs the perceived difference genuine, or perhaps, as people like to say, ‚Äúa\\nstatistical Ô¨Çuke‚Äù? The classic answer to this question is via a two-sample\\nt-statistic,\\n\\nt D\\n\\nAML (cid:0) ALL\\n\\nbsd\\n\\n;\\n\\n(1.6)\\n\\nwhere bsd is an estimate of the numerator‚Äôs standard deviation.4\\n\\nDividing by bsd allows us (under Gaussian assumptions discussed in\\nChapter 5) to compare the observed value of t with a standard ‚Äúnull‚Äù dis-\\ntribution, in this case a Student‚Äôs t distribution with 70 degrees of freedom.\\nWe obtain t D 3:01 from (1.6), which would classically be considered very\\nstrong evidence that the apparent difference (1.5) is genuine; in standard\\nterminology, ‚Äúwith two-sided signiÔ¨Åcance level 0.0036.‚Äù\\n\\nA small signiÔ¨Åcance level (or ‚Äúp-value‚Äù) is a statement of statistical sur-\\nprise: something very unusual has happened if in fact there is no difference\\nin gene 136 expression levels between ALL and AML patients. We are less\\nsurprised by t D 3:01 if gene 136 is just one candidate out of thousands\\nthat might have produced ‚Äúinteresting‚Äù results.\\n\\nThat is the case here. Figure 1.5 shows the histogram of the two-sample\\nt-statistics for the panel of 7128 genes. Now t D 3:01 looks less unusual;\\n400 other genes have t exceeding 3.01, about 5.6% of them.\\n\\nThis doesn‚Äôt mean that gene 136 is ‚ÄúsigniÔ¨Åcant at the 0.056 level.‚Äù There\\n\\nare two powerful complicating factors:\\n\\n1 Large numbers of candidates, 7128 here, will produce some large t-\\nvalues even if there is really no difference in genetic expression between\\nALL and AML patients.\\n\\n2 The histogram implies that in this study there is something wrong with\\nthe theoretical null distribution (‚ÄúStudent‚Äôs t with 70 degrees of free-\\ndom‚Äù), the smooth curve in Figure 1.5. It is much too narrow at the cen-\\nter, where presumably most of the genes are reporting non-signiÔ¨Åcant\\nresults.\\n\\nWe will see in Chapter 15 that a low false-discovery rate, i.e., a low\\nchance of crying wolf over an innocuous gene, requires t exceeding 6.16\\nin the ALL/AML study. Only 47 of the 7128 genes make the cut. False-\\ndiscovery-rate theory is an impressive advance in statistical inference, in-\\ncorporating Bayesian, frequentist, and empirical Bayesian (Chapter 6) el-\\n\\n4 Formally, a standard error is the standard deviation of a summary statistic, and bsd might\\n\\nbetter be called bse, but we will follow the distinction less than punctiliously here.\\n\\n\\x0c10\\n\\nAlgorithms and Inference\\n\\nFigure 1.5 Two-sample t -statistics for 7128 genes, leukemia\\ndata. The smooth curve is the theoretical null density for the\\nt-statistic.\\n\\nements. It was a necessary advance in a scientiÔ¨Åc world where computer-\\nbased technology routinely presents thousands of comparisons to be eval-\\nuated at once.\\n\\nThere is one more thing to say about the algorithm/inference statistical\\ncycle. Important new algorithms often arise outside the world of profes-\\nsional statisticians: neural nets, support vector machines, and boosting are\\nthree famous examples. None of this is surprising. New sources of data,\\nsatellite imagery for example, or medical microarrays, inspire novel meth-\\nodology from the observing scientists. The early literature tends toward the\\nenthusiastic, with claims of enormous applicability and power.\\n\\nIn the second phase, statisticians try to locate the new metholodogy\\nwithin the framework of statistical theory. In other words, they carry out\\nthe statistical inference part of the cycle, placing the new methodology\\nwithin the known Bayesian and frequentist limits of performance. (Boost-\\ning offers a nice example, Chapter 17.) This is a healthy chain of events,\\ngood both for the hybrid vigor of the statistics profession and for the further\\nprogress of algorithmic technology.\\n\\n t statisticsFrequency‚àí10‚àí5051001002003004005006007003.01\\x0c1.3 Notes\\n\\n1.3 Notes\\n\\n11\\n\\nLegendre published the least squares algorithm in 1805, causing Gauss\\nto state that he had been using the method in astronomical orbit-Ô¨Åtting\\nsince 1795. Given Gauss‚Äô astonishing production of major mathematical\\nadvances, this says something about the importance attached to the least\\nsquares idea. Chapter 8 includes its usual algebraic formulation, as well as\\nGauss‚Äô formula for the standard errors, line 2 of Table 1.1.\\n\\nOur division between algorithms and inference brings to mind Tukey‚Äôs\\nexploratory/conÔ¨Årmatory system. However the current algorithmic world\\nis often bolder in its claims than the word ‚Äúexploratory‚Äù implies, while to\\nour minds ‚Äúinference‚Äù conveys something richer than mere conÔ¨Årmation.\\n(cid:142)1 [p. 6] lowess was devised by William Cleveland (Cleveland, 1981) and\\nis available in the R statistical computing language. It is applied to the\\nkidney data in Efron (2004). The kidney data originated in the nephrology\\nlaboratory of Dr. Brian Myers, Stanford University, and is available from\\nthis book‚Äôs web site.\\n\\n\\x0c2\\n\\nFrequentist Inference\\n\\nBefore the computer age there was the calculator age, and before ‚Äúbig data‚Äù\\nthere were small data sets, often a few hundred numbers or fewer, labori-\\nously collected by individual scientists working under restrictive experi-\\nmental constraints. Precious data calls for maximally efÔ¨Åcient statistical\\nanalysis. A remarkably effective theory, feasible for execution on mechan-\\nical desk calculators, was developed beginning in 1900 by Pearson, Fisher,\\nNeyman, Hotelling, and others, and grew to dominate twentieth-century\\nstatistical practice. The theory, now referred to as classical, relied almost\\nentirely on frequentist inferential ideas. This chapter sketches a quick and\\nsimpliÔ¨Åed picture of frequentist inference, particularly as employed in clas-\\nsical applications.\\n\\nWe begin with another example from Dr. Myers‚Äô nephrology laboratory:\\n211 kidney patients have had their glomerular Ô¨Åltration rates measured,\\nwith the results shown in Figure 2.1; gfr is an important indicator of kid-\\nney function, with low values suggesting trouble. (It is a key component of\\ntot in Figure 1.1.) The mean and standard error (1.1)‚Äì(1.2) are Nx D 54:25\\nand bse D 0:95, typically reported as\\n\\n54:25 Àô 0:95I\\n\\n(2.1)\\n\\nÀô0:95 denotes a frequentist inference for the accuracy of the estimate Nx D\\n54:25, and suggests that we shouldn‚Äôt take the ‚Äú.25‚Äù very seriously, even\\nthe ‚Äú4‚Äù being open to doubt. Where the inference comes from and what\\nexactly it means remains to be said.\\n\\nStatistical inference usually begins with the assumption that some prob-\\nability model has produced the observed data x, in our case the vector of\\nn D 211 gfr measurements x D .x1; x2; : : : ; xn/. Let X D .X1; X2; : : : ;\\nXn/ indicate n independent draws from a probability distribution F , writ-\\nten\\n\\nF ! X ;\\n\\n12\\n\\n(2.2)\\n\\n\\x0cFrequentist Inference\\n\\n13\\n\\nFigure 2.1 Glomerular Ô¨Åltration rates for 211 kidney patients;\\nmean 54.25, standard error .95.\\n\\nF being the underlying distribution of possible gfr scores here. A realiza-\\ntion X D x of (2.2) has been observed, and the statistician wishes to infer\\nsome property of the unknown distribution F .\\n\\nSuppose the desired property is the expectation of a single random draw\\n\\nX from F , denoted\\n\\n(cid:18) D EF fXg\\n(2.3)\\n(which also equals the expectation of the average NX D P Xi =n of random\\nvector (2.2)1). The obvious estimate of (cid:18) is O(cid:18) D Nx, the sample average. If\\nn were enormous, say 1010, we would expect O(cid:18) to nearly equal (cid:18), but oth-\\nerwise there is room for error. How much error is the inferential question.\\nThe estimate O(cid:18) is calculated from x according to some known algorithm,\\n\\nsay\\n\\nO(cid:18) D t.x/;\\n(2.4)\\nt .x/ in our example being the averaging function Nx D P xi =n; O(cid:18) is a\\n\\n1 The fact that EF f NX g equals EF fX g is a crucial, though easily proved, probabilistic\\n\\nresult.\\n\\n gfrFrequency20406080100051015202530\\x0c14\\n\\nFrequentist Inference\\n\\nrealization of\\n\\nO‚Äö D t.X /;\\nthe output of t.(cid:1)/ applied to a theoretical sample X from F (2.2). We have\\nchosen t.X /, we hope, to make O‚Äö a good estimator of (cid:18), the desired prop-\\nerty of F .\\n\\n(2.5)\\n\\nWe can now give a Ô¨Årst deÔ¨Ånition of frequentist inference: the accu-\\nracy of an observed estimate O(cid:18) D t.x/ is the probabilistic accuracy of\\nO‚Äö D t.X / as an estimator of (cid:18). This may seem more a tautology than a\\ndeÔ¨Ånition, but it contains a powerful idea: O(cid:18) is just a single number but O‚Äö\\ntakes on a range of values whose spread can deÔ¨Åne measures of accuracy.\\nBias and variance are familiar examples of frequentist inference. DeÔ¨Åne\\n\\n(cid:22) to be the expectation of O‚Äö D t.X / under model (2.2),\\n\\n(cid:22) D EF f O‚Äög:\\n\\n(2.6)\\n\\nThen the bias and variance attributed to estimate O(cid:18) of parameter (cid:18) are\\n\\nbias D (cid:22) (cid:0) (cid:18)\\n\\nand\\n\\nvar D EF\\n\\n. O‚Äö (cid:0) (cid:22)/2o\\nn\\n\\n:\\n\\n(2.7)\\n\\nAgain, what keeps this from tautology is the attribution to the single num-\\nber O(cid:18) of the probabilistic properties of O‚Äö following from model (2.2). If\\nall of this seems too obvious to worry about, the Bayesian criticisms of\\nChapter 3 may come as a shock.\\n\\nFrequentism is often deÔ¨Åned with respect to ‚Äúan inÔ¨Ånite sequence of\\nfuture trials.‚Äù We imagine hypothetical data sets X .1/; X .2/; X .3/; : : : gen-\\nerated by the same mechanism as x providing corresponding values O‚Äö.1/;\\nO‚Äö.2/, O‚Äö.3/; : : : as in (2.5). The frequentist principle is then to attribute for\\nO(cid:18) the accuracy properties of the ensemble of O‚Äö values.2 If the O‚Äös have\\nempirical variance of, say, 0.04, then O(cid:18) is claimed to have standard error\\n0:2 D\\n0:04, etc. This amounts to a more picturesque restatement of the\\nprevious deÔ¨Ånition.\\n\\np\\n\\n2.1 Frequentism in Practice\\n\\nOur working deÔ¨Ånition of frequentism is that the probabilistic properties\\nof a procedure of interest are derived and then applied verbatim to the\\nprocedure‚Äôs output for the observed data. This has an obvious defect: it\\nrequires calculating the properties of estimators O‚Äö D t.X / obtained from\\n\\n2 In essence, frequentists ask themselves ‚ÄúWhat would I see if I reran the same situation\\n\\nagain (and again and again. . . )?‚Äù\\n\\n\\x0c2.1 Frequentism in Practice\\n\\n15\\n\\nthe true distribution F , even though F is unknown. Practical frequentism\\nuses a collection of more or less ingenious devices to circumvent the defect.\\n\\n1. The plug-in principle. A simple formula relates the standard error of\\nNX D P Xi =n to varF .X/, the variance of a single X drawn from F ,\\n\\nse (cid:0) NX(cid:1) D ≈ívarF .X/=n(cid:141)1=2:\\nBut having observed x D .x1; x2; : : : ; xn/ we can estimate varF .X/ with-\\nout bias by\\n\\n(2.8)\\n\\ncvarF D X\\n\\n.xi (cid:0) Nx/2 =.n (cid:0) 1/:\\n\\n(2.9)\\n\\nPlugging formula (2.9) into (2.8) gives bse (1.2), the usual estimate for the\\nstandard error of an average Nx. In other words, the frequentist accuracy\\nestimate for Nx is itself estimated from the observed data.3\\n\\nStatistics O(cid:18) D t.x/ more complicated\\n2. Taylor-series approximations.\\nthan Nx can often be related back to the plug-in formula by local linear\\napproximations, sometimes known as the ‚Äúdelta method.‚Äù (cid:142) For example, (cid:142)1\\nO(cid:18) D Nx2 has d O(cid:18)=d Nx D 2 Nx. Thinking of 2 Nx as a constant gives\\n\\nse (cid:0) Nx2(cid:1) :D 2 j Nxj\\n\\nbse;\\n\\n(2.10)\\n\\nwith bse as in (1.2). Large sample calculations, as sample size n goes to\\ninÔ¨Ånity, validate the delta method which, fortunately, often performs well\\nin small samples.\\n\\n3. Parametric families and maximum likelihood theory.\\nTheoretical ex-\\npressions for the standard error of a maximum likelihood estimate (MLE)\\nare discussed in Chapters 4 and 5, in the context of parametric families\\nof distributions. These combine Fisherian theory, Taylor-series approxima-\\ntions, and the plug-in principle in an easy-to-apply package.\\n\\n4. Simulation and the bootstrap. Modern computation has opened up the\\npossibility of numerically implementing the ‚ÄúinÔ¨Ånite sequence of future\\ntrials‚Äù deÔ¨Ånition, except for the inÔ¨Ånite part. An estimate OF of F , perhaps\\nthe MLE, is found, and values O‚Äö.k/ D t.X .k// simulated from OF for k D\\n1; 2; : : : ; B, say B D 1000. The empirical standard deviation of the O‚Äös is\\nthen the frequentist estimate of standard error for O(cid:18) D t.x/, and similarly\\nwith other measures of accuracy.\\n\\nThis is a good description of the bootstrap, Chapter 10. (Notice that\\n\\n3 The most familiar example is the observed proportion p of heads in n Ô¨Çips of a coin\\nhaving true probability (cid:25): the actual standard error is ≈í(cid:25).1 (cid:0) (cid:25)/=n(cid:141)1=2 but we can\\nonly report the plug-in estimate ≈íp.1 (cid:0) p/=n(cid:141)1=2.\\n\\n\\x0c16\\n\\nFrequentist Inference\\n\\nTable 2.1 Three estimates of location for the gfr data, and their\\nestimated standard errors; last two standard errors using the bootstrap,\\nB D 1000.\\n\\nEstimate\\n\\nStandard error\\n\\nmean\\n25% Winsorized mean\\nmedian\\n\\n54.25\\n52.61\\n52.24\\n\\n.95\\n.78\\n.87\\n\\nOF for F , comes Ô¨Årst rather than at the end of\\nhere the plugging-in, of\\nthe process.) The classical methods 1‚Äì3 above are restricted to estimates\\nO(cid:18) D t.x/ that are smoothly deÔ¨Åned functions of various sample means.\\nSimulation calculations remove this restriction. Table 2.1 shows three ‚Äúlo-\\ncation‚Äù estimates for the gfr data, the mean, the 25% Winsorized mean,4\\nand the median, along with their standard errors, the last two computed\\nby the bootstrap. A happy feature of computer-age statistical inference is\\nthe tremendous expansion of useful and usable statistics t.x/ in the statis-\\ntician‚Äôs working toolbox, the lowess algorithm in Figures 1.2 and 1.3\\nproviding a nice example.\\n5. Pivotal statistics. A pivotal statistic O(cid:18) D t.x/ is one whose distri-\\nbution does not depend upon the underlying probability distribution F . In\\nsuch a case the theoretical distribution of O‚Äö D t.X / applies exactly to O(cid:18),\\nremoving the need for devices 1‚Äì4 above. The classic example concerns\\nStudent‚Äôs two-sample t-test.\\n\\nIn a two-sample problem the statistician observes two sets of numbers,\\n\\nx1 D .x11; x12; : : : ; x1n1/ x2 D .x21; x22; : : : ; x2n2/;\\n\\n(2.11)\\n\\nand wishes to test the null hypothesis that they come from the same dis-\\ntribution (as opposed to, say, the second set tending toward larger values\\nthan the Ô¨Årst). It is assumed that the distribution F1 for x1 is normal, or\\nGaussian,\\n\\nind(cid:24)\\n\\nX1i\\n\\nN\\n\\n.(cid:22)1; (cid:27) 2/;\\n\\ni D 1; 2; : : : ; n1;\\n\\n(2.12)\\n\\nthe notation indicating n1 independent draws from a normal distribution5\\n\\n4 All observations below the 25th percentile of the 211 observations are moved up to that\\npoint, similarly those above the 75th percentile are moved down, and Ô¨Ånally the mean is\\ntaken.\\n\\n5 Each draw having probability density .2(cid:25)(cid:27) 2/(cid:0)1=2 expf(cid:0)0:5 (cid:1) .x (cid:0) (cid:22)1/2=(cid:27) 2g.\\n\\n\\x0c2.1 Frequentism in Practice\\n\\n17\\n\\nwith expectation (cid:22)1 and variance (cid:27) 2. Likewise\\n\\nX2i\\n\\nind(cid:24)\\n\\nN\\n\\n.(cid:22)2; (cid:27) 2/\\n\\ni D 1; 2; : : : ; n2:\\n\\n(2.13)\\n\\nWe wish to test the null hypothesis\\n\\nH0 W (cid:22)1 D (cid:22)2:\\n(2.14)\\nThe obvious test statistic O(cid:18) D Nx2 (cid:0) Nx1, the difference of the means, has\\ndistribution\\n\\nO(cid:18) (cid:24)\\n\\n0; (cid:27) 2 (cid:16) 1\\n(cid:16)\\n\\nn1\\n\\nC 1\\nn2\\n\\n(cid:17)(cid:17)\\n\\nN\\nunder H0. We could plug in the unbiased estimate of (cid:27) 2,\\n\\n(2.15)\\n\\n\" n1X\\n\\n.x1i (cid:0) Nx1/2 C\\n\\nn2X\\n\\n.x2i (cid:0) Nx2/2\\n\\n#,\\n\\nO(cid:27) 2 D\\n\\n.n1 C n2 (cid:0) 2/;\\n\\n(2.16)\\n\\n1\\n\\n1\\n\\nbut Student provided a more elegant solution: instead of O(cid:18) , we test H0\\nusing the two-sample t-statistic\\n\\nt D\\n\\nNx2 (cid:0) Nx1\\nbsd\\n\\n;\\n\\nwhere bsd D O(cid:27)\\n\\n(cid:16) 1\\nn1\\n\\nC 1\\nn2\\n\\n(cid:17)1=2\\n\\n:\\n\\n(2.17)\\n\\nUnder H0, t is pivotal, having the same distribution (Student‚Äôs t distribu-\\ntion with n1 C n2 (cid:0) 2 degrees of freedom), no matter what the value of the\\n‚Äúnuisance parameter‚Äù (cid:27).\\n\\nFor n1 C n2 (cid:0) 2 D 70, as in the leukemia example (1.5)‚Äì(1.6), Student‚Äôs\\n\\ndistribution gives\\n\\nPrH0\\n\\nf(cid:0)1:99 (cid:20) t (cid:20) 1:99g D 0:95:\\n\\n(2.18)\\n\\nThe hypothesis test that rejects H0 if jtj exceeds 1.99 has probability ex-\\nactly 0.05 of mistaken rejection. Similarly,\\n\\nNx2 (cid:0) Nx1 Àô 1:99 (cid:1) bsd\\n\\n(2.19)\\n\\nis an exact 0.95 conÔ¨Ådence interval for the difference (cid:22)2 (cid:0) (cid:22)1, covering\\nthe true value in 95% of repetitions of probability model (2.12)‚Äì(2.13).6\\n\\n6 Occasionally, one sees frequentism deÔ¨Åned in careerist terms, e.g., ‚ÄúA statistician who\\nalways rejects null hypotheses at the 95% level will over time make only 5% errors of\\nthe Ô¨Årst kind.‚Äù This is not a comforting criterion for the statistician‚Äôs clients, who are\\ninterested in their own situations, not everyone else‚Äôs. Here we are only assuming\\nhypothetical repetitions of the speciÔ¨Åc problem at hand.\\n\\n\\x0c18\\n\\nFrequentist Inference\\n\\nWhat might be called the strong deÔ¨Ånition of frequentism insists on exact\\nfrequentist correctness under experimental repetitions. Pivotality, unfortu-\\nnately, is unavailable in most statistical situations. Our looser deÔ¨Ånition\\nof frequentism, supplemented by devices such as those above,7 presents a\\nmore realistic picture of actual frequentist practice.\\n\\n2.2 Frequentist Optimality\\n\\nThe popularity of frequentist methods reÔ¨Çects their relatively modest math-\\nematical modeling assumptions: only a probability model F (more exactly\\na family of probabilities, Chapter 3) and an algorithm of choice t.x/. This\\nÔ¨Çexibility is also a defect in that the principle of frequentist correctness\\ndoesn‚Äôt help with the choice of algorithm. Should we use the sample mean\\nto estimate the location of the gfr distribution? Maybe the 25% Win-\\nsorized mean would be better, as Table 2.1 suggests.\\n\\nThe years 1920‚Äì1935 saw the development of two key results on fre-\\nquentist optimality, that is, Ô¨Ånding the best choice of t.x/ given model F .\\nThe Ô¨Årst of these was Fisher‚Äôs theory of maximum likelihood estimation\\nand the Fisher information bound: in parametric probability models of the\\ntype discussed in Chapter 4, the MLE is the optimum estimate in terms of\\nminimum (asymptotic) standard error.\\n\\nIn the same spirit, the Neyman‚ÄìPearson lemma provides an optimum\\nhypothesis-testing algorithm. This is perhaps the most elegant of frequen-\\ntist constructions. In its simplest formulation, the NP lemma assumes we\\nare trying to decide between two possible probability density functions for\\nthe observed data x, a null hypothesis density f0.x/ and an alternative\\ndensity f1.x/. A testing rule t .x/ says which choice, 0 or 1, we will make\\nhaving observed data x. Any such rule has two associated frequentist error\\nprobabilities: choosing f1 when actually f0 generated x, and vice versa,\\n\\nÀõ D Prf0\\nÀá D Prf1\\n\\nft.x/ D 1g ;\\nft.x/ D 0g :\\n\\nLet L.x/ be the likelihood ratio,\\n\\nL.x/ D f1.x/=f0.x/\\n\\n(2.20)\\n\\n(2.21)\\n\\n7 The list of devices is not complete. Asymptotic calculations play a major role, as do\\n\\nmore elaborate combinations of pivotality and the plug-in principle; see the discussion\\nof approximate bootstrap conÔ¨Ådence intervals in Chapter 11.\\n\\n\\x0c2.2 Frequentist Optimality\\n\\n19\\n\\nand deÔ¨Åne the testing rule tc.x/ by\\n\\ntc.x/ D\\n\\n(\\n1 if log L.x/ (cid:21) c\\n0 if log L.x/ < c:\\n\\n(2.22)\\n\\nThere is one such rule for each choice of the cutoff c. The Neyman‚ÄìPearson\\nlemma says that only rules of form (2.22) can be optimum; for any other\\nrule t.x/ there will be a rule tc.x/ having smaller errors of both kinds,8\\n\\nÀõc < Àõ and Àác < Àá:\\n\\n(2.23)\\n\\nFigure 2.2 Neyman‚ÄìPearson alpha‚Äìbeta curve for f0 (cid:24)\\nf1 (cid:24)\\ncutoffs c D 2:75; 1:75; :75; : : : ; (cid:0)3:25.\\n\\n.:5; 1/, and sample size n D 10. Red dots correspond to\\n\\nN\\n\\nN\\n\\n.0; 1/,\\n\\nFigure 2.2 graphs .Àõc; Àác/ as a function of the cutoff c, for the case\\nwhere x D .x1; x2; : : : ; x10/ is obtained by independent sampling from a\\n.0:5; 1/ for f1. The NP lemma\\nnormal distribution,\\nsays that any rule not of form (2.22) must have its .Àõ; Àá/ point lying above\\nthe curve.\\n\\n.0; 1/ for f0 versus\\n\\nN\\n\\nN\\n\\n8 Here we are ignoring some minor deÔ¨Ånitional difÔ¨Åculties that can occur if f0 and f1 are\\n\\ndiscrete.\\n\\n0.00.20.40.60.81.00.00.20.40.60.81.0abc = .75a  = .10,  b = .38\\x0c20\\n\\nFrequentist Inference\\n\\nFrequentist optimality theory, both for estimation and for testing, an-\\nchored statistical practice in the twentieth century. The larger data sets and\\nmore complicated inferential questions of the current era have strained the\\ncapabilities of that theory. Computer-age statistical inference, as we will\\nsee, often displays an unsettling ad hoc character. Perhaps some contem-\\nporary Fishers and Neymans will provide us with a more capacious opti-\\nmality theory equal to the challenges of current practice, but for now that\\nis only a hope.\\n\\nFrequentism cannot claim to be a seamless philosophy of statistical in-\\nference. Paradoxes and contradictions abound within its borders, as will\\nbe shown in the next chapter. That being said, frequentist methods have\\na natural appeal to working scientists, an impressive history of success-\\nful application, and, as our list of Ô¨Åve ‚Äúdevices‚Äù suggests, the capacity to\\nencourage clever methodology. The story that follows is not one of aban-\\ndonment of frequentist thinking, but rather a broadening of connections\\nwith other methods.\\n\\n2.3 Notes and Details\\n\\nThe name ‚Äúfrequentism‚Äù seems to have been suggested by Neyman as a\\nstatistical analogue of Richard von Mises‚Äô frequentist theory of probability,\\nthe connection being made explicit in his 1977 paper, ‚ÄúFrequentist prob-\\nability and frequentist statistics.‚Äù ‚ÄúBehaviorism‚Äù might have been a more\\ndescriptive name9 since the theory revolves around the long-run behavior\\nof statistics t .x/, but in any case ‚Äúfrequentism‚Äù has stuck, replacing the\\nolder (sometimes disparaging) term ‚Äúobjectivism.‚Äù Neyman‚Äôs attempt at a\\ncomplete frequentist theory of statistical inference, ‚Äúinductive behavior,‚Äù\\nis not much quoted today, but can claim to be an important inÔ¨Çuence on\\nWald‚Äôs development of decision theory.\\n\\nR. A. Fisher‚Äôs work on maximum likelihood estimation is featured in\\nChapter 4. Fisher, arguably the founder of frequentist optimality theory,\\nwas not a pure frequentist himself, as discussed in Chapter 4 and Efron\\n(1998), ‚ÄúR. A. Fisher in the 21st Century.‚Äù (Now that we are well into the\\ntwenty-Ô¨Årst century, the author‚Äôs talents as a prognosticator can be frequen-\\ntistically evaluated.)\\n\\n(cid:142)1 [p. 15] Delta method. The delta method uses a Ô¨Årst-order Taylor series to\\napproximate the variance of a function s. O(cid:18)/ of a statistic O(cid:18) . Suppose O(cid:18)\\nhas mean/variance .(cid:18); (cid:27) 2/, and consider the approximation s. O(cid:18)/ (cid:25) s.(cid:18)/ C\\n\\n9 That name is already spoken for in the psychology literature.\\n\\n\\x0c2.3 Notes and Details\\n21\\ns0.(cid:18)/. O(cid:18) (cid:0) (cid:18)/. Hence varfs. O(cid:18)/g (cid:25) js0.(cid:18)/j2(cid:27) 2. We typically plug-in O(cid:18) for (cid:18),\\nand use an estimate for (cid:27) 2.\\n\\n\\x0c3\\n\\nBayesian Inference\\n\\nThe human mind is an inference machine: ‚ÄúIt‚Äôs getting windy, the sky is\\ndarkening, I‚Äôd better bring my umbrella with me.‚Äù Unfortunately, it‚Äôs not a\\nvery dependable machine, especially when weighing complicated choices\\nagainst past experience. Bayes‚Äô theorem is a surprisingly simple mathemat-\\nical guide to accurate inference. The theorem (or ‚Äúrule‚Äù), now 250 years\\nold, marked the beginning of statistical inference as a serious scientiÔ¨Åc sub-\\nject. It has waxed and waned in inÔ¨Çuence over the centuries, now waxing\\nagain in the service of computer-age applications.\\n\\nBayesian inference, if not directly opposed to frequentism, is at least or-\\nthogonal. It reveals some worrisome Ô¨Çaws in the frequentist point of view,\\nwhile at the same time exposing itself to the criticism of dangerous overuse.\\nThe struggle to combine the virtues of the two philosophies has become\\nmore acute in an era of massively complicated data sets. Much of what\\nfollows in succeeding chapters concerns this struggle. Here we will review\\nsome basic Bayesian ideas and the ways they impinge on frequentism.\\n\\nThe fundamental unit of statistical inference both for frequentists and\\n\\nfor Bayesians is a family of probability densities\\n\\nD Àöf(cid:22).x/I x 2\\n\\n; (cid:22) 2 (cid:127)(cid:9) I\\n\\nX\\n\\nF\\n\\n(3.1)\\n\\nx, the observed data, is a point1 in the sample space\\n, while the unob-\\nserved parameter (cid:22) is a point in the parameter space (cid:127). The statistician\\nobserves x from f(cid:22).x/, and infers the value of (cid:22).\\n\\nX\\n\\nPerhaps the most familiar case is the normal family\\n\\nf(cid:22).x/ D 1p\\n2(cid:25)\\n\\ne(cid:0) 1\\n\\n2 .x(cid:0)(cid:22)/2\\n\\n(3.2)\\n\\n1 Both x and (cid:22) may be scalars, vectors, or more complicated objects. Other names for the\\ngeneric ‚Äúx‚Äù and ‚Äú(cid:22)‚Äù occur in speciÔ¨Åc situations, for instance x for x in Chapter 2. We\\nwill also call F a ‚Äúfamily of probability distributions.‚Äù\\n\\n22\\n\\n\\x0cBayesian Inference\\n\\n23\\n\\n(more exactly, the one-dimensional normal translation family2 with vari-\\n1, the entire real line .(cid:0)1; 1/.\\nance 1), with both\\nAnother central example is the Poisson family\\n\\nand (cid:127) equaling\\n\\nR\\n\\nX\\n\\nf(cid:22).x/ D e(cid:0)(cid:22)(cid:22)x=x≈†;\\n\\n(3.3)\\n\\nis the nonnegative integers f0; 1; 2; : : : g and (cid:127) is the nonnegative\\nwhere\\nreal line .0; 1/. (Here the ‚Äúdensity‚Äù (3.3) speciÔ¨Åes the atoms of probability\\non the discrete points of\\n\\nX\\n\\n.)\\n\\nBayesian inference requires one crucial assumption in addition to the\\n, the knowledge of a prior density\\n\\nprobability family\\n\\nX\\n\\nF\\n\\ng.(cid:22)/;\\n\\n(cid:22) 2 (cid:127)I\\n\\n(3.4)\\n\\ng.(cid:22)/ represents prior information concerning the parameter (cid:22), available to\\nthe statistician before the observation of x. For instance, in an application\\nof the normal model (3.2), it could be known that (cid:22) is positive, while past\\nexperience shows it never exceeding 10, in which case we might take g.(cid:22)/\\nto be the uniform density g.(cid:22)/ D 1=10 on the interval ≈í0; 10(cid:141). Exactly\\nwhat constitutes ‚Äúprior knowledge‚Äù is a crucial question we will consider\\nin ongoing discussions of Bayes‚Äô theorem.\\n\\nBayes‚Äô theorem is a rule for combining the prior knowledge in g.(cid:22)/ with\\nthe current evidence in x. Let g.(cid:22)jx/ denote the posterior density of (cid:22), that\\nis, our update of the prior density g.(cid:22)/ after taking account of observation\\nx. Bayes‚Äô rule provides a simple expression for g.(cid:22)jx/ in terms of g.(cid:22)/\\nand\\n\\n.\\n\\nF\\nBayes‚Äô Rule: g.(cid:22)jx/ D g.(cid:22)/f(cid:22).x/=f .x/;\\n\\n(cid:22) 2 (cid:127);\\n\\n(3.5)\\n\\nwhere f .x/ is the marginal density of x,\\n\\nf .x/ D\\n\\nZ\\n\\n(cid:127)\\n\\nf(cid:22).x/g.(cid:22)/ d(cid:22):\\n\\n(3.6)\\n\\n(The integral in (3.6) would be a sum if (cid:127) were discrete.) The Rule is a\\nstraightforward exercise in conditional probability,3 and yet has far-reaching\\nand sometimes surprising consequences.\\n\\nIn Bayes‚Äô formula (3.5), x is Ô¨Åxed at its observed value while (cid:22) varies\\nover (cid:127), just the opposite of frequentist calculations. We can emphasize this\\n\\n2 Standard notation is x (cid:24) N .(cid:22); (cid:27) 2/ for a normal distribution with expectation (cid:22) and\\n\\nvariance (cid:27) 2, so (3.2) has x (cid:24) N .(cid:22); 1/.\\n\\n3 g.(cid:22)jx/ is the ratio of g.(cid:22)/f(cid:22).x/, the joint probability of the pair .(cid:22); x/, and f .x/,\\n\\nthe marginal probability of x.\\n\\n\\x0c24\\n\\nBayesian Inference\\n\\nby rewriting (3.5) as\\n\\ng.(cid:22)jx/ D cxLx.(cid:22)/g.(cid:22)/;\\n\\n(3.7)\\n\\nwhere Lx.(cid:22)/ is the likelihood function, that is, f(cid:22).x/ with x Ô¨Åxed and (cid:22)\\nvarying. Having computed Lx.(cid:22)/g.(cid:22)/, the constant cx can be determined\\nnumerically from the requirement that g.(cid:22)jx/ integrate to 1, obviating the\\ncalculation of f .x/ (3.6).\\n\\nNote Multiplying the likelihood function by any Ô¨Åxed constant c0 has no\\neffect on (3.7) since c0 can be absorbed into cx. So for the Poisson family\\n(3.3) we can take Lx.(cid:22)/ D e(cid:0)(cid:22)(cid:22)x, ignoring the x≈† factor, which acts as a\\nconstant in Bayes‚Äô rule. The luxury of ignoring factors depending only on\\nx often simpliÔ¨Åes Bayesian calculations.\\n\\nFor any two points (cid:22)1 and (cid:22)2 in (cid:127), the ratio of posterior densities is, by\\n\\ndivision in (3.5),\\n\\ng.(cid:22)1jx/\\ng.(cid:22)2jx/\\n\\nD g.(cid:22)1/\\ng.(cid:22)2/\\n\\nf(cid:22)1.x/\\nf(cid:22)2.x/\\n\\n(3.8)\\n\\n(no longer involving the marginal density f .x/), that is, ‚Äúthe posterior odds\\nratio is the prior odds ratio times the likelihood ratio,‚Äù a memorable restate-\\nment of Bayes‚Äô rule.\\n\\n3.1 Two Examples\\n\\nA simple but genuine example of Bayes‚Äô rule in action is provided by the\\nstory of the Physicist‚Äôs Twins: thanks to sonograms, a physicist found out\\nshe was going to have twin boys. ‚ÄúWhat is the probability my twins will\\nbe Identical, rather than Fraternal?‚Äù she asked. The doctor answered that\\none-third of twin births were Identicals, and two-thirds Fraternals.\\n\\nIn this situation (cid:22), the unknown parameter (or ‚Äústate of nature‚Äù) is either\\nIdentical or Fraternal with prior probability 1/3 or 2/3; X , the possible\\nsonogram results for twin births, is either Same Sex or Different Sexes, and\\nx D Same Sex was observed. (We can ignore sex since that does not affect\\nthe calculation.) A crucial fact is that identical twins are always same-sex\\nwhile fraternals have probability 0.5 of same or different, so Same Sex in\\nthe sonogram is twice as likely if the twins are Identical. Applying Bayes‚Äô\\n\\n\\x0c3.1 Two Examples\\n\\nrule in ratio form (3.8) answers the physicist‚Äôs question:\\n\\ng.Identical j Same/\\ng.Fraternal j Same/\\n\\n(cid:1) fIdentical.Same/\\nfFraternal.Same/\\n\\nD g.Identical/\\ng.Fraternal/\\n(cid:1) 1\\n1=2\\n\\nD 1=3\\n2=3\\n\\nD 1:\\n\\n25\\n\\n(3.9)\\n\\nThat is, the posterior odds are even, and the physicist‚Äôs twins have equal\\nprobabilities 0.5 of being Identical or Fraternal.4 Here the doctor‚Äôs prior\\nodds ratio, 2 to 1 in favor of Fraternal, is balanced out by the sonogram‚Äôs\\nlikelihood ratio of 2 to 1 in favor of Identical.\\n\\nFigure 3.1 Analyzing the twins problem.\\n\\nThere are only four possible combinations of parameter (cid:22) and outcome\\nx in the twins problem, labeled a, b, c, and d in Figure 3.1. Cell b has\\nprobability 0 since Identicals cannot be of Different Sexes. Cells c and d\\nhave equal probabilities because of the random sexes of Fraternals. Finally,\\na C b must have total probability 1/3, and c C d total probability 2/3,\\naccording to the doctor‚Äôs prior distribution. Putting all this together, we\\ncan Ô¨Åll in the probabilities for all four cells, as shown. The physicist knows\\nshe is in the Ô¨Årst column of the table, where the conditional probabilities\\nof Identical or Fraternal are equal, just as provided by Bayes‚Äô rule in (3.9).\\nPresumably the doctor‚Äôs prior distribution came from some enormous\\nstate or national database, say three million previous twin births, one mil-\\nlion Identical pairs and two million Fraternals. We deduce that cells a, c,\\nand d must have had one million entries each in the database, while cell\\nb was empty. Bayes‚Äô rule can be thought of as a big book with one page\\n\\n4 They turned out to be Fraternal.\\n\\n1\\t\\n \\xa0Identical Twins are: Fraternal Same sex Different Physicist Sonogram shows: Doctor 2/3 1/3 1/3 1/3 0 1/3 b a c d \\x0c26\\n\\nBayesian Inference\\n\\nfor each possible outcome x. (The book has only two pages in Figure 3.1.)\\nThe physicist turns to the page ‚ÄúSame Sex‚Äù and sees two million previous\\ntwin births, half Identical and half Fraternal, correctly concluding that the\\nodds are equal in her situation.\\n\\nGiven any prior distribution g.(cid:22)/ and any family of densities f(cid:22).x/,\\nBayes‚Äô rule will always provide a version of the big book. That doesn‚Äôt\\nmean that the book‚Äôs contents will always be equally convincing. The prior\\nfor the twins problems was based on a large amount of relevant previous\\nexperience. Such experience is most often unavailable. Modern Bayesian\\npractice uses various strategies to construct an appropriate ‚Äúprior‚Äù g.(cid:22)/\\nin the absence of prior experience, leaving many statisticians unconvinced\\nby the resulting Bayesian inferences. Our second example illustrates the\\ndifÔ¨Åculty.\\n\\nTable 3.1 Scores from two tests taken by 22 students, mechanics and\\nvectors.\\n\\nmechanics\\nvectors\\n\\nmechanics\\nvectors\\n\\n1\\n\\n7\\n51\\n\\n12\\n\\n36\\n59\\n\\n2\\n\\n44\\n69\\n\\n13\\n\\n42\\n60\\n\\n3\\n\\n49\\n41\\n\\n14\\n\\n5\\n30\\n\\n4\\n\\n59\\n70\\n\\n15\\n\\n22\\n58\\n\\n5\\n\\n34\\n42\\n\\n16\\n\\n18\\n51\\n\\n6\\n\\n46\\n40\\n\\n17\\n\\n41\\n63\\n\\n7\\n\\n0\\n40\\n\\n18\\n\\n48\\n38\\n\\n8\\n\\n32\\n45\\n\\n19\\n\\n31\\n42\\n\\n9\\n\\n49\\n57\\n\\n20\\n\\n42\\n69\\n\\n10\\n\\n52\\n64\\n\\n21\\n\\n46\\n49\\n\\n11\\n\\n44\\n61\\n\\n22\\n\\n63\\n63\\n\\nTable 3.1 shows the scores on two tests, mechanics and vectors,\\nachieved by n D 22 students. The sample correlation coefÔ¨Åcient between\\nthe two scores is O(cid:18) D 0:498,\\n\\nO(cid:18) D\\n\\n22\\nX\\n\\n.mi (cid:0) Nm/.vi (cid:0) Nv/\\n\\n,\" 22\\nX\\n\\n.mi (cid:0) Nm/2\\n\\n22\\nX\\n\\n.vi (cid:0) Nv/2\\n\\n#1=2\\n\\n; (3.10)\\n\\ni D1\\n\\ni D1\\n\\niD1\\n\\nwith m and v short for mechanics and vectors, Nm and Nv their aver-\\nages. We wish to assign a Bayesian measure of posterior accuracy to the\\ntrue correlation coefÔ¨Åcient (cid:18), ‚Äútrue‚Äù meaning the correlation for the hypo-\\nthetical population of all students, of which we observed only 22.\\n\\nIf we assume that the joint .m; v/ distribution is bivariate normal (as\\ndiscussed in Chapter 5), then the density of O(cid:18) as a function of (cid:18) has a\\nknown form,(cid:142)\\n\\n(cid:142)1\\n\\n\\x0c3.1 Two Examples\\n\\n27\\n\\n(cid:17)\\n\\n(cid:16) O(cid:18)\\n\\nD\\n\\nf(cid:18)\\n\\n.n (cid:0) 2/.1 (cid:0) (cid:18) 2/.n(cid:0)1/=2 (cid:16)\\n\\n1 (cid:0) O(cid:18) 2(cid:17).n(cid:0)4/=2\\n\\n(cid:25)\\n\\nZ 1\\n\\n0\\n\\ndw\\n(cid:16)\\ncosh w (cid:0) (cid:18) O(cid:18)\\n\\n(cid:17)n(cid:0)1 :\\n\\n(3.11)\\nIn terms of our general Bayes notation, parameter (cid:22) is (cid:18), observation x is\\nO(cid:18), and family\\nequaling the interval\\n≈í(cid:0)1; 1(cid:141). Formula (3.11) looks formidable to the human eye but not to the\\ncomputer eye, which makes quick work of it.\\n\\nis given by (3.11), with both (cid:127) and\\n\\nF\\n\\nX\\n\\nFigure 3.2 Student scores data; posterior density of correlation (cid:18)\\nfor three possible priors.\\n\\nIn this case, as in the majority of scientiÔ¨Åc situations, we don‚Äôt have a\\ntrove of relevant past experience ready to provide a prior g.(cid:18)/. One expe-\\ndient, going back to Laplace, is the ‚Äúprinciple of insufÔ¨Åcient reason,‚Äù that\\nis, we take (cid:18) to be uniformly distributed over (cid:127),\\n\\ng.(cid:18)/ D 1\\n2\\n\\nfor (cid:0) 1 (cid:20) (cid:18) (cid:20) 1;\\n\\n(3.12)\\n\\na ‚ÄúÔ¨Çat prior.‚Äù The solid black curve in Figure 3.2 shows the resulting poste-\\nrior density (3.5), which is just the likelihood f(cid:18) .0:498/ plotted as a func-\\ntion of (cid:18) (and scaled to have integral 1).\\n\\n‚àí0.20.00.20.40.60.81.00.00.51.01.52.02.5qg(q|q^)MLE .498flat priorJeffreysTriangularl.093.750\\x0c28\\n\\nBayesian Inference\\n\\nJeffreys‚Äô prior,\\n\\ngJeff.(cid:18)/ D 1=.1 (cid:0) (cid:18) 2/;\\n(3.13)\\nyields posterior density g.(cid:18) j O(cid:18)/ shown by the dashed red curve. It suggests\\nsomewhat bigger values for the unknown parameter (cid:18). Formula (3.13)\\narises from a theory of ‚Äúuninformative priors‚Äù discussed in the next sec-\\ntion, an improvement on the principle of insufÔ¨Åcient reason; (3.13) is an\\nimproper density in that R 1\\n(cid:0)1 g.(cid:18)/ d(cid:18) D 1, but it still provides proper pos-\\nterior densities when deployed in Bayes‚Äô rule (3.5).\\n\\nThe dotted blue curve in Figure 3.2 is posterior density g.(cid:18)j O(cid:18)/ obtained\\n\\nfrom the triangular-shaped prior\\n\\ng.(cid:18)/ D 1 (cid:0) j(cid:18)j:\\n\\n(3.14)\\n\\nThis is a primitive example of a shrinkage prior, one designed to favor\\nsmaller values of (cid:18). Its effect is seen in the leftward shift of the posterior\\ndensity. Shrinkage priors will play a major role in our discussion of large-\\nscale estimation and testing problems, where we are hoping to Ô¨Ånd a few\\nlarge effects hidden among thousands of negligible ones.\\n\\n3.2 Uninformative Prior Distributions\\n\\nGiven a convincing prior distribution, Bayes‚Äô rule is easier to use and pro-\\nduces more satisfactory inferences than frequentist methods. The domi-\\nnance of frequentist practice reÔ¨Çects the scarcity of useful prior information\\nin day-to-day scientiÔ¨Åc applications. But the Bayesian impulse is strong,\\nand almost from its inception 250 years ago there have been proposals for\\nthe construction of ‚Äúpriors‚Äù that permit the use of Bayes‚Äô rule in the ab-\\nsence of relevant experience.\\n\\nOne approach, perhaps the most inÔ¨Çuential in current practice, is the\\nemployment of uninformative priors. ‚ÄúUninformative‚Äù has a positive con-\\nnotation here, implying that the use of such a prior in Bayes‚Äô rule does not\\ntacitly bias the resulting inference. Laplace‚Äôs principle of insufÔ¨Åcient rea-\\nson, i.e., assigning uniform prior distributions to unknown parameters, is\\nan obvious attempt at this goal. Its use went unchallenged for more than a\\ncentury, perhaps because of Laplace‚Äôs inÔ¨Çuence more than its own virtues.\\nVenn (of the Venn diagram) in the 1860s, and Fisher in the 1920s, attack-\\ning the routine use of Bayes‚Äô theorem, pointed out that Laplace‚Äôs principle\\ncould not be applied consistently. In the student correlation example, for\\ninstance, a uniform prior distribution for (cid:18) would not be uniform if we\\n\\n\\x0c3.2 Uninformative Prior Distributions\\n\\n29\\n\\nchanged parameters to (cid:13) D e(cid:18) ; posterior probabilities such as\\nn\\n(cid:18) > 0j O(cid:18)\\n\\nn\\n(cid:13) > 1j O(cid:18)\\n\\nD Pr\\n\\nPr\\n\\no\\n\\no\\n\\n(3.15)\\n\\nwould depend on whether (cid:18) or (cid:13) was taken to be uniform a priori. Neither\\nchoice then could be considered uninformative.\\n\\nA more sophisticated version of Laplace‚Äôs principle was put forward by\\nJeffreys beginning in the 1930s. It depends, interestingly enough, on the\\nfrequentist notion of Fisher information (Chapter 4). For a one-parameter\\nfamily f(cid:22).x/, where the parameter space (cid:127) is an interval of the real line\\n\\nR\\n\\n1, the Fisher information is deÔ¨Åned to be\\n\\n(cid:22) D E(cid:22)\\nI\\n\\n((cid:18) @\\n@(cid:22)\\n\\n(cid:19)2)\\n\\nlog f(cid:22).x/\\n\\n:\\n\\n(3.16)\\n\\n(For the Poisson family (3.3), @=@(cid:22).log f(cid:22).x// D x=(cid:22)(cid:0)1 and\\nThe Jeffreys‚Äô prior gJeff.(cid:22)/ is by deÔ¨Ånition\\n\\n(cid:22) D 1=(cid:22).)\\nI\\n\\ngJeff.(cid:22)/ D\\n\\n1=2\\n(cid:22) :\\nI\\n\\n(3.17)\\n\\n(cid:22) equals, approximately, the variance (cid:27) 2\\nBecause 1=\\nI\\nequivalent deÔ¨Ånition is\\n\\n(cid:22) of the MLE O(cid:22), an\\n\\ngJeff.(cid:22)/ D 1=(cid:27)(cid:22):\\n\\n(3.18)\\n\\nFormula (3.17) does in fact transform correctly under parameter changes,\\navoiding the Venn‚ÄìFisher criticism.(cid:142)It is known that O(cid:18) in family (3.11) has (cid:142)2\\napproximate standard deviation\\n\\n(cid:27)(cid:18) D c.1 (cid:0) (cid:18) 2/;\\n\\n(3.19)\\n\\nyielding Jeffreys‚Äô prior (3.13) from (3.18), the constant factor c having no\\neffect on Bayes‚Äô rule (3.5)‚Äì(3.6).\\n\\nThe red triangles in Figure 3.2 indicate the ‚Äú95% credible interval‚Äù [0.093,\\n\\n0.750] for (cid:18) , based on Jeffreys‚Äô prior. That is, the posterior probability\\n0:093 (cid:20) (cid:18) (cid:20) 0:750 equals 0.95,\\n\\nZ 0:750\\n\\n0:093\\n\\ngJeff (cid:16)\\n\\n(cid:18)j O(cid:18)\\n\\n(cid:17)\\n\\nd(cid:18) D 0:95;\\n\\n(3.20)\\n\\nwith probability 0.025 for (cid:18) < 0:093 or (cid:18) > 0:750. It is not an accident that\\nthis nearly equals the standard Neyman 95% conÔ¨Ådence interval based on\\nf(cid:18) . O(cid:18)/ (3.11). Jeffreys‚Äô prior tends to induce this nice connection between\\nthe Bayesian and frequentist worlds, at least in one-parameter families.\\n\\nMultiparameter probability families, Chapter 4, make everything more\\n\\n\\x0c30\\n\\nBayesian Inference\\n\\ndifÔ¨Åcult. Suppose, for instance, the statistician observes 10 independent\\nversions of the normal model (3.2), with possibly different values of (cid:22),\\n\\nind(cid:24)\\n\\nxi\\n\\n.(cid:22)i ; 1/\\n\\nN\\n\\nfor i D 1; 2; : : : ; 10;\\n\\n(3.21)\\n\\nin standard notation. Jeffreys‚Äô prior is Ô¨Çat for any one of the 10 problems,\\nwhich is reasonable for dealing with them separately, but the joint Jeffreys‚Äô\\nprior\\n\\ng.(cid:22)1; (cid:22)2; : : : ; (cid:22)10/ D constant;\\n\\n(3.22)\\n\\nalso Ô¨Çat, can produce disastrous overall results, as discussed in Chapter 13.\\nComputer-age applications are often more like (3.21) than (3.11), except\\nwith hundreds or thousands of cases rather than 10 to consider simultane-\\nously. Uninformative priors of many sorts, including Jeffreys‚Äô, are highly\\npopular in current applications, as we will discuss. This leads to an inter-\\nplay between Bayesian and frequentist methodology, the latter intended to\\ncontrol possible biases in the former, exemplifying our general theme of\\ncomputer-age statistical inference.\\n\\n3.3 Flaws in Frequentist Inference\\n\\nBayesian statistics provides an internally consistent (‚Äúcoherent‚Äù) program\\nof inference. The same cannot be said of frequentism. The apocryphal story\\nof the meter reader makes the point: an engineer measures the voltages on\\na batch of 12 tubes, using a voltmeter that is normally calibrated,\\n\\nx (cid:24)\\n\\n.(cid:22); 1/;\\n\\n(3.23)\\n\\nN\\nx being any one measurement and (cid:22) the true batch voltage. The measure-\\nments range from 82 to 99, with an average of Nx D 92, which he reports\\nback as an unbiased estimate of (cid:22).(cid:142)\\n\\nThe next day he discovers a glitch in his voltmeter such that any volt-\\nage exceeding 100 would have been reported as x D 100. His frequentist\\nstatistician tells him that Nx D 92 is no longer unbiased for the true expecta-\\ntion (cid:22) since (3.23) no longer completely describes the probability family.\\n(The statistician says that 92 is a little too small.) The fact that the glitch\\ndidn‚Äôt affect any of the actual measurements doesn‚Äôt let him off the hook;\\nNX from the actual\\nNx would not be unbiased for (cid:22) in future realizations of\\nprobability model.\\n\\nA Bayesian statistician comes to the meter reader‚Äôs rescue. For any prior\\ndensity g.(cid:22)/, the posterior density g.(cid:22)jx/ D g.(cid:22)/f(cid:22).x/=f .x/, where\\nx is the vector of 12 measurements, depends only on the data x actually\\n\\n(cid:142)3\\n\\n\\x0c3.3 Flaws in Frequentist Inference\\n\\n31\\n\\nobserved, and not on other potential data sets X that might have been\\nseen. The Ô¨Çat Jeffreys‚Äô prior g.(cid:22)/ D constant yields posterior expectation\\nNx D 92 for (cid:22), irrespective of whether or not the glitch would have affected\\nreadings above 100.\\n\\nFigure 3.3 Z-values against null hypothesis (cid:22) D 0 for months 1\\nthrough 30.\\n\\nA less contrived version of the same phenomenon is illustrated in Fig-\\nure 3.3. An ongoing experiment is being run. Each month i an independent\\nnormal variate is observed,\\n\\n(3.24)\\nN\\nwith the intention of testing the null hypothesis H0 W (cid:22) D 0 versus the\\nalternative (cid:22) > 0. The plotted points are test statistics\\n\\n.(cid:22); 1/;\\n\\nxi (cid:24)\\n\\nZi D\\n\\n.p\\n\\ni ;\\n\\nxj\\n\\ni\\nX\\n\\nj D1\\n\\na ‚Äúz-value‚Äù based on all the data up to month i,\\n(cid:17)\\n\\n(cid:16)p\\n\\nZi (cid:24)\\n\\nN\\n\\ni (cid:22); 1\\n\\n:\\n\\nAt month 30, the scheduled end of the experiment, Z30 D 1:66, just ex-\\n.0; 1/ distribution. Victory!\\nceeding 1.645, the upper 95% point for a\\nThe investigators get to claim ‚ÄúsigniÔ¨Åcant‚Äù rejection of H0 at level 0.05.\\n\\nN\\n\\n(3.25)\\n\\n(3.26)\\n\\nllllllllllllllllllllllllllllll051015202530‚àí1.5‚àí1.0‚àí0.50.00.51.01.52.0month iz value1.645\\x0c32\\n\\nBayesian Inference\\n\\nUnfortunately, it turns out that the investigators broke protocol and peek-\\ned at the data at month 20, in the hope of being able to stop an expensive\\nexperiment early. This proved a vain hope, Z20 D 0:79 not being anywhere\\nnear signiÔ¨Åcance, so they continued on to month 30 as originally planned.\\nThis means they effectively used the stopping rule ‚Äústop and declare signif-\\nicance if either Z20 or Z30 exceeds 1.645.‚Äù Some computation shows that\\nthis rule had probability 0.074, not 0.05, of rejecting H0 if it were true.\\nVictory has turned into defeat according to the honored frequentist 0.05\\ncriterion.\\n\\nOnce again, the Bayesian statistician is more lenient. The likelihood\\n\\nfunction for the full data set x D .x1; x2; : : : ; x30/,\\n\\nLx.(cid:22)/ D\\n\\n30\\nY\\n\\ni D1\\n\\ne(cid:0) 1\\n\\n2 .xi (cid:0)(cid:22)/2\\n\\n;\\n\\n(3.27)\\n\\nis the same irrespective of whether or not the experiment might have stopped\\nearly. The stopping rule doesn‚Äôt affect the posterior distribution g.(cid:22)jx/,\\nwhich depends on x only through the likelihood (3.7).\\n\\nFigure 3.4 Unbiased effect-size estimates for 6033 genes,\\nprostate cancer study. The estimate for gene 610 is x610 D 5:29.\\nWhat is its effect size?\\n\\nThe lenient nature of Bayesian inference can look less benign in multi-\\n\\n effect‚àísize estimatesFrequency‚àí4‚àí20240100200300400gene 6105.29\\x0c3.4 A Bayesian/Frequentist Comparison List\\n\\n33\\n\\nparameter settings. Figure 3.4 concerns a prostate cancer study comparing\\n52 patients with 50 healthy controls. Each man had his genetic activity\\nmeasured for a panel of N D 6033 genes. A statistic x was computed for\\neach gene,5 comparing the patients with controls, say(cid:142)\\n\\n(cid:142)4\\n\\nxi (cid:24)\\n\\nN\\n\\n.(cid:22)i ; 1/\\n\\ni D 1; 2; : : : ; N;\\n\\n(3.28)\\n\\nwhere (cid:22)i represents the true effect size for gene i. Most of the genes, prob-\\nably not being involved in prostate cancer, would be expected to have effect\\nsizes near 0, but the investigators hoped to spot a few large (cid:22)i values, either\\npositive or negative.\\n\\nThe histogram of the 6033 xi values does in fact reveal some large val-\\nues, x610 D 5:29 being the winner. Question: what estimate should we\\ngive for (cid:22)610? Even though x610 was individually unbiased for (cid:22)610, a fre-\\nquentist would (correctly) worry that focusing attention on the largest of\\n6033 values would produce an upward bias, and that our estimate should\\ndownwardly correct 5.29. ‚ÄúSelection bias,‚Äù ‚Äúregression to the mean,‚Äù and\\n‚Äúthe winner‚Äôs curse‚Äù are three names for this phenomenon.\\n\\nBayesian inference, surprisingly, is immune to selection bias.(cid:142) Irrespec- (cid:142)5\\n\\ntive of whether gene 610 was prespeciÔ¨Åed for particular attention or only\\ncame to attention as the ‚Äúwinner,‚Äù the Bayes‚Äô estimate for (cid:22)610 given all\\nthe data stays the same. This isn‚Äôt obvious, but follows from the fact that\\nany data-based selection process does not affect the likelihood function in\\n(3.7).\\n\\nWhat does affect Bayesian inference is the prior g.(cid:22)/ for the full vector\\n(cid:22) of 6033 effect sizes. The Ô¨Çat prior, g.(cid:22)/ constant, results in the danger-\\nous overestimate O(cid:22)610 D x610 D 5:29. A more appropriate uninformative\\nprior appears as part of the empirical Bayes calculations of Chapter 15\\n(and gives O(cid:22)610 D 4:11). The operative point here is that there is a price to\\nbe paid for the desirable properties of Bayesian inference. Attention shifts\\nfrom choosing a good frequentist procedure to choosing an appropriate\\nprior distribution. This can be a formidable task in high-dimensional prob-\\nlems, the very kinds featured in computer-age inference.\\n\\n3.4 A Bayesian/Frequentist Comparison List\\n\\nBayesians and frequentists start out on the same playing Ô¨Åeld, a family\\nof probability distributions f(cid:22).x/ (3.1), but play the game in orthogonal\\n\\n5 The statistic was the two-sample t-statistic (2.17) transformed to normality (3.28); see\\n\\nthe endnotes.\\n\\n\\x0c34\\n\\nBayesian Inference\\n\\ndirections, as indicated schematically in Figure 3.5: Bayesian inference\\nproceeds vertically, with x Ô¨Åxed, according to the posterior distribution\\ng.(cid:22)jx/, while frequentists reason horizontally, with (cid:22) Ô¨Åxed and x varying.\\nAdvantages and disadvantages accrue to both strategies, some of which are\\ncompared next.\\n\\nFigure 3.5 Bayesian inference proceeds vertically, given x;\\nfrequentist inference proceeds horizontally, given (cid:22).\\n\\n(cid:15) Bayesian inference requires a prior distribution g.(cid:22)/. When past experi-\\nence provides g.(cid:22)/, as in the twins example, there is every good reason to\\nemploy Bayes‚Äô theorem. If not, techniques such as those of Jeffreys still\\npermit the use of Bayes‚Äô rule, but the results lack the full logical force\\nof the theorem; the Bayesian‚Äôs right to ignore selection bias, for instance,\\nmust then be treated with caution.\\n\\n(cid:15) Frequentism replaces the choice of a prior with the choice of a method,\\nor algorithm, t .x/, designed to answer the speciÔ¨Åc question at hand. This\\nadds an arbitrary element to the inferential process, and can lead to meter-\\nreader kinds of contradictions. Optimal choice of t.x/ reduces arbitrary\\nbehavior, but computer-age applications typically move outside the safe\\nwaters of classical optimality theory, lending an ad-hoc character to fre-\\nquentist analyses.\\n\\n(cid:15) Modern data-analysis problems are often approached via a favored meth-\\n\\n\\x0c3.4 A Bayesian/Frequentist Comparison List\\n\\n35\\n\\nodology, such as logistic regression or regression trees in the examples of\\nChapter 8. This plays into the methodological orientation of frequentism,\\nwhich is more Ô¨Çexible than Bayes‚Äô rule in dealing with speciÔ¨Åc algorithms\\n(though one always hopes for a reasonable Bayesian justiÔ¨Åcation for the\\nmethod at hand).\\n\\n(cid:15) Having chosen g.(cid:22)/, only a single probability distribution g.(cid:22)jx/ is in\\nplay for Bayesians. Frequentists, by contrast, must struggle to balance\\nthe behavior of t.x/ over a family of possible distributions, since (cid:22) in\\nFigure 3.5 is unknown. The growing popularity of Bayesian applications\\n(usually begun with uninformative priors) reÔ¨Çects their simplicity of ap-\\nplication and interpretation.\\n\\n(cid:15) The simplicity argument cuts both ways. The Bayesian essentially bets it\\nall on the choice of his or her prior being correct, or at least not harmful.\\nFrequentism takes a more defensive posture, hoping to do well, or at least\\nnot poorly, whatever (cid:22) might be.\\n\\n(cid:15) A Bayesian analysis answers all possible questions at once, for example,\\nestimating Efgfrg or Prfgfr < 40g or anything else relating to Figure 2.1.\\nFrequentism focuses on the problem at hand, requiring different estima-\\ntors for different questions. This is more work, but allows for more intense\\ninspection of particular problems. In situation (2.9) for example, estima-\\ntors of the form\\n\\nX\\n\\n.xi (cid:0) Nx/2=.n (cid:0) c/\\n\\n(3.29)\\n\\nmight be investigated for different choices of the constant c, hoping to\\nreduce expected mean-squared error.\\n\\n(cid:15) The simplicity of the Bayesian approach is especially appealing in dy-\\nnamic contexts, where data arrives sequentially and updating one‚Äôs beliefs\\nis a natural practice. Bayes‚Äô rule was used to devastating effect before the\\n2012 US presidential election, updating sequential polling results to cor-\\nrectly predict the outcome in all 50 states. Bayes‚Äô theorem is an excellent\\ntool in general for combining statistical evidence from disparate sources,\\nthe closest frequentist analog being maximum likelihood estimation.\\n(cid:15) In the absence of genuine prior information, a whiff of subjectivity6 hangs\\nover Bayesian results, even those based on uninformative priors. Classical\\nfrequentism claimed for itself the high ground of scientiÔ¨Åc objectivity,\\nespecially in contentious areas such as drug testing and approval, where\\nskeptics as well as friends hang on the statistical details.\\n\\nFigure 3.5 is soothingly misleading in its schematics: (cid:22) and x will\\n\\n6 Here we are not discussing the important subjectivist school of Bayesian inference, of\\n\\nSavage, de Finetti, and others, covered in Chapter 13.\\n\\n\\x0c36\\n\\nBayesian Inference\\n\\ntypically be high-dimensional in the chapters that follow, sometimes very\\nhigh-dimensional, straining to the breaking point both the frequentist and\\nthe Bayesian paradigms. Computer-age statistical inference at its most\\nsuccessful combines elements of the two philosophies, as for instance in\\nthe empirical Bayes methods of Chapter 6, and the lasso in Chapter 16.\\nThere are two potent arrows in the statistician‚Äôs philosophical quiver, and\\nfaced, say, with 1000 parameters and 1,000,000 data points, there‚Äôs no\\nneed to go hunting armed with just one of them.\\n\\n3.5 Notes and Details\\n\\nThomas Bayes, if transferred to modern times, might well be employed as\\na successful professor of mathematics. Actually, he was a mid-eighteenth-\\ncentury nonconformist English minister with substantial mathematical in-\\nterests. Richard Price, a leading Ô¨Ågure of letters, science, and politics, had\\nBayes‚Äô theorem published in the 1763 Transactions of the Royal Society\\n(two years after Bayes‚Äô death), his interest being partly theological, with\\nthe rule somehow proving the existence of God. Bellhouse‚Äôs (2004) biog-\\nraphy includes some of Bayes‚Äô other mathematical accomplishments.\\n\\nHarold Jeffreys was another part-time statistician, working from his day\\njob as the world‚Äôs premier geophysicist of the inter-war period (and Ô¨Åerce\\nopponent of the theory of continental drift). What we called uninformative\\npriors are also called noninformative or objective. Jeffreys‚Äô brand of Bayes-\\nianism had a dubious reputation among Bayesians in the period 1950‚Äì\\n1990, with preference going to subjective analysis of the type advocated\\nby Savage and de Finetti. The introduction of Markov chain Monte Carlo\\nmethodology was the kind of technological innovation that changes philoso-\\nphies. MCMC (Chapter 13), being very well suited to Jeffreys-style anal-\\nysis of Big Data problems, moved Bayesian statistics out of the textbooks\\nand into the world of computer-age applications. Berger (2006) makes a\\nspirited case for the objective Bayes approach.\\n\\n(cid:142)1 [p. 26] Correlation coefÔ¨Åcient density. Formula (3.11) for the correlation\\ncoefÔ¨Åcient density was R. A. Fisher‚Äôs debut contribution to the statistics\\nliterature. Chapter 32 of Johnson and Kotz (1970b) gives several equivalent\\nforms. The constant c in (3.19) is often taken to be .n (cid:0) 3/(cid:0)1=2, with n the\\nsample size.\\n\\n(cid:142)2 [p. 29] Jeffreys‚Äô prior and transformations. Suppose we change parame-\\nters from (cid:22) to Q(cid:22) in a smoothly differentiable way. The new family Qf Q(cid:22).x/\\n\\n\\x0c3.5 Notes and Details\\n\\n37\\n\\nsatisÔ¨Åes\\n\\n@\\n@ Q(cid:22)\\n\\nlog Qf Q(cid:22).x/ D @(cid:22)\\n@ Q(cid:22)\\n\\n@\\n@(cid:22)\\n\\nlog f(cid:22).x/:\\n\\n(3.30)\\n\\nÀá\\nThen Q\\nÀá\\nÀá\\nI\\nsays that gJeff.(cid:22)/ transforms correctly to QgJeff. Q(cid:22)/.\\n\\n(cid:22) (3.16) and QgJeff. Q(cid:22)/ D\\nI\\n\\n(cid:16) @(cid:22)\\n@ Q(cid:22)\\n\\nQ(cid:22) D\\n\\n(cid:17)2\\n\\n@(cid:22)\\n@ Q(cid:22)\\n\\nÀá\\nÀá\\nÀá gJeff.(cid:22)/. But this just\\n\\n(cid:142)3 [p. 30] The meter-reader fable is taken from Edwards‚Äô (1992) book Likeli-\\nhood, where he credits John Pratt. It nicely makes the point that frequentist\\ninferences, which are calibrated in terms of possible observed data sets X,\\nmay be inappropriate for the actual observation x. This is the difference\\nbetween working in the horizontal and vertical directions of Figure 3.5.\\n(cid:142)4 [p. 33] Two-sample t -statistic. Applied to gene i‚Äôs data in the prostate\\nstudy, the two-sample t -statistic ti (2.17) has theoretical null hypothesis\\ndistribution t100, a Student‚Äôs t distribution with 100 degrees of freedom; xi\\nin (3.28) is ÀÜ(cid:0)1.F100.ti //, where ÀÜ and F100 are the cumulative distribu-\\ntion functions of standard normal and t100 variables. Section 7.4 of Efron\\n(2010) motivates approximation (3.28).\\n\\n(cid:142)5 [p. 33] Selection bias. Senn (2008) discusses the immunity of Bayesian\\ninferences to selection bias and other ‚Äúparadoxes,‚Äù crediting Phil Dawid for\\nthe original idea. The article catches the possible uneasiness of following\\nBayes‚Äô theorem too literally in applications.\\n\\nThe 22 students in Table 3.1 were randomly selected from a larger data\\nset of 88 in Mardia et al. (1979) (which gave O(cid:18) D 0:553). Welch and\\nPeers (1963) initiated the study of priors whose credible intervals, such\\nas ≈í0:093; 0:750(cid:141) in Figure 3.2, match frequentist conÔ¨Ådence intervals. In\\none-parameter problems, Jeffreys‚Äô priors provide good matches, but not\\nususally in multiparameter situations. In fact, no single multiparameter\\nprior can give good matches for all one-parameter subproblems, a source of\\ntension between Bayesian and frequentist methods revisited in Chapter 11.\\n\\n\\x0c4\\n\\nFisherian Inference and Maximum\\nLikelihood Estimation\\n\\nSir Ronald Fisher was arguably the most inÔ¨Çuential anti-Bayesian of all\\ntime, but that did not make him a conventional frequentist. His key data-\\nanalytic methods‚Äîanalysis of variance, signiÔ¨Åcance testing, and maxi-\\nmum likelihood estimation‚Äîwere almost always applied frequentistically.\\nTheir Fisherian rationale, however, often drew on ideas neither Bayesian\\nnor frequentist in nature, or sometimes the two in combination. Fisher‚Äôs\\nwork held a central place in twentieth-century applied statistics, and some\\nof it, particularly maximum likelihood estimation, has moved forcefully\\ninto computer-age practice. This chapter‚Äôs brief review of Fisherian meth-\\nodology sketches parts of its unique philosophical structure, while concen-\\ntrating on those topics of greatest current importance.\\n\\n4.1 Likelihood and Maximum Likelihood\\n\\nFisher‚Äôs seminal work on estimation focused on the likelihood function,\\nor more exactly its logarithm. For a family of probability densities f(cid:22).x/\\n(3.1), the log likelihood function is\\n\\nlx.(cid:22)/ D logff(cid:22).x/g;\\n\\n(4.1)\\n\\nthe notation lx.(cid:22)/ emphasizing that the parameter vector (cid:22) is varying\\nwhile the observed data vector x is Ô¨Åxed. The maximum likelihood esti-\\nmate (MLE) is the value of (cid:22) in parameter space (cid:127) that maximizes lx.(cid:22)/,\\n\\nMLE W\\n\\nO(cid:22) D arg max\\n\\nflx.(cid:22)/g:\\n\\n(cid:22)2(cid:127)\\n\\n(4.2)\\n\\nIt can happen that O(cid:22) doesn‚Äôt exist or that there are multiple maximizers, but\\nhere we will assume the usual case where O(cid:22) exists uniquely. More careful\\nreferences are provided in the endnotes.\\n\\nDeÔ¨Ånition (4.2) is extended to provide maximum likelihood estimates\\n\\n38\\n\\n\\x0c4.1 Likelihood and Maximum Likelihood\\n\\n39\\n\\nfor a function (cid:18) D T .(cid:22)/ of (cid:22) according to the simple plug-in rule\\n\\nO(cid:18) D T . O(cid:22)/;\\n\\n(4.3)\\n\\nmost often with (cid:18) being a scalar parameter of particular interest, such as\\nthe regression coefÔ¨Åcient of an important covariate in a linear model.\\n\\nMaximum likelihood estimation came to dominate classical applied es-\\ntimation practice. Less dominant now, for reasons we will be investigating\\nin subsequent chapters, the MLE algorithm still has iconic status, being of-\\nten the method of Ô¨Årst choice in any novel situation. There are several good\\nreasons for its ubiquity.\\n\\n1 The MLE algorithm is automatic: in theory, and almost in practice, a\\nsingle numerical algorithm produces O(cid:22) without further statistical input.\\nThis contrasts with unbiased estimation, for instance, where each new\\nsituation requires clever theoretical calculations.\\n\\n2 The MLE enjoys excellent frequentist properties. In large-sample situa-\\ntions, maximum likelihood estimates tend to be nearly unbiased, with the\\nleast possible variance. Even in small samples, MLEs are usually quite\\nefÔ¨Åcient, within say a few percent of the best possible performance.\\n3 The MLE also has reasonable Bayesian justiÔ¨Åcation. Looking at Bayes‚Äô\\n\\nrule (3.7),\\n\\ng.(cid:22)jx/ D cxg.(cid:22)/elx .(cid:22)/;\\n(4.4)\\nwe see that O(cid:22) is the maximizer of the posterior density g.(cid:22)jx/ if the prior\\ng.(cid:22)/ is Ô¨Çat, that is, constant. Because the MLE depends on the family\\nonly through the likelihood function, anomalies of the meter-reader\\n\\nF\\ntype are averted.\\n\\nFigure 4.1 displays two maximum likelihood estimates for the gfr data\\nof Figure 2.1. Here the data1 is the vector x D .x1; x2; : : : ; xn/, n D 211.\\nWe assume that x was obtained as a random sample of size n from a density\\nf(cid:22).x/,\\n\\nxi\\n\\niid(cid:24) f(cid:22).x/\\n\\nfor i D 1; 2; : : : ; n;\\n\\n(4.5)\\n\\n‚Äúiid‚Äù abbreviating ‚Äúindependent and identically distributed.‚Äù Two families\\nare considered for the component density f(cid:22).x/, the normal, with (cid:22) D\\n.(cid:18); (cid:27)/,\\n\\nf(cid:22).x/ D\\n\\n1p\\n\\n2(cid:25)(cid:27) 2\\n\\n2 . x(cid:0)(cid:18)\\n\\n(cid:27) /2\\n\\ne(cid:0) 1\\n\\n;\\n\\n(4.6)\\n\\n1 Now x is what we have been calling ‚Äúx‚Äù before, while we will henceforth use x as a\\n\\nsymbol for the individual components of x.\\n\\n\\x0c40\\n\\nFisherian Inference and MLE\\n\\nFigure 4.1 Glomerular Ô¨Åltration data of Figure 2.1 and two\\nmaximum-likelihood density estimates, normal (solid black), and\\ngamma (dashed blue).\\n\\nand the gamma,2 with (cid:22) D .(cid:21); (cid:27); (cid:23)/,\\nf(cid:22).x/ D .x (cid:0) (cid:21)/(cid:23)(cid:0)1\\n(cid:27) (cid:23)(cid:128).(cid:23)/\\n\\ne(cid:0) x(cid:0)(cid:21)\\n\\n(cid:27)\\n\\n(for x (cid:21) (cid:21), 0 otherwise):\\n\\n(4.7)\\n\\nSince\\n\\nf(cid:22).x/ D\\n\\nn\\nY\\n\\ni D1\\n\\nf(cid:22).xi /\\n\\nunder iid sampling, we have\\n\\nlx.(cid:22)/ D\\n\\nn\\nX\\n\\niD1\\n\\nlog f(cid:22).xi / D\\n\\nn\\nX\\n\\niD1\\n\\nlxi .(cid:22)/:\\n\\n(4.8)\\n\\n(4.9)\\n\\nMaximum likelihood estimates were found by maximizing lx.(cid:22)/. For the\\nnormal model (4.6),\\n(cid:16) O(cid:18); O(cid:27)\\n\\nD .54:3; 13:7/ D\\n\\n.xi (cid:0) Nx/2 =n\\n\\ni1=2(cid:19)\\n\\n(4.10)\\n\\nhX\\n\\nNx;\\n\\n(cid:18)\\n\\n(cid:17)\\n\\n:\\n\\n2 The gamma distribution is usually deÔ¨Åned with (cid:21) D 0 as the lower limit of x. Here we\\n\\nare allowing the lower limit (cid:21) to vary as a free parameter.\\n\\n gfrFrequency20406080100051015202530NormalGamma\\x0c4.2 Fisher Information and the MLE\\n\\n41\\n\\nThere is no closed-form solution for gamma model (4.7), where numerical\\nmaximization gave\\n\\n(cid:16) O(cid:21); O(cid:27); O(cid:23)\\n\\n(cid:17)\\n\\nD .21:4; 5:47; 6:0/:\\n\\n(4.11)\\n\\nThe plotted curves in Figure 4.1 are the two MLE densities f O(cid:22).x/. The\\ngamma model gives a better Ô¨Åt than the normal, but neither is really satis-\\nfactory. (A more ambitious maximum likelihood Ô¨Åt appears in Figure 5.7.)\\nMost MLEs require numerical maximization, as for the gamma model.\\nWhen introduced in the 1920s, maximum likelihood was criticized as com-\\nputationally difÔ¨Åcult, invidious comparisons being made with the older\\nmethod of moments, which relied only on sample moments of various\\nkinds.\\n\\nThere is a downside to maximum likelihood estimation that remained\\nnearly invisible in classical applications: it is dangerous to rely upon in\\nproblems involving large numbers of parameters. If the parameter vector\\n(cid:22) has 1000 components, each component individually may be well esti-\\nmated by maximum likelihood, while the MLE O(cid:18) D T . O(cid:22)/ for a quantity of\\nparticular interest can be grossly misleading.\\n\\nFor the prostate data of Figure 3.4, model (4.6) gives MLE O(cid:22)i D xi for\\neach of the 6033 genes. This seems reasonable, but if we are interested in\\nthe maximum coordinate value\\n\\n(cid:18) D T .(cid:22)/ D max\\n\\ni\\n\\nf(cid:22)i g;\\n\\n(4.12)\\n\\nthe MLE is O(cid:18) D 5:29, almost certainly a Ô¨Çagrant overestimate. ‚ÄúRegular-\\nized‚Äù versions of maximum likelihood estimation more suitable for high-\\ndimensional applications play an important role in succeeding chapters.\\n\\n4.2 Fisher Information and the MLE\\n\\nFisher was not the Ô¨Årst to suggest the maximum likelihood algorithm for\\nparameter estimation. His paradigm-shifting work concerned the favorable\\ninferential properties of the MLE, and in particular its achievement of the\\nFisher information bound. Only a brief heuristic review will be provided\\nhere, with more careful derivations referenced in the endnotes.\\n\\nWe begin3 with a one-parameter family of densities\\n\\nX\\n3 The multiparameter case is considered in the next chapter.\\n\\nF\\n\\nD ff(cid:18) .x/; (cid:18) 2 (cid:127); x 2\\n\\ng ;\\n\\n(4.13)\\n\\n\\x0c42\\n\\nFisherian Inference and MLE\\n\\nX\\n\\nwhere (cid:127) is an interval of the real line, possibly inÔ¨Ånite, while the sam-\\nple space\\nmay be multidimensional. (As in the Poisson example (3.3),\\nf(cid:18) .x/ can represent a discrete density, but for convenience we assume here\\nthe continuous case, with the probability of set A equaling R\\nA f(cid:18) .x/ dx,\\netc.) The log likelihood function is lx.(cid:18)/ D log f(cid:18) .x/ and the MLE O(cid:18) D\\narg maxflx.(cid:18)/g, with (cid:18) replacing (cid:22) in (4.1)‚Äì(4.2) in the one-dimensional\\ncase.\\n\\nDots will indicate differentiation with respect to (cid:18), e.g., for the score\\n\\nfunction\\n\\nPlx.(cid:18)/ D @\\n@(cid:18)\\n\\nlog f(cid:18) .x/ D Pf(cid:18) .x/=f(cid:18) .x/:\\n\\n(4.14)\\n\\nThe score function has expectation 0,\\n\\nPlx.(cid:18)/f(cid:18) .x/ dx D\\n\\nZ\\n\\nX\\n\\nZ\\n\\nX\\n\\nPf(cid:18) .x/ dx D @\\n@(cid:18)\\nD @\\n@(cid:18)\\n\\nZ\\n\\nf(cid:18) .x/ dx\\n\\nX\\n1 D 0;\\n\\n(4.15)\\n\\nwhere we are assuming the regularity conditions necessary for differenti-\\nating under the integral sign at the third step.\\n\\nThe Fisher information\\n\\nfunction,\\n\\n(cid:18) is deÔ¨Åned to be the variance of the score\\n\\nI\\n\\n(cid:18) D\\nI\\n\\nZ\\n\\nX\\n\\nPlx.(cid:18)/2f(cid:18) .x/ dx;\\n\\n(4.16)\\n\\nthe notation\\n\\nPlx.(cid:18)/ (cid:24) .0;\\n\\n(cid:18) /\\nI\\nindicating that Plx.(cid:18) / has mean 0 and variance\\n(cid:18) . The term ‚Äúinformation‚Äù is\\nwell chosen. The main result for maximum likelihood estimation, sketched\\nnext, is that the MLE O(cid:18) has an approximately normal distribution with\\nmean (cid:18) and variance 1=\\n\\n(4.17)\\n\\nI\\n\\n(cid:18) ,\\n\\nI\\n\\nO(cid:18) P(cid:24)\\n\\nN\\n\\n.(cid:18); 1=\\n\\n(cid:18) /;\\n\\nI\\n\\n(4.18)\\n\\nand that no ‚Äúnearly unbiased‚Äù estimator of (cid:18) can do better. In other words,\\nbigger Fisher information implies smaller variance for the MLE.\\n\\nThe second derivative of the log likelihood function\\n\\nRlx.(cid:18) / D @2\\n\\n@(cid:18) 2 log f(cid:18) .x/ D\\n\\nRf(cid:18) .x/\\nf(cid:18) .x/\\n\\n(cid:0)\\n\\n  Pf(cid:18) .x/\\nf(cid:18) .x/\\n\\n!2\\n\\n(4.19)\\n\\n\\x0c4.2 Fisher Information and the MLE\\n\\n43\\n\\nhas expectation\\n\\no\\nn Rlx.(cid:18)/\\n(the Rf(cid:18) .x/=f(cid:18) .x/ term having expectation 0 as in (4.15)). We can write\\n\\nD (cid:0)\\n\\n(cid:18)\\nI\\n\\nE(cid:18)\\n\\n(4.20)\\n\\n(cid:0) Rlx.(cid:18)/ (cid:24) .\\n\\n(cid:18) ;\\n\\nI\\n\\n(cid:18) /;\\n\\nJ\\n\\n(4.21)\\n\\nwhere\\n\\n(cid:18) is the variance of Rlx.(cid:18)/.\\nJ\\n\\nNow suppose that x D .x1; x2; : : : ; xn/ is an iid sample from f(cid:18) .x/, as\\n\\nin (4.5), so that the total score function Plx.(cid:18)/, as in (4.9), is\\n\\nand similarly\\n\\nPlx.(cid:18)/ D\\n\\n(cid:0) Rlx.(cid:18)/ D\\n\\nn\\nX\\n\\niD1\\n\\nn\\nX\\n\\nPlxi .(cid:18)/;\\n\\n(cid:0) Rlxi .(cid:18)/:\\n\\n(4.22)\\n\\n(4.23)\\n\\niD1\\nThe MLE O(cid:18) based on the full sample x satisÔ¨Åes the maximizing condition\\nPlx. O(cid:18)/ D 0. A Ô¨Årst-order Taylor series gives the approximation\\n\\n0 D Plx\\n\\n(cid:16) O(cid:18)\\n\\n(cid:17) :D Plx.(cid:18)/ C Rlx.(cid:18)/\\n\\n(cid:16) O(cid:18) (cid:0) (cid:18)\\n\\n(cid:17)\\n\\n;\\n\\nor\\n\\nO(cid:18)\\n\\n:D (cid:18) C\\n\\nPlx.(cid:18)/=n\\n(cid:0) Rlx.(cid:18)/=n\\n\\n:\\n\\n(4.24)\\n\\n(4.25)\\n\\nUnder reasonable regularity conditions, (4.17) and the central limit theo-\\nrem imply that\\n\\nPlx.(cid:18) /=n P(cid:24)\\n\\n.0;\\n\\n(cid:18) =n/;\\n\\nN\\n\\nI\\n\\n(4.26)\\n\\nwhile the law of large numbers has (cid:0) Rlx.(cid:18)/=n approaching the constant\\n(4.21).\\n\\n(cid:18)\\nI\\n\\nPutting all of this together, (4.25) produces Fisher‚Äôs fundamental theo-\\n\\nrem for the MLE, that in large samples\\n\\nO(cid:18) P(cid:24)\\n\\nN\\n\\n.(cid:18); 1=.n\\n\\n(cid:18) // :\\n\\nI\\n\\n(4.27)\\n\\nThis is the same as result (4.18) since the total Fisher information in an iid\\nsample (4.5) is n\\n\\n(cid:18) , as can be seen by taking expectations in (4.23).\\n\\nI\\nIn the case of normal sampling,\\n\\niid(cid:24)\\n\\nxi\\n\\nN\\n\\n.(cid:18); (cid:27) 2/\\n\\nfor i D 1; 2; : : : ; n;\\n\\n(4.28)\\n\\n\\x0c44\\n\\nFisherian Inference and MLE\\n\\nwith (cid:27) 2 known, we compute the log likelihood\\n\\nlx.(cid:18)/ D (cid:0) 1\\n2\\n\\nn\\nX\\n\\ni D1\\n\\n.xi (cid:0) (cid:18)/2\\n(cid:27) 2\\n\\n(cid:0) n\\n2\\n\\nlog.2(cid:25)(cid:27) 2/:\\n\\n(4.29)\\n\\nThis gives\\n\\nPlx.(cid:18)/ D 1\\n(cid:27) 2\\n\\nn\\nX\\n\\n.xi (cid:0) (cid:18)/\\n\\niD1\\n\\nand (cid:0) Rlx.(cid:18)/ D n\\n(cid:27) 2 ;\\n\\nyielding the familiar result O(cid:18) D Nx and, since\\nI\\n.(cid:18); (cid:27) 2=n/\\n\\nO(cid:18) (cid:24)\\n\\nN\\n\\n(cid:18) D 1=(cid:27) 2,\\n\\n(4.30)\\n\\n(4.31)\\n\\nfrom (4.27).\\n\\nThis brings us to an aspect of Fisherian inference neither Bayesian nor\\nfrequentist. Fisher believed there was a ‚Äúlogic of inductive inference‚Äù that\\nwould produce the correct answer to any statistical question, in the same\\nway ordinary logic solves deductive problems. His principal tactic was to\\nlogically reduce a complicated inferential question to a simple form where\\nthe solution should be obvious to all.\\n\\nFisher‚Äôs favorite target for the obvious was (4.31), where a single scalar\\nobservation O(cid:18) is normally distributed around the unknown parameter of\\ninterest (cid:18), with known variance (cid:27) 2=n. Then everyone should agree in the\\nabsence of prior information that O(cid:18) is the best estimate of (cid:18), that (cid:18) has\\nabout 95% chance of lying in the interval O(cid:18) Àô 1:96 O(cid:27)=\\n\\nn, etc.\\n\\np\\n\\nFisher was astoundingly resourceful at reducing statistical problems to\\nthe form (4.31). SufÔ¨Åciency, efÔ¨Åciency, conditionality, and ancillarity were\\nall brought to bear, with the maximum likelihood approximation (4.27)\\nbeing the most inÔ¨Çuential example. Fisher‚Äôs logical system is not in favor\\nthese days, but its conclusions remain as staples of conventional statistical\\npractice.\\n\\nSuppose that Q(cid:18) D t .x/ is any unbiased estimate of (cid:18) based on an iid\\n\\nsample x D .x1; x2; : : : ; xn/ from f(cid:18) .x/. That is,\\n\\n(cid:18) D E(cid:18) ft.x/g:\\n\\n(4.32)\\n\\n(cid:142)1\\n\\nThen the Cram¬¥er‚ÄìRao lower bound, described in the endnotes, says that\\nthe variance of Q(cid:18) exceeds the Fisher information bound (4.27),(cid:142)\\nn Q(cid:18)\\n\\n(cid:21) 1=.n\\n\\n(4.33)\\n\\no\\n\\nvar(cid:18)\\n\\n(cid:18) /:\\n\\nI\\n\\nA loose interpretation is that the MLE has variance at least as small as\\nthe best unbiased estimate of (cid:18). The MLE is generally not unbiased, but\\n\\n\\x0c4.3 Conditional Inference\\n\\n45\\n\\np\\n\\nits bias is small (of order 1=n, compared with standard deviation of order\\nn), making the comparison with unbiased estimates and the Cram¬¥er‚Äì\\n1=\\nRao bound appropriate.\\n\\n4.3 Conditional Inference\\n\\nA simple example gets across the idea of conditional inference: an i.i.d.\\nsample\\n\\niid(cid:24)\\n\\nxi\\n\\n(4.34)\\nhas produced estimate O(cid:18) D Nx. The investigators originally disagreed on an\\naffordable sample size n and Ô¨Çipped a fair coin to decide,\\n\\n.(cid:18); 1/;\\n\\ni D 1; 2; : : : ; n;\\n\\nN\\n\\nn D\\n\\n(\\n\\n25\\n100\\n\\nprobability 1/2\\nprobability 1/2I\\n\\n(4.35)\\n\\nn D 25 won. Question: What is the standard deviation of Nx?\\n\\np\\n\\nIf you answered 1=\\n\\n25 D 0:2 then you, like Fisher, are an advocate\\nof conditional inference. The unconditional frequentist answer says that Nx\\n.(cid:18); 1=25/ with equal probability, yield-\\ncould have been\\ning standard deviation ≈í.0:01 C 0:04/=2(cid:141)1=2 D 0:158. Some less obvious\\n(and less trivial) examples follow in this section, and in Chapter 9, where\\nconditional inference plays a central role.\\n\\n.(cid:18); 1=100/ or\\n\\nN\\n\\nN\\n\\nThe data for a typical regression problem consists of pairs .xi ; yi /, i D\\n1; 2; : : : ; n, where xi is a p-dimensional vector of covariates for the ith\\nsubject and yi is a scalar response. In Figure 1.1, xi is age and yi the\\nkidney Ô¨Åtness measure tot. Let x be the n (cid:2) p matrix having xi as its ith\\nrow, and y the vector of responses. A regression algorithm uses x and y\\nto construct a function rx;y .x/ predicting y for any value of x, as in (1.3),\\nwhere OÀá0 and OÀá1 were obtained using least squares.\\n\\nHow accurate is rx;y .x/? This question is usually answered under the\\nassumption that x is Ô¨Åxed, not random: in other words, by conditioning\\non the observed value of x. The standard errors in the second line of Ta-\\nble 1.1 are conditional in this sense; they are frequentist standard deviations\\nof OÀá0 C OÀá1x, assuming that the 157 values for age are Ô¨Åxed as observed.\\n(A correlation analysis between age and tot would not make this as-\\nsumption.)\\n\\nFisher argued for conditional inference on two grounds.\\n\\n\\x0c46\\n\\nFisherian Inference and MLE\\n\\n1 More relevant inferences. The conditional standard deviation in situ-\\nation (4.35) seems obviously more relevant to the accuracy of the ob-\\nserved O(cid:18) for estimating (cid:18) . It is less obvious in the regression example,\\nthough arguably still the case.\\n\\n2 Simpler inferences. Conditional inferences are often simpler to exe-\\ncute and interpret. This is the case with regression, where the statistician\\ndoesn‚Äôt have to worry about correlation relationships among the covari-\\nates, and also with our next example, a Fisherian classic.\\n\\nTable 4.1 shows the results of a randomized trial on 45 ulcer patients,\\ncomparing new and old surgical treatments. Was the new surgery signiÔ¨Å-\\ncantly better? Fisher argued for carrying out the hypothesis test conditional\\non the marginals of the table .16; 29; 21; 24/. With the marginals Ô¨Åxed, the\\nnumber y in the upper left cell determines the other three cells by subtrac-\\ntion. We need only test whether the number y D 9 is too big under the null\\nhypothesis of no treatment difference, instead of trying to test the numbers\\nin all four cells.4\\n\\nTable 4.1 Forty-Ô¨Åve ulcer patients randomly assigned to either new or\\nold surgery, with results evaluated as either success or failure.\\nWas the new surgery signiÔ¨Åcantly better?\\n\\nsuccess\\n\\nfailure\\n\\nnew\\n\\nold\\n\\n9\\n\\n7\\n\\n16\\n\\n12\\n\\n17\\n\\n29\\n\\n21\\n\\n24\\n\\n45\\n\\nAn ancillary statistic (again, Fisher‚Äôs terminology) is one that contains\\nno direct information by itself, but does determine the conditioning frame-\\nwork for frequentist calculations. Our three examples of ancillaries were\\nthe sample size n, the covariate matrix x, and the table‚Äôs marginals. ‚ÄúCon-\\ntains no information‚Äù is a contentious claim. More realistically, the two ad-\\nvantages of conditioning, relevance and simplicity, are thought to outweigh\\nthe loss of information that comes from treating the ancillary statistic as\\nnonrandom. Chapter 9 makes this case speciÔ¨Åcally for standard survival\\nanalysis methods.\\n\\n4 Section 9.3 gives the details of such tests; in the surgery example, the difference was not\\n\\nsigniÔ¨Åcant.\\n\\n\\x0c4.3 Conditional Inference\\n\\n47\\n\\nOur Ô¨Ånal example concerns the accuracy of a maximum likelihood esti-\\n\\nmate O(cid:18). Rather than\\n\\nO(cid:18) P(cid:24)\\n\\n(cid:0)(cid:18); 1ƒ± (cid:0)n\\nO(cid:18)\\nI\\n\\n(cid:1)(cid:1) ;\\n\\nN\\n\\nthe plug-in version of (4.27), Fisher suggested using\\n\\nO(cid:18) P(cid:24)\\n\\nN\\n\\n.(cid:18); 1=I.x// ;\\n\\n(4.36)\\n\\n(4.37)\\n\\nwhere I.x/ is the observed Fisher information\\nD (cid:0) @2\\n\\nI.x/ D (cid:0) Rlx\\n\\n(cid:16) O(cid:18)\\n\\n(cid:17)\\n\\nÀá\\nÀá\\n@(cid:18) 2 lx.(cid:18)/\\nÀá\\nÀá O(cid:18)\\n\\n:\\n\\n(4.38)\\n\\nThe expectation of I.x/ is n\\n(cid:18) , so in large samples the distribution (4.37)\\nconverges to (4.36). Before convergence, however, Fisher suggested that\\n(4.37) gives a better idea of O(cid:18)‚Äôs accuracy.\\n\\nI\\n\\nAs a check, a simulation was run involving i.i.d. samples x of size n D\\n\\n20 drawn from a Cauchy density\\n\\nf(cid:18) .x/ D 1\\n(cid:25)\\n\\n1\\n1 C .x (cid:0) (cid:18)/2 :\\n\\n(4.39)\\n\\n10,000 samples x of size n D 20 were drawn (with (cid:18) D 0) and the ob-\\nserved information bound 1=I.x/ computed for each. The 10,000 O(cid:18) values\\nwere grouped according to deciles of 1=I.x/, and the observed empirical\\nvariance of O(cid:18) within each group was then calculated.\\n\\nThis amounts to calculating a somewhat crude estimate of the condi-\\ntional variance of the MLE O(cid:18), given the observed information bound 1=I.x/.\\nFigure 4.2 shows the results. We see that the conditional variance is close\\nto 1=I.x/, as Fisher predicted. The conditioning effect is quite substan-\\ntial; the unconditional variance 1=n\\n(cid:18) is 0.10 here, while the conditional\\nvariance ranges from 0.05 to 0.20.\\n\\nI\\n\\nThe observed Fisher information I.x/ acts as an approximate ancillary,\\nenjoying both of the virtues claimed by Fisher: it is more relevant than the\\nunconditional information n\\nO(cid:18) , and it is usually easier to calculate. Once\\nI\\nO(cid:18) has been found, I.x/ is obtained by numerical second differentiation.\\nUnlike\\n\\n(cid:18) , no probability calculations are required.\\n\\nThere is a strong Bayesian current Ô¨Çowing here. A narrow peak for the\\nlog likelihood function, i.e., a large value of I.x/, also implies a narrow\\nposterior distribution for (cid:18) given x. Conditional inference, of which Fig-\\nure 4.2 is an evocative example, helps counter the central Bayesian criti-\\ncism of frequentist inference: that the frequentist properties relate to data\\nsets possibly much different than the one actually observed. The maximum\\n\\nI\\n\\n\\x0c48\\n\\nFisherian Inference and MLE\\n\\nFigure 4.2 Conditional variance of MLE for Cauchy samples of\\nsize 20, plotted versus the observed information bound 1=I.x/.\\nObserved information bounds are grouped by quantile intervals\\nfor variance calculations (in percentages): (0‚Äì5), (5‚Äì15), : : : ,\\n(85‚Äì95), (95‚Äì100). The broken red horizontal line is the\\nunconditional variance 1=n\\n\\n(cid:18) .\\n\\nI\\n\\nlikelihood algorithm can be interpreted both vertically and horizontally in\\nFigure 3.5, acting as a connection between the Bayesian and frequentist\\nworlds.\\n\\nThe equivalent of result (4.37) for multiparameter families, Section 5.3,\\n\\nO(cid:22) P(cid:24)\\n\\np\\n\\nN\\n\\n(cid:0)(cid:22); I.x/(cid:0)1(cid:1) ;\\n\\n(4.40)\\n\\nplays an important role in succeeding chapters, with (cid:0)I.x/ the p(cid:2)p matrix\\nof second derivatives\\n\\nI.x/ D (cid:0) Rlx.(cid:22)/ D (cid:0)\\n\\n(cid:20)\\n\\n@2\\n@(cid:22)i @(cid:22)j\\n\\nlog f(cid:22).x/\\n\\n(cid:21)\\n\\n:\\n\\nO(cid:22)\\n\\n(4.41)\\n\\nlllllllllll0.050.100.150.200.250.000.050.100.150.200.25Observed Information BoundMLE variance\\x0c4.4 Permutation and Randomization\\n\\n49\\n\\n4.4 Permutation and Randomization\\n\\nFisherian methodology faced criticism for its overdependence on normal\\nsampling assumptions. Consider the comparison between the 47 ALL and\\n25 AML patients in the gene 136 leukemia example of Figure 1.4. The two-\\nsample t -statistic (1.6) had value 3.01, with two-sided signiÔ¨Åcance level\\n0.0036 according to a Student-t null distribution with 70 degrees of free-\\ndom. All of this depended on the Gaussian, or normal, assumptions (2.12)‚Äì\\n(2.13).\\n\\nAs an alternative signiÔ¨Åcance-level calculation, Fisher suggested using\\npermutations of the 72 data points. The 72 values are randomly divided\\ninto disjoint sets of size 47 and 25, and the two-sample t-statistic (2.17) is\\nrecomputed. This is done some large number B times, yielding permuta-\\ntion t-values t (cid:3)\\nB . The two-sided permutation signiÔ¨Åcance level\\nfor the original value t is then the proportion of the t (cid:3)\\ni values exceeding t\\nin absolute value,\\n\\n2 ; : : : ; t (cid:3)\\n\\n1 ; t (cid:3)\\n\\n# fjt (cid:3)\\ni\\n\\nj (cid:21) jtjg =B:\\n\\n(4.42)\\n\\nFigure 4.3 10,000 permutation t (cid:3)-values for testing ALL vs AML,\\nfor gene 136 in the leukemia data of Figure 1.3. Of these, 26\\nt (cid:3)-values (red ticks) exceeded in absolute value the observed\\nt-statistic 3.01, giving permutation signiÔ¨Åcance level 0.0026.\\n\\n‚àí4‚àí20240200400600800t* valuesfrequency||||||||||||||||||||||||||‚àí3.013.01originalt‚àístatistic\\x0c50\\n\\nFisherian Inference and MLE\\n\\nFigure 4.3 shows the histogram of B D 10,000 t (cid:3)\\n\\ni values for the gene\\n136 data in Figure 1.3: 26 of these exceeded t D 3:01 in absolute value,\\nyielding signiÔ¨Åcance level 0.0026 against the null hypothesis of no ALL/AML\\ndifference, close to the normal-theory signiÔ¨Åcance level 0.0036. (We were\\na little lucky here.)\\n\\nWhy should we believe the permutation signiÔ¨Åcance level (4.42)? Fisher\\n\\nprovided two arguments.\\n(cid:15) Suppose we assume as a null hypothesis that the n D 72 observed mea-\\nsurements x are an iid sample obtained from the same distribution f(cid:22).x/,\\n\\nxi\\n\\niid(cid:24) f(cid:22).x/\\n\\nfor i D 1; 2; : : : ; n:\\n\\n(4.43)\\n\\n.(cid:18); (cid:27) 2/.)\\n\\nN\\n\\n(There is no normal assumption here, say that f(cid:22).x/ is\\n\\nLet o indicate the order statistic of x, i.e., the 72 numbers ordered\\nfrom smallest to largest, with their AML or ALL labels removed. Then it\\ncan be shown that all 72≈†=.47≈†25≈†/ ways of obtaining x by dividing o\\ninto disjoint subsets of sizes 47 and 25 are equally likely under null hy-\\npothesis (4.43). A small value of the permutation signiÔ¨Åcance level (4.42)\\nindicates that the actual division of AML/ALL measurements was not ran-\\ndom, but rather resulted from negation of the null hypothesis (4.43). This\\nmight be considered an example of Fisher‚Äôs logic of inductive inference,\\nwhere the conclusion ‚Äúshould be obvious to all.‚Äù It is certainly an exam-\\nple of conditional inference, now with conditioning used to avoid speciÔ¨Åc\\nassumptions about the sampling density f(cid:22).x/.\\n\\n(cid:15) In experimental situations, Fisher forcefully argued for randomization,\\nthat is for randomly assigning the experimental units to the possible treat-\\nment groups. Most famously, in a clinical trial comparing drug A with\\ndrug B, each patient should be randomly assigned to A or B.\\n\\nRandomization greatly strengthens the conclusions of a permutation\\ntest. In the AML/ALL gene-136 situation, where randomization wasn‚Äôt fea-\\nsible, we wind up almost certain that the AML group has systematically\\nlarger numbers, but cannot be certain that it is the different disease states\\ncausing the difference. Perhaps the AML patients are older, or heavier, or\\nhave more of some other characteristic affecting gene 136. Experimen-\\ntal randomization almost guarantees that age, weight, etc., will be well-\\nbalanced between the treatment groups. Fisher‚Äôs RCT (randomized clini-\\ncal trial) was and is the gold standard for statistical inference in medical\\ntrials.\\n\\nPermutation testing is frequentistic: a statistician following the proce-\\ndure has 5% chance of rejecting a valid null hypothesis at level 0.05, etc.\\n\\n\\x0c4.5 Notes and Details\\n\\n51\\n\\nRandomization inference is somewhat different, amounting to a kind of\\nforced frequentism, with the statistician imposing his or her preferred prob-\\nability mechanism upon the data. Permutation methods are enjoying a healthy\\ncomputer-age revival, in contexts far beyond Fisher‚Äôs original justiÔ¨Åcation\\nfor the t-test, as we will see in Chapter 15.\\n\\n4.5 Notes and Details\\n\\nOn a linear scale that puts Bayesian on the left and frequentist on the right,\\nFisherian inference winds up somewhere in the middle. Fisher rejected\\nBayesianism early on, but later criticized as ‚Äúwooden‚Äù the hard-line fre-\\nquentism of the Neyman‚ÄìWald decision-theoretic school. Efron (1998) lo-\\ncates Fisher along the Bayes‚Äìfrequentist scale for several different criteria;\\nsee in particular Figure 1 of that paper.\\n\\nBayesians, of course, believe there is only one true logic of inductive in-\\nference. Fisher disagreed. His most ambitious attempt to ‚Äúenjoy the Bayes-\\nian omelette without breaking the Bayesian eggs‚Äù5 was Ô¨Åducial inference.\\nThe simplest example concerns the normal translation model x (cid:24)\\n.(cid:18); 1/,\\nwhere (cid:18) (cid:0) x has a standard\\n.0; 1/ distribution, the Ô¨Åducial distribution of\\n(cid:18) given x then being\\n.x; 1/. Among Fisher‚Äôs many contributions, Ô¨Ådu-\\ncial inference was the only outright popular bust. Nevertheless the idea has\\npopped up again in the current literature under the name ‚ÄúconÔ¨Ådence dis-\\ntribution;‚Äù see Efron (1993) and Xie and Singh (2013). A brief discussion\\nappears in Chapter 11.\\n\\nN\\n\\nN\\n\\nN\\n\\n(cid:142)1 [p. 44] For an unbiased estimator Q(cid:18) D t.x/ (4.32), we have\\n\\nZ\\n\\nX\\n\\nt .x/ Plx.(cid:18) /f(cid:18) .x/ d x D\\n\\nZ\\n\\nX\\n\\nt.x/ Pf(cid:18) .x/ d x D @\\n@(cid:18)\\nD @\\n@(cid:18)\\n\\nZ\\n\\nX\\n\\nt.x/f(cid:18) .x/ d x\\n\\n(cid:18) D 1:\\n\\n(4.44)\\n\\nn, the sample space of x D .x1; x2; : : : ; xn/, and we are as-\\nHere X is\\nsuming the conditions necessary for differentiating under the integral sign;\\n(4.44) gives R .t .x/ (cid:0) (cid:18)/ Plx.(cid:18) /f(cid:18) .x/ d x D 1 (since Plx.(cid:18)/ has expectation\\n\\nX\\n\\n5 Attributed to the important Bayesian theorist L. J. Savage.\\n\\n\\x0c52\\n\\nFisherian Inference and MLE\\n\\n0), and then, applying the Cauchy‚ÄìSchwarz inequality,\\n\\n(cid:20)Z\\n\\nX\\n\\nor\\n\\n.t.x/ (cid:0) (cid:18)/ Plx.(cid:18)/f(cid:18) .x/ d x\\n\\n(cid:21)2\\n\\n(cid:20)Z\\n\\n(cid:20)\\n\\nX\\n\\n.t.x/ (cid:0) (cid:18)/2 f(cid:18) .x/d x\\n\\n(cid:21) (cid:20)Z\\n\\nPlx.(cid:18)/2f(cid:18) .x/ d x\\n\\n(cid:21)\\n\\n;\\n\\n(4.45)\\n\\n1 (cid:20) var(cid:18)\\n\\no\\n\\nn Q(cid:18)\\n\\nX\\n\\n(cid:18) :\\n\\nI\\n\\n(4.46)\\n\\nThis veriÔ¨Åes the Cram¬¥er‚ÄìRao lower bound (4.33): the optimal variance for\\nan unbiased estimator is one over the Fisher information.\\n\\nOptimality results are a sign of scientiÔ¨Åc maturity. Fisher information\\nand its estimation bound mark the transition of statistics from a collection\\nof ad-hoc techniques to a coherent discipline. (We have lost some ground\\nrecently, where, as discussed in Chapter 1, ad-hoc algorithmic coinages\\nhave outrun their inferential justiÔ¨Åcation.) Fisher‚Äôs information bound was\\na major mathematical innovation, closely related to and predating, Heisen-\\nberg‚Äôs uncertainty principle and Shannon‚Äôs information bound; see Dembo\\net al. (1991).\\n\\nUnbiased estimation has strong appeal in statistical applications, where\\n‚Äúbiased,‚Äù its opposite, carries a hint of self-interested data manipulation.\\nIn large-scale settings, such as the prostate study of Figure 3.4, one can,\\nhowever, strongly argue for biased estimates. We saw this for gene 610,\\nwhere the usual unbiased estimate O(cid:22)610 D 5:29 is almost certainly too\\nlarge. Biased estimation will play a major role in our subsequent chapters.\\nMaximum likelihood estimation is effectively unbiased in most situa-\\n\\ntions. Under repeated sampling, the expected mean squared error\\n\\nMSE D E\\n\\n(cid:26)(cid:16) O(cid:18) (cid:0) (cid:18)\\n\\n(cid:17)2(cid:27)\\n\\nD variance C bias2\\n\\n(4.47)\\n\\nhas order-of-magnitude variance D O.1=n/ and bias2 D O.1=n2/, the\\nlatter usually becoming negligible as sample size n increases. (Important\\nexceptions, where bias is substantial, can occur if O(cid:18) D T . O(cid:22)/ when O(cid:22) is\\nhigh-dimensional, as in the James‚ÄìStein situation of Chapter 7.) Section\\n10 of Efron (1975) provides a detailed analysis.\\n\\nSection 9.2 of Cox and Hinkley (1974) gives a careful and wide-ranging\\naccount of the MLE and Fisher information. Lehmann (1983) covers the\\nsame ground, somewhat more technically, in his Chapter 6.\\n\\n\\x0c5\\n\\nParametric Models and Exponential Families\\n\\nWe have been reviewing classic approaches to statistical inference‚Äîfre-\\nquentist, Bayesian, and Fisherian‚Äîwith an eye toward examining their\\nstrengths and limitations in modern applications. Putting philosophical dif-\\nferences aside, there is a common methodological theme in classical statis-\\ntics: a strong preference for low-dimensional parametric models; that is, for\\nmodeling data-analysis problems using parametric families of probability\\ndensities (3.1),\\n\\nD Àöf(cid:22).x/I x 2\\n\\n; (cid:22) 2 (cid:127)(cid:9) ;\\n\\nX\\n\\nF\\n\\n(5.1)\\n\\nwhere the dimension of parameter (cid:22) is small, perhaps no greater than 5\\nor 10 or 20. The inverted nomenclature ‚Äúnonparametric‚Äù suggests the pre-\\ndominance of classical parametric methods.\\n\\nTwo words explain the classic preference for parametric models: math-\\nematical tractability. In a world of sliderules and slow mechanical arith-\\nmetic, mathematical formulation, by necessity, becomes the computational\\ntool of choice. Our new computation-rich environment has unplugged the\\nmathematical bottleneck, giving us a more realistic, Ô¨Çexible, and far-reach-\\ning body of statistical techniques. But the classic parametric families still\\nplay an important role in computer-age statistics, often assembled as small\\nparts of larger methodologies (as with the generalized linear models of\\nChapter 8). This chapter1 presents a brief review of the most widely used\\nparametric models, ending with an overview of exponential families, the\\ngreat connecting thread of classical theory and a player of continuing im-\\nportance in computer-age applications.\\n\\n1 This chapter covers a large amount of technical material for use later, and may be\\n\\nreviewed lightly at Ô¨Årst reading.\\n\\n53\\n\\n\\x0c54\\n\\nParametric Models\\n\\n5.1 Univariate Families\\n\\nof observation\\nUnivariate parametric families, in which the sample space\\n1, are the building blocks of most statistical\\nx is a subset of the real line\\nanalyses. Table 5.1 names and describes the Ô¨Åve most familiar univariate\\nfamilies: normal, Poisson, binomial, gamma, and beta. (The chi-squared\\ndistribution with n degrees of freedom (cid:31)2\\nn is also included since it is dis-\\ntributed as 2 (cid:1) Gam.n=2; 1/.) The normal distribution\\n.(cid:22); (cid:27) 2/ is a shifted\\nand scaled version of the\\n\\n.0; 1/ distribution2 used in (3.27),\\n\\nR\\n\\nN\\n\\nX\\n\\nN\\n.(cid:22); (cid:27) 2/ (cid:24) (cid:22) C (cid:27)\\n\\nN\\n\\n.0; 1/:\\n\\nN\\n\\n(5.2)\\n\\nTable 5.1 Five familiar univariate densities, and their sample spaces\\nparameter spaces (cid:127), and expectations and variances; chi-squared\\ndistribution with n degrees of freedom is 2 Gam.n=2; 1/.\\n\\n,\\n\\nX\\n\\nName,\\nNotation\\n\\nNormal\\nN .(cid:22); (cid:27) 2/\\n\\nPoisson\\nPoi.(cid:22)/\\n\\nBinomial\\nBi.n; (cid:25)/\\n\\nGamma\\nGam.(cid:23); (cid:27) /\\n\\nBeta\\nBe.(cid:23)1; (cid:23)2/\\n\\nDensity\\n\\n1\\np\\n\\n(cid:27)\\n\\n2(cid:25)\\n\\ne(cid:0) 1\\n\\n2 . x(cid:0)(cid:22)\\n\\n(cid:27) /2\\n\\nX\\n\\nR1\\n\\n(cid:127)\\n\\n(cid:22) 2 R1\\n(cid:27) 2 > 0\\n\\ne(cid:0)(cid:22)(cid:22)x\\nx≈†\\n\\nf0; 1; : : : g\\n\\n(cid:22) > 0\\n\\nn≈†\\n\\nx≈†.n(cid:0)x/≈† (cid:25) x.1 (cid:0) (cid:25)/n(cid:0)x\\n\\nf0; 1; : : : ; ng 0 < (cid:25) < 1\\n\\nx(cid:23)(cid:0)1e(cid:0)x=(cid:27)\\n(cid:27) (cid:23) (cid:128).(cid:23)/\\n\\nx (cid:21) 0\\n\\n(cid:128).(cid:23)1C(cid:23)2/\\n(cid:128).(cid:23)1/(cid:128).(cid:23)2/ x(cid:23)1(cid:0)1.1 (cid:0) x/(cid:23)2(cid:0)1 0 (cid:20) x (cid:20) 1\\n\\n(cid:23) > 0\\n(cid:27) > 0\\n\\n(cid:23)1 > 0\\n(cid:23)2 > 0\\n\\nExpectation,\\nVariance\\n\\n(cid:22)\\n(cid:27) 2\\n\\n(cid:22)\\n(cid:22)\\n\\nn(cid:25)\\nn(cid:25).1 (cid:0) (cid:25)/\\n\\n(cid:27)(cid:23)\\n(cid:27) 2(cid:23)\\n\\n(cid:23)1=.(cid:23)1 C (cid:23)2/\\n(cid:23)1(cid:23)2\\n.(cid:23)1C(cid:23)2/2.(cid:23)1C(cid:23)2C1/\\n\\nRelationships abound among the table‚Äôs families. For instance, indepen-\\ndent gamma variables Gam.(cid:23)1; (cid:27)/ and Gam.(cid:23)2; (cid:27)/ yield a beta variate ac-\\ncording to\\n\\nBe.(cid:23)1; (cid:23)2/ (cid:24)\\n\\nGam.(cid:23)1; (cid:27)/\\nGam.(cid:23)1; (cid:27)/ C Gam.(cid:23)2; (cid:27)/\\n\\n:\\n\\n(5.3)\\n\\nThe binomial and Poisson are particularly close cousins. A Bi.n; (cid:25)/ distri-\\nbution (the number of heads in n independent Ô¨Çips of a coin with probabil-\\n\\n2 The notation in (5.2) indicates that if X (cid:24) N .(cid:22); (cid:27) 2/ and Y (cid:24) N .0; 1/ then X and\\n\\n(cid:22) C (cid:27)Y have the same distribution.\\n\\n\\x0c5.2 The Multivariate Normal Distribution\\n\\n55\\n\\nFigure 5.1 Comparison of the binomial distribution Bi.30; 0:2/\\n(black lines) with the Poisson Poi.6/ (red dots). In the legend we\\nshow the mean and standard deviation for each distribution.\\n\\nity of heads (cid:25)) approaches a Poi.n(cid:25)/ distribution,\\n\\nBi.n; (cid:25)/ P(cid:24) Poi.n(cid:25)/\\n\\n(5.4)\\n\\nas n grows large and (cid:25) small, the notation P(cid:24) indicating approximate equal-\\nity of the two distributions. Figure 5.1 shows the approximation already\\nworking quite effectively for n D 30 and (cid:25) D 0:2.\\n\\nThe Ô¨Åve families in Table 5.1 have Ô¨Åve different sample spaces, making\\nthem appropriate in different situations. Beta distributions, for example,\\nare natural candidates for modeling continuous data on the unit interval\\n≈í0; 1(cid:141). Choices of the two parameters .(cid:23)1; (cid:23)2/ provide a variety of possible\\nshapes, as illustrated in Figure 5.2. Later we will discuss general exponen-\\ntial families, unavailable in classical theory, that greatly expand the catalog\\nof possible shapes.\\n\\n5.2 The Multivariate Normal Distribution\\n\\nClassical statistics produced a less rich catalog of multivariate distribu-\\np, p-dimensional Eu-\\ntions, ones where the sample space\\n\\nexists in\\n\\nX\\n\\nR\\n\\n0.000.050.100.150.20xf(x)llllllllllllllllll01234567891011121314151617lBinomial: (6, 2.19)Poisson:  (6, 2.45)\\x0c56\\n\\nParametric Models\\n\\nFigure 5.2 Three beta densities, with .(cid:23)1; (cid:23)2/ indicated.\\n\\nclidean space, p > 1. By far the greatest amount of attention focused on\\nthe multivariate normal distribution.\\n\\nA random vector x D .x1; x2; : : : ; xp/0; normally distributed or not, has\\n\\nmean vector\\n\\n(cid:22) D Efxg D (cid:0)Efx1g; Efx2g; : : : ; Efxpg(cid:1)0\\n\\n(5.5)\\n\\nand p (cid:2) p covariance matrix3\\n\\n‚Ä† D E Àö.x (cid:0) (cid:22)/.x (cid:0) (cid:22)/0(cid:9) D (cid:0)E Àö.xi (cid:0) (cid:22)i /.xj (cid:0) (cid:22)j /(cid:9)(cid:1) :\\n\\n(5.6)\\n\\n(The outer product uv0 of vectors u and v is the matrix having elements\\nui vj .) We will use the convenient notation\\n\\nx (cid:24) .(cid:22); ‚Ä†/\\n\\n(5.7)\\n\\nfor (5.5) and (5.6), reducing to the familiar form x (cid:24) .(cid:22); (cid:27) 2/ in the uni-\\nvariate case.\\n\\nDenoting the entries of ‚Ä† by (cid:27)ij , for i and j equaling 1; 2; : : : ; p, the\\n\\ndiagonal elements are variances,\\n\\n(cid:27)i i D var.xi /:\\n\\n(5.8)\\n\\n3 The notation ‚Ä† D .(cid:27)ij / deÔ¨Ånes the ij th element of a matrix.\\n\\n0.00.20.40.60.81.00.00.51.01.52.02.53.0xf(x)( n1,n2 )( 8, 4)( 2, 4)(.5,.5)\\x0c5.2 The Multivariate Normal Distribution\\n\\n57\\n\\nThe off-diagonal elements relate to the correlations between the coordi-\\nnates of x,\\n\\ncor.xi ; xj / D (cid:27)ijp\\n\\n(cid:27)i i (cid:27)jj\\n\\n:\\n\\n(5.9)\\n\\nThe multivariate normal distribution extends the univariate deÔ¨Ånition\\n.(cid:22); (cid:27) 2/ in Table 5.1. To begin with, let z D .z1; z2; : : : ; zp/0 be a vector\\n\\n.0; 1/ variates, with probability density function\\n\\nN\\nof p independent\\n\\nN\\n\\nf .z/ D .2(cid:25)/(cid:0) p\\n\\n2 e(cid:0) 1\\n\\n2\\n\\nPp\\n\\n1 z2\\n\\ni D .2(cid:25)/(cid:0) p\\n\\n2 e(cid:0) 1\\n\\n2 z0z\\n\\n(5.10)\\n\\naccording to line 1 of Table 5.1.\\n\\nThe multivariate normal family is obtained by linear transformations of\\nz: let (cid:22) be a p-dimensional vector and T a p (cid:2) p nonsingular matrix, and\\ndeÔ¨Åne the random vector\\n\\nx D (cid:22) C T z:\\n\\n(5.11)\\n\\nFollowing the usual rules of probability transformations yields the density\\nof x,\\n\\nf(cid:22);‚Ä†.x/ D .2(cid:25)/(cid:0)p=2\\nj‚Ä†j1=2\\n\\ne(cid:0) 1\\n\\n2 .x(cid:0)(cid:22)/0‚Ä†(cid:0)1.x(cid:0)(cid:22)/;\\n\\n(5.12)\\n\\nwhere ‚Ä† is the p (cid:2) p symmetric positive deÔ¨Ånite matrix\\n\\n‚Ä† D T T 0\\n\\n(5.13)\\n\\nand j‚Ä†j its determinant; (cid:142) f(cid:22);‚Ä†.x/, the p-dimensional multivariate normal (cid:142)1\\ndistribution with mean (cid:22) and covariance ‚Ä†, is denoted\\n\\nx (cid:24)\\n\\nN\\n\\np.(cid:22); ‚Ä†/:\\n\\n(5.14)\\n\\nFigure 5.3 illustrates the bivariate normal distribution with (cid:22) D .0; 0/0\\nand ‚Ä† having (cid:27)11 D (cid:27)22 D 1 and (cid:27)12 D 0:5 (so cor.x1; x2/ D 0:5). The\\nbell-shaped mountain on the left is a plot of density (5.12). The right panel\\nshows a scatterplot of 2000 points drawn from this distribution. Concentric\\nellipses illustrate curves of constant density,\\n\\n.x (cid:0) (cid:22)/0‚Ä†(cid:0)1.x (cid:0) (cid:22)/ D constant:\\n\\n(5.15)\\n\\nClassical multivariate analysis was the study of the multivariate normal\\ndistribution, both of its probabilistic and statistical properties. The notes\\nreference some important (and lengthy) multivariate texts. Here we will\\njust recall a couple of results useful in the chapters to follow.\\n\\n\\x0c58\\n\\nParametric Models\\n\\nFigure 5.3 Left: bivariate normal density, with var.x1/ D\\nvar.x2/ D 1 and cor.x1; x2/ D 0:5. Right: sample of 2000\\n.x1; x2/ pairs from this bivariate normal density.\\n\\nSuppose that x D .x1; x2; : : : ; xp/0 is partitioned into\\nx.1/ D .x1; x2; : : : ; xp1/0\\n\\nand x.2/ D .xp1C1; xp1C2; : : : ; xp1Cp2/0;\\n\\np1 C p2 D p, with (cid:22) and ‚Ä† similarly partitioned,\\n\\n!\\n\\nx.1/\\nx.2/\\n\\n(cid:24)\\n\\np\\n\\nN\\n\\n!\\n;\\n\\n(cid:22).1/\\n(cid:22).2/\\n\\n(cid:18)‚Ä†11 ‚Ä†12\\n‚Ä†21 ‚Ä†22\\n\\n(cid:19)!\\n\\n(5.16)\\n\\n(5.17)\\n\\n(cid:142)2\\n\\n(so ‚Ä†11 is p1 (cid:2) p1, ‚Ä†12 is p1 (cid:2) p2, etc.). Then the conditional distribution\\nof x.2/ given x.1/ is itself normal,(cid:142)\\n(cid:0)(cid:22).2/ C ‚Ä†21‚Ä†(cid:0)1\\n\\n11 .x.1/ (cid:0) (cid:22).1//; ‚Ä†22 (cid:0) ‚Ä†21‚Ä†(cid:0)1\\n\\nx.2/jx.1/ (cid:24)\\n\\n11 ‚Ä†12\\n\\np2\\n\\nN\\n\\n(cid:1) :\\n(5.18)\\n\\nIf p1 D p2 D 1, then (5.18) reduces to\\n\\nx2jx1 (cid:24)\\n\\n(cid:18)\\n(cid:22)2 C (cid:27)12\\n(cid:27)11\\n\\n.x1 (cid:0) (cid:22)1/; (cid:27)22 (cid:0) (cid:27) 2\\n12\\n(cid:27)11\\n\\n(cid:19)\\n\\nI\\n\\nN\\n\\n(5.19)\\n\\nhere (cid:27)12=(cid:27)11 is familiar as the linear regression coefÔ¨Åcient of x2 as a func-\\ntion of x1, while (cid:27) 2\\n12=(cid:27)11(cid:27)22 equals cor.x1; x2/2, the squared proportion\\nR2 of the variance of x2 explained by x1. Hence we can write the (unex-\\nplained) variance term in (5.19) as (cid:27)22.1 (cid:0) R2/.\\n\\nBayesian statistics also makes good use of the normal family. It helps to\\n.(cid:22); (cid:27) 2/, where now we assume that\\n\\nbegin with the univariate case x (cid:24)\\n\\nN\\n\\n****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************‚àí2‚àí1012‚àí2‚àí1012x1x1x2x2 \\n  \\n\\x0c5.3 Fisher‚Äôs Information Bound\\n\\n59\\n\\nthe expectation vector itself has a normal prior distribution\\n\\n(cid:22) (cid:24)\\n\\nN\\n\\n.M; A/\\n\\nand xj(cid:22) (cid:24)\\n\\n.(cid:22); (cid:27) 2/:\\n\\nN\\n\\n.M; A/:\\n\\nN\\n\\n(5.20)\\n\\nBayes‚Äô theorem and some algebra show that the posterior distribution of (cid:22)\\nhaving observed x is normal,(cid:142)\\n\\n(cid:142)3\\n\\n(cid:22)jx (cid:24)\\n\\nN\\n\\n(cid:18)\\nM C A\\n\\nA C (cid:27) 2 .x (cid:0) M /;\\n\\nA(cid:27) 2\\nA C (cid:27) 2\\n\\n(cid:19)\\n\\n:\\n\\n(5.21)\\n\\nThe posterior expectation O(cid:22)Bayes D M C.A=.AC(cid:27) 2//.x (cid:0)M / is a shrink-\\nage estimator of (cid:22): if, say, A equals (cid:27) 2, then O(cid:22)Bayes D M C .x (cid:0) M /=2\\nis shrunk half the way back from the unbiased estimate O(cid:22) D x toward the\\nprior mean M , while the posterior variance (cid:27) 2=2 of O(cid:22)Bayes is only one-half\\nthat of O(cid:22).\\n\\nThe multivariate version of the Bayesian setup (5.20) is\\n\\n(cid:22) (cid:24)\\n\\np.M; A/\\n\\nand xj(cid:22) (cid:24)\\n\\np.(cid:22); ‚Ä†/;\\n\\n(5.22)\\n\\nN\\n\\nN\\n\\nnow with M and (cid:22) p-vectors, and A and ‚Ä† positive deÔ¨Ånite p(cid:2)p matrices.\\nAs indicated in the notes, the posterior distribution of (cid:22) given x is then\\n\\n(cid:22)jx (cid:24)\\n\\n(cid:0)M C A.A C ‚Ä†/(cid:0)1.x (cid:0) M /; A.A C ‚Ä†/(cid:0)1‚Ä†(cid:1) ;\\n\\np\\n\\n(5.23)\\n\\nN\\n\\nwhich reduces to (5.21) when p D 1.\\n\\n5.3 Fisher‚Äôs Information Bound for Multiparameter Families\\n\\nThe multivariate normal distribution plays its biggest role in applications\\nas a large-sample approximation for maximum likelihood estimates. We\\nsuppose that the parametric family of densities ff(cid:22).x/g, normal or not, is\\nsmoothly deÔ¨Åned in terms of its p-dimensional parameter vector (cid:22). (In\\nterms of (5.1), (cid:127) is a subset of\\n\\np.)\\n\\nThe MLE deÔ¨Ånitions and results are direct analogues of the single-param-\\neter calculations beginning at (4.14) in Chapter 4. The score function Plx.(cid:22)/\\nis now deÔ¨Åned as the gradient of logff(cid:22).x/g,\\n\\nR\\n\\nPlx.(cid:22)/ D r(cid:22)\\n\\nÀölog f(cid:22).x/(cid:9) D\\n\\n(cid:18)\\n\\n: : : ;\\n\\n@ log f(cid:22).x/\\n@(cid:22)i\\n\\n(cid:19)0\\n\\n; : : :\\n\\n;\\n\\n(5.24)\\n\\nthe p-vector of partial derivatives of log f(cid:22).x/ with respect to the coordi-\\nnates of (cid:22). It has mean zero,\\no\\nn Plx.(cid:22)/\\n\\nD 0 D .0; 0; 0; : : : ; 0/0:\\n\\n(5.25)\\n\\nE(cid:22)\\n\\n\\x0c60\\n\\nParametric Models\\n\\nBy deÔ¨Ånition, the Fisher information matrix I (cid:22) for (cid:22) is the p (cid:2) p covari-\\nance matrix of Plx.(cid:22)/; using outer product notation,\\n(cid:26) @ log f(cid:22).x/\\n@(cid:22)i\\n\\n@ log f(cid:22).x/\\n@(cid:22)j\\n\\nn Plx.(cid:22)/ Plx.(cid:22)/0o\\n\\nI (cid:22) D E(cid:22)\\n\\n(5.26)\\n\\nE(cid:22)\\n\\n(cid:27)(cid:19)\\n\\nD\\n\\n(cid:18)\\n\\n:\\n\\nThe key result is that the MLE O(cid:22) D arg max(cid:22)ff(cid:22).x/g has an approxi-\\n\\nmately normal distribution with covariance matrix I (cid:0)1\\n(cid:22) ,\\n\\nO(cid:22) P(cid:24)\\n\\nN\\n\\np.(cid:22); I (cid:0)1\\n(cid:22) /:\\n\\n(5.27)\\n\\nR\\n\\nApproximation (5.27) is justiÔ¨Åed by large-sample arguments, say with x\\np, .x1; x2; : : : ; xn/, n going to inÔ¨Ånity.\\nan iid sample in\\nSuppose the statistician is particularly interested in (cid:22)1, the Ô¨Årst coordi-\\nnate of (cid:22). Let (cid:22).2/ D .(cid:22)2; (cid:22)3; : : : ; (cid:22)p/ denote the other p (cid:0) 1 coordinates\\nof (cid:22), which are now ‚Äúnuisance parameters‚Äù as far as the estimation of (cid:22)1\\ngoes. According to (5.27), the MLE O(cid:22)1, which is the Ô¨Årst coordinate of O(cid:22),\\nhas\\n\\nO(cid:22)1 P(cid:24)\\n\\nN\\n\\n(cid:0)(cid:22)1; .I (cid:0)1\\n\\n(cid:22) /11\\n\\n(cid:1) ;\\n\\n(5.28)\\n\\nwhere the notation indicates the upper leftmost entry of I (cid:0)1\\n(cid:22) .\\n\\nWe can partition the information matrix I (cid:22) into the two parts corre-\\n\\nsponding to (cid:22)1 and (cid:22).2/,\\n\\nI (cid:22) D\\n\\n(cid:19)\\n\\n(cid:18)\\n\\n(cid:22)11\\n(cid:22)1.2/\\nI\\nI\\n(cid:22).2/1 I (cid:22).22/\\nI\\n\\n(5.29)\\n\\n(cid:22)1.2/ D\\n(with\\nI\\nThe endnotes show that(cid:142)\\n\\n0\\n\\n(cid:22).2/1 of dimension 1(cid:2).p(cid:0)1/ and I (cid:22).22/ .p(cid:0)1/(cid:2).p(cid:0)1/).\\nI\\n\\n.I (cid:0)1\\n\\n(cid:22) /11 D (cid:0)\\n\\n(cid:22)1.2/I (cid:0)1\\nI\\nThe subtracted term on the right side of (5.30) is nonnegative, implying\\nthat\\n\\n(cid:22)11 (cid:0)\\nI\\n\\n(cid:22).22/I\\n\\n(5.30)\\n\\n(cid:22).2/1\\n\\n:\\n\\n(cid:1)(cid:0)1\\n\\n.I (cid:0)1\\n\\n(cid:22) /11 (cid:21)\\n\\n(cid:0)1\\n(cid:22)11:\\nI\\n\\n(5.31)\\n\\nIf (cid:22).2/ were known to the statistician, rather than requiring estimation,\\nthen f(cid:22)1(cid:22).2/.x/ would be a one-parameter family, with Fisher information\\n\\n(cid:22)11 for estimating (cid:22)1, giving\\n\\nI\\n\\nO(cid:22)1 P(cid:24)\\n\\n.(cid:22)1;\\n\\n(cid:0)1\\n(cid:22)11/:\\n\\nI\\n\\nN\\n\\n(5.32)\\n\\n(cid:142)4\\n\\n\\x0c5.4 The Multinomial Distribution\\n\\n61\\n\\nComparing (5.28) with (5.32), (5.31) shows that the variance of the MLE\\nO(cid:22)1 must always increase4 in the presence of nuisance parameters.(cid:142)\\n\\n(cid:142)5\\n\\nMaximum likelihood, and in fact any form of unbiased or nearly unbi-\\nased estimation, pays a nuisance tax for the presence of ‚Äúother‚Äù parameters.\\nModern applications often involve thousands of others; think of regression\\nÔ¨Åts with too many predictors. In some circumstances, biased estimation\\nmethods can reverse the situation, using the others to actually improve esti-\\nmation of a target parameter; see Chapter 6 on empirical Bayes techniques,\\nand Chapter 16 on `1 regularized regression models.\\n\\n5.4 The Multinomial Distribution\\n\\nSecond in the small catalog of well-known classic multivariate distribu-\\ntions is the multinomial. The multinomial applies to situations in which\\nthe observations take on only a Ô¨Ånite number of discrete values, say L of\\nthem. The 2(cid:2)2 ulcer surgery of Table 4.1 is repeated in Table 5.2, now with\\nthe cells labeled 1; 2; 3; and 4. Here there are L D 4 possible outcomes\\nfor each patient: (new, success), (new, failure), (old, success),\\n(old, failure).\\n\\nTable 5.2 The ulcer study of Table 4.1, now with the cells numbered 1\\nthrough 4 as shown.\\n\\nsuccess\\n\\nfailure\\n\\nnew\\n\\nold\\n\\n1\\n\\n3\\n\\n9\\n\\n7\\n\\n2\\n\\n4\\n\\n12\\n\\n17\\n\\nA number n of cases has been observed, n D 45 in Table 5.2. Let x D\\n\\n.x1; x2; : : : ; xL/ be the vector of counts for the L possible outcomes,\\n\\nxl D #fcases having outcome lg;\\n(5.33)\\nx D .9; 12; 7; 17/0 for the ulcer data. It is convenient to code the outcomes\\nin terms of the coordinate vectors el of length L,\\n\\nel D .0; 0; : : : ; 0; 1; 0; : : : ; 0/0;\\n\\n(5.34)\\n\\nwith a 1 in the lth place.\\n\\n4 Unless I(cid:22)1.2/ is a vector of zeros, a condition that amounts to approximate\\n\\nindependence of O(cid:22)1 and O(cid:22).2/.\\n\\n\\x0c62\\n\\nParametric Models\\n\\nFigure 5.4 The simplex\\nangle to the coordinate axes in\\n\\nS\\n\\n3 is an equilateral triangle set at an\\n\\n3.\\n\\nR\\n\\nThe multinomial probability model assumes that the n cases are inde-\\npendent of each other, with each case having probability (cid:25)l for outcome\\nel ,\\n\\nLet\\n\\n(cid:25)l D Prfel g;\\n\\nl D 1; 2; : : : ; L:\\n\\n(cid:25) D .(cid:25)1; (cid:25)2; : : : ; (cid:25)L/0\\n\\n(5.35)\\n\\n(5.36)\\n\\nindicate the vector of probabilities. The count vector x then follows the\\nmultinomial distribution,\\n\\ndenoted\\n\\nf(cid:25).x/ D\\n\\nn≈†\\nx1≈†x2≈† : : : xL≈†\\n\\nL\\nY\\n\\nlD1\\n\\n(cid:25) xl\\nl\\n\\n;\\n\\nx (cid:24) MultL.n; (cid:25)/\\n\\n(5.37)\\n\\n(5.38)\\n\\n(for n observations, L outcomes, probability vector (cid:25)).\\n\\nThe parameter space (cid:127) for (cid:25) is the simplex\\n\\n(\\n(cid:25) W (cid:25)l (cid:21) 0 and\\n\\nL D\\nS\\n\\nL\\nX\\n\\nlD1\\n\\nL,\\n\\nS\\n\\n)\\n(cid:25)l D 1\\n\\n:\\n\\n(5.39)\\n\\n3, an equilateral triangle sitting at an angle to the coordi-\\nFigure 5.4 shows\\nnate axes e1; e2; and e3. The midpoint of the triangle (cid:25) D .1=3; 1=3; 1=3/\\n\\nS\\n\\n\\x0c5.4 The Multinomial Distribution\\n\\n63\\n\\ncorresponds to a multinomial distribution putting equal probability on the\\nthree possible outcomes.\\n\\nFigure 5.5 Sample space\\nindicate .x1; x2; x3/.\\n\\nX\\n\\nfor x (cid:24) Mult3.4; (cid:25)/; numbers\\n\\nS\\n\\nX\\n\\nThe sample space\\n\\nfor x is the subset of n\\nL (the set of nonnegative\\nvectors summing to n) having integer components. Figure 5.5 illustrates\\nthe case n D 4 and L D 3, now with the triangle of Figure 5.4 multiplied\\nby 4 and set Ô¨Çat on the page. The point 121 indicates x D .1; 2; 1/, with\\nprobability 12 (cid:1) (cid:25)1(cid:25) 2\\n\\n2 (cid:25)3 according to (5.37), etc.\\n\\nIn the dichotomous case, L D 2, the multinomial distribution reduces\\nto the binomial, with .(cid:25)1; (cid:25)2/ equaling .(cid:25); 1 (cid:0) (cid:25)/ in line 3 of Table 5.1,\\nand .x1; x2/ equaling .x; n (cid:0) x/. The mean vector and covariance matrix\\nof MultL.n; (cid:25)/, for any value of L, are(cid:142)\\n\\nx (cid:24) (cid:0)n(cid:25); n (cid:2)diag.(cid:25)/ (cid:0) (cid:25)(cid:25) 0(cid:3)(cid:1)\\n(diag.(cid:25)/ is the diagonal matrix with diagonal elements (cid:25)l ), so var.xl / D\\nn(cid:25)l .1 (cid:0) (cid:25)l / and covariance .xl ; xj / D (cid:0)n(cid:25)l (cid:25)j ; (5.40) generalizes the\\nbinomial mean and variance .n(cid:25); n(cid:25).1 (cid:0) (cid:25)//.\\n\\n(5.40)\\n\\nThere is a useful relationship between the multinomial distribution and\\nthe Poisson. Suppose S1; S2; : : : ; SL are independent Poissons having pos-\\nsibly different parameters,\\n\\nSl\\n\\nind(cid:24) Poi.(cid:22)l /;\\n\\nl D 1; 2; : : : ; L;\\n\\n(5.41)\\n\\nor, more concisely,\\n\\nS (cid:24) Poi.(cid:22)/\\n(5.42)\\nwith S D .S1; S2; : : : ; SL/0 and (cid:22) D .(cid:22)1; (cid:22)2; : : : ; (cid:22)L/0, the independence\\n\\n(cid:142)6\\n\\n103\\t\\n \\xa0202\\t\\n \\xa0301\\t\\n \\xa0004\\t\\n \\xa0400\\t\\n \\xa0211\\t\\n \\xa0121\\t\\n \\xa0112\\t\\n \\xa0310\\t\\n \\xa0220\\t\\n \\xa0130\\t\\n \\xa0040\\t\\n \\xa0031\\t\\n \\xa0022\\t\\n \\xa0013\\t\\n \\xa0\\x0c64\\n\\nParametric Models\\n\\nbeing assumed in notation (5.42). Then the conditional distribution of S\\ngiven the sum SC D P Sl is multinomial,(cid:142)\\n\\n(cid:142)7\\n\\nS jSC (cid:24) MultL.SC; (cid:22)=(cid:22)C/;\\n\\n(5.43)\\n\\n(cid:22)C D P (cid:22)l .\\n\\nGoing in the other direction, suppose N (cid:24) Poi.n/. Then the uncondi-\\n\\ntional or marginal distribution of MultL.N; (cid:25)/ is Poisson,\\n\\nMultL.N; (cid:25)/ (cid:24) Poi.n(cid:25)/\\n\\n(5.44)\\nCalculations involving x (cid:24) MultL.n; (cid:25)/ are sometimes complicated by\\nthe multinomial‚Äôs correlations. The approximation x P(cid:24) Poi.n(cid:25)/ removes\\nthe correlations and is usually quite accurate if n is large.\\n\\nif N (cid:24) Poi.n/:\\n\\nThere is one more important thing to say about the multinomial family: it\\ncontains all distributions on a sample space\\ncomposed of L discrete cat-\\negories. In this sense it is a model for nonparametric inference on\\n. The\\nnonparametric bootstrap calculations of Chapter 10 use the multinomial in\\nthis way. Nonparametrics, and the multinomial, have played a larger role\\nin the modern environment of large, difÔ¨Åcult to model, data sets.\\n\\nX\\n\\nX\\n\\n5.5 Exponential Families\\n\\nClassic parametric families dominated statistical theory and practice for a\\ncentury and more, with an enormous catalog of their individual properties‚Äî\\nmeans, variances, tail areas, etc.‚Äîbeing compiled. A surprise, though a\\nslowly emerging one beginning in the 1930s, was that all of them were\\nexamples of a powerful general construction: exponential families. What\\nfollows here is a brief introduction to the basic theory, with further devel-\\nopment to come in subsequent chapters.\\n\\nTo begin with, consider the Poisson family, line 2 of Table 5.1. The ratio\\n\\nof Poisson densities at two parameter values (cid:22) and (cid:22)0 is\\n\\nf(cid:22).x/\\nf(cid:22)0.x/\\n\\nD e(cid:0).(cid:22)(cid:0)(cid:22)0/\\n\\n(cid:19)x\\n\\n;\\n\\n(cid:18) (cid:22)\\n(cid:22)0\\n\\nwhich can be re-expressed as\\n\\nf(cid:22).x/ D eÀõx(cid:0) .Àõ/f(cid:22)0.x/;\\n\\nwhere we have deÔ¨Åned\\n\\n(5.45)\\n\\n(5.46)\\n\\nÀõ D logf(cid:22)=(cid:22)0g\\n\\nand  .Àõ/ D (cid:22)0.eÀõ (cid:0) 1/:\\n\\n(5.47)\\n\\nLooking at (5.46), we can describe the Poisson family in three steps.\\n\\n\\x0c5.5 Exponential Families\\n\\n65\\n\\n1 Start with any one Poisson distribution f(cid:22)0.x/.\\n2 For any value of (cid:22) > 0 let Àõ D logf(cid:22)=(cid:22)0g and calculate\\n\\nQf(cid:22).x/ D eÀõxf(cid:22)0.x/\\n\\nfor x D 0; 1; 2; : : : :\\n\\n(5.48)\\n\\n3 Finally, divide Qf(cid:22).x/ by exp. .Àõ// to get the Poisson density f(cid:22).x/.\\nIn other words, we ‚Äútilt‚Äù f(cid:22)0.x/ with the exponential factor eÀõx to get\\nQf(cid:22).x/, and then renormalize Qf(cid:22).x/ to sum to 1. Notice that (5.46) gives\\nexp.(cid:0) .Àõ// as the renormalizing constant since\\n\\ne .Àõ/ D\\n\\n1\\nX\\n\\n0\\n\\neÀõxf(cid:22)0.x/:\\n\\n(5.49)\\n\\nFigure 5.6 Poisson densities for (cid:22) D 3; 6; 9; 12; 15; 18; heavy\\ngreen curve with dots for (cid:22) D 12.\\n\\nFigure 5.6 graphs the Poisson density f(cid:22).x/ for (cid:22) D 3; 6; 9; 12; 15; 18.\\nEach Poisson density is a renormalized exponential tilt of any other Poisson\\ndensity. So for instance f6.x/ is obtained from f12.x/ via the tilt eÀõx with\\nÀõ D logf6=12g D (cid:0)0:693.5\\n\\n5 Alternate expressions for f(cid:22).x/ as an exponential family are available, for example\\n\\nexp.Àõx (cid:0)  .Àõ//f0.x/, where Àõ D log (cid:22),  .Àõ/ D exp.Àõ/, and f0.x/ D 1=x≈†. (It\\nisn‚Äôt necessary for f0.x/ to be a member of the family.)\\n\\n0510152025300.000.050.100.150.20xf(x)lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\\x0c(cid:142)8\\n\\n66\\n\\nParametric Models\\n\\nThe Poisson is a one-parameter exponential family, in that Àõ and x in\\nexpression (5.46) are one-dimensional. A p-parameter exponential family\\nhas the form\\n\\nfÀõ.x/ D eÀõ0y(cid:0) .Àõ/f0.x/\\n\\nfor Àõ 2 A;\\n\\n(5.50)\\n\\np. Here Àõ is the\\nwhere Àõ and y are p-vectors and A is contained in\\n‚Äúcanonical‚Äù or ‚Äúnatural‚Äù parameter vector and y D t.x/ is the ‚ÄúsufÔ¨Åcient\\nstatistic‚Äù vector. The normalizing function  .Àõ/, which makes fÀõ.x/ inte-\\ngrate (or sum) to one, satisÔ¨Åes\\n\\nR\\n\\nZ\\n\\ne .Àõ/ D\\n\\neÀõ0yf0.x/ dx;\\n\\n(5.51)\\n\\nX\\n\\nand it can be shown that the parameter space A for which the integral is\\nÔ¨Ånite is a convex set (cid:142) in\\np. As an example, the gamma family on line 4\\nof Table 5.1 is a two-parameter exponential family, with Àõ and y D t.x/\\ngiven by\\n\\nR\\n\\n.Àõ1; Àõ2/ D\\n\\n(cid:18)\\n(cid:0) 1\\n(cid:27)\\n\\n(cid:19)\\n\\n; (cid:23)\\n\\n;\\n\\n.y1; y2/ D .x; log x/ ;\\n\\n(5.52)\\n\\nand\\n\\n .Àõ/ D (cid:23) log (cid:27) C log (cid:128).(cid:23)/\\n\\nD (cid:0)Àõ2 logf(cid:0)Àõ1g C log f(cid:128).Àõ2/g :\\n\\n(5.53)\\n\\nThe parameter space A is fÀõ1 < 0 and Àõ2 > 0g.\\n\\nWhy are we interested in exponential tilting rather than some other trans-\\nformational form? The answer has to do with repeated sampling. Suppose\\nx D .x1; x2; : : : ; xn/ is an iid sample from a p-parameter exponential\\nfamily (5.50). Then, letting yi D t.xi / denote the sufÔ¨Åcient vector corre-\\nsponding to xi ,\\n\\nfÀõ.x/ D\\n\\nn\\nY\\n\\niD1\\n\\neÀõ0yi (cid:0) .Àõ/f0.xi /\\n\\nD en.Àõ0 Ny(cid:0) .Àõ//f0.x/;\\n\\n(5.54)\\n\\nwhere Ny D Pn\\n1 yi =n. This is still a p-parameter exponential family, now\\nwith natural parameter nÀõ, sufÔ¨Åcient statistic Ny, and normalizer n .Àõ/.\\nNo matter how large n may be, the statistician can still compress all the\\ninferential information into a p-dimensional statistic Ny. Only exponential\\nfamilies enjoy this property.\\n\\nEven though they were discovered and developed in quite different con-\\ntexts, and at quite different times, all of the distributions discussed in this\\n\\n\\x0c5.5 Exponential Families\\n\\n67\\n\\nchapter exist in exponential families. This isn‚Äôt quite the coincidence it\\nseems. Mathematical tractability was the prized property of classic para-\\nmetric distributions, and tractability was greatly facilitated by exponential\\nstructure, even if that structure went unrecognized.\\n\\nIn one-parameter exponential families, the normalizer  .Àõ/ is also known\\nas the cumulant generating function. Derivatives of  .Àõ/ yield the cumu-\\nlants of y,6 the Ô¨Årst two giving the mean and variance(cid:142)\\n\\n(cid:142)9\\n\\nP .Àõ/ D EÀõfyg\\n\\nand\\n\\nR .Àõ/ D varÀõfyg:\\n\\n(5.55)\\n\\nSimilarly, in p-parametric families\\n\\nP .Àõ/ D .: : : @ =@Àõj : : : /0 D EÀõfyg\\n\\nand\\n\\nR .Àõ/ D\\n\\n(cid:19)\\n\\n(cid:18) @2 .Àõ/\\n@Àõj @Àõk\\n\\nD covÀõfyg:\\n\\nThe p-dimensional expectation parameter, denoted\\n\\nÀá D EÀõfyg;\\n\\n(5.56)\\n\\n(5.57)\\n\\n(5.58)\\n\\nis a one-to-one function of the natural parameter Àõ. Let VÀõ indicate the\\np (cid:2) p covariance matrix,\\n\\nVÀõ D covÀõfyg:\\n\\nThen the p (cid:2) p derivative matrix of Àá with respect to Àõ is\\n\\ndÀá\\ndÀõ\\n\\nD .@Àáj =@Àõk/ D VÀõ;\\n\\n(5.59)\\n\\n(5.60)\\n\\nthis following from (5.56)‚Äì(5.57), the inverse mapping being dÀõ=dÀá D\\nV (cid:0)1\\nÀõ . As a one-parameter example, the Poisson in Table 5.1 has Àõ D log (cid:22),\\nÀá D (cid:22), y D x, and dÀá=dÀõ D 1=.dÀõ=dÀá/ D (cid:22) D VÀõ.\\n\\nThe maximum likelihood estimate for the expectation parameter Àá is\\nsimply y (or Ny under repeated sampling (5.54)), which makes it immediate\\nto calculate in most situations.(cid:142) Less immediate is the MLE for the natural (cid:142)10\\nparameter Àõ: the one-to-one mapping Àá D P .Àõ/ (5.56) has inverse Àõ D\\nP  (cid:0)1.Àá/, so\\n\\nOÀõ D P  (cid:0)1.y/;\\n\\n(5.61)\\n\\n6 The simpliÔ¨Åed dot notation leads to more compact expressions: P .Àõ/ D d .Àõ/=dÀõ\\n\\nand R .Àõ/ D d 2 .Àõ/=dÀõ2.\\n\\n\\x0cParametric Models\\n68\\ne.g., OÀõ D log y for the Poisson. The trouble is that P  (cid:0)1.(cid:1)/ is usually un-\\navailable in closed form. Numerical approximation algorithms are neces-\\nsary to calculate OÀõ in most cases.\\n\\nAll of the classic exponential families have closed-form expressions for\\n .Àõ/ (and fÀõ.x/), yielding pleasant formulas for the mean Àá and covar-\\niance VÀõ, (5.56)‚Äì(5.57). Modern computational technology allows us to\\nwork with general exponential families, designed for speciÔ¨Åc tasks, with-\\nout concern for mathematical tractability.\\n\\nFigure 5.7 A seven-parameter exponential family Ô¨Åt to the gfr\\ndata of Figure 2.1 (solid) compared with gamma Ô¨Åt of Figure 4.1\\n(dashed).\\n\\nAs an example we again consider Ô¨Åtting the gfr data of Figure 2.1.\\nFor our exponential family of possible densities we take f0.x/ (cid:17) 1, and\\nsufÔ¨Åcient statistic vector\\n\\ny.x/ D .x; x2; : : : ; x7/;\\n(5.62)\\nso Àõ0y in (5.50) can represent all 7th-order polynomials in x, the gfr\\nmeasurement.7 (Stopping at power 2 gives the\\n.(cid:22); (cid:27) 2/ family, which we\\nalready know Ô¨Åts poorly from Figure 4.1.) The heavy curve in Figure 5.7\\nshows the MLE Ô¨Åt f OÀõ.x/ now following the gfr histogram quite closely.\\nChapter 10 discusses ‚ÄúLindsey‚Äôs method,‚Äù a simpliÔ¨Åed algorithm for cal-\\nculating the MLE OÀõ.\\n\\nN\\n\\n7 Any intercept in the polynomial is absorbed into the  .Àõ/ term in (5.57).\\n\\n gfrFrequency20406080100051015202530GammaExponential Family\\x0c5.6 Notes and Details\\n\\n69\\n\\nA more exotic example concerns the generation of random graphs on a\\nÔ¨Åxed set of N nodes. Each possible graph has a certain total number E of\\nedges, and T of triangles. A popular choice for generating such graphs is\\nthe two-parameter exponential family having y D .E; T /, so that larger\\nvalues of Àõ1 and Àõ2 yield more connections.\\n\\n5.6 Notes and Details\\n\\nThe notion of sufÔ¨Åcient statistics, ones that contain all available inferen-\\ntial information, was perhaps Fisher‚Äôs happiest contribution to the classic\\ncorpus. He noticed that in the exponential family form (5.50), the fact that\\nthe parameter Àõ interacts with the data x only through the factor exp.Àõ0y/\\nmakes y.x/ sufÔ¨Åcient for estimating Àõ. In 1935‚Äì36, a trio of authors, work-\\ning independently in different countries, Pitman, Darmois, and Koopmans,\\nshowed that exponential families are the only ones that enjoy Ô¨Åxed-dimen-\\nsional sufÔ¨Åcient statistics under repeated independent sampling. Until the\\nlate 1950s such distributions were called Pitman‚ÄìDarmois‚ÄìKoopmans fam-\\nilies, the long name suggesting infrequent usage.\\n\\nGeneralized linear models, Chapter 8, show the continuing impact of\\nsufÔ¨Åciency on statistical practice. Peter Bickel has pointed out that data\\ncompression, a lively topic in areas such as image transmission, is a mod-\\nern, less stringent, version of sufÔ¨Åciency.\\n\\nOur only nonexponential family so far was (4.39), the Cauchy transla-\\ntional model. Efron and Hinkley (1978) analyze the Cauchy family in terms\\nof curved exponential families, a generalization of model (5.50).\\n\\nProperties of classical distributions (lots of properties and lots of distri-\\nbutions) are covered in Johnson and Kotz‚Äôs invaluable series of reference\\nbooks, 1969‚Äì1972. Two classic multivariate analysis texts are Anderson\\n(2003) and Mardia et al. (1979).\\n\\n(cid:142)1 [p. 57] Formula (5.12). From z D T (cid:0)1.x (cid:0) (cid:22)/ we have dz=dx D T (cid:0)1\\n\\nand\\n\\nf(cid:22);‚Ä†.x/ D f .z/jT (cid:0)1j D .2(cid:25)/(cid:0) p\\n\\n2 jT (cid:0)1je(cid:0) 1\\n\\n2 .x(cid:0)(cid:22)/0T (cid:0)1\\n\\n0\\n\\nT (cid:0)1.x(cid:0)(cid:22)/;\\n\\n(5.63)\\n\\nso (5.12) follows from T T 0 D ‚Ä† and jT j D j‚Ä†j1=2.\\n\\n(cid:142)2 [p. 58] Formula (5.18). Let ∆í D ‚Ä†(cid:0)1 be partitioned as in (5.17). Then\\n\\n(cid:19)\\n\\n(cid:18)∆í11 ∆í12\\n∆í21 ∆í22\\n\\nD\\n\\n (cid:0)‚Ä†11 (cid:0) ‚Ä†12‚Ä†(cid:0)1\\n(cid:0)‚Ä†(cid:0)1\\n\\n22 ‚Ä†21\\n22 ‚Ä†21∆í11\\n\\n(cid:1)(cid:0)1\\n\\n(cid:0)‚Ä†(cid:0)1\\n(cid:0)‚Ä†22 (cid:0) ‚Ä†21‚Ä†(cid:0)1\\n\\n11 ‚Ä†12∆í22\\n11 ‚Ä†12\\n\\n(cid:1)(cid:0)1\\n\\n!\\n\\n;\\n\\n(5.64)\\ndirect multiplication showing that ∆í‚Ä† D I, the identity matrix. If ‚Ä† is\\n\\n\\x0c70\\n\\nParametric Models\\n\\nsymmetric then ∆í21 D ∆í0\\n12. By redeÔ¨Åning x to be x (cid:0) (cid:22) we can set (cid:22).1/\\nand (cid:22).2/ equal to zero in (5.18). The quadratic form in the exponent of\\n(5.12) is\\n.1/; x0\\n.x0\\n\\n.2//∆í (cid:0)x.1/; x.2/\\n\\n.2/∆í22x.2/ C 2x0\\n\\n.1/∆í12x.2/ C x0\\n\\n.1/∆í11x.1/:\\n\\n(cid:1) D x0\\n\\nBut, using (5.64), this matches the quadratic form from (5.18),\\n\\n(cid:0)x.2/ (cid:0) ‚Ä†21‚Ä†(cid:0)1\\n\\n11 x.1/\\n\\n(cid:1)0\\n\\n∆í22\\n\\n(cid:0)x.2/ (cid:0) ‚Ä†21‚Ä†(cid:0)1\\n\\n11 x.1/\\n\\n(cid:1)\\n\\n(5.65)\\n\\n(5.66)\\n\\nexcept for an added term that does not involve x.2/. For a multivariate nor-\\nmal distribution, this is sufÔ¨Åcient to show that the conditional distribution\\nof x.2/ given x.1/ is indeed (5.18) (see (cid:142)3).\\n\\n(cid:142)3 [p. 59] Formulas (5.21) and (5.23). Suppose that the continuous univariate\\n\\nrandom variable z has density of the form\\n\\nf .z/ D c0e(cid:0) 1\\n\\n2 Q.z/;\\n\\nwhere Q.z/ D az2 C 2bz C c1;\\n\\n(5.67)\\n\\na; b; c0 and c1 constants, a > 0. Then, by ‚Äúcompleting the square,‚Äù\\n\\nf .z/ D c2e(cid:0) 1\\n\\n2 a.z(cid:0) b\\n\\na /2\\n\\n;\\n\\n(5.68)\\n\\nand we see that z (cid:24)\\n.b=a; 1=a/. The key point is that form (5.67) spec-\\niÔ¨Åes z as normal, with mean and variance uniquely determined by a and\\nb. The multivariate version of this fact was used in the derivation of for-\\nmula (5.18).\\n\\nN\\n\\nBy redeÔ¨Åning (cid:22) and x as (cid:22) (cid:0) M and x (cid:0) M , we can take M D 0 in\\n(5.21). Setting B D A=.A C (cid:27) 2/, density (5.21) for (cid:22)jx is of form (5.67),\\nwith\\n\\nQ.(cid:22)/ D (cid:22)2\\nB(cid:27) 2\\n\\n(cid:0) 2x(cid:22)\\n(cid:27) 2\\n\\nC Bx2\\n(cid:27) 2 :\\n\\n(5.69)\\n\\nBut Bayes‚Äô rule says that the density of (cid:22)jx is proportional to g.(cid:22)/f(cid:22).x/,\\nalso of form (5.67), now with\\n(cid:18) 1\\nA\\n\\n(cid:22)2 (cid:0) 2x(cid:22)\\n(cid:27) 2\\n\\nC x2\\n(cid:27) 2 :\\n\\nC 1\\n(cid:27) 2\\n\\nQ.(cid:22)/ D\\n\\n(5.70)\\n\\n(cid:19)\\n\\nA little algebra shows that the quadratic and linear coefÔ¨Åcients of (cid:22) match\\nin (5.69)‚Äì(5.70), verifying (5.21).\\n\\nWe verify the multivariate result (5.23) using a different argument. The\\n\\n2p vector .(cid:22); x/0 has joint distribution\\n(cid:18)A\\nA A C ‚Ä†\\n\\n(cid:18)(cid:18)M\\nM\\n\\nA\\n\\n(cid:19)\\n\\n;\\n\\nN\\n\\n(cid:19)(cid:19)\\n\\n:\\n\\n(5.71)\\n\\n\\x0c5.6 Notes and Details\\n\\n71\\n\\nNow we employ (5.18) and a little manipulation to get (5.23).\\n\\n(cid:142)4 [p. 60] Formula (5.30). This is the matrix identity (5.64), now with ‚Ä†\\n\\nequaling\\n\\n(cid:22).\\n\\nI\\n\\n(cid:142)5 [p. 61] Multivariate Gaussian and nuisance parameters. The cautionary\\nmessage here‚Äîthat increasing the number of unknown nuisance parame-\\nters decreases the accuracy of the estimate of interest‚Äîcan be stated more\\npositively: if some nuisance parameters are actually known, then the MLE\\nof the parameter of interest becomes more accurate. Suppose, for example,\\nwe wish to estimate (cid:22)1 from a sample of size n in a bivariate normal model\\nx (cid:24)\\n2.(cid:22); ‚Ä†/ (5.14). The MLE Nx1 has variance (cid:27)11=n in notation (5.19).\\nBut if (cid:22)2 is known then the MLE of (cid:22)1 becomes Nx1 (cid:0) .(cid:27)12=(cid:27)22/. Nx2 (cid:0) (cid:22)2/\\nwith variance .(cid:27)11=n/ (cid:1) .1 (cid:0) (cid:26)2/, (cid:26) being the correlation (cid:27)12=\\n\\n(cid:142)6 [p. 63] Formula (5.40). x D Pn\\n\\ni D1 xi , where the xi are iid observations\\nhaving Prfxi D ei g D (cid:25)l , as in (5.35). The mean and covariance of each\\nxi are\\n\\n(cid:27)11(cid:27)22.\\n\\np\\n\\nN\\n\\nEfxi g D\\n\\nL\\nX\\n\\n1\\n\\n(cid:25)l el D (cid:25)\\n\\nand\\n\\ncovfxi g D Efxi x0\\n\\ng (cid:0) Efxi gEfx0\\n\\ng D X\\n\\n(cid:25)l el e0\\n\\nl\\n\\n(cid:0) (cid:25)(cid:25) 0\\n\\ni\\n\\ni\\nD diag.(cid:25)/ (cid:0) (cid:25)(cid:25) 0:\\n\\n(5.72)\\n\\n(5.73)\\n\\nFormula (5.40) follows from Efxg D P Efxi g and covfxg D P covfxi g.\\n\\n(cid:142)7 [p. 64] Formula (5.43). The densities of S (5.42) and SC D P Sl are\\n\\nf(cid:22).S / D\\n\\nL\\nY\\n\\nlD1\\n\\ne(cid:0)(cid:22)l (cid:22)Sl\\n\\nl =Sl ≈†\\n\\nand f(cid:22)C.SC/ D e(cid:0)(cid:22)C(cid:22)SC\\n\\nC =SC≈†:\\n\\n(5.74)\\n\\nThe conditional density of S given SC is the ratio\\n\\nf(cid:22).S jSC/ D\\n\\nSC≈†\\n1 Sl ≈†\\n\\nQL\\n\\n! L\\nY\\n\\nlD1\\n\\n(cid:18) (cid:22)l\\n(cid:22)C\\n\\n(cid:19)Sl\\n\\n;\\n\\n(5.75)\\n\\nwhich is (5.43).\\n\\n(cid:142)8 [p. 66] Formula (5.51) and the convexity of A. Suppose Àõ1 and Àõ2 are any\\ntwo points in A, i.e., values of Àõ having the integral in (5.51) Ô¨Ånite. For any\\nvalue of c in the interval ≈í0; 1(cid:141), and any value of y, we have\\n1y C .1 (cid:0) c/eÀõ0\\n\\n2y (cid:21) e≈ícÀõ1C.1(cid:0)c/Àõ2(cid:141)\\n\\n(5.76)\\n\\nceÀõ0\\n\\ny\\n\\n0\\n\\nbecause of the convexity in c of the function on the right (veriÔ¨Åed by show-\\ning that its second derivative is positive). Integrating both sides of (5.76)\\n\\n \\n\\x0c72\\n\\nParametric Models\\n\\nwith respect to f0.x/ shows that the integral on the right must be\\n\\nover\\nÔ¨Ånite: that is, cÀõ1 C .1 (cid:0) c/Àõ2 is in A, verifying A‚Äôs convexity.\\n\\nX\\n\\n(cid:142)9 [p. 67] Formula (5.55). In the univariate case, differentiating both sides of\\n\\n(5.51) with respect to Àõ gives\\n\\nP .Àõ/e .Àõ/ D\\n\\nZ\\n\\nyeÀõyf0.x/ dxI\\n\\n(5.77)\\n\\ndividing by e .Àõ/ shows that P .Àõ/ D EÀõfyg. Differentiating (5.77) again\\ngives\\n\\nX\\n\\nor\\n\\n(cid:0) R .Àõ/ C P .Àõ/2(cid:1) e .Àõ/ D\\n\\nZ\\n\\nX\\n\\ny2eÀõyf0.x/ dx;\\n\\n(5.78)\\n\\nR .Àõ/ D EÀõfy2g (cid:0) EÀõfyg2 D varÀõfyg:\\n\\n(5.79)\\n\\nSuccessive derivatives of  .Àõ/ yield the higher cumulants of y, its skew-\\nness, kurtosis, etc.\\n\\n(cid:142)10 [p. 67] MLE for Àá. The gradient with respect to Àõ of log fÀõ.y/ (5.50) is\\n\\nrÀõ\\n\\n(cid:0)Àõ0y (cid:0)  .Àõ/(cid:1) D y (cid:0) P .Àõ/ D y (cid:0) EÀõfy(cid:3)g;\\n(5.80)\\n(5.56), where y(cid:3) represents a hypothetical realization y.x(cid:3)/ drawn from\\nfÀõ.(cid:1)/. We achieve the MLE OÀõ at r OÀõ D 0, or\\nE OÀõfy(cid:3)g D y:\\nIn other words the MLE OÀõ is the value of Àõ that makes the expectation\\nEÀõfy(cid:3)g match the observed y. Thus (5.58) implies that the MLE of pa-\\nrameter Àá is y.\\n\\n(5.81)\\n\\n\\x0cPart II\\n\\nEarly Computer-Age Methods\\n\\n\\x0c\\x0c6\\n\\nEmpirical Bayes\\n\\nThe constraints of slow mechanical computation molded classical statistics\\ninto a mathematically ingenious theory of sharply delimited scope. Emerg-\\ning after the Second World War, electronic computation loosened the com-\\nputational stranglehold, allowing a more expansive and useful statistical\\nmethodology.\\n\\nSome revolutions start slowly. The journals of the 1950s continued to\\nemphasize classical themes: pure mathematical development typically cen-\\ntered around the normal distribution. Change came gradually, but by the\\n1990s a new statistical technology, computer enabled, was Ô¨Årmly in place.\\nKey developments from this period are described in the next several chap-\\nters. The ideas, for the most part, would not startle a pre-war statistician,\\nbut their computational demands, factors of 100 or 1000 times those of\\nclassical methods, would. More factors of a thousand lay ahead, as will be\\ntold in Part III, the story of statistics in the twenty-Ô¨Årst century.\\n\\nEmpirical Bayes methodology, this chapter‚Äôs topic, has been a particu-\\nlarly slow developer despite an early start in the 1940s. The roadblock here\\nwas not so much the computational demands of the theory as a lack of ap-\\npropriate data sets. Modern scientiÔ¨Åc equipment now provides ample grist\\nfor the empirical Bayes mill, as will be illustrated later in the chapter, and\\nmore dramatically in Chapters 15‚Äì21.\\n\\n6.1 Robbins‚Äô Formula\\n\\nTable 6.1 shows one year of claims data for a European automobile insur-\\nance company; 7840 of the 9461 policy holders made no claims during the\\nyear, 1317 made a single claim, 239 made two claims each, etc., with Ta-\\nble 6.1 continuing to the one person who made seven claims. Of course the\\ninsurance company is concerned about the claims each policy holder will\\nmake in the next year.\\n\\nBayes‚Äô formula seems promising here. We suppose that xk, the number\\n\\n75\\n\\n\\x0c76\\n\\nEmpirical Bayes\\n\\nTable 6.1 Counts yx of number of claims x made in a single year by\\n9461 automobile insurance policy holders. Robbins‚Äô formula (6.7)\\nestimates the number of claims expected in a succeeding year, for instance\\n0:168 for a customer in the x D 0 category. Parametric maximum\\nlikelihood analysis based on a gamma prior gives less noisy estimates.\\n\\nClaims x\\n\\n0\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\nCounts yx\\nFormula (6.7)\\nGamma MLE\\n\\n7840\\n.168\\n.164\\n\\n1317\\n.363\\n.398\\n\\n239\\n.527\\n.633\\n\\n42\\n1.33\\n.87\\n\\n14\\n1.43\\n1.10\\n\\n4\\n6.00\\n1.34\\n\\n4\\n1.75\\n1.57\\n\\n7\\n\\n1\\n\\nof claims to be made in a single year by policy holder k, follows a Poisson\\ndistribution with parameter (cid:18)k,\\n\\nPrfxk D xg D p(cid:18)k .x/ D e(cid:0)(cid:18)k (cid:18) x\\n\\nk =x≈†;\\n\\n(6.1)\\n\\nfor x D 0; 1; 2; 3; : : : ; (cid:18)k is the expected value of xk. A good customer,\\nfrom the company‚Äôs point of view, has a small value of (cid:18)k, though in any\\none year his or her actual number of accidents xk will vary randomly ac-\\ncording to probability density (6.1).\\n\\nSuppose we knew the prior density g.(cid:18)/ for the customers‚Äô (cid:18) values.\\n\\nThen Bayes‚Äô rule (3.5) would yield\\n\\nEf(cid:18) jxg D\\n\\nR 1\\n0 (cid:18)p(cid:18) .x/g.(cid:18)/ d(cid:18)\\nR 1\\n0 p(cid:18) .x/g.(cid:18)/ d(cid:18)\\n\\n(6.2)\\n\\nfor the expected value of (cid:18) of a customer observed to make x claims in a\\nsingle year. This would answer the insurance company‚Äôs question of what\\nnumber of claims X to expect the next year from the same customer, since\\nEf(cid:18) jxg is also EfXjxg ((cid:18) being the expectation of X).\\n\\nFormula (6.2) is just the ticket if the prior g.(cid:18)/ is known to the company,\\nbut what if it is not? A clever rewriting of (6.2) provides a way forward.\\nUsing (6.1), (6.2) becomes\\n\\nEf(cid:18)jxg D\\n\\nR 1\\n0\\nR 1\\n0\\n\\n(cid:2)e(cid:0)(cid:18) (cid:18) xC1=x≈†(cid:3) g.(cid:18)/ d(cid:18)\\n(cid:2)e(cid:0)(cid:18) (cid:18) x=x≈†(cid:3) g.(cid:18)/ d(cid:18)\\n\\nD\\n\\n.x C 1/ R 1\\n0\\nR 1\\n0\\n\\n(cid:2)e(cid:0)(cid:18) (cid:18) xC1=.x C 1/≈†(cid:3) g.(cid:18)/ d(cid:18)\\n(cid:2)e(cid:0)(cid:18) (cid:18) x=x≈†(cid:3) g.(cid:18)/ d(cid:18)\\n\\n:\\n\\n(6.3)\\n\\n\\x0c6.1 Robbins‚Äô Formula\\n\\n77\\n\\nThe marginal density of x, integrating p(cid:18) .x/ over the prior g.(cid:18)/, is\\n\\nf .x/ D\\n\\nZ 1\\n\\n0\\n\\np(cid:18) .x/g.(cid:18)/ d(cid:18) D\\n\\nZ 1\\n\\nh\\n\\ne(cid:0)(cid:18) (cid:18) x=x≈†\\n\\ni\\n\\n0\\n\\ng.(cid:18)/ d(cid:18):\\n\\n(6.4)\\n\\nComparing (6.3) with (6.4) gives Robbins‚Äô formula,\\n\\nEf(cid:18)jxg D .x C 1/f .x C 1/=f .x/:\\n\\n(6.5)\\n\\nThe surprising and gratifying fact is that, even with no knowledge of the\\nprior density g.(cid:18)/, the insurance company can estimate Ef(cid:18) jxg (6.2) from\\nformula (6.5). The obvious estimate of the marginal density f .x/ is the\\nproportion of total counts in category x,\\n\\nOf .x/ D yx=N; with N D P\\n\\n(6.6)\\nOf .1/ D 1317=9461, etc. This yields an empirical\\n\\nx yx; the total count;\\n\\nOf .0/ D 7840=9461,\\nversion of Robbins‚Äô formula,\\n\\nOEf(cid:18) jxg D .x C 1/ Of .x C 1/ƒ± Of .x/ D .x C 1/yxC1=yx;\\n\\n(6.7)\\nthe Ô¨Ånal expression not requiring N . Table 6.1 gives OEf(cid:18)j0g D 0:168:\\ncustomers who made zero claims in one year had expectation 0.168 of a\\nclaim the next year; those with one claim had expectation 0.363, and so on.\\nRobbins‚Äô formula came as a surprise1 to the statistical world of the\\n1950s: the expectation Ef(cid:18)kjxkg for a single customer, unavailable without\\nthe prior g.(cid:18) /, somehow becomes available in the context of a large study.\\nThe terminology empirical Bayes is apt here: Bayesian formula (6.5) for a\\nsingle subject is estimated empirically (i.e., frequentistically) from a col-\\nlection of similar cases. The crucial point, and the surprise, is that large\\ndata sets of parallel situations carry within them their own Bayesian in-\\nformation. Large parallel data sets are a hallmark of twenty-Ô¨Årst-century\\nscientiÔ¨Åc investigation, promoting the popularity of empirical Bayes meth-\\nods.\\n\\nFormula (6.7) goes awry at the right end of Table 6.1, where it is destabi-\\nlized by small count numbers. A parametric approach gives more depend-\\nable results: now we assume that the prior density g.(cid:18)/ for the customers‚Äô\\n(cid:18)k values has a gamma form (Table 5.1)\\ng.(cid:18)/ D (cid:18) (cid:23)(cid:0)1e(cid:0)(cid:18)=(cid:27)\\n(cid:27) (cid:23)(cid:128).(cid:23)/\\n\\nfor (cid:18) (cid:21) 0;\\n\\n(6.8)\\n\\n;\\n\\nbut with parameters (cid:23) and (cid:27) unknown. Estimates . O(cid:23); O(cid:27)/ are obtained by\\n\\n1 Perhaps it shouldn‚Äôt have; estimation methods similar to (6.7) were familiar in the\\n\\nactuarial literature.\\n\\n\\x0c78\\n\\nEmpirical Bayes\\n\\nmaximum likelihood Ô¨Åtting to the counts yx, yielding a parametrically es-\\ntimated marginal density(cid:142)\\n\\n(cid:142)1\\n\\nOf .x/ D f O(cid:23); O(cid:27) .x/;\\n\\n(6.9)\\n\\nor equivalently Oyx D Nf O(cid:23); O(cid:27) .x/.\\n\\nFigure 6.1 Auto accident data; log(counts) vs claims for 9461\\nauto insurance policies. The dashed line is a gamma MLE Ô¨Åt.\\n\\nThe bottom row of Table 6.1 gives parametric estimates E O(cid:23); O(cid:27) f(cid:18)jxg D\\n.x C 1/ OyxC1= Oyx, which are seen to be less eccentric for large x. Figure 6.1\\ncompares (on the log scale) the raw counts yx with their parametric cousins\\nOyx.\\n\\n6.2 The Missing-Species Problem\\n\\nThe very Ô¨Årst empirical Bayes success story related to the butterÔ¨Çy data of\\nTable 6.2. Even in the midst of World War II Alexander Corbet, a leading\\nnaturalist, had been trapping butterÔ¨Çies for two years in Malaysia (then\\nMalaya): 118 species were so rare that he had trapped only one specimen\\neach, 74 species had been trapped twice each, Table 6.2 going on to show\\nthat 44 species were trapped three times each, and so on. Some of the more\\n\\n012345670246810claimslog(counts)llllllll\\x0c6.2 The Missing-Species Problem\\n\\n79\\n\\ncommon species had appeared hundreds of times each, but of course Corbet\\nwas interested in the rarer specimens.\\n\\nTable 6.2 ButterÔ¨Çy data; number y of species seen x times each in two\\nyears of trapping; 118 species trapped just once, 74 trapped twice each,\\netc.\\n\\nx\\n\\ny\\n\\nx\\n\\ny\\n\\n1\\n\\n118\\n\\n13\\n\\n6\\n\\n2\\n\\n74\\n\\n14\\n\\n12\\n\\n3\\n\\n44\\n\\n15\\n\\n6\\n\\n4\\n\\n24\\n\\n16\\n\\n9\\n\\n5\\n\\n29\\n\\n17\\n\\n9\\n\\n6\\n\\n22\\n\\n18\\n\\n6\\n\\n7\\n\\n20\\n\\n19\\n\\n10\\n\\n8\\n\\n19\\n\\n20\\n\\n10\\n\\n9\\n\\n20\\n\\n21\\n\\n11\\n\\n10\\n\\n15\\n\\n22\\n\\n5\\n\\n11\\n\\n12\\n\\n23\\n\\n3\\n\\n12\\n\\n14\\n\\n24\\n\\n3\\n\\nCorbet then asked a seemingly impossible question: if he trapped for one\\nadditional year, how many new species would he expect to capture? The\\nquestion relates to the absent entry in Table 6.2, x D 0, the species that\\nhaven‚Äôt been seen yet. Do we really have any evidence at all for answering\\nCorbet? Fortunately he asked the right man: R. A. Fisher, who produced a\\nsurprisingly satisfying solution for the ‚Äúmissing-species problem.‚Äù\\n\\nSuppose there are S species in all, seen or unseen, and that xk, the num-\\nber of times species k is trapped in one time unit,2 follows a Poisson dis-\\ntribution with parameter (cid:18)k as in (6.1),\\n\\nxk (cid:24) Poi.(cid:18)k/;\\n\\nfor k D 1; 2; : : : ; S:\\n\\n(6.10)\\n\\nThe entries in Table 6.2 are\\n\\nyx D #fxk D xg;\\n\\nfor x D 1; 2; : : : ; 24;\\n\\n(6.11)\\n\\nthe number of species trapped exactly x times each.\\n\\nNow consider a further trapping period of t time units, t D 1=2 in Cor-\\nbet‚Äôs question, and let xk.t / be the number of times species k is trapped in\\nthe new period. Fisher‚Äôs key assumption is that\\n\\nxk.t/ (cid:24) Poi.(cid:18)kt/\\n\\n(6.12)\\n\\nindependently of xk. That is, any one species is trapped independently over\\ntime3 at a rate proportional to its parameter (cid:18)k.\\n\\nThe probability that species k is not seen in the initial trapping period\\n\\n2 One time unit equals two years in Corbet‚Äôs situation.\\n3 This is the deÔ¨Ånition of a Poisson process.\\n\\n\\x0c80\\n\\nEmpirical Bayes\\n\\nbut is seen in the new period, that is xk D 0 and xk.t/ > 0, is\\n\\ne(cid:0)(cid:18)k\\n\\n(cid:16)\\n\\n1 (cid:0) e(cid:0)(cid:18)k t (cid:17)\\n\\n;\\n\\n(6.13)\\n\\nso that E.t/, the expected number of new species seen in the new trapping\\nperiod, is\\n\\nE.t/ D\\n\\nS\\nX\\n\\nkD1\\n\\ne(cid:0)(cid:18)k\\n\\n1 (cid:0) e(cid:0)(cid:18)k t (cid:17)\\n(cid:16)\\n\\n:\\n\\n(6.14)\\n\\nIt is convenient to write (6.14) as an integral,\\n\\nE.t/ D S\\n\\nZ 1\\n\\n0\\n\\ne(cid:0)(cid:18) (cid:16)\\n\\n1 (cid:0) e(cid:0)(cid:18) t (cid:17)\\n\\ng.(cid:18)/ d(cid:18);\\n\\n(6.15)\\n\\nwhere g.(cid:18)/ is the ‚Äúempirical density‚Äù putting probability 1=S on each of\\nthe (cid:18)k values. (Later we will think of g.(cid:18)/ as a continuous prior density on\\nthe possible (cid:18)k values.)\\n\\nExpanding 1 (cid:0) e(cid:0)(cid:18) t gives\\nZ 1\\n\\nE.t / D S\\n\\ne(cid:0)(cid:18) (cid:2)(cid:18) t (cid:0) .(cid:18) t/2=2≈† C .(cid:18) t/3=3≈† (cid:0) (cid:1) (cid:1) (cid:1) (cid:3) g.(cid:18)/ d(cid:18):\\n\\n(6.16)\\n\\n0\\n\\nNotice that the expected value ex of yx is the sum of the probabilities of\\nbeing seen exactly x times in the initial period,\\n\\nex D Efyxg D\\n\\nS\\nX\\n\\nkD1\\n\\ne(cid:0)(cid:18)k (cid:18) x\\n\\nk =x≈†\\n\\nD S\\n\\nZ 1\\n\\n0\\n\\ni\\nh\\ne(cid:0)(cid:18) (cid:18) x=x≈†\\n\\ng.(cid:18)/ d(cid:18):\\n\\n(6.17)\\n\\nComparing (6.16) with (6.17) provides a surprising result,\\n\\nE.t / D e1t (cid:0) e2t 2 C e3t 3 (cid:0) (cid:1) (cid:1) (cid:1) :\\n\\n(6.18)\\n\\nWe don‚Äôt know the ex values but, as in Robbins‚Äô formula, we can esti-\\n\\nmate them by the yx values, yielding an answer to Corbet‚Äôs question,\\n\\nOE.t/ D y1t (cid:0) y2t 2 C y3t 3 (cid:0) (cid:1) (cid:1) (cid:1) :\\n\\nCorbet speciÔ¨Åed t D 1=2, so4\\n\\nOE.1=2/ D 118.1=2/ (cid:0) 74.1=2/2 C 44.1=2/3 (cid:0) (cid:1) (cid:1) (cid:1)\\n\\nD 45:2:\\n\\n(6.19)\\n\\n(6.20)\\n\\n4 This may have been discouraging; there were no new trapping results reported.\\n\\n\\x0c6.2 The Missing-Species Problem\\n\\n81\\n\\nTable 6.3 Expectation (6.19) and its standard error (6.21) for the number\\nof new species captured in t additional fractional units of trapping time.\\n\\nt\\n\\n0.0\\n\\n0.1\\n\\n0.2\\n\\n0.3\\n\\n0.4\\n\\n0.5\\n\\n0.6\\n\\n0.7\\n\\n0.8\\n\\n0.9\\n\\n1.0\\n\\nE.t/\\nbsd.t/\\n\\n0 11.10 20.96 29.79 37.79 45.2 52.1 58.9 65.6 71.6 75.0\\n8.95 11.2 13.4 15.7 17.9 20.1 22.4\\n0\\n\\n2.24\\n\\n4.48\\n\\n6.71\\n\\nFormulas (6.18) and (6.19) do not require the butterÔ¨Çies to arrive inde-\\npendently. If we are willing to add the assumption that the xk‚Äôs are mutually\\nindependent, we can calculate(cid:142)\\n\\n(cid:142)2\\n\\nbsd.t/ D\\n\\n!1=2\\n\\nyxt 2x\\n\\n  24\\nX\\n\\nxD1\\n\\n(6.21)\\n\\nas an approximate standard error for OE.t/. Table 6.3 shows OE.t/ and bsd.t/\\nfor t D 0; 0:1; 0:2; : : : ; 1; in particular,\\n\\nOE.0:5/ D 45:2 Àô 11:2:\\n\\n(6.22)\\n\\nFormula (6.19) becomes unstable for t > 1. This is our price for sub-\\nstituting the nonparametric estimates yx for ex in (6.18). Fisher actually\\nanswered Corbet using a parametric empirical Bayes model in which the\\nprior g.(cid:18) / for the Poisson parameters (cid:18)k (6.12) was assumed to be of the\\ngamma form (6.8). It can be shown(cid:142) that then E.t/ (6.15) is given by\\n\\n(cid:142)3\\n\\nE.t / D e1 f1 (cid:0) .1 C (cid:13) t/(cid:0)(cid:23)g ƒ±.(cid:13)(cid:23)/;\\nwhere (cid:13) D (cid:27)=.1 C (cid:27)/. Taking Oe1 D y1, maximum likelihood estimation\\ngave\\n\\n(6.23)\\n\\nO(cid:23) D 0:104 and\\n\\nO(cid:27) D 89:79:\\n\\n(6.24)\\n\\nFigure 6.2 shows that the parametric estimate of E.t/ (6.23) using Oe1,\\nO(cid:23), and O(cid:27) is just slightly greater than the nonparametric estimate (6.19) over\\nthe range 0 (cid:20) t (cid:20) 1. Fisher‚Äôs parametric estimate, however, gives reason-\\nable results for t > 1, OE.2/ D 123 for instance, for a future trapping period\\nof 2 units (4 years). ‚ÄúReasonable‚Äù does not necessarily mean dependable.\\nThe gamma prior is a mathematical convenience, not a fact of nature; pro-\\njections into the far future fall into the category of educated guessing.\\n\\nThe missing-species problem encompasses more than butterÔ¨Çies. There\\nare 884,647 words in total in the recognized Shakespearean canon, of which\\n14,376 are so rare they appear just once each, 4343 appear twice each, etc.,\\n\\n\\x0c82\\n\\nEmpirical Bayes\\n\\nFigure 6.2 ButterÔ¨Çy data; expected number of new species in t\\nunits of additional trapping time. Nonparametric Ô¨Åt (solid) Àô 1\\nstandard deviation; gamma model (dashed).\\n\\nTable 6.4 Shakespeare‚Äôs word counts; 14,376 distinct words appeared\\nonce each in the canon, 4343 distinct words twice each, etc. The canon\\nhas 884,647 words in total, counting repeats.\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n0C 14376\\n10C\\n305\\n20C\\n104\\n30C\\n73\\n40C\\n49\\n50C\\n25\\n60C\\n30\\n70C\\n13\\n80C\\n13\\n90C\\n4\\n\\n4343\\n259\\n105\\n47\\n41\\n19\\n19\\n12\\n12\\n7\\n\\n2292\\n242\\n99\\n56\\n30\\n28\\n21\\n10\\n11\\n6\\n\\n1463\\n223\\n112\\n59\\n35\\n27\\n18\\n16\\n8\\n7\\n\\n1043\\n187\\n93\\n53\\n37\\n31\\n15\\n18\\n10\\n10\\n\\n6\\n\\n837\\n181\\n74\\n45\\n21\\n19\\n10\\n11\\n11\\n10\\n\\n7\\n\\n638\\n179\\n83\\n34\\n41\\n19\\n15\\n8\\n7\\n15\\n\\n8\\n\\n519\\n130\\n76\\n49\\n30\\n22\\n14\\n15\\n12\\n7\\n\\n9\\n\\n430\\n127\\n72\\n45\\n28\\n23\\n11\\n12\\n9\\n7\\n\\n10\\n\\n364\\n128\\n63\\n52\\n19\\n14\\n16\\n7\\n8\\n5\\n\\nas in Table 6.4, which goes on to the Ô¨Åve words appearing 100 times each.\\nAll told, 31,534 distinct words appear (including those that appear more\\nthan 100 times each), this being the observed size of Shakespeare‚Äôs vocab-\\nulary. But what of the words Shakespeare knew but didn‚Äôt use? These are\\nthe ‚Äúmissing species‚Äù in Table 6.4.\\n\\n0.00.20.40.60.81.0020406080time tE^(t) Gamma model  E^(2) = 123E^(4) = 176E^(8) = 233\\x0c6.2 The Missing-Species Problem\\n\\n83\\n\\nSuppose another quantity of previously unknown Shakespeare manu-\\nscripts was discovered, comprising 884647 (cid:1) t words (so t D 1 would rep-\\nresent a new canon just as large as the old one). How many previously\\nunseen distinct words would we expect to discover?\\n\\nEmploying formulas (6.19) and (6.21) gives\\n\\n11430 Àô 178\\n\\n(6.25)\\n\\nfor the expected number of distinct new words if t D 1. This is a very con-\\nservative lower bound on how many words Shakespeare knew but didn‚Äôt\\nuse. We can imagine t rising toward inÔ¨Ånity, revealing ever more unseen\\nvocabulary. Formula (6.19) fails for t > 1, and Fisher‚Äôs gamma assump-\\ntion is just that, but more elaborate empirical Bayes calculations give a Ô¨Årm\\nlower bound of 35; 000C on Shakespeare‚Äôs unseen vocabulary, exceeding\\nthe visible portion!\\n\\nMissing mass is an easier version of the missing-species problem, in\\nwhich we only ask for the proportion of the total sum of (cid:18)k values corre-\\nsponding to the species that went unseen in the original trapping period,\\n\\nM D X\\n\\n(cid:18)k\\n\\n(cid:30) X\\n\\n(cid:18)k:\\n\\nunseen\\n\\nall\\n\\n(6.26)\\n\\nThe numerator has expectation\\n\\n(cid:18)ke(cid:0)(cid:18)k D S\\n\\nX\\n\\nall\\n\\nZ 1\\n\\n0\\n\\n(cid:18)e(cid:0)(cid:18) g.(cid:18)/d(cid:18) D e1\\n\\n(6.27)\\n\\nas in (6.17), while the expectation of the denominator is\\n\\nX\\n\\n(cid:18)k D X\\n\\nEfxsg D E\\n\\nall\\n\\nall\\n\\n(\\n\\nX\\n\\nall\\n\\n)\\n\\nxs\\n\\nD EfN g;\\n\\n(6.28)\\n\\nwhere N is the total number of butterÔ¨Çies trapped. The obvious missing-\\nmass estimate is then\\n\\nOM D y1=N:\\n\\nFor the Shakespeare data,\\n\\nOM D 14376=884647 D 0:016:\\n\\n(6.29)\\n\\n(6.30)\\n\\nWe have seen most of Shakespeare‚Äôs vocabulary, as weighted by his usage,\\nthough not by his vocabulary count.\\n\\nAll of this seems to live in the rareÔ¨Åed world of mathematical abstrac-\\ntion, but in fact some previously unknown Shakespearean work might have\\n\\n\\x0c84\\n\\nEmpirical Bayes\\n\\nbeen discovered in 1985. A short poem, ‚ÄúShall I die?,‚Äù was found in the\\narchives of the Bodleian Library and, controversially, attributed to Shake-\\nspeare by some but not all experts.\\n\\nThe poem of 429 words provided a new ‚Äútrapping period‚Äù of length only\\n\\nt D 429=884647 D 4:85 (cid:1) 10(cid:0)4;\\n\\nand a prediction from (6.19) of\\n\\nEft g D 6:97\\n\\n(6.31)\\n\\n(6.32)\\n\\nnew ‚Äúspecies,‚Äù i.e., distinct words not appearing in the canon. In fact there\\nwere nine such words in the poem. Similar empirical Bayes predictions\\nfor the number of words appearing once each in the canon, twice each,\\netc., showed reasonable agreement with the poem‚Äôs counts, but not enough\\nto stiÔ¨Çe doubters. ‚ÄúShall I die?‚Äù is currently grouped with other canonical\\napocrypha by a majority of experts.\\n\\n6.3 A Medical Example\\n\\nThe reader may have noticed that our examples so far have not been par-\\nticularly computer intensive; all of the calculations could have been (and\\noriginally were) done by hand.5 This section discusses a medical study\\nwhere the empirical Bayes analysis is more elaborate.\\n\\nCancer surgery sometimes involves the removal of surrounding lymph\\nnodes as well as the primary target at the site. Figure 6.3 concerns N D 844\\nsurgeries, each reporting\\n\\nn D # nodes removed\\n\\nand x D # nodes found positive;\\n\\n(6.33)\\n\\n‚Äúpositive‚Äù meaning malignant. The ratios\\n\\npk D xk=nk;\\n\\nk D 1; 2; : : : ; N;\\n\\n(6.34)\\n\\nare described in the histogram. A large proportion of them, 340=844 or\\n40%, were zero, the remainder spreading unevenly between zero and one.\\nThe denominators nk ranged from 1 to 69, with a mean of 19 and standard\\ndeviation of 11.\\n\\nWe suppose that each patient has some true probability of a node being\\n\\n5 Not so collecting the data. Corbet‚Äôs work was pre-computer but Shakespeare‚Äôs word\\n\\ncounts were done electronically. Twenty-Ô¨Årst-century scientiÔ¨Åc technology excels at the\\nproduction of the large parallel-structured data sets conducive to empirical Bayes\\nanalysis.\\n\\n\\x0c6.3 A Medical Example\\n\\n85\\n\\nFigure 6.3 Nodes study; ratio p D x=n for 844 patients; n D\\nnumber of nodes removed, x D number positive.\\n\\npositive, say probability (cid:18)k for patient k, and that his or her nodal results\\noccur independently of each other, making xk binomial,\\n\\nxk (cid:24) Bi.nk; (cid:18)k/:\\n\\nThis gives pk D xk=nk with mean and variance\\n\\npk (cid:24) .(cid:18)k; (cid:18)k.1 (cid:0) (cid:18)k/=nk/ ;\\n\\n(6.35)\\n\\n(6.36)\\n\\nso that (cid:18)k is estimated more accurately when nk is large.\\n\\nA Bayesian analysis would begin with the assumption of a prior density\\n\\ng.(cid:18)/ for the (cid:18)k values,\\n\\n(cid:18)k (cid:24) g.(cid:18)/;\\n\\nfor k D 1; 2; : : : ; N D 844:\\n\\n(6.37)\\n\\nWe don‚Äôt know g.(cid:18)/, but the parallel nature of the nodes data set‚Äî844\\nsimilar cases‚Äîsuggests an empirical Bayes approach. As a Ô¨Årst try for the\\nnodes study, we assume that logfg.(cid:18)/g is a fourth-degree polynomial in (cid:18),\\n\\nlog fgÀõ.(cid:18)/g D a0 C\\n\\n4\\nX\\n\\nj D1\\n\\nÀõj (cid:18) j I\\n\\n(6.38)\\n\\n p = x/nFrequency0.00.20.40.60.81.0020406080100*340\\x0c86\\n\\nEmpirical Bayes\\n\\ngÀõ.(cid:18)/ is determined by the parameter vector Àõ D .Àõ1; Àõ2; Àõ3; Àõ4/ since,\\ngiven Àõ, a0 can be calculated from the requirement that\\n)\\n\\nZ 1\\n\\ngÀõ.(cid:18)/ d(cid:18) D 1 D\\n\\nZ 1\\n\\n(\\na0 C\\n\\n4\\nX\\n\\nexp\\n\\nÀõj (cid:18) j\\n\\nd(cid:18):\\n\\n(6.39)\\n\\n0\\n\\n0\\n\\n1\\n\\nFor a given choice of Àõ, let fÀõ.xk/ be the marginal probability of the\\n\\nobserved value xk for patient k,\\n\\nfÀõ.xk/ D\\n\\nZ 1\\n\\n0\\n\\nnk\\nxk\\n\\n!\\n\\n(cid:18) xk .1 (cid:0) (cid:18)/nk (cid:0)xk gÀõ.(cid:18)/ d(cid:18):\\n\\n(6.40)\\n\\nThe maximum likelihood estimate of Àõ is the maximizer\\n\\nOÀõ D arg max\\n\\nÀõ\\n\\n)\\n\\nlog fÀõ.xk/\\n\\n:\\n\\n( N\\nX\\n\\nkD1\\n\\n(6.41)\\n\\nFigure 6.4 Estimated prior density g.(cid:18)/ for the nodes study;\\n59% of patients have (cid:18) (cid:20) 0:2, 7% have (cid:18) (cid:21) 0:8.\\n\\nFigure 6.4 graphs g OÀõ.(cid:18) /, the empirical Bayes estimate for the prior dis-\\ntribution of the (cid:18)k values. The huge spike at zero in Figure 6.3 is now\\nreduced: Prf(cid:18)k (cid:20) 0:01g D 0:12 compared with the 38% of the pk values\\n\\n0.00.20.40.60.81.00.000.020.040.060.080.100.12qg^(q)‚Äì sd \\n\\x0c6.3 A Medical Example\\n\\n87\\n\\nless than 0.01. Small (cid:18) values are still the rule though, for instance\\n\\nZ 0:20\\n\\ng OÀõ.(cid:18)/ d(cid:18) D 0:59 compared with\\n\\nZ 1:00\\n\\ng OÀõ.(cid:18)/ d(cid:18) D 0:07: (6.42)\\n\\n0\\nThe vertical bars in Figure 6.4 indicate Àô one standard error for the es-\\ntimation of g.(cid:18)/. The curve seems to have been estimated very accurately,\\nat least if we assume the adequacy of model (6.37). Chapter 21 describes\\nthe computations involved in Figure 6.4.\\n\\n0:80\\n\\nThe posterior distribution of (cid:18)k given xk and nk is estimated according\\n\\nto Bayes‚Äô rule (3.5) to be\\n\\nOg.(cid:18)jxk; nk/ D g OÀõ.(cid:18)/\\n\\n!\\n\\n(cid:18) xk .1 (cid:0) (cid:18)/nk (cid:0)xk\\n\\n(cid:30)\\n\\nnk\\nxk\\n\\nf OÀõ.xk/;\\n\\n(6.43)\\n\\nwith f OÀõ.xk/ from (6.40).\\n\\nFigure 6.5 Empirical Bayes posterior densities of (cid:18) for three\\npatients, given x D number of positive nodes, n D number of\\nnodes.\\n\\nFigure 6.5 graphs Og.(cid:18)jxk; nk/ for three choices of .xk; nk/: .7; 32/, .3; 6/,\\nand .17; 18/. If we take (cid:18) (cid:21) 0:50 as indicating poor prognosis (and sug-\\ngesting more aggressive follow-up therapy), then the Ô¨Årst patient is almost\\nsurely on safe ground, the third patient almost surely needs more follow-up\\ntherapy and the situation of the second is uncertain.\\n\\n0.00.20.40.60.81.00246qg(q | x, n)x=7 n=32x=17 n=18x=3 n=60.5 \\n\\x0c88\\n\\nEmpirical Bayes\\n\\n6.4 Indirect Evidence 1\\n\\nA good deÔ¨Ånition of a statistical argument is one in which many small\\npieces of evidence, often contradictory, are combined to produce an overall\\nconclusion. In the clinical trial of a new drug, for instance, we don‚Äôt expect\\nthe drug to cure every patient, or the placebo to always fail, but eventually\\nperhaps we will obtain convincing evidence of the new drug‚Äôs efÔ¨Åcacy.\\n\\nThe clinical trial is collecting direct statistical evidence, in which each\\nsubject‚Äôs success or failure bears directly upon the question of interest. Di-\\nrect evidence, interpreted by frequentist methods, was the dominant mode\\nof statistical application in the twentieth century, being strongly connected\\nto the idea of scientiÔ¨Åc objectivity.\\n\\nBayesian inference provides a theoretical basis for incorporating indi-\\nrect evidence, for example the doctor‚Äôs prior experience with twin sexes in\\nSection 3.1. The assertion of a prior density g.(cid:18)/ amounts to a claim for\\nthe relevance of past data to the case at hand.\\n\\nEmpirical Bayes removes the Bayes scaffolding. In place of a reassuring\\nprior g.(cid:18) /, the statistician must put his or her faith in the relevance of the\\n‚Äúother‚Äù cases in a large data set to the case of direct interest. For the second\\npatient in Figure 6.5, the direct estimate of his (cid:18) value is O(cid:18) D 3=6 D 0:50.\\nThe empirical Bayes estimate is a little less,\\n\\nO(cid:18) EB D\\n\\nZ 1\\n\\n0\\n\\n(cid:18) Og.(cid:18)jxk D 3; nk D 6/ D 0:446:\\n\\n(6.44)\\n\\nA small difference, but we will see bigger ones in succeeding chapters.\\n\\nThe changes in twenty-Ô¨Årst-century statistics have largely been demand\\ndriven, responding to the massive data sets enabled by modern scientiÔ¨Åc\\nequipment. Philosophically, as opposed to methodologically, the biggest\\nchange has been the increased acceptance of indirect evidence, especially\\nas seen in empirical Bayes and objective (‚Äúuninformative‚Äù) Bayes appli-\\ncations. False-discovery rates, Chapter 15, provide a particularly striking\\nshift from direct to indirect evidence in hypothesis testing. Indirect evi-\\ndence in estimation is the subject of our next chapter.\\n\\n6.5 Notes and Details\\n\\nRobbins (1956) introduced the term ‚Äúempirical Bayes‚Äù as well as rule (6.7)\\nas part of a general theory of empirical Bayes estimation. 1956 was also the\\npublication year for Good and Toulmin‚Äôs solution (6.19) to the missing-\\nspecies problem. Good went out of his way to credit his famous Bletchley\\n\\n\\x0c6.5 Notes and Details\\n\\n89\\n\\ncolleague Alan Turing for some of the ideas. The auto accident data is taken\\nfrom Table 3.1 of Carlin and Louis (1996), who provide a more complete\\ndiscussion. Empirical Bayes estimates such as 11430 in (6.25) do not de-\\npend on independence among the ‚Äúspecies,‚Äù but accuracies such as Àô178\\ndo; and similarly for the error bars in Figures 6.2 and 6.4.\\n\\nCorbet‚Äôs enormous efforts illustrate the difÔ¨Åculties of amassing large\\ndata sets in pre-computer times. Dependable data is still hard to come by,\\nbut these days it is often the statistician‚Äôs job to pry it out of enormous\\ndatabases. Efron and Thisted (1976) apply formula (6.19) to the Shake-\\nspeare word counts, and then use linear programming methods to bound\\nShakespeare‚Äôs unseen vocabulary from below at 35,000 words. (Shake-\\nspeare was actually less ‚Äúwordy‚Äù than his contemporaries, Marlow and\\nDonne.) ‚ÄúShall I die,‚Äù the possibly Shakespearean poem recovered in 1985,\\nis analyzed by a variety of empirical Bayes techniques in Thisted and Efron\\n(1987). Comparisons are made with other Elizabethan authors, none of\\nwhom seem likely candidates for authorship.\\n\\nThe Shakespeare word counts are from Spevack‚Äôs (1968) concordance.\\n(The Ô¨Årst concordance was compiled by hand in the mid 1800s, listing\\nevery word Shakespeare wrote and where it appeared, a full life‚Äôs labor.)\\nThe nodes example, Figure 6.3, is taken from Gholami et al. (2015).\\n\\n(cid:142)1 [p. 78] Formula (6.9). For any positive numbers c and d we have\\n\\nZ 1\\n\\n0\\n\\n(cid:18) c(cid:0)1e(cid:0)(cid:18)=d d(cid:18) D d c(cid:128).c/;\\n\\n(6.45)\\n\\nso combining gamma prior (6.8) with Poisson density (6.1) gives marginal\\ndensity\\n\\nf(cid:23);(cid:27) .x/ D\\n\\nR 1\\n0 (cid:18) (cid:23)Cx(cid:0)1e(cid:0)(cid:18)=(cid:13) d(cid:18)\\n(cid:27) (cid:23)(cid:128).(cid:23)/x≈†\\nD (cid:13) (cid:23)Cx(cid:128).(cid:23) C x/\\n(cid:27) (cid:23)(cid:128).(cid:23)/x≈†\\n\\n;\\n\\n(6.46)\\n\\nwhere (cid:13) D (cid:27)=.1 C (cid:27)/. Assuming independence among the counts yx\\n(which is exactly true if the customers act independently of each other and\\nN , the total number of them, is itself Poisson), the log likelihood function\\nfor the accident data is\\n\\nxmaxX\\n\\nxD0\\n\\nyx log ff(cid:23);(cid:27) .x/g :\\n\\n(6.47)\\n\\nHere xmax is some notional upper bound on the maximum possible number\\n\\n\\x0c90\\n\\nEmpirical Bayes\\n\\nof accidents for a single customer; since yx D 0 for x > 7 the choice of\\nxmax is irrelevant. The values . O(cid:23); O(cid:27)/ in (6.8) maximize (6.47).\\n\\n(cid:142)2 [p. 81] Formula (6.21). If N D P yx, the total number trapped, is assumed\\nto be Poisson, and if the N observed values xk are mutually independent,\\nthen a useful property of the Poisson distribution implies that the counts yx\\nare themselves approximately independent Poisson variates\\n\\nyx\\n\\nind(cid:24) Poi.ex/;\\n\\nfor x D 0; 1; 2; : : : ;\\n\\nin notation (6.17). Formula (6.19) and varfyxg D ex then give\\no\\nn OE.t/\\n\\next 2x:\\n\\nvar\\n\\nD X\\nx(cid:21)1\\n\\n(6.48)\\n\\n(6.49)\\n\\nSubstituting yx for ex produces (6.21). Section 11.5 of Efron (2010) shows\\nthat (6.49) is an upper bound on varf OE.t/g if N is considered Ô¨Åxed rather\\nthan Poisson.\\n\\n(cid:142)3 [p. 81] Formula (6.23). Combining the case x D 1 in (6.17) with (6.15)\\n\\nyields\\n\\ne1\\n\\n(cid:2)R 1\\n\\nE.t / D\\n\\n0 e(cid:0)(cid:18) g.(cid:18)/ d(cid:18) (cid:0) R 1\\nR 1\\n0 (cid:18)e(cid:0)(cid:18) g.(cid:18)/ d(cid:18)\\nSubstituting the gamma prior (6.8) for g.(cid:18)/, and using (6.45) three times,\\ngives formula (6.23).\\n\\n0 e(cid:0)(cid:18).1Ct/g.(cid:18)/ d(cid:18)(cid:3)\\n\\n(6.50)\\n\\n:\\n\\n\\x0c7\\n\\nJames‚ÄìStein Estimation and Ridge\\nRegression\\n\\nIf Fisher had lived in the era of ‚Äúapps,‚Äù maximum likelihood estimation\\nmight have made him a billionaire. Arguably the twentieth century‚Äôs most\\ninÔ¨Çuential piece of applied mathematics, maximum likelihood continues to\\nbe a prime method of choice in the statistician‚Äôs toolkit. Roughly speaking,\\nmaximum likelihood provides nearly unbiased estimates of nearly mini-\\nmum variance, and does so in an automatic way.\\n\\nThat being said, maximum likelihood estimation has shown itself to be\\nan inadequate and dangerous tool in many twenty-Ô¨Årst-century applica-\\ntions. Again speaking roughly, unbiasedness can be an unaffordable luxury\\nwhen there are hundreds or thousands of parameters to estimate at the same\\ntime.\\n\\nThe James‚ÄìStein estimator made this point dramatically in 1961, and\\nmade it in the context of just a few unknown parameters, not hundreds or\\nthousands. It begins the story of shrinkage estimation, in which deliberate\\nbiases are introduced to improve overall performance, at a possible danger\\nto individual estimates. Chapters 7 and 21 will carry on the story in its\\nmodern implementations.\\n\\n7.1 The James‚ÄìStein Estimator\\n\\nSuppose we wish to estimate a single parameter (cid:22) from observation x in\\nthe Bayesian situation\\n(cid:22) (cid:24)\\n\\nand xj(cid:22) (cid:24)\\n\\n.M; A/\\n\\n.(cid:22); 1/;\\n\\n(7.1)\\n\\nN\\n\\nN\\n\\nin which case (cid:22) has posterior distribution\\n\\n(cid:22)jx (cid:24)\\n\\n.M C B.x (cid:0) M /; B/\\n\\n(7.2)\\nas given in (5.21) (where we take (cid:27) 2 D 1 for convenience). The Bayes\\nestimator of (cid:22),\\n\\n≈íB D A=.A C 1/(cid:141)\\n\\nN\\n\\nO(cid:22)Bayes D M C B.x (cid:0) M /;\\n\\n(7.3)\\n\\n91\\n\\n\\x0c92\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\n\\nhas expected squared error\\n\\nn(cid:0) O(cid:22)Bayes (cid:0) (cid:22)(cid:1)2o\\n\\nE\\n\\nD B;\\n\\ncompared with 1 for the MLE O(cid:22)MLE D x,\\nn(cid:0) O(cid:22)MLE (cid:0) (cid:22)(cid:1)2o\\n\\nE\\n\\nD 1:\\n\\n(7.4)\\n\\n(7.5)\\n\\nIf, say, A D 1 in (7.1) then B D 1=2 and O(cid:22)Bayes has only half the risk of\\nthe MLE.\\n\\nThe same calculation applies to a situation where we have N indepen-\\n\\ndent versions of (7.1), say\\n\\n(cid:22) D .(cid:22)1; (cid:22)2; : : : ; (cid:22)N /0\\n\\nand x D .x1; x2; : : : ; xN /0;\\n\\n(7.6)\\n\\nwith\\n\\n(cid:22)i (cid:24)\\n\\n.M; A/\\n\\n(7.7)\\nindependently for i D 1; 2; : : : ; N . (Notice that the (cid:22)i differ from each\\nother, and that this situation is not the same as (5.22)‚Äì(5.23).) Let O(cid:22)Bayes\\nindicate the vector of individual Bayes estimates O(cid:22)Bayes\\nD M CB.xi (cid:0)M /,\\n\\n.(cid:22)i ; 1/;\\n\\nand xi j(cid:22)i (cid:24)\\n\\nN\\n\\nN\\n\\ni\\n\\nO(cid:22)Bayes D M C B.x (cid:0) M /;\\n\\n(cid:2)M D .M; M; : : : ; M /0(cid:3) ;\\n\\n(7.8)\\n\\nand similarly\\n\\nUsing (7.4) the total squared error risk of O(cid:22)Bayes is\\n\\nO(cid:22)MLE D x:\\n\\nn(cid:13)\\n(cid:13) O(cid:22)Bayes (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\n2o\\n\\nE\\n\\nD E\\n\\n(cid:16)\\nO(cid:22)Bayes\\ni\\n\\n(cid:0) (cid:22)i\\n\\n)\\n\\n(cid:17)2\\n\\n( N\\nX\\n\\ni D1\\n\\nD N (cid:1) B\\n\\n(7.9)\\n\\ncompared with\\n\\nn(cid:13)\\n(cid:13) O(cid:22)MLE (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\n2o\\n\\nE\\n\\nD N:\\n\\n(7.10)\\n\\nAgain, O(cid:22)Bayes has only B times the risk of O(cid:22)MLE.\\n\\nThis is Ô¨Åne if we know M and A (or equivalently M and B) in (7.1). If\\nnot, we might try to estimate them from x D .x1; x2; : : : ; xN /. Marginally,\\n(7.7) gives\\n\\nxi\\nThen OM D Nx is an unbiased estimate of M . Moreover,\\n\\n.M; A C 1/:\\n\\nN\\n\\nind(cid:24)\\n\\n(7.11)\\n\\nOB D 1 (cid:0) .N (cid:0) 3/=S\\n\\n\"\\n\\nS D\\n\\nN\\nX\\n\\niD1\\n\\n#\\n\\n.xi (cid:0) Nx/2\\n\\n(7.12)\\n\\n\\x0c7.1 The James‚ÄìStein Estimator\\n\\n93\\n\\nunbiasedly estimates B, as long as N > 3. (cid:142) The James‚ÄìStein estimator is (cid:142)1\\nthe plug-in version of (7.3),\\n\\nO(cid:22)JS\\ni\\n\\nD OM C OB\\n\\n(cid:16)\\nxi (cid:0) OM\\n\\n(cid:17)\\n\\nfor i D 1; 2; : : : ; N;\\n\\n(7.13)\\n\\nor equivalently O(cid:22)JS D OM C OB.x (cid:0) OM /, with OM D . OM ; OM ; : : : ; OM /0.\\n\\nAt this point the terminology ‚Äúempirical Bayes‚Äù seems especially apt:\\nBayesian model (7.7) leads to the Bayes estimator (7.8), which itself is\\nestimated empirically (i.e., frequentistically) from all the data x, and then\\napplied to the individual cases. Of course O(cid:22)JS cannot perform as well as\\nthe actual Bayes‚Äô rule O(cid:22)Bayes, but the increased risk is surprisingly modest.\\nThe expected squared risk of O(cid:22)JS under model (7.7) is(cid:142)\\nn(cid:13)\\n(cid:13) O(cid:22)JS (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\nD NB C 3.1 (cid:0) B/:\\n\\n(7.14)\\n\\n2o\\n\\nE\\n\\n(cid:142)2\\n\\nIf, say, N D 20 and A D 1, then (7.14) equals 11.5, compared with true\\nBayes risk 10 from (7.9), much less than risk 20 for O(cid:22)MLE.\\n\\nA defender of maximum likelihood might respond that none of this is\\nsurprising: Bayesian model (7.7) speciÔ¨Åes the parameters (cid:22)i to be clustered\\nmore or less closely around a central point M , while O(cid:22)MLE makes no such\\nassumption, and cannot be expected to perform as well. Wrong! Removing\\nthe Bayesian assumptions does not rescue O(cid:22)MLE, as James and Stein proved\\nin 1961:\\n\\nJames‚ÄìStein Theorem Suppose that\\n\\nindependently for i D 1; 2; : : : ; N , with N (cid:21) 4. Then\\n\\nxi j(cid:22)i (cid:24)\\n\\n.(cid:22)i ; 1/\\n\\nN\\n\\nn(cid:13)\\n(cid:13) O(cid:22)JS (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\n2o\\n\\nE\\n\\n< N D E\\n\\nn(cid:13)\\n(cid:13) O(cid:22)MLE (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\n2o\\n\\n(7.15)\\n\\n(7.16)\\n\\nfor all choices of (cid:22) 2\\nand x varying according to (7.15).)\\n\\nR\\n\\nN . (The expectations in (7.16) are with (cid:22) Ô¨Åxed\\n\\nIn the language of decision theory, equation (7.16) says that O(cid:22)MLE is\\ninadmissible: (cid:142) its total squared error risk exceeds that of O(cid:22)JS no matter (cid:142)3\\nwhat (cid:22) may be. This is a strong frequentist form of defeat for O(cid:22)MLE, not\\ndepending on Bayesian assumptions.\\n\\nThe James‚ÄìStein theorem came as a rude shock to the statistical world\\nof 1961. First of all, the defeat came on MLE‚Äôs home Ô¨Åeld: normal observa-\\ntions with squared error loss. Fisher‚Äôs ‚Äúlogic of inductive inference,‚Äù Chap-\\nter 4, claimed that O(cid:22)MLE D x was the obviously correct estimator in the uni-\\nvariate case, an assumption tacitly carried forward to multiparameter linear\\n\\n\\x0c94\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\n\\nregression problems, where versions of O(cid:22)MLE were predominant. There are\\nstill some good reasons for sticking with O(cid:22)MLE in low-dimensional prob-\\nlems, as discussed in Section 7.4. But shrinkage estimation, as exempliÔ¨Åed\\nby the James‚ÄìStein rule, has become a necessity in the high-dimensional\\nsituations of modern practice.\\n\\n7.2 The Baseball Players\\nThe James‚ÄìStein theorem doesn‚Äôt say by how much O(cid:22)JS beats O(cid:22)MLE. If the\\nimprovement were inÔ¨Ånitesimal nobody except theorists would be inter-\\nested. In favorable situations the gains can in fact be substantial, as sug-\\ngested by (7.14). One such situation appears in Table 7.1. The batting av-\\nerages1 of 18 Major League players have been observed over the 1970 sea-\\nson. The column labeled MLE reports the player‚Äôs observed average over\\nhis Ô¨Årst 90 at bats; TRUTH is the average over the remainder of the 1970\\nseason (370 further at bats on average). We would like to predict TRUTH\\nfrom the early-season observations.\\n\\nThe column labeled JS in Table 7.1 is from a version of the James‚Äì\\nStein estimator applied to the 18 MLE numbers. We suppose that each\\nplayer‚Äôs MLE value pi (his batting average in the Ô¨Årst 90 tries) is a binomial\\nproportion,\\n\\npi (cid:24) Bi.90; Pi /=90:\\n\\n(7.17)\\n\\nHere Pi is his true average, how he would perform over an inÔ¨Ånite number\\nof tries; TRUTHi is itself a binomial proportion, taken over an average of\\n370 more tries per player.\\n\\nAt this point there are two ways to proceed. The simplest uses a normal\\n\\napproximation to (7.17),\\n\\nwhere (cid:27) 2\\n\\n0 is the binomial variance\\n\\npi P(cid:24)\\n\\n.Pi ; (cid:27) 2\\n\\n0 /;\\n\\nN\\n\\n(7.18)\\n\\n(7.19)\\nwith Np D 0:254 the average of the pi values. Letting xi D pi =(cid:27)0, applying\\n(7.13), and transforming back to OpJS\\ni , gives James‚ÄìStein estimates\\ni\\n\\nD Np.1 (cid:0) Np/=90;\\n\\n(cid:27) 2\\n0\\n\\nD (cid:27)0 O(cid:22)JS\\n(cid:20)\\n1 (cid:0) .N (cid:0) 3/(cid:27) 2\\nP.pi (cid:0) Np/2\\n\\n0\\n\\nOpJS\\ni\\n\\nD Np C\\n\\n(cid:21)\\n\\n.pi (cid:0) Np/:\\n\\n(7.20)\\n\\n1 Batting average D # hits =# at bats, that is, the success rate. For example, Player 1 hits\\n\\nsuccessfully 31 times in his Ô¨Årst 90 tries, for batting average 31=90 D 0:345. This data\\nis based on 1970 Major League performances, but is partly artiÔ¨Åcial; see the endnotes.\\n\\n\\x0c7.2 The Baseball Players\\n\\n95\\n\\nTable 7.1 Eighteen baseball players; MLE is batting average in Ô¨Årst 90 at\\nbats; TRUTH is average in remainder of 1970 season; James‚ÄìStein\\nestimator JS is based on arcsin transformation of MLEs. Sum of squared\\nerrors for predicting TRUTH: MLE .0425, JS .0218.\\n\\nPlayer\\n\\nMLE\\n\\nJS\\n\\nTRUTH\\n\\nx\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n\\n.345\\n.333\\n.322\\n.311\\n.289\\n.289\\n.278\\n.255\\n.244\\n.233\\n.233\\n.222\\n.222\\n.222\\n.211\\n.211\\n.200\\n.145\\n\\n.283\\n.279\\n.276\\n.272\\n.265\\n.264\\n.261\\n.253\\n.249\\n.245\\n.245\\n.242\\n.241\\n.241\\n.238\\n.238\\n.234\\n.212\\n\\n.298\\n.346\\n.222\\n.276\\n.263\\n.273\\n.303\\n.270\\n.230\\n.264\\n.264\\n.210\\n.256\\n.269\\n.316\\n.226\\n.285\\n.200\\n\\n11.96\\n11.74\\n11.51\\n11.29\\n10.83\\n10.83\\n10.60\\n10.13\\n9.88\\n9.64\\n9.64\\n9.40\\n9.39\\n9.39\\n9.14\\n9.14\\n8.88\\n7.50\\n\\nA second approach begins with the arcsin transformation\\n(cid:19)1=2#\\n\\nxi D 2.n C 0:5/1=2 sin(cid:0)1\\n\\n\"(cid:18) npi C 0:375\\nn C 0:75\\n\\n;\\n\\n(7.21)\\n\\nn D 90 (column labeled x in Table 7.1), a classical device that produces\\napproximate normal deviates of variance 1,\\n\\nxi P(cid:24)\\n\\n.(cid:22)i ; 1/;\\n\\n(7.22)\\n\\nN\\nwhere (cid:22)i is transformation (7.21) applied to TRUTHi . Using (7.13) gives\\nO(cid:22)JS\\n\\ni , which is Ô¨Ånally inverted back to the binomial scale,\\n\\nOpJS\\ni\\n\\nD 1\\nn\\n\\nh\\n.n C 0:75/\\n\\n(cid:16)\\n\\nsin\\n\\n(cid:16)\\n\\np\\n\\nO(cid:22)JS\\ni\\nn C 0:5\\n\\n2\\n\\n(cid:17)(cid:17)2\\n\\ni\\n(cid:0) 0:375\\n\\n(7.23)\\n\\nFormulas (7.20) and (7.23) yielded nearly the same estimates for the\\nbaseball players; the JS column in Table 7.1 is from (7.23). James and\\nStein‚Äôs theorem requires normality, but the James‚ÄìStein estimator often\\n\\n\\x0c96\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\n\\nworks perfectly well in less ideal situations. That is the case in Table 7.1:\\n\\n18\\nX\\n\\n.MLEi (cid:0)TRUTHi /2 D 0:0425 while\\n\\n18\\nX\\n\\n.JSi (cid:0)TRUTHi /2 D 0:0218:\\n\\ni D1\\n\\ni D1\\n\\n(7.24)\\nIn other words, the James‚ÄìStein estimator reduced total predictive squared\\nerror by about 50%.\\n\\nFigure 7.1 Eighteen baseball players; top line MLE, middle\\nJames‚ÄìStein, bottom true values. Only 13 points are visible, since\\nthere are ties.\\n\\nThe James‚ÄìStein rule describes a shrinkage estimator, each MLE value\\nxi being shrunk by factor OB toward the grand mean OM D Nx (7.13). ( OB D\\n0:34 in (7.20).) Figure 7.1 illustrates the shrinking process for the baseball\\nplayers.\\n\\nTo see why shrinking might make sense, let us return to the original\\nBayes model (7.8) and take M D 0 for simplicity, so that the xi are\\n.0; A C 1/ (7.11). Even though each xi is unbiased for its\\nmarginally\\nparameter (cid:22)i , as a group they are ‚Äúoverdispersed,‚Äù\\n\\nN\\n\\n( N\\nX\\n\\n)\\n\\nx2\\ni\\n\\nE\\n\\ni D1\\n\\nD N.A C 1/\\n\\ncompared with E\\n\\n( N\\nX\\n\\n)\\n\\n(cid:22)2\\ni\\n\\ni D1\\n\\nD NA: (7.25)\\n\\nThe sum of squares of the MLEs exceeds that of the true values by expected\\namount N ; shrinkage improves group estimation by removing the excess.\\n\\n0.150.200.250.300.35 llllllllllllllllllMLEllllllllllllllllllJAMES‚àíSTEINllllllllllllllllllTRUEBatting averages\\x0c7.3 Ridge Regression\\n\\n97\\n\\nIn fact the James‚ÄìStein rule overshrinks the data, as seen in the bottom\\ntwo lines of Figure 7.1, a property it inherits from the underlying Bayes\\nmodel: the Bayes estimates O(cid:22)Bayes\\n\\nD Bxi have\\n\\ni\\n\\nE\\n\\n( N\\nX\\n\\niD1\\n\\n)\\n\\n(cid:16)\\nO(cid:22)Bayes\\ni\\n\\n(cid:17)2\\n\\nD NB 2.A C 1/ D NA\\n\\nA\\nA C 1\\n\\n;\\n\\n(7.26)\\n\\novershrinking E.P (cid:22)2\\ni / D NA by factor A=.A C 1/. We could use the\\nless extreme shrinking rule Q(cid:22)i D\\nBxi , which gives the correct expected\\nsum of squares NA, but a larger expected sum of squared estimation errors\\nEfP. Q(cid:22)i (cid:0) (cid:22)i /2jxg.\\n\\np\\n\\nThe most extreme shrinkage rule would be ‚Äúall the way,‚Äù that is, to\\n\\nO(cid:22)NULL\\ni\\n\\nD Nx\\n\\nfor i D 1; 2; : : : ; N;\\n\\n(7.27)\\n\\nNULL indicating that in a classical sense we have accepted the null hy-\\npothesis of no differences among the (cid:22)i values. (This gave P.Pi (cid:0) Np/2 D\\n0:0266 for the baseball data (7.24).) The James‚ÄìStein estimator is a data-\\nbased rule for compromising between the null hypothesis of no differences\\nand the MLE‚Äôs tacit assumption of no relationship at all among the (cid:22)i\\nvalues. In this sense it blurs the classical distinction between hypothesis\\ntesting and estimation.\\n\\n7.3 Ridge Regression\\n\\nLinear regression, perhaps the most widely used estimation technique, is\\nbased on a version of O(cid:22)MLE. In the usual notation, we observe an n-dimen-\\nsional vector y D .y1; y2; : : : ; yn/0 from the linear model\\n\\ny D X Àá C (cid:15):\\n\\n(7.28)\\n\\nHere X is a known n (cid:2) p structure matrix, Àá is an unknown p-dimensional\\nparameter vector, while the noise vector (cid:15) D .(cid:15)1; (cid:15)2; : : : ; (cid:15)n/0 has its com-\\nponents uncorrelated and with constant variance (cid:27) 2,\\n\\n(cid:15) (cid:24) .0; (cid:27) 2I/;\\n\\n(7.29)\\n\\nwhere I is the n (cid:2) n identity matrix. Often (cid:15) is assumed to be multivariate\\nnormal,\\n\\n(cid:15) (cid:24)\\n\\nN\\n\\nn.0; (cid:27) 2I/;\\n\\n(7.30)\\n\\nbut that is not required for most of what follows.\\n\\n\\x0c98\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\nThe least squares estimate OÀá, going back to Gauss and Legendre in the\\n\\nearly 1800s, is the minimizer of the total sum of squared errors,\\n\\nIt is given by\\n\\nOÀá D arg min\\n\\nÀöky (cid:0) X Àák2(cid:9) :\\n\\nÀá\\n\\nOÀá D S (cid:0)1X 0y;\\n\\nwhere S is the p (cid:2) p inner product matrix\\n\\nS D X 0X I\\n\\n(7.31)\\n\\n(7.32)\\n\\n(7.33)\\n\\nOÀá is unbiased for Àá and has covariance matrix (cid:27) 2S (cid:0)1,\\n\\nOÀá (cid:24) (cid:0)Àá; (cid:27) 2S (cid:0)1(cid:1) :\\n(7.34)\\nIn the normal case (7.30) OÀá is the MLE of Àá. Before 1950 a great deal\\nof effort went into designing matrices X such that S (cid:0)1 could be feasibly\\ncalculated, which is now no longer a concern.\\n\\nA great advantage of the linear model is that it reduces the number of\\nunknown parameters to p (or p C 1 including (cid:27) 2), no matter how large n\\nmay be. In the kidney data example of Section 1.1, n D 157 while p D 2.\\nIn modern applications, however, p has grown larger and larger, sometimes\\ninto the thousands or more, as we will see in Part III, causing statisticians\\nagain to confront the limitations of high-dimensional unbiased estimation.\\nRidge regression is a shrinkage method designed to improve the estima-\\ntion of Àá in linear models. By transformations (cid:142) we can standardize (7.28)\\nso that the columns of X each have mean 0 and sum of squares 1, that is,\\n\\nSi i D 1\\n\\nfor i D 1; 2; : : : ; p:\\n\\n(7.35)\\n\\n(This puts the regression coefÔ¨Åcients Àá1; Àá2; : : : ; Àáp on comparable scales.)\\nFor convenience, we also assume Ny D 0. A ridge regression estimate OÀá.(cid:21)/\\nis deÔ¨Åned, for (cid:21) (cid:21) 0, to be\\n\\nOÀá.(cid:21)/ D .S C (cid:21)I/(cid:0)1X 0y D .S C (cid:21)I/(cid:0)1S OÀá\\n(7.36)\\n(using (7.32)); OÀá.(cid:21)/ is a shrunken version of OÀá, the bigger (cid:21) the more\\nextreme the shrinkage: OÀá.0/ D OÀá while OÀá.1/ equals the vector of zeros.\\nRidge regression effects can be quite dramatic. As an example, con-\\nsider the diabetes data, partially shown in Table 7.2, in which 10 prediction\\nvariables measured at baseline‚Äîage, sex, bmi (body mass index), map\\n(mean arterial blood pressure), and six blood serum measurements‚Äîhave\\n\\n(cid:142)4\\n\\n\\x0c7.3 Ridge Regression\\n\\n99\\n\\nTable 7.2 First 7 of n D 442 patients in the diabetes study; we wish to\\npredict disease progression at one year ‚Äúprog‚Äù from the 10 baseline\\nmeasurements age, sex, . . . , glu.\\n\\nage\\n\\nsex\\n\\nbmi\\n\\nmap\\n\\ntc\\n\\nldl\\n\\nhdl\\n\\ntch\\n\\nltg\\n\\nglu\\n\\nprog\\n\\n59\\n48\\n72\\n24\\n50\\n23\\n36\\n:::\\n\\n1\\n0\\n1\\n0\\n0\\n0\\n1\\n:::\\n\\n32.1\\n21.6\\n30.5\\n25.3\\n23.0\\n22.6\\n22.0\\n:::\\n\\n101\\n87\\n93\\n84\\n101\\n89\\n90\\n:::\\n\\n157\\n183\\n156\\n198\\n192\\n139\\n160\\n:::\\n\\n93.2\\n103.2\\n93.6\\n131.4\\n125.4\\n64.8\\n99.6\\n:::\\n\\n38\\n70\\n41\\n40\\n52\\n61\\n50\\n:::\\n\\n4\\n3\\n4\\n5\\n4\\n2\\n3\\n:::\\n\\n2.11\\n1.69\\n2.03\\n2.12\\n1.86\\n1.82\\n1.72\\n:::\\n\\n87\\n69\\n85\\n89\\n80\\n68\\n82\\n:::\\n\\n151\\n75\\n141\\n206\\n135\\n97\\n138\\n:::\\n\\nbeen obtained for n D 442 patients. We wish to use the 10 variables to pre-\\ndict prog, a quantitative assessment of disease progression one year after\\nbaseline. In this case X is the 442 (cid:2) 10 matrix of standardized predictor\\nvariables, and y is prog with its mean subtracted off.\\n\\nFigure 7.2 Ridge coefÔ¨Åcient trace for the standardized diabetes\\ndata.\\n\\n‚àí5000500lb^(l)0.000.050.150.200.250.1llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllagesexbmimaptcldlhdltchltgglu\\x0c100\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\n\\nTable 7.3 Ordinary least squares estimate OÀá.0/ compared with ridge\\nregression estimate OÀá.0:1/ with (cid:21) D 0:1. The columns sd(0) and sd(0.1)\\nare their estimated standard errors. (Here (cid:27) was taken to be 54.1, the\\nusual OLS estimate based on model (7.28).)\\n\\nOÀá.0:1/\\n\\nOÀá.0/\\n(cid:0)10.0\\nage\\n1.3\\nsex (cid:0)239.8 (cid:0)207.2\\nbmi\\n489.7\\n519.8\\nmap\\n301.8\\n324.4\\n(cid:0)83.5\\n(cid:0)792.2\\ntc\\n(cid:0)70.8\\nldl\\n476.7\\n101.0 (cid:0)188.7\\nhdl\\ntch\\n115.7\\n177.1\\nltg\\n443.8\\n751.3\\nglu\\n86.7\\n67.6\\n\\nsd(0)\\n\\nsd(0.1)\\n\\n59.7\\n61.2\\n66.5\\n65.3\\n416.2\\n338.6\\n212.3\\n161.3\\n171.7\\n65.9\\n\\n52.7\\n53.2\\n56.3\\n55.7\\n43.6\\n52.4\\n58.4\\n70.8\\n58.4\\n56.6\\n\\nFigure 7.2 vertically plots the 10 coordinates of OÀá.(cid:21)/ as the ridge pa-\\nrameter (cid:21) increases from 0 to 0.25. Four of the coefÔ¨Åcients change rapidly\\nat Ô¨Årst. Table 7.3 compares OÀá.0/, that is the usual estimate OÀá, with OÀá.0:1/.\\nPositive coefÔ¨Åcients predict increased disease progression. Notice that ldl,\\nthe ‚Äúbad cholesterol‚Äù measurement, goes from being a strongly positive\\npredictor in OÀá to a mildly negative one in OÀá.0:1/.\\n\\nThere is a Bayesian rationale for ridge regression. Assume that the noise\\n\\nvector (cid:15) is normal as in (7.30), so that\\n\\nOÀá (cid:24)\\n\\np\\n\\nN\\n\\n(cid:0)Àá; (cid:27) 2S (cid:0)1(cid:1)\\n\\nrather than just (7.34). Then the Bayesian prior\\n\\nmakes\\n\\nÀá (cid:24)\\n\\np\\nN\\n\\n(cid:18)\\n\\n0;\\n\\n(cid:19)\\n\\n(cid:27) 2\\n(cid:21)\\n\\nI\\n\\no\\n\\nn\\nÀáj OÀá\\n\\nE\\n\\nD .S C (cid:21)I/(cid:0)1S OÀá;\\n\\n(7.37)\\n\\n(7.38)\\n\\n(7.39)\\n\\nthe same as the ridge regression estimate OÀá.(cid:21)/ (using (5.23) with M D 0,\\nA D .(cid:27) 2=(cid:21)/I, and ‚Ä† D .S =(cid:27) 2/(cid:0)1). Ridge regression amounts to an\\nincreased prior belief that Àá lies near 0.\\n\\n(cid:142)5\\n\\nThe last two columns of Table 7.3 compare the standard deviations (cid:142) of\\nOÀá and OÀá.0:1/. Ridging has greatly reduced the variability of the estimated\\n\\n\\x0c7.3 Ridge Regression\\n\\n101\\n\\nregression coefÔ¨Åcients. This does not guarantee that the corresponding es-\\ntimate of (cid:22) D X Àá,\\n\\nO(cid:22).(cid:21)/ D X OÀá.(cid:21)/;\\n(7.40)\\nwill be more accurate than the ordinary least squares estimate O(cid:22) D X OÀá.\\nWe have (deliberately) introduced bias, and the squared bias term coun-\\nteracts some of the advantage of reduced variability. The Cp calculations\\nof Chapter 12 suggest that the two effects nearly offset each other for the\\ndiabetes data. However, if interest centers on the coefÔ¨Åcients of Àá, then\\nridging can be crucial, as Table 7.3 emphasizes.\\n\\nBy current standards, p D 10 is a small number of predictors. Data sets\\nwith p in the thousands, and more, will show up in Part III. In such situa-\\ntions the scientist is often looking for a few interesting predictor variables\\nhidden in a sea of uninteresting ones: the prior belief is that most of the Àái\\nvalues lie near zero. Biasing the maximum likelihood estimates OÀái toward\\nzero then becomes a necessity.\\n\\nThere is still another way to motivate the ridge regression estimator\\n\\nOÀá.(cid:21)/:\\n\\nOÀá.(cid:21)/ D arg min\\n\\nfky (cid:0) X Àák2 C (cid:21)kÀák2g:\\n\\n(7.41)\\n\\nÀá\\n\\nDifferentiating the term in brackets with respect to Àá shows that OÀá.(cid:21)/ D\\n.S C (cid:21)I/(cid:0)1X 0y as in (7.36). If (cid:21) D 0 then (7.41) describes the ordinary\\nleast squares algorithm; (cid:21) > 0 penalizes choices of Àá having kÀák large,\\nbiasing OÀá.(cid:21)/ toward the origin.\\n\\nVarious terminologies are used to describe algorithms such as (7.41): pe-\\nnalized least squares; penalized likelihood; maximized a-posteriori proba-\\nbility (MAP);(cid:142)and, generically, regularization describes almost any method (cid:142)6\\nthat tamps down statistical variability in high-dimensional estimation or\\nprediction problems.\\n\\nA wide variety of penalty terms are in current use, the most inÔ¨Çuential\\n\\none involving the ‚Äú`1 norm‚Äù kÀák1 D Pp\\n\\nQÀá.(cid:21)/ D arg min\\n\\n1\\n\\njÀáj j,\\nfky (cid:0) X Àák2 C (cid:21)kÀák1g;\\n\\n(7.42)\\n\\nÀá\\n\\nthe so-called lasso estimator, Chapter 16. Despite the Bayesian provenance,\\nmost regularization research is carried out frequentistically, with various\\npenalty terms investigated for their probabilistic behavior regarding esti-\\nmation, prediction, and variable selection.\\n\\nIf we apply the James‚ÄìStein rule to the normal model (7.37), we get a\\n\\ndifferent shrinkage rule(cid:142) for OÀá, say QÀáJS,\\n\\n(cid:142)7\\n\\n\\x0c102\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\n\\n\"\\n\\nQÀáJS D\\n\\n1 (cid:0) .p (cid:0) 2/(cid:27) 2\\nOÀá0S OÀá\\n\\n#\\n\\nOÀá:\\n\\n(7.43)\\n\\nLetting Q(cid:22)JS D X QÀáJS be the corresponding estimator of (cid:22) D Efyg in\\n(7.28), the James‚ÄìStein Theorem guarantees that\\n\\nn(cid:13)\\n(cid:13) Q(cid:22)JS (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\n2o\\n\\nE\\n\\n< p(cid:27) 2\\n\\n(7.44)\\n\\nno matter what Àá is, as long as p (cid:21) 3.2 There is no such guarantee for\\nridge regression, and no foolproof way to choose the ridge parameter (cid:21).\\nOn the other hand, QÀáJS does not stabilize the coordinate standard devia-\\ntions, as in the sd(0.1) column of Table 7.3. The main point here is that at\\npresent there is no optimality theory for shrinkage estimation. Fisher pro-\\nvided an elegant theory for optimal unbiased estimation. It remains to be\\nseen whether biased estimation can be neatly codiÔ¨Åed.\\n\\n7.4 Indirect Evidence 2\\n\\nThere is a downside to shrinkage estimation, which we can examine by\\nreturning to the baseball data of Table 7.1. One thousand simulations were\\nrun, each one generating simulated batting averages\\n\\np(cid:3)\\ni\\n\\n(cid:24) Bi.90; TRUTHi /=90\\n\\ni D 1; 2; : : : ; 18:\\n\\n(7.45)\\n\\nThese gave corresponding James‚ÄìStein (JS) estimates (7.20), with (cid:27) 2\\n0\\nNp(cid:3).1 (cid:0) Np(cid:3)/=90.\\n\\nD\\n\\nTable 7.4 shows the root mean square error for the MLE and JS estimates\\n\\nover 1000 simulations for each of the 18 players,\\n\\n2\\n\\n4\\n\\n1\\n1000\\n\\n1000\\nX\\n\\nj D1\\n\\n3\\n\\n1=2\\n\\n.p(cid:3)\\nij\\n\\n(cid:0) TRUTHi /2\\n\\n5\\n\\nand\\n\\n2\\n\\n4\\n\\n1\\n1000\\n\\n1000\\nX\\n\\nj D1\\n\\n3\\n\\n1=2\\n\\n. Op(cid:3) JS\\nij\\n\\n(cid:0) TRUTHi /2\\n\\n5\\n\\n(7.46)\\nAs foretold by the James‚ÄìStein Theorem, the JS estimates are easy victors\\nin terms of total squared error (summing over all 18 players). However,\\nOp(cid:3) JS\\ni for 4 of the 18 players, losing badly in the case\\ni\\nof player 2.\\n\\nloses to Op(cid:3) MLE\\n\\nD p(cid:3)\\n\\ni\\n\\nHistograms comparing the 1000 simulations of p(cid:3)\\n\\nfor player 2 appear in Figure 7.3. Strikingly, all 1000 of the Op(cid:3) JS\\n\\ni with those of Op(cid:3) JS\\n2j values lie\\n\\ni\\n\\n2 Of course we are assuming (cid:27) 2 is known in (7.43); if it is estimated, some of the\\n\\nimprovement erodes away.\\n\\n\\x0c7.4 Indirect Evidence 2\\n\\n103\\n\\nTable 7.4 Simulation study comparing root mean square errors for MLE\\nand JS estimators (7.20) as estimates of TRUTH. Total mean square errors\\n.0384 (MLE) and .0235 (JS). Asterisks indicate four players for whom\\nrmsJS exceeded rmsMLE; these have two largest and two smallest\\nTRUTH values (player 2 is Clemente). Column rmsJS1 is for the limited\\ntranslation version of JS that bounds shrinkage to within one standard\\ndeviation of the MLE.\\n\\nPlayer\\n\\nTRUTH\\n\\nrmsMLE\\n\\nrmsJS\\n\\nrmsJS1\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n\\n.298\\n.346*\\n.222\\n.276\\n.263\\n.273\\n.303\\n.270\\n.230\\n.264\\n.264\\n.210*\\n.256\\n.269\\n.316*\\n.226\\n.285\\n.200*\\n\\n.046\\n.049\\n.044\\n.048\\n.047\\n.046\\n.047\\n.049\\n.044\\n.047\\n.047\\n.043\\n.045\\n.048\\n.048\\n.045\\n.046\\n.043\\n\\n.033\\n.077\\n.042\\n.015\\n.011\\n.014\\n.037\\n.012\\n.034\\n.011\\n.012\\n.053\\n.014\\n.012\\n.049\\n.038\\n.022\\n.062\\n\\n.032\\n.056\\n.038\\n.023\\n.020\\n.021\\n.035\\n.022\\n.033\\n.021\\n.020\\n.044\\n.020\\n.021\\n.043\\n.036\\n.026\\n.048\\n\\nbelow TRUTH2 D 0:346. Player 2 could have had a legitimate complaint if\\nthe James‚ÄìStein estimate were used to set his next year‚Äôs salary.\\n\\ni\\n\\nThe four losing cases for Op(cid:3) JS\\n\\nare the players with the two largest and\\ntwo smallest values of the TRUTH. Shrinkage estimators work against cases\\nthat are genuinely outstanding (in a positive or negative sense). Player 2\\nwas Roberto Clemente. A better informed Bayesian, that is, a baseball fan,\\nwould know that Clemente had led the league in batting over the previ-\\nous several years, and shouldn‚Äôt be thrown into a shrinkage pool with 17\\nordinary hitters.\\n\\nOf course the James‚ÄìStein estimates were more accurate for 14 of the\\n18 players. Shrinkage estimation tends to produce better results in general,\\nat the possible expense of extreme cases. Nobody cares much about Cold\\nWar batting averages, but if the context were the efÔ¨Åcacies of 18 new anti-\\ncancer drugs the stakes would be higher.\\n\\n\\x0c104\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\n\\nFigure 7.3 Comparing MLE estimates (solid) with JS estimates\\n(line) for Clemente; 1000 simulations, 90 at bats each.\\n\\nCompromise methods are available. The rmsJS1 column of Table 7.4\\nin which shrinkage is not al-\\n\\nrefers to a limited translation version of OpJS\\ni\\nlowed to diverge more than one (cid:27)0 unit from Opi ; in formulaic terms,\\n\\nOpJS 1\\ni\\n\\nD min Àömax (cid:0) OpJS\\n\\ni ; Opi (cid:0) (cid:27)0\\n\\n(cid:1) ; Opi C (cid:27)0\\n\\n(cid:9) :\\n\\n(7.47)\\n\\nThis mitigates the Clemente problem while still gaining most of the shrink-\\nage advantages.\\n\\nThe use of indirect evidence amounts to learning from the experience\\nof others, each batter learning from the 17 others in the baseball exam-\\nples. ‚ÄúWhich others?‚Äù is a key question in applying computer-age methods.\\nChapter 15 returns to the question in the context of false-discovery rates.\\n\\n7.5 Notes and Details\\n\\nThe Bayesian motivation emphasized in Chapters 6 and 7 is anachronistic:\\noriginally the work emerged mainly from frequentist considerations and\\nwas justiÔ¨Åed frequentistically, as in Robbins (1956). Stein (1956) proved\\nthe inadmissibility of O(cid:22)MLE, the neat version of O(cid:22)JS appearing in James\\nand Stein (1961) (Willard James was Stein‚Äôs graduate student); O(cid:22)JS is it-\\nself inadmissible, being everywhere improvable by changing OB in (7.13)\\n\\n p^Frequency0.200.250.300.350.400.450.500.55050100150200250300350Truth 0.346p^ MLEp^ James‚àíStein\\x0c7.5 Notes and Details\\n105\\nto max. OB; 0/. This in turn is inadmissible, but further gains tend to the\\nminuscule.\\n\\nIn a series of papers in the early 1970s, Efron and Morris emphasized\\nthe empirical Bayes motivation of the James‚ÄìStein rule, Efron and Morris\\n(1972) giving the limited translation version (7.47). The baseball data in its\\noriginal form appears in Table 1.1 of Efron (2010). Here the original 45 at\\nbats recorded for each player have been artiÔ¨Åcially augmented by adding\\n45 binomial draws, Bi.45; TRUTHi / for player i. This gives a somewhat\\nless optimistic view of the James‚ÄìStein rule‚Äôs performance.\\n\\n‚ÄúStein‚Äôs paradox in statistics,‚Äù Efron and Morris‚Äô title for their 1977 Sci-\\nentiÔ¨Åc American article, catches the statistics world‚Äôs sense of discomfort\\nwith the James‚ÄìStein theorem. Why should our estimate for Player A go\\nup or down depending on the other players‚Äô performances? This is the\\nquestion of direct versus indirect evidence, raised again in the context of\\nhypothesis testing in Chapter 15. Unbiased estimation has great scientiÔ¨Åc\\nappeal, so the argument is by no means settled.\\n\\nRidge regression was introduced into the statistics literature by Hoerl\\nand Kennard (1970). It appeared previously in the numerical analysis liter-\\nature as Tikhonov regularization.\\n\\nof freedom, Z (cid:24) (cid:31)2\\n\\n(cid:142)1 [p. 93] Formula (7.12). If Z has a chi-squared distribution with (cid:23) degrees\\n(cid:23) (that is, Z (cid:24) Gam.(cid:23)=2; 2/ in Table 5.1), it has density\\nf .z/ D z(cid:23)=2(cid:0)1e(cid:0)z=2\\n2(cid:23)=2(cid:128).(cid:23)=2/\\n\\nfor z (cid:21) 0;\\n\\n(7.48)\\n\\nyielding\\n(cid:26) 1\\n(cid:27)\\nz\\n\\nE\\n\\nZ 1\\n\\nD\\n\\n0\\n\\nz(cid:23)=2(cid:0)2e(cid:0)z=2\\n2(cid:23)=2(cid:128).(cid:23)=2/\\n\\ndz D 2(cid:23)=2(cid:0)1\\n2(cid:23)=2\\n\\n(cid:128).(cid:23)=2 (cid:0) 1/\\n(cid:128).(cid:23)=2/\\n\\nD 1\\n\\n(cid:23) (cid:0) 2\\n\\n:\\n\\n(7.49)\\n\\nBut standard results, starting from (7.11), show that S (cid:24) .A C 1/(cid:31)2\\nWith (cid:23) D N (cid:0) 1 in (7.49),\\n\\nN (cid:0)1.\\n\\nE\\n\\n(cid:27)\\n\\n(cid:26) N (cid:0) 3\\nS\\n\\nD 1\\n\\nA C 1\\n\\n;\\n\\n(7.50)\\n\\nverifying (7.12).\\n\\n(cid:142)2 [p. 93] Formula (7.14). First consider the simpler situation where M in\\n(7.11) is known to equal zero, in which case the James‚ÄìStein estimator is\\n\\nD OBxi\\n\\nwith OB D 1 (cid:0) .N (cid:0) 2/=S;\\n\\nwhere S D PN\\n\\ni . For convenient notation let\\nOC D 1 (cid:0) OB D .N (cid:0) 2/=S and C D 1 (cid:0) B D 1=.A C 1/:\\n\\nO(cid:22)JS\\ni\\n1 x2\\n\\n(7.51)\\n\\n(7.52)\\n\\n\\x0c106\\n\\nJames‚ÄìStein Estimation and Ridge Regression\\n\\nThe conditional distribution (cid:22)i jx (cid:24)\\nn(cid:0) O(cid:22)JS\\n\\n(cid:0) (cid:22)i\\n\\nE\\n\\no\\n\\n(cid:1)2Àá\\nÀá\\nÀáx\\n\\ni\\n\\n.Bxi ; B/ gives\\n\\nN\\n\\nD B C . OC (cid:0) C /2x2\\ni ;\\n\\n(7.53)\\n\\nand, adding over the N coordinates,\\n\\nn(cid:13)\\n(cid:13) O(cid:22)JS (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\nE\\n\\no\\n\\n2Àá\\nÀá\\nÀáx\\n\\nD NB C . OC (cid:0) C /2S:\\n\\n(7.54)\\n\\nThe marginal distribution S (cid:24) .A C 1/(cid:31)2\\ncalculation,\\n\\nN and (7.49) yields, after a little\\n\\nn\\n. OC (cid:0) C /2S\\n\\no\\n\\nE\\n\\nD 2.1 (cid:0) B/;\\n\\nand so\\n\\nn(cid:13)\\n(cid:13) O(cid:22)JS (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\n2o\\n\\nE\\n\\nD NB C 2.1 (cid:0) B/:\\n\\n(7.55)\\n\\n(7.56)\\n\\nBy orthogonal transformations, in situation (7.7), where M is not as-\\nsumed to be zero, O(cid:22)JS can be represented as the sum of two parts: a JS\\nestimate in N (cid:0) 1 dimensions but with M D 0 as in (7.51), and a MLE\\nestimate of the remaining one coordinate. Using (7.56) this gives\\n\\nn(cid:13)\\n(cid:13) O(cid:22)JS (cid:0) (cid:22)(cid:13)\\n(cid:13)\\n\\n2o\\n\\nE\\n\\nD .N (cid:0) 1/B C 2.1 (cid:0) B/ C 1\\n\\nD NB C 3.1 (cid:0) B/;\\n\\n(7.57)\\n\\nwhich is (7.14).\\n\\n(cid:142)3 [p. 93] The James‚ÄìStein Theorem. Stein (1981) derived a simpler proof of\\n\\nthe JS Theorem that appears in Section 1.2 of Efron (2010).\\n\\nestimates map in the obvious way:\\n\\n(cid:142)4 [p. 98] Transformations to form (7.35). The linear regression model (7.28)\\nis equivariant under scale changes of the variables xj . What this means\\nis that the space of Ô¨Åts using linear combinations of the xj is the same as\\nthe space of linear combinations using scaled versions Qxj D xj =sj , with\\nsj > 0. Furthermore, the least squares Ô¨Åts are the same, and the coefÔ¨Åcient\\nOQÀáj D sj\\nNot so for ridge regression. Changing the scales of the columns of X\\nwill generally lead to different Ô¨Åts. Using the penalty version (7.41) of\\nridge regression, we see that the penalty term kÀák2 D P\\nj treats all the\\ncoefÔ¨Åcients as equals. This penalty is most natural if all the variables are\\nmeasured on the same scale. Hence we typically use for sj the standard\\ndeviation of variable xj , which leads to (7.35). Furthermore, with ridge\\nregression we typically do not penalize the intercept. This can be achieved\\n\\nj Àá2\\n\\nOÀáj .\\n\\n\\x0c7.5 Notes and Details\\n\\n107\\n\\nby centering and scaling each of the variables, Qxj D .xj (cid:0) 1 Nxj /=sj , where\\n(cid:20) 1\\nn\\n\\nxij =n and sj D\\n\\n.xij (cid:0) Nxj /2\\n\\nNxj D\\n\\n(7.58)\\n\\nn\\nX\\n\\n(cid:21)1=2\\n\\nX\\n\\n;\\n\\niD1\\n\\nwith 1 the n-vector of 1s. We now work with QX D . Qx1; Qx2; : : : ; Qxp/ rather\\nthan X , and the intercept is estimated separately as Ny.\\n\\n(cid:142)5 [p. 100] Standard deviations in Table 7.3. From the Ô¨Årst equality in (7.36)\\n\\nwe calculate the covariance matrix of OÀá.(cid:21)/ to be\\n\\nCov(cid:21) D (cid:27) 2.S C (cid:21)I/(cid:0)1S .S C (cid:21)I/(cid:0)1:\\n\\n(7.59)\\n\\nThe entries sd(0.1) in Table 7.3 are square roots of the diagonal elements\\nof Cov(cid:21), substituting the ordinary least squares estimate O(cid:27) D 54:1 for (cid:27) 2.\\n(cid:142)6 [p. 101] Penalized likelihood and MAP. With (cid:27) 2 Ô¨Åxed and known in the\\nn.X Àá; (cid:27) 2I/, minimizing ky (cid:0) X Àák2 is the\\n\\nnormal linear model y (cid:24)\\nsame as maximizing the log density function\\n\\nN\\n\\nlog fÀá .y/ D (cid:0) 1\\n2\\n\\nky (cid:0) X Àák2 C constant:\\n\\n(7.60)\\n\\nIn this sense, the term (cid:21)kÀák2 in (7.41) penalizes the likelihood log fÀá .y/\\nconnected with Àá in proportion to the magnitude kÀák2. Under the prior\\ndistribution (7.38), the log posterior density of Àá given y (the log of (3.5))\\nis\\n\\n(cid:0) 1\\n2(cid:27) 2\\n\\nÀöky (cid:0) X Àák2 C (cid:21)kÀák2(cid:9) ;\\n\\n(7.61)\\n\\nplus a term that doesn‚Äôt depend on Àá. That makes the maximizer of (7.41)\\nalso the maximizer of the posterior density of Àá given y, or the MAP.\\n(cid:142)7 [p. 101] Formula (7.43). Let (cid:13) D .S 1=2=(cid:27)/Àá and O(cid:13) D .S 1=2=(cid:27)/ OÀá in\\n(7.37), where S 1=2 is a matrix square root of S , .S 1=2/2 D S . Then\\n\\nand the M D 0 form of the James‚ÄìStein rule (7.51) is\\n\\nO(cid:13) (cid:24)\\n\\np.(cid:13); I/;\\n\\nN\\n\\nO(cid:13) JS D\\n\\n(cid:21)\\n\\n(cid:20)\\n1 (cid:0) p (cid:0) 2\\nk O(cid:13) k2\\n\\nO(cid:13):\\n\\nTransforming back to the Àá scale gives (7.43).\\n\\n(7.62)\\n\\n(7.63)\\n\\n\\x0c8\\n\\nGeneralized Linear Models and Regression\\nTrees\\n\\nIndirect evidence is not the sole property of Bayesians. Regression models\\nare the frequentist method of choice for incorporating the experience of\\n‚Äúothers.‚Äù As an example, Figure 8.1 returns to the kidney Ô¨Åtness data of\\nSection 1.1. A potential new donor, aged 55, has appeared, and we wish\\nto assess his kidney Ô¨Åtness without subjecting him to an arduous series of\\nmedical tests. Only one of the 157 previously tested volunteers was age\\n55, his tot score being (cid:0)0:01 (the upper large dot in Figure 8.1). Most\\napplied statisticians, though, would prefer to read off the height of the least\\nsquares regression line at age D 55 (the green dot on the regression line),\\ndtot D (cid:0)1:46. The former is the only direct evidence we have, while the\\n\\nFigure 8.1 Kidney data; a new volunteer donor is aged 55.\\nWhich prediction is preferred for his kidney function?\\n\\n108\\n\\n*************************************************************************************************************************************************************2030405060708090‚àí6‚àí4‚àí2024Agetotll55\\x0c8.1 Logistic Regression\\n\\n109\\n\\nregression line lets us incorporate indirect evidence for age 55 from all 157\\nprevious cases.\\n\\nIncreasingly aggressive use of regression techniques is a hallmark of\\nmodern statistical practice, ‚Äúaggressive‚Äù applying to the number and type\\nof predictor variables, the coinage of new methodology, and the sheer size\\nof the target data sets. Generalized linear models, this chapter‚Äôs main topic,\\nhave been the most pervasively inÔ¨Çuential of the new methods. The chapter\\nends with a brief review of regression trees, a completely different regres-\\nsion methodology that will play an important role in the prediction algo-\\nrithms of Chapter 17.\\n\\n8.1 Logistic Regression\\n\\nAn experimental new anti-cancer drug called Xilathon is under devel-\\nopment. Before human testing can begin, animal studies are needed to de-\\ntermine safe dosages. To this end, a bioassay or dose‚Äìresponse experiment\\nwas carried out: 11 groups of n D 10 mice each were injected with in-\\ncreasing amounts of Xilathon, dosages coded1 1; 2; : : : ; 11.\\n\\nLet\\n\\nyi D # mice dying in ith group:\\n\\nThe points in Figure 8.2 show the proportion of deaths\\n\\npi D yi =10;\\n\\n(8.1)\\n\\n(8.2)\\n\\nlethality generally increasing with dose. The counts yi are modeled as in-\\ndependent binomials,\\n\\nyi\\n\\nind(cid:24) Bi.ni ; (cid:25)i /\\n\\nfor i D 1; 2; : : : ; N;\\n\\n(8.3)\\n\\nN D 11 and all ni equaling 10 here; (cid:25)i is the true death rate in group\\ni, estimated unbiasedly by pi , the direct evidence for (cid:25)i . The regression\\ncurve in Figure 8.2 uses all the doses to give a better picture of the true\\ndose‚Äìresponse relation.\\n\\nLogistic regression is a specialized technique for regression analysis of\\n\\ncount or proportion data. The logit parameter (cid:21) is deÔ¨Åned as\\n\\n(cid:21) D log\\n\\nn (cid:25)\\n1 (cid:0) (cid:25)\\n\\no\\n\\n;\\n\\n(8.4)\\n\\n1 Dose would usually be labeled on a log scale, each one, say, 50% larger than its\\n\\npredecessor.\\n\\n\\x0c110\\n\\nGLMs and Regression Trees\\n\\nFigure 8.2 Dose‚Äìresponse study; groups of 10 mice exposed to\\nincreasing doses of experimental drug. The points are the\\nobserved proportions that died in each group. The Ô¨Åtted curve is\\nthe maximum-likelihoood estimate of the linear logistic\\nregression model. The open circle on the curve is the LD50, the\\nestimated dose for 50% mortality.\\n\\nwith (cid:21) increasing from (cid:0)1 to 1 as (cid:25) increases from 0 to 1. A linear lo-\\ngistic regression dose‚Äìresponse analysis begins with binomial model (8.3),\\nand assumes that the logit is a linear function of dose,\\n\\n(cid:21)i D log\\n\\n(cid:27)\\n\\n(cid:26) (cid:25)i\\n1 (cid:0) (cid:25)i\\n\\nD Àõ0 C Àõ1xi :\\n\\nMaximum likelihood gives estimates . OÀõ0; OÀõ1/, and Ô¨Åtted curve\\n\\nO(cid:21).x/ D OÀõ0 C OÀõ1x:\\n\\nSince the inverse transformation of (8.4) is\\n\\n(cid:25) D\\n\\n1 C e(cid:0)(cid:21)(cid:17)(cid:0)1\\n(cid:16)\\n\\nwe obtain from (8.6) the linear logistic regression curve\\n\\nO(cid:25).x/ D\\n\\n1 C e(cid:0). OÀõ0C OÀõ1x/(cid:17)(cid:0)1\\n(cid:16)\\n\\n(8.5)\\n\\n(8.6)\\n\\n(8.7)\\n\\n(8.8)\\n\\npictured in Figure 8.2.\\n\\nTable 8.1 compares the standard deviation of the estimated regression\\n\\nlllllllllllDoseProportion of deaths12345678910110.000.250.500.751.00llLD50 = 5.69\\x0c8.1 Logistic Regression\\n\\n111\\n\\nTable 8.1 Standard deviation estimates for O(cid:25).x/ in Figure 8.1. The Ô¨Årst\\nrow is for the linear logistic regression Ô¨Åt (8.8); the second row is based\\non the individual binomial estimates pi .\\n\\nx\\n\\nsd O(cid:25).x/\\nsd pi\\n\\n1\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n8\\n\\n9\\n\\n10\\n\\n11\\n\\n.015\\n.045\\n\\n.027\\n.066\\n\\n.043\\n.094\\n\\n.061\\n.126\\n\\n.071\\n.152\\n\\n.072\\n.157\\n\\n.065\\n.138\\n\\n.050\\n.106\\n\\n.032\\n.076\\n\\n.019\\n.052\\n\\n.010\\n.035\\n\\ncurve (8.8) at x D 1; 2; : : : ; 11 (as discussed in the next section) with\\nthe usual binomial standard deviation estimate ≈ípi .1 (cid:0) pi /=10(cid:141)1=2 obtained\\nby considering the 11 doses separately.2 Regression has reduced error by\\nbetter than 50%, the price being possible bias if model (8.5) goes seriously\\nwrong.\\n\\nOne advantage of the logit transformation is that (cid:21) isn‚Äôt restricted to the\\nrange ≈í0; 1(cid:141), so model (8.5) never verges on forbidden territory. A better\\nreason has to do with the exploitation of exponential family properties. We\\ncan rewrite the density function for Bi.n; y/ as\\n!\\n\\n!\\n\\nn\\ny\\n\\n(cid:25) y.1 (cid:0) (cid:25)/n(cid:0)y D e(cid:21)y(cid:0)n .(cid:21)/\\n\\nn\\ny\\n\\nwith (cid:21) the logit parameter (8.4) and\\n\\n .(cid:21)/ D logf1 C e(cid:21)gI\\n\\n(8.9) is a one-parameter exponential family3 as described in Section 5.5,\\nwith (cid:21) the natural parameter, called Àõ there.\\n\\nLet y D .y1; y2; : : : ; yN / denote the full data set, N D 11 in Figure 8.2.\\nUsing (8.5), (8.9), and the independence of the yi gives the probability\\ndensity of y as a function of .Àõ0; Àõ1/,\\n\\nfÀõ0;Àõ1.y/ D\\n\\nN\\nY\\n\\ni D1\\n\\ne(cid:21)i yi (cid:0)ni  .(cid:21)i /\\n\\n!\\n\\nni\\nyi\\n\\nD eÀõ0S0CÀõ1S1 (cid:1) e(cid:0) PN\\n\\n1 ni  .Àõ0CÀõ1xi / (cid:1)\\n\\n(8.11)\\n\\nN\\nY\\n\\ni D1\\n\\n!\\n;\\n\\nni\\nyi\\n\\n2 For the separate-dose standard error, pi was taken equal to the Ô¨Åtted value from the\\n\\ncurve in Figure 8.2.\\n\\n3 It is not necessary for f(cid:22)0 .x/ in (5.46) on page 64 to be a probability density function,\\n\\nonly that it not depend on the parameter (cid:22).\\n\\n(8.9)\\n\\n(8.10)\\n\\n \\n \\n \\n \\n\\x0c112\\n\\nwhere\\n\\nGLMs and Regression Trees\\n\\nS0 D\\n\\nN\\nX\\n\\niD1\\n\\nyi\\n\\nand S1 D\\n\\nN\\nX\\n\\niD1\\n\\nxi yi :\\n\\n(8.12)\\n\\nFormula (8.11) expresses fÀõ0;Àõ1.y/ as the product of three factors,\\nfÀõ0;Àõ1.y/ D gÀõ0;Àõ1.S0; S1/h.Àõ0; Àõ1/j.y/;\\n\\n(8.13)\\n\\n(cid:142)1\\n\\nonly the Ô¨Årst of which involves both the parameters and the data. This im-\\nplies that .S0; S1/ is a sufÔ¨Åcient statistic:(cid:142) no matter how large N might be\\n(later we will have N in the thousands), just the two numbers .S0; S1/ con-\\ntain all of the experiment‚Äôs information. Only the logistic parameterization\\n(8.4) makes this happen.4\\n\\nA more intuitive picture of logistic regression depends on D.pi ; O(cid:25)i /, the\\n\\ndeviance between an observed proportion pi (8.2) and an estimate O(cid:25)i ,\\n(cid:19)\\n\\n(cid:19)(cid:21)\\n\\n(cid:20)\\n\\nD .pi ; O(cid:25)i / D 2ni\\n\\npi log\\n\\nC .1 (cid:0) pi / log\\n\\n:\\n\\n(8.14)\\n\\n(cid:18) 1 (cid:0) pi\\n1 (cid:0) O(cid:25)i\\n\\n(cid:18) pi\\nO(cid:25)i\\n\\nThe deviance5 is zero if O(cid:25)i D pi , otherwise it increases as O(cid:25)i departs\\nfurther from pi .\\n\\nThe logistic regression MLE value . OÀõ0; OÀõ1/ also turns out to be the\\nchoice of .Àõ0; Àõ1/ minimizing the total deviance between the N points pi\\nand their corresponding estimates O(cid:25)i D (cid:25) OÀõ0; OÀõ1.xi / (8.8):\\n\\n. OÀõ0; OÀõ1/ D arg min\\n.Àõ0;Àõ1/\\n\\nN\\nX\\n\\ni D1\\n\\nD .pi ; (cid:25)Àõ0;Àõ1.xi // :\\n\\n(8.15)\\n\\nThe solid line in Figure 8.2 is the linear logistic curve coming closest to\\nthe 11 points, when distance is measured by total deviance. In this way the\\n200-year-old notion of least squares is generalized to binomial regression,\\nas discussed in the next section. A more sophisticated notion of distance\\nbetween data and models is one of the accomplishments of modern statis-\\ntics.\\n\\nTable 8.2 reports on the data for a more structured logistic regression\\nanalysis. Human muscle cell colonies were infused with mouse nuclei in\\nÔ¨Åve different ratios, cultured over time periods ranging from one to Ô¨Åve\\n\\n4 Where the name ‚Äúlogistic regression‚Äù comes from is explained in the endnotes, along\\n\\nwith a description of its nonexponential family predecessor probit analysis.\\n\\n5 Deviance is analogous to squared error in ordinary regression theory, as discussed in\\nwhat follows. It is twice the ‚ÄúKullback‚ÄìLeibler distance,‚Äù the preferred name in the\\ninformation-theory literature.\\n\\n\\x0c8.1 Logistic Regression\\n\\n113\\n\\nTable 8.2 Cell infusion data; human cell colonies infused with mouse\\nnuclei in Ô¨Åve ratios over 1 to 5 days and observed to see whether they did\\nor did not thrive. Green numbers are estimates O(cid:25)ij from the logistic\\nregression model. For example, 5 of 31 colonies in the lowest ratio/days\\ncategory thrived, with observed proportion 5=31 D 0:16, and logistic\\nregression estimate O(cid:25)11 D 0:11:\\n\\n1\\n\\n5/31\\n.11\\n\\n2\\n\\n3/28\\n.25\\n\\n15/77\\n.24\\n\\n36/78\\n.45\\n\\n1\\n\\n2\\n\\nTime\\n\\n3\\n\\n20/45\\n.42\\n\\n43/71\\n.64\\n\\n4\\n\\n24/47\\n.54\\n\\n56/71\\n.74\\n\\n5\\n\\n29/35\\n.75\\n\\n66/74\\n.88\\n\\nRatio 3\\n\\n48/126\\n.38\\n\\n68/116\\n.62\\n\\n145/171\\n.77\\n\\n98/119\\n.85\\n\\n114/129\\n.93\\n\\n4\\n\\n5\\n\\n29/92\\n.32\\n\\n11/53\\n.18\\n\\n35/52\\n.56\\n\\n20/52\\n.37\\n\\n57/85\\n.73\\n\\n20/48\\n.55\\n\\n38/50\\n.81\\n\\n40/55\\n.67\\n\\n72/77\\n.92\\n\\n52/61\\n.84\\n\\ndays, and observed to see whether they thrived. For example, of the 126\\ncolonies having the third ratio and shortest time period, 48 thrived.\\n\\nLet (cid:25)ij denote the true probability of thriving for ratio i during time\\nperiod j , and (cid:21)ij its logit logf(cid:25)ij =.1 (cid:0) (cid:25)ij /g. A two-way additive logistic\\nregression was Ô¨Åt to the data,6\\n\\n(cid:21)ij D (cid:22) C Àõi C Àáj ;\\n\\ni D 1; 2; : : : ; 5; j D 1; 2; : : : ; 5:\\n\\n(8.16)\\n\\nThe green numbers in Table 8.2 show the maximum likelihood estimates\\n\\n. (cid:20)\\n\\nO(cid:25)ij D 1\\n\\n(cid:16)\\nO(cid:22)C OÀõi C OÀáj\\n\\n(cid:0)\\n\\n(cid:17)(cid:21)\\n\\n:\\n\\n1 C e\\n\\n(8.17)\\n\\nModel (8.16) has nine free parameters (taking into account the con-\\nstraints P Àõi D P Àáj D 0 necessary to avoid deÔ¨Ånitional difÔ¨Åculties)\\ncompared with just two in the dose‚Äìresponse experiment. The count can\\neasily go much higher these days.\\n\\nTable 8.3 reports on a 57-variable logistic regression applied to the spam\\ndata. A researcher (named George) labeled N D 4601 of his email mes-\\n\\n6 Using the statistical computing language R; see the endnotes.\\n\\n\\x0c114\\n\\nGLMs and Regression Trees\\n\\nTable 8.3 Logistic regression analysis of the spam data, model (8.17);\\nestimated regression coefÔ¨Åcients, standard errors, and z D estimate=se,\\nfor 57 keyword predictors. The notation char$ means the relative\\nnumber of times $ appears, etc. The last three entries measure\\ncharacteristics such as length of capital-letter strings. The word george\\nis special, since the recipient of the email is named George, and the goal\\nhere is to build a customized spam Ô¨Ålter.\\n\\nEstimate\\n\\nse\\n\\nz-value\\n\\nEstimate\\n\\nse\\n\\nz-value\\n\\nintercept\\nmake\\naddress\\nall\\n3d\\nour\\nover\\nremove\\ninternet\\norder\\nmail\\nreceive\\nwill\\npeople\\nreport\\naddresses\\nfree\\nbusiness\\nemail\\nyou\\ncredit\\nyour\\nfont\\n000\\nmoney\\nhp\\nhpl\\ngeorge\\n650\\n\\n(cid:0)12.27\\n(cid:0).12\\n(cid:0).19\\n.06\\n3.14\\n.38\\n.24\\n.89\\n.23\\n.20\\n.08\\n(cid:0).05\\n(cid:0).12\\n(cid:0).02\\n.05\\n.32\\n.86\\n.43\\n.06\\n.14\\n.53\\n.29\\n.21\\n.79\\n.19\\n(cid:0)3.21\\n(cid:0).92\\n(cid:0)39.62\\n.24\\n\\n1.99\\n.07\\n.09\\n.06\\n2.10\\n.07\\n.07\\n.13\\n.07\\n.08\\n.05\\n.06\\n.06\\n.07\\n.05\\n.19\\n.12\\n.10\\n.06\\n.06\\n.27\\n.06\\n.17\\n.16\\n.07\\n.52\\n.39\\n7.12\\n.11\\n\\n(cid:0)6.16 lab\\n(cid:0)1.68 labs\\n(cid:0)2.10 telnet\\n1.03 857\\n1.49 data\\n5.52 415\\n3.53 85\\n6.85 technology\\n3.39 1999\\n2.58 parts\\n1.75 pm\\n(cid:0) .86 direct\\n(cid:0)1.87 cs\\n(cid:0) .35 meeting\\n1.06 original\\n1.70 project\\n7.13 re\\n4.26 edu\\n1.03 table\\n2.32 conference\\n1.95 char;\\n4.62 char(\\n1.24 char\\n4.76 char!\\n2.63 char$\\n(cid:0)6.14 char#\\n(cid:0)2.37 cap.ave\\n(cid:0)5.57 cap.long\\n2.24 cap.tot\\n\\n(cid:0)1.48\\n(cid:0).15\\n(cid:0).07\\n.84\\n(cid:0).41\\n.22\\n(cid:0)1.09\\n.37\\n.02\\n(cid:0).13\\n(cid:0).38\\n(cid:0).11\\n(cid:0)16.27\\n(cid:0)2.06\\n(cid:0).28\\n(cid:0).98\\n(cid:0).80\\n(cid:0)1.33\\n(cid:0).18\\n(cid:0)1.15\\n(cid:0).31\\n(cid:0).05\\n(cid:0).07\\n.28\\n1.31\\n1.03\\n.38\\n1.78\\n.51\\n\\n.89\\n.14\\n.19\\n1.08\\n.17\\n.53\\n.42\\n.12\\n.07\\n.09\\n.17\\n.13\\n9.61\\n.64\\n.18\\n.33\\n.16\\n.24\\n.13\\n.46\\n.11\\n.07\\n.09\\n.07\\n.17\\n.48\\n.60\\n.49\\n.14\\n\\n(cid:0)1.66\\n(cid:0)1.05\\n(cid:0) .35\\n.78\\n(cid:0)2.37\\n.42\\n(cid:0)2.61\\n2.99\\n.26\\n(cid:0)1.41\\n(cid:0)2.26\\n(cid:0) .84\\n(cid:0)1.69\\n(cid:0)3.21\\n(cid:0)1.55\\n(cid:0)2.97\\n(cid:0)5.09\\n(cid:0)5.43\\n(cid:0)1.40\\n(cid:0)2.49\\n(cid:0)2.92\\n(cid:0) .75\\n(cid:0) .78\\n3.89\\n7.55\\n2.16\\n.64\\n3.62\\n3.75\\n\\nsages as either spam or ham (nonspam7), say\\n\\nyi D\\n\\n(\\n\\n1 if email i is spam\\n0 if email i is ham\\n\\n(8.18)\\n\\n7 ‚ÄúHam‚Äù refers to ‚Äúnonspam‚Äù or good email; this is a playful connection to the processed\\n\\n\\x0c8.1 Logistic Regression\\n\\n115\\n\\n(40% of the messages were spam). The p D 57 predictor variables repre-\\nsent the most frequently used words and tokens in George‚Äôs corpus of email\\n(excluding trivial words such as articles), and are in fact the relative fre-\\nquencies of these chosen words in each email (standardized by the length\\nof the email). The goal of the study was to predict whether future emails\\nare spam or ham using these keywords; that is, to build a customized spam\\nÔ¨Ålter.\\n\\nLet xij denote the relative frequency of keyword j in email i, and (cid:25)i\\nrepresent the probability that email i is spam. Letting (cid:21)i be the logit trans-\\nform logf(cid:25)i =.1 (cid:0) (cid:25)i /g, we Ô¨Åt the additive logistic model\\n\\n(cid:21)i D Àõ0 C\\n\\n57\\nX\\n\\nj D1\\n\\nÀõj xij :\\n\\n(8.19)\\n\\nTable 8.3 shows OÀõi for each word‚Äîfor example, (cid:0)0:12 for make‚Äîas well\\nas the estimated standard error and the z-value: estimate=se.\\n\\nIt looks like certain words, such as free and your, are good spam\\npredictors. However, the table as a whole has an unstable appearance, with\\noccasional very large estimates OÀõi accompanied by very large standard de-\\nviations.8 The dangers of high-dimensional maximum likelihood estima-\\ntion are apparent here. Some sort of shrinkage estimation is called for, as\\ndiscussed in Chapter 16.\\n\\n(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)\\n\\nRegression analysis, either in its classical form or in modern formula-\\ntions, requires covariate information x to put the various cases into some\\nsort of geometrical relationship. Given such information, regression is the\\nstatistician‚Äôs most powerful tool for bringing ‚Äúother‚Äù results to bear on a\\ncase of primary interest: for instance, the age-55 volunteer in Figure 8.1.\\n\\nEmpirical Bayes methods do not require covariate information but may\\nbe improvable if it exists. If, for example, the player‚Äôs age were an impor-\\ntant covariate in the baseball example of Table 7.1, we might Ô¨Årst regress\\nthe MLE values on age, and then shrink them toward the regression line\\nrather than toward the grand mean Np as in (7.20). In this way, two different\\nsorts of indirect evidence would be brought to bear on the estimation of\\neach player‚Äôs ability.\\n\\nspam that was fake ham during WWII, and has been adopted by the machine-learning\\ncommunity.\\n\\n8 The 4601 (cid:2) 57 X matrix .xij / was standardized, so disparate scalings are not the\\ncause of these discrepancies. Some of the features have mostly ‚Äúzero‚Äù observations,\\nwhich may account for their unstable estimation.\\n\\n\\x0c116\\n\\nGLMs and Regression Trees\\n\\n8.2 Generalized Linear Models9\\n\\nLogistic regression is a special case of generalized linear models (GLMs),\\na key 1970s methodology having both algorithmic and inferential inÔ¨Çu-\\nence. GLMs extend ordinary linear regression, that is least squares curve-\\nÔ¨Åtting, to situations where the response variables are binomial, Poisson,\\ngamma, beta, or in fact any exponential family form.\\nWe begin with a one-parameter exponential family,\\n\\nn\\nf(cid:21).y/ D e(cid:21)y(cid:0)(cid:13).(cid:21)/f0.y/; (cid:21) 2 ∆í\\n\\no\\n\\n;\\n\\n(8.20)\\n\\nas in (5.46) (now with Àõ and x replaced by (cid:21) and y, and  .Àõ/ replaced by\\n(cid:13).(cid:21)/, for clearer notation in what follows). Here (cid:21) is the natural parameter\\nand y the sufÔ¨Åcient statistic, both being one-dimensional in usual applica-\\ntions; (cid:21) takes its values in an interval of the real line. Each coordinate yi\\nof an observed data set y D .y1; y2; : : : ; yi ; : : : ; yN /0 is assumed to come\\nfrom a member of family (8.20),\\n\\nyi (cid:24) f(cid:21)i .(cid:1)/ independently for i D 1; 2; : : : ; N:\\n\\n(8.21)\\n\\nTable 8.4 lists (cid:21) and y for the Ô¨Årst four families in Table 5.1, as well as\\ntheir deviance and normalizing functions.\\n\\nBy itself, model (8.21) requires N parameters (cid:21)1; (cid:21)2; : : : ; (cid:21)N , usually\\ntoo many for effective individual estimation. A key GLM tactic is to specify\\nthe (cid:21)s in terms of a linear regression equation. Let X be an N (cid:2)p ‚Äústructure\\nmatrix,‚Äù with ith row say x0\\ni , and Àõ an unknown vector of p parameters;\\nthe N -vector (cid:21) D .(cid:21)1; (cid:21)2; : : : ; (cid:21)N /0 is then speciÔ¨Åed by\\n\\n(cid:21) D X Àõ:\\n\\n(8.22)\\n\\nIn the dose‚Äìresponse experiment of Figure 8.2 and model (8.5), X is N (cid:2) 2\\nwith ith row .1; xi / and parameter vector Àõ D .Àõ0; Àõ1/.\\n\\nThe probability density function fÀõ.y/ of the data vector y is\\n\\nfÀõ.y/ D\\n\\nN\\nY\\n\\ni D1\\n\\nf(cid:21)i .yi / D e\\n\\nPN\\n\\n1 .(cid:21)i yi (cid:0)(cid:13).(cid:21)i //\\n\\nN\\nY\\n\\niD1\\n\\nf0.yi /;\\n\\n(8.23)\\n\\nwhich can be written as\\n\\nfÀõ.y/ D eÀõ0z(cid:0) .Àõ/f0.y/;\\n\\n(8.24)\\n\\n9 Some of the more technical points raised in this section are referred to in later chapters,\\n\\nand can be scanned or omitted at Ô¨Årst reading.\\n\\n\\x0c8.2 Generalized Linear Models\\n\\n117\\n\\nTable 8.4 Exponential family form for Ô¨Årst four cases in Table 5.1;\\nnatural parameter (cid:21), sufÔ¨Åcient statistic y, deviance (8.31) between family\\nmembers f1 and f2, D.f1; f2/, and normalizing function (cid:13).(cid:21)/.\\n\\n(cid:21)\\n\\n1. Normal\\n\\n(cid:22)=(cid:27) 2\\n\\nN .(cid:22); (cid:27) 2/,\\n(cid:27) 2 known\\n\\ny\\n\\nx\\n\\nD.f1; f2/\\n\\n(cid:16) (cid:22)1(cid:0)(cid:22)2\\n(cid:27)\\n\\n(cid:17)2\\n\\n(cid:13).(cid:21)/\\n\\n(cid:27) 2(cid:21)2=2\\n\\n2. Poisson\\n\\nlog (cid:22)\\n\\nx\\n\\n2(cid:22)1\\n\\nh(cid:16) (cid:22)2\\n(cid:22)1\\n\\n(cid:17)\\n(cid:0) 1\\n\\n(cid:0) log (cid:22)2\\n(cid:22)1\\n\\ni\\n\\ne(cid:21)\\n\\nPoi.(cid:22)/\\n\\n3. Binomial\\n\\nBi.n; (cid:25)/\\n\\nlog (cid:25)\\n\\n1(cid:0)(cid:25) x 2n\\n\\nh\\n\\n(cid:25)1 log (cid:25)1\\n(cid:25)2\\n\\nC .1 (cid:0) (cid:25)1/ log 1(cid:0)(cid:25)1\\n1(cid:0)(cid:25)2\\n\\ni\\n\\nn log.1 C e(cid:21)/\\n\\n4. Gamma\\n\\n(cid:0)1=(cid:27)\\n\\nx\\n\\n2(cid:23)\\n\\nh(cid:16) (cid:27)1\\n(cid:27)2\\n\\n(cid:17)\\n(cid:0) 1\\n\\n(cid:0) log (cid:27)1\\n(cid:27)2\\n\\ni\\n\\n(cid:0)(cid:23) log.(cid:0)(cid:21)/\\n\\nGam.(cid:23); (cid:27) /,\\n(cid:23) known\\n\\nwhere\\n\\nz D X 0y and  .Àõ/ D\\n\\nN\\nX\\n\\niD1\\n\\n(cid:13).x0\\n\\ni Àõ/;\\n\\n(8.25)\\n\\na p-parameter exponential family (5.50), with natural parameter vector Àõ\\nand sufÔ¨Åcient statistic vector z. The main point is that all the information\\nfrom a p-parameter GLM is summarized in the p-dimensional vector z,\\nno matter how large N may be, making it easier both to understand and to\\nanalyze.\\n\\nWe have now reduced the N -parameter model (8.20)‚Äì(8.21) to the p-\\nparameter exponential family (8.24), with p usually much smaller than N ,\\nin this way avoiding the difÔ¨Åculties of high-dimensional estimation. The\\nmoments of the one-parameter constituents (8.20) determine the estimation\\nproperties in model (8.22)‚Äì(8.24). Let .(cid:22)(cid:21); (cid:27) 2\\n(cid:21)/ denote the expectation and\\nvariance of univariate density f(cid:21).y/ (8.20),\\n\\n(8.26)\\n(cid:21)/ D .e(cid:21); e(cid:21)/ for the Poisson. The N -vector y obtained\\n\\nfor instance .(cid:22)(cid:21); (cid:27) 2\\nfrom GLM (8.22) then has mean vector and covariance matrix\\n\\n(cid:21)/;\\n\\ny (cid:24) .(cid:22)(cid:21); (cid:27) 2\\n\\ny (cid:24) .(cid:22).Àõ/; ‚Ä†.Àõ// ;\\n\\n(8.27)\\n\\n\\x0c118\\n\\nGLMs and Regression Trees\\n\\nwhere (cid:22).Àõ/ is the vector with ith component (cid:22)(cid:21)i with (cid:21)i D x0\\nis the N (cid:2) N diagonal matrix having diagonal elements (cid:27) 2\\n.\\n(cid:21)i\\n\\nThe maximum likelihood estimate OÀõ of the parameter vector Àõ can be\\n\\ni Àõ, and ‚Ä†.Àõ/\\n\\n(cid:142)2\\n\\nshown to satisfy the simple equation(cid:142)\\n\\nX 0 ≈íy (cid:0) (cid:22) . OÀõ/(cid:141) D 0:\\n\\n(8.28)\\n\\nFor the normal case where yi (cid:24)\\n.(cid:22)i ; (cid:27) 2/ in (8.21), that is, for ordinary\\nlinear regression, (cid:22). OÀõ/ D X OÀõ and (8.28) becomes X 0.y (cid:0) X OÀõ/ D 0, with\\nthe familiar solution\\n\\nN\\n\\nOÀõ D .X 0X /(cid:0)1X 0yI\\n\\n(8.29)\\n\\notherwise, (cid:22).Àõ/ is a nonlinear function of Àõ, and (8.28) must be solved\\nby numerical iteration. This is made easier by the fact that, for GLMs,\\nlog fÀõ.y/, the likelihood function we wish to maximize, is a concave func-\\ntion of Àõ. The MLE OÀõ has approximate expectation and covariance(cid:142)\\n\\n(cid:142)3\\n\\nOÀõ P(cid:24) .Àõ; (cid:0)X 0‚Ä†.Àõ/X (cid:1)(cid:0)1\\n\\n/;\\n\\n(8.30)\\n\\n(cid:142)4\\n\\nsimilar to the exact OLS result OÀõ (cid:24) .Àõ; (cid:27) (cid:0)2.X 0X /(cid:0)1/.(cid:142)\\n\\nGeneralizing the binomial deÔ¨Ånition (8.14), the deviance between den-\\n\\nsities f1.y/ and f2.y/ is deÔ¨Åned to be\\n\\nD.f1; f2/ D 2\\n\\nf1.y/ log\\n\\n(cid:27)\\n\\n(cid:26) f1.y/\\nf2.y/\\n\\ndy;\\n\\nZ\\n\\nY\\n\\n(8.31)\\n\\nthe integral (or sum for discrete distributions) being over their common\\n. D.f1; f2/ is always nonnegative, equaling zero only if\\nsample space\\nf1 and f2 are the same; in general D.f1; f2/ does not equal D.f2; f1/.\\nDeviance does not depend on how the two densities are named, for example\\n(8.14) having the same expression as the Binomial entry in Table 8.4.\\n\\nY\\n\\nIn what follows it will sometimes be useful to label the family (8.20) by\\nits expectation parameter (cid:22) D E(cid:21)fyg rather than by the natural parameter\\n(cid:21):\\n\\nf(cid:22).y/ D e(cid:21)y(cid:0)(cid:13).(cid:21)/f0.y/;\\n\\n(8.32)\\n\\nmeaning the same thing as (8.20), only the names attached to the individ-\\nual family members being changed. In this notation it is easy to show a\\nfundamental result sometimes known as\\n\\n(cid:142)5\\n\\nHoeffding‚Äôs Lemma (cid:142)\\nThe maximum likelihood estimate of (cid:22) given y\\nis y itself, and the log likelihood log f(cid:22).y/ decreases from its maximum\\nlog fy.y/ by an amount that depends on the deviance D.y; (cid:22)/,\\n\\nf(cid:22).y/ D fy.y/e(cid:0)D.y;(cid:22)/=2:\\n\\n(8.33)\\n\\n\\x0c8.2 Generalized Linear Models\\n\\n119\\n\\nReturning to the GLM framework (8.21)‚Äì(8.22), parameter vector Àõ\\ngives (cid:21).Àõ/ D X Àõ, which in turn gives the vector of expectation param-\\neters\\n\\n(cid:22).Àõ/ D .: : : (cid:22)i .Àõ/ : : : /0;\\n(8.34)\\nfor instance (cid:22)i .Àõ/ D expf(cid:21)i .Àõ/g for the Poisson family. Multiplying Hoeff-\\nding‚Äôs lemma (8.33) over the N cases y D .y1; y2; : : : ; yN /0 yields\\n\" N\\nY\\n\\nN\\nY\\n\\n#\\n\\nfyi .yi /\\n\\ne(cid:0) PN\\n\\n1 D.yi ;(cid:22)i .Àõ//:\\n\\nfÀõ.y/ D\\n\\nf(cid:22)i .Àõ/.yi / D\\n\\n(8.35)\\n\\ni D1\\n\\niD1\\n\\nThis has an important consequence: the MLE OÀõ is the choice of Àõ that\\nminimizes the total deviance PN\\n1 D.yi ; (cid:22)i .Àõ//. As in Figure 8.2, GLM\\nmaximum likelihood Ô¨Åtting is ‚Äúleast total deviance‚Äù in the same way that\\nordinary linear regression is least sum of squares.\\n(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)\\nThe inner circle of Figure 8.3 represents normal theory, the preferred\\nvenue of classical applied statistics. Exact inferences‚Äît-tests, F distribu-\\ntions, most of multivariate analysis‚Äîwere feasible within the circle. Out-\\nside the circle was a general theory based mainly on asymptotic (large-\\nsample) approximations involving Taylor expansions and the central limit\\ntheorem.\\n\\nFigure 8.3 Three levels of statistical modeling.\\n\\nA few useful exact results lay outside the normal theory circle, relating\\n\\n NORMAL THEORY(exact calculations)EXPONENTIAL FAMILIES(partly exact)GENERAL THEORY(asymptotics)Figure 8.3. Three levels of statistical modeling\\x0c120\\n\\nGLMs and Regression Trees\\n\\nto a few special families: the binomial, Poisson, gamma, beta, and others\\nless well known. Exponential family theory, the second circle in Figure 8.3,\\nuniÔ¨Åed the special cases into a coherent whole. It has a ‚Äúpartly exact‚Äù Ô¨Ça-\\nvor, with some ideal counterparts to normal theory‚Äîconvex likelihood sur-\\nfaces, least deviance regression‚Äîbut with some approximations necessary,\\nas in (8.30). Even the approximations, though, are often more convincing\\nthan those of general theory, exponential families‚Äô Ô¨Åxed-dimension sufÔ¨Å-\\ncient statistics making the asymptotics more transparent.\\n\\nLogistic regression has banished its predecessors (such as probit anal-\\nysis) almost entirely from the Ô¨Åeld, and not only because of estimating\\nefÔ¨Åciencies and computational advantages (which are actually rather mod-\\nest), but also because it is seen as a clearer analogue to ordinary least\\nsquares, our 200-year-old dependable standby. GLM research development\\nhas been mostly frequentist, but with a substantial admixture of likelihood-\\nbased reasoning, and a hint of Fisher‚Äôs ‚Äúlogic of inductive inference.‚Äù\\n\\nHelping the statistician choose between competing methodologies is the\\njob of statistical inference. In the case of generalized linear models the\\nchoice has been made, at least partly, in terms of aesthetics as well as phi-\\nlosophy.\\n\\n8.3 Poisson Regression\\n\\nThe third most-used member of the GLM family, after normal theory least\\nsquares and logistic regression, is Poisson regression. N independent Pois-\\nson variates are observed,\\n\\nyi\\n\\nind(cid:24) Poi.(cid:22)i /;\\n\\ni D 1; 2; : : : ; N;\\n\\nwhere (cid:21)i D log (cid:22)i is assumed to follow a linear model,\\n\\n(cid:21).Àõ/ D X Àõ;\\n\\n(8.36)\\n\\n(8.37)\\n\\nwhere X is a known N (cid:2) p structure matrix and Àõ an unknown p-vector\\nof regression coefÔ¨Åcients. That is, (cid:21)i D x0\\ni Àõ for i D 1; 2; : : : ; N , where x0\\nis the ith row of X .\\n\\ni\\n\\nIn the chapters that follow we will see Poisson regression come to the\\nrescue in what at Ô¨Årst appear to be awkward data-analytic situations. Here\\nwe will settle for an example involving density estimation from a spatially\\ntruncated sample.\\n\\nTable 8.5 shows galaxy counts (cid:142) from a small portion of the sky: 487\\ngalaxies have had their redshifts r and apparent magnitudes m measured.\\n\\n(cid:142)6\\n\\n\\x0c8.3 Poisson Regression\\n\\n121\\n\\nTable 8.5 Counts for a truncated sample of 487 galaxies, binned by\\nredshift and magnitude.\\n\\n\"\\n\\n18\\n17\\n16\\n15\\n14\\n13\\n12\\n11\\nmagnitude 10\\n9\\n8\\n7\\n6\\n5\\n4\\n3\\n2\\n1\\n\\n(dimmer)\\n\\n1\\n\\n1\\n3\\n3\\n1\\n1\\n3\\n2\\n4\\n1\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n2\\n\\n6\\n2\\n2\\n1\\n3\\n2\\n0\\n1\\n0\\n1\\n0\\n1\\n0\\n3\\n0\\n1\\n1\\n1\\n\\n3\\n\\n6\\n3\\n3\\n4\\n2\\n4\\n2\\n1\\n0\\n0\\n0\\n0\\n3\\n1\\n1\\n0\\n0\\n0\\n\\n4\\n\\n3\\n4\\n3\\n3\\n3\\n5\\n4\\n4\\n2\\n2\\n0\\n1\\n1\\n1\\n1\\n0\\n0\\n0\\n\\nredshift (farther) (cid:0)!\\n11\\n8\\n5\\n\\n10\\n\\n6\\n\\n7\\n\\n9\\n\\n1\\n0\\n3\\n4\\n3\\n3\\n5\\n7\\n2\\n2\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n\\n4\\n5\\n2\\n3\\n4\\n6\\n4\\n3\\n2\\n2\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n6\\n7\\n9\\n2\\n5\\n4\\n2\\n3\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n8\\n6\\n9\\n3\\n7\\n3\\n3\\n1\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n8\\n6\\n6\\n8\\n6\\n2\\n3\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n20\\n7\\n3\\n9\\n7\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n10\\n5\\n5\\n4\\n3\\n5\\n1\\n1\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n12\\n\\n7\\n7\\n4\\n3\\n4\\n1\\n2\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n13\\n\\n16\\n6\\n5\\n4\\n0\\n0\\n0\\n0\\n2\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n14\\n\\n15\\n\\n9\\n8\\n2\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n4\\n5\\n1\\n1\\n1\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\nDistance from earth is an increasing function of r, while apparent bright-\\nness is a decreasing function10 of m. In this survey, counts were limited to\\ngalaxies having\\n\\n1:22 (cid:20) r (cid:20) 3:32\\n\\nand 17:2 (cid:20) m (cid:20) 21:5;\\n\\n(8.38)\\n\\nthe upper limit reÔ¨Çecting the difÔ¨Åculty of measuring very dim galaxies.\\n\\nThe range of log r has been divided into 15 equal intervals and likewise\\n18 equal intervals for m. Table 8.5 gives the counts of the 487 galaxies in\\nthe 18 (cid:2) 15 D 270 bins. (The lower right corner of the table is empty be-\\ncause distant galaxies always appear dim.) The multinomial/Poisson con-\\nnection (5.44) helps motivate model (8.36), picturing the table as a multi-\\nnomial observation on 270 categories, in which the sample size N was\\nitself Poisson.\\n\\nWe can imagine Table 8.5 as a small portion of a much more extensive\\ntable, hypothetically available if the data were not truncated. Experience\\nsuggests that we might then Ô¨Åt an appropriate bivariate normal density to\\nthe data, as in Figure 5.3. It seems like it might be awkward to Ô¨Åt part of a\\nbivariate normal density to truncated data, but Poisson regression offers an\\neasy solution.\\n\\n10 An object of the second magnitude is less bright than one of the Ô¨Årst, and so on, a\\n\\nclassiÔ¨Åcation system owing to the Greeks.\\n\\n\\x0c122\\n\\nGLMs and Regression Trees\\n\\nLet r be the 270-vector listing the values of r in each bin of the table\\n(in column order), and likewise m for the 270 m values‚Äîfor instance m D\\n.18; 17; : : : ; 1/ repeated 15 times‚Äîand deÔ¨Åne the 270 (cid:2) 5 matrix X as\\n\\nX D ≈ír; m; r 2; rm; m2(cid:141);\\n\\n(8.39)\\n\\nwhere r 2 is the vector whose components are the square of r‚Äôs, etc. The\\nlog density of a bivariate normal distribution in .r; m/ is of the form Àõ1r C\\nÀõ2m C Àõ3r 2 C Àõ4rm C Àõ5m2, agreeing with log (cid:22)i D x0\\ni Àõ as speciÔ¨Åed\\nby (8.39). We can use a Poisson GLM, with yi the ith bin‚Äôs count, to es-\\ntimate the portion of our hypothesized bivariate normal distribution in the\\ntruncation region (8.38).\\n\\nFigure 8.4 Left galaxy data; binned counts. Right Poisson GLM\\ndensity estimate.\\n\\nThe left panel of Figure 8.4 is a perspective picture of the raw counts\\nin Table 8.5. On the right is the Ô¨Åtted density from the Poisson regression.\\nIrrespective of density estimation, Poisson regression has done a useful job\\nof smoothing the raw bin counts.\\n\\nContours of equal value of the Ô¨Åtted log density\\n\\nOÀõ0 C OÀõ1r C OÀõ2m C OÀõ3r 2 C OÀõ4rm C OÀõ5m2\\n\\n(8.40)\\n\\nare shown in Figure 8.5. One can imagine the contours as truncated por-\\ntions of ellipsoids, of the type shown in Figure 5.3. The right panel of\\nFigure 8.4 makes it clear that we are nowhere near the center of the hypo-\\nthetical bivariate normal density, which must lie well beyond our dimness\\nlimit.\\n\\nDimmerFartherCountDimmerFartherDensity\\x0c8.3 Poisson Regression\\n\\n123\\n\\nFigure 8.5 Contour curves for Poisson GLM density estimate for\\nthe galaxy data. The red dot shows the point of maximum density.\\n\\nThe Poisson deviance residual Z between an observed count y and a\\n\\nÔ¨Åtted value O(cid:22) is\\n\\nZ D sign.y (cid:0) O(cid:22)/D.y; O(cid:22)/1=2;\\n\\n(8.41)\\n\\nwith D the Poisson deviance from Table 8.4. Zj k, the deviance residual\\nbetween the count yij in the ij th bin of Table 8.5 and the Ô¨Åtted value O(cid:22)j k\\nfrom the Poisson GLM, was calculated for all 270 bins. Standard frequen-\\ntist GLM theory says that S D P\\nj k Z2\\nj k should be about 270 if the bivari-\\nate normal model (8.39) is correct.11 Actually the Ô¨Åt was poor: S D 610.\\nIn practice we might try adding columns to X in (8.39), e.g., rm2 or\\nr 2m2, improving the Ô¨Åt where it was worst, near the boundaries of the ta-\\nble. Chapter 12 demonstrates some other examples of Poisson density esti-\\nmation. In general, Poisson GLMs reduce density estimation to regression\\nmodel Ô¨Åtting, a familiar and Ô¨Çexible inferential technology.\\n\\n11 This is a modern version of the classic chi-squared goodness-of-Ô¨Åt test.\\n\\n‚àí1.5‚àí1.0‚àí0.50.0‚àí21‚àí20‚àí19‚àí18‚àí17FartherDimmer 0.5  1  1.5  2  2.5  3  3.5  4  4.5  5  5.5  6  6.5  7  7.5  8  8.5  9 l\\x0c124\\n\\nGLMs and Regression Trees\\n\\n8.4 Regression Trees\\n\\nThe data set d for a regression problem typically consists of N pairs .xi ; yi /,\\n\\nd D f.xi ; yi /; i D 1; 2; : : : ; N g ;\\n\\n(8.42)\\n\\nX\\n\\nwhere xi is a vector of predictors, or ‚Äúcovariates,‚Äù taking its value in some\\n, and yi is the response, assumed to be univariate in what follows.\\nspace\\nThe regression algorithm, perhaps a Poisson GLM, inputs d and outputs\\n, rd .x/ produces an estimate Oy for a\\na rule rd .x/: for any value of x in\\npossible future value of y,\\n\\nX\\n\\nOy D rd .x/:\\n\\n(8.43)\\n\\nIn the logistic regression example (8.8), rd .x/ is O(cid:25).x/.\\nThere are three principal uses for the rule rd .x/.\\n\\n1 For prediction: Given a new observation of x, but not of its correspond-\\ning y, we use Oy D rd .x/ to predict y. In the spam example, the 57\\nkeywords of an incoming message could be used to predict whether or\\nnot it is spam.12 (See Chapter 12.)\\n\\n2 For estimation: The rule rd .x/ describes a ‚Äúregression surface‚Äù OS over\\n\\n,\\n\\nX\\n\\nOS D frd .x/; x 2\\n\\n(8.44)\\nThe right panel of Figure 8.4 shows OS for the galaxy example. OS can be\\nthought of as estimating S, the true regression surface, often deÔ¨Åned in\\nthe form of conditional expectation,\\n\\nX\\n\\ng :\\n\\nS D fEfyjxg; x 2\\n\\ng :\\n\\nX\\n\\n(8.45)\\n\\nX\\n\\ng.)\\n\\n(In a dichotomous situation where y is coded as 0 or 1, S D fPrfy D\\n1jxg; x 2\\nFor estimation, but not necessarily for prediction, we want OS to accu-\\nrately portray S. The right panel of Figure 8.4 shows the estimated galaxy\\ndensity still increasing monotonically in dimmer at the top end of the\\ntruncation region, but not so in farther, perhaps an important clue for\\ndirecting future search counts.13 The Ô¨Çat region in the kidney function re-\\ngression curve of Figure 1.2 makes almost no difference to prediction, but\\nis of scientiÔ¨Åc interest if accurate.\\n\\n12 Prediction of dichotomous outcomes is often called ‚ÄúclassiÔ¨Åcation.‚Äù\\n13 Physicists call a regression-based search for new objects ‚Äúbump hunting.‚Äù\\n\\n\\x0c8.4 Regression Trees\\n\\n125\\n\\n3 For explanation: The 10 predictors for the diabetes data of Section 7.3,\\nage, sex, bmi,. . . , were selected by the researcher in the hope of ex-\\nplaining the etiology of diabetes progression. The relative contribution\\nof the different predictors to rd .x/ is then of interest. How the regression\\nsurface is composed is of prime concern in this use, but not in use 1 or 2\\nabove.\\n\\nThe three different uses of rd .x/ raise different inferential questions.\\nUse 1 calls for estimates of prediction error. In a dichotomous situation\\nsuch as the spam study, we would want to know both error probabilities\\n\\nPr f Oy D spamjy D hamg\\n\\nand\\n\\nPr f Oy D hamjy D spamg :\\n\\n(8.46)\\n\\nFor estimation, the accuracy of rd .x/ as a function of x, perhaps in stan-\\ndard deviation terms,\\n\\nsd.x/ D sd. Oyjx/;\\n(8.47)\\nwould tell how closely OS approximates S. Use 3, explanation, requires\\nmore elaborate inferential tools, saying for example which of the regression\\ncoefÔ¨Åcients Àõi in (8.19) can safely be set to zero.\\n\\nFigure 8.6 Left a hypothetical regression tree based on two\\npredictors X1 and X2. Right corresponding regression surface.\\n\\nRegression trees use a simple but intuitively appealing technique to form\\na regression surface: recursive partitioning. The left panel of Figure 8.6\\nillustrates the method for a hypothetical situation involving two predictor\\nvariables, X1 and X2 (e.g., r and m in the galaxy example). At the top of\\n\\n|t1t2t3t4R1R1R2R2R3R3R4R4R5R5X1X1X1X2X2X2X1‚â§t1X2‚â§t2X1‚â§t3X2‚â§t4\\x0c126\\n\\nGLMs and Regression Trees\\n\\nthe tree, the sample population of N cases has been split into two groups:\\nthose with X1 equal to or less than value t1 go to the left, those with X1 >\\nt1 to the right. The leftward group is itself then divided into two groups\\ndepending on whether or not X2 (cid:20) t2. The division stops there, leaving\\ntwo terminal nodes R1 and R2. On the tree‚Äôs right side, two other splits\\ngive terminal nodes R3, R4, and R5.\\n\\nA prediction value OyRj is attached to each terminal node Rj . The predic-\\ntion Oy applying to a new observation x D .x1; x2/ is calculated by starting\\nx at the top of the tree and following the splits downward until a terminal\\nnode, and its attached prediction OyRj , is reached. The corresponding re-\\ngression surface OS is shown in the right panel of Figure 8.6 (here the OyRj\\nhappen to be in ascending order).\\n\\nVarious algorithmic rules are used to decide which variable to split and\\nwhich splitting value t to take at each step of the tree‚Äôs construction. Here\\nis the most common method: suppose at step k of the algorithm, groupk of\\nNk cases remains to be split, those cases having mean and sum of squares\\n\\nmk D X\\ni 2groupk\\n\\nyi =Nk\\n\\nand s2\\nk\\n\\nD X\\ni 2groupk\\n\\n.yi (cid:0) mk/2:\\n\\n(8.48)\\n\\nDividing groupk into groupk;left and groupk;right produces means mk;left and\\nmk;right, and corresponding sums of squares s2\\nk;right. The algorithm\\nproceeds by choosing the splitting variable Xk and the threshold tk to min-\\nimize\\n\\nk;left and s2\\n\\ns2\\nk;left\\n\\nC s2\\n\\nk;right:\\n\\n(8.49)\\n\\nIn other words, it splits groupk into two groups that are as different from\\neach other as possible.(cid:142)\\n\\n(cid:142)7\\n\\nCross-validation estimates of prediction error, Chapter 12, are used to\\ndecide when the splitting process should stop. If groupk is not to be further\\ndivided, it becomes terminal node Rk, with prediction value OyRk\\nD mk.\\nNone of this would be feasible without electronic computation, but even\\nquite large prediction problems can be short work for modern computers.\\nFigure 8.7 shows a regression tree analysis14 of the spam data, Ta-\\nble 8.3. There are seven terminal nodes, labeled 0 or 1 for decision ham\\nor spam. The leftmost node, say R1, is a 0, and contains 2462 ham cases\\nand 275 spam (compared with 2788 and 1813 in the full data set). Starting\\nat the top of the tree, R1 is reached if it has a low proportion of $ symbols\\n\\n14 Using the R program rpart, in classiÔ¨Åcation mode, employing a different splitting rule\\n\\nthan the version based on (8.49).\\n\\n\\x0c8.4 Regression Trees\\n\\n127\\n\\nFigure 8.7 Regression tree on the spam data; 0 D ham, 1 D\\nspam. Error rates: ham 5.2%, spam 17.4%. Captions indicate\\nleftward (ham) moves.\\n\\nchar$, a low proportion of the word remove, and a low proportion of\\nexclamation marks char!.\\n\\nRegression trees are easy to interpret (‚ÄúToo many dollar signs means\\nspam!‚Äù) seemingly suiting them for use 3, explanation. Unfortunately, they\\nare also easy to overinterpret, with a reputation for being unstable in prac-\\ntice. Discontinuous regression surfaces OS, as in Figure 8.6, disqualify them\\nfor use 2, estimation. Their principal use in what follows will be as key\\nparts of prediction algorithms, use 1. The tree in Figure 8.6 has apparent\\nerror rates (8.46) of 5.2% and 17.4%. This can be much improved upon\\nby ‚Äúbagging‚Äù (bootstrap aggregation), Chapters 17 and 20, and by other\\ncomputer-intensive techniques.\\n\\nCompared with generalized linear models, regression trees represent a\\nbreak from classical methodology that is more stark. First of all, they are\\ntotally nonparametric; bigger but less structured data sets have promoted\\nnonparametrics in twenty-Ô¨Årst-century statistics. Regression trees are more\\ncomputer-intensive and less efÔ¨Åcient than GLMs but, as will be seen in Part\\nIII, the availability of massive data sets and modern computational equip-\\n\\n|char$< ‚àí0.0826remove< ‚àí0.1513char!< 0.1335capruntot< ‚àí0.3757free< 0.7219hp>=‚àí0.0894502462/2750129/3211/20133/189130/300063/7170/990Figure 8.7 . Regression Tree, Spam Data: 0=nonspam,  1=spam,Error Rates: nonspam 5.2%,  spam 17.4%Captions indicate leftward (nonspam) moves\\x0c128\\n\\nGLMs and Regression Trees\\n\\nment has diminished the appeal of efÔ¨Åciency in favor of easy assumption-\\nfree application.\\n\\n8.5 Notes and Details\\n\\nComputer-age algorithms depend for their utility on statistical computing\\nlanguages. After a period of evolution, the language S (Becker et al., 1988)\\nand its open-source successor R (R Core Team, 2015), have come to dom-\\ninate applied practice.15 Generalized linear models are available from a\\nsingle R command, e.g.,\\n\\nglm(y(cid:24)X,family=binomial)\\n\\nfor logistic regression (Chambers and Hastie, 1993), and similarly for re-\\ngression trees and hundreds of other applications.\\n\\nThe classic version of bioassay, probit analysis, assumes that each test\\nanimal has its own lethal dose level X, and that the population distribution\\nof X is normal,\\n\\nPrfX (cid:20) xg D ÀÜ.Àõ0 C Àõ1x/\\n\\n(8.50)\\n\\nfor unknown parameters .Àõ0; Àõ1/ and standard normal cdf ÀÜ. Then the\\nnumber of animals dying at dose x is binomial Bi.nx; (cid:25)x/ as in (8.3), with\\n(cid:25)x D ÀÜ.Àõ0 C Àõ1x/, or\\n\\nÀÜ(cid:0)1.(cid:25)x/ D Àõ0 C Àõ1x:\\n(8.51)\\nReplacing the standard normal cdf ÀÜ.z/ with the logistic cdf 1=.1 C e(cid:0)z/\\n(which resembles ÀÜ), changes (8.51) into logistic regression (8.5). The\\nusual goal of bioassay was to estimate ‚ÄúLD50,‚Äù the dose lethal to 50%\\nof the test population; it is indicated by the open circle in Figure 8.2.\\n\\nCox (1970), the classic text on logistic regression, lists Berkson (1944)\\nas an early practitioner. Wedderburn (1974) is credited with generalized\\nlinear models in McCullagh and Nelder‚Äôs inÔ¨Çuential text of that name, Ô¨Årst\\nedition 1983; Birch (1964) developed an important and suggestive special\\ncase of GLM theory.\\n\\nThe twenty-Ô¨Årst century has seen an efÔ¨Çorescence of computer-based re-\\ngression techniques, as described extensively in Hastie et al. (2009). The\\ndiscussion of regression trees here is taken from their Section 9.2, including\\nour Figure 8.6. They use the spam data as a central example; it is publicly\\n\\n15 Previous computer packages such as SAS and SPSS continue to play a major role in\\n\\napplication areas such as the social sciences, biomedical statistics, and the\\npharmaceutical industry.\\n\\n\\x0c8.5 Notes and Details\\n\\n129\\n\\navailable at ftp.ics.uci.edu. Breiman et al. (1984) propelled regres-\\nsion trees into wide use with their CART algorithm.\\n\\n(cid:142)1 [p. 112] SufÔ¨Åciency as in (8.13). The Fisher‚ÄìNeyman criterion says that if\\nfÀõ.x/ D hÀõ.S.x//g.x/, when g.(cid:1)/ does not depend on Àõ, then S.x/ is\\nsufÔ¨Åcient for Àõ.\\n\\n(cid:142)2 [p. 118] Equation (8.28). From (8.24)‚Äì(8.25) we have the log likelihood\\n\\nfunction\\n\\nlÀõ.y/ D Àõ0z (cid:0)  .Àõ/\\n\\n(8.52)\\n\\nwith sufÔ¨Åcient statistic z D X 0y and  .Àõ/ D PN\\ning with respect to Àõ,\\n\\ni D1 (cid:13).x0\\n\\ni Àõ/. Differentiat-\\n\\nPlÀõ.y/ D z (cid:0) P .Àõ/ D X 0y (cid:0) X 0(cid:22).Àõ/;\\n\\n(8.53)\\n\\nwhere we have used d(cid:13)=d(cid:21) D (cid:22)(cid:21) (5.55), so P(cid:13).x0\\n(8.53) says PlÀõ.y/ D X 0.y (cid:0) (cid:22).Àõ//, verifying the MLE equation (8.28).\\n(cid:142)3 [p. 118] Concavity of the log likelihood. From (8.53), the second derivative\\n\\ni Àõ/ D x0\\n\\ni (cid:22)i .Àõ/. But\\n\\nmatrix RlÀõ.y/ with respect to Àõ is\\n\\n(cid:0) R .Àõ/ D (cid:0) covÀõ.z/;\\n\\n(5.57)‚Äì(5.59). But z D X 0y has\\n\\ncovÀõ.z/ D X 0‚Ä†.Àõ/X ;\\n\\n(8.54)\\n\\n(8.55)\\n\\na positive deÔ¨Ånite p (cid:2) p matrix, verifying the concavity of lÀõ.y/ (which in\\nfact applies to any exponential family, not only GLMs).\\n\\n(cid:142)4 [p. 118] Formula (8.30). The sufÔ¨Åcient statistic z has mean vector and co-\\n\\nvariance matrix\\n\\nz (cid:24) .Àá; VÀõ/;\\n\\n(8.56)\\n\\nwith Àá D EÀõfzg (5.58) and VÀõ D X 0‚Ä†.Àõ/X (8.55). Using (5.60), the\\nÔ¨Årst-order Taylor series for OÀõ as a function of z is\\n\\nOÀõ\\n\\n:D Àõ C V (cid:0)1\\n\\nÀõ .z (cid:0) Àá/:\\n\\n(8.57)\\n\\nTaken literally, (8.57) gives (8.30). In the OLS formula, we have (cid:27) (cid:0)2 rather\\nthan (cid:27) 2 since the natural parameter Àõ for the Normal entry in Table 8.4 is\\n(cid:22)=(cid:27) 2.\\n\\n(cid:142)5 [p. 118] Formula (8.33). This formula, attributed to Hoeffding (1965), is a\\nkey result in the interpretation of GLM Ô¨Åtting. Applying deÔ¨Ånition (8.31)\\n\\n\\x0c130\\n\\nGLMs and Regression Trees\\n\\nto family (8.32) gives\\n\\n1\\n2\\n\\nD.(cid:21)1; (cid:21)2/ D E(cid:21)1\\n\\nf.(cid:21)1 (cid:0) (cid:21)2/y (cid:0) ≈í(cid:13).(cid:21)1/ (cid:0) (cid:13).(cid:21)2/(cid:141)g\\n\\n(8.58)\\n\\nD .(cid:21)1 (cid:0) (cid:21)2/(cid:22)1 (cid:0) ≈í(cid:13).(cid:21)1/ (cid:0) (cid:13).(cid:21)2/(cid:141) :\\nIf (cid:21)1 is the MLE O(cid:21) then (cid:22)1 D y (from the maximum likelihood equation\\n0 D d ≈ílog f(cid:21).y/(cid:141)=d(cid:21) D y (cid:0) P(cid:13).(cid:21)/ D y (cid:0) (cid:22)(cid:21)), giving16\\n\\n(cid:17)\\n(cid:16) O(cid:21); (cid:21)\\n\\nD\\n\\nD\\n\\n(cid:16) O(cid:21) (cid:0) (cid:21)\\n\\n(cid:17)\\n\\ny (cid:0)\\n\\nh\\n(cid:13)\\n\\n(cid:17)\\n\\n(cid:16) O(cid:21)\\n\\ni\\n(cid:0) (cid:13).(cid:21)/\\n\\n(8.59)\\n\\n1\\n2\\n\\nfor any choice of (cid:21). But the right-hand side of (8.59) is (cid:0) log≈íf(cid:21).y/=fy.y/(cid:141),\\nverifying (8.33).\\n\\n(cid:142)6 [p. 120] Table 8.5. The galaxy counts are from Loh and Spillar‚Äôs 1988\\n\\nredshift survey, as discussed in Efron and Petrosian (1992).\\n\\n(cid:142)7 [p. 126] Criteria (8.49). Abbreviating ‚Äúleft‚Äù and ‚Äúright‚Äù by l and r, we\\n\\nhave\\n\\ns2\\nk\\n\\nD s2\\nkl\\n\\nC s2\\nkr\\n\\nC Nkl Nkr\\nNk\\n\\n.mkl (cid:0) mkr /2;\\n\\n(8.60)\\n\\nwith Nkl and Nkr the subgroup sizes, showing that minimizing (8.49) is\\nthe same as maximizing the last term in (8.60). Intuitively, a good split is\\none that makes the left and right groups as different as possible, the ideal\\nbeing all 0s on the left and all 1s on the right, making the terminal nodes\\n‚Äúpure.‚Äù\\n\\n16 In some cases O(cid:21) is undeÔ¨Åned; for example, when y D 0 for a Poisson response,\\n\\nO(cid:21) D log.y/ which is undeÔ¨Åned. But, in (8.59), we assume that O(cid:21)y D 0. Similarly for\\nbinary y and the binomial family.\\n\\n\\x0c9\\n\\nSurvival Analysis and the EM Algorithm\\n\\nSurvival analysis had its roots in governmental and actuarial statistics,\\nspanning centuries of use in assessing life expectancies, insurance rates,\\nand annuities. In the 20 years between 1955 and 1975, survival analysis\\nwas adapted by statisticians for application to biomedical studies. Three\\nof the most popular post-war statistical methodologies emerged during\\nthis period: the Kaplan‚ÄìMeier estimate, the log-rank test,1 and Cox‚Äôs pro-\\nportional hazards model, the succession showing increased computational\\ndemands along with increasingly sophisticated inferential justiÔ¨Åcation. A\\nconnection with one of Fisher‚Äôs ideas on maximum likelihood estimation\\nleads in the last section of this chapter to another statistical method that has\\n‚Äúgone platinum,‚Äù the EM algorithm.\\n\\n9.1 Life Tables and Hazard Rates\\n\\nAn insurance company‚Äôs life table appears in Table 9.1, showing its number\\nof clients (that is, life insurance policy holders) by age, and the number of\\ndeaths during the past year in each age group,2 for example Ô¨Åve deaths\\namong the 312 clients aged 59. The column labeled OS is of great interest\\nto the company‚Äôs actuaries, who have to set rates for new policy holders.\\nIt is an estimate of survival probability: probability 0.893 of a person aged\\n30 (the beginning of the table) surviving past age 59, etc. OS is calculated\\naccording to an ancient but ingenious algorithm.\\n\\nLet X represent a typical lifetime, so\\n\\nfi D PrfX D ig\\n\\n(9.1)\\n\\n1 Also known as the Mantel‚ÄìHaenszel or Cochran‚ÄìMantel‚ÄìHaenszel test.\\n2 The insurance company is Ô¨Åctitious but the deaths y are based on the true 2010 rates for\\n\\nUS men, per Social Security Administration data.\\n\\n131\\n\\n\\x0c132\\n\\nSurvival Analysis and the EM Algorithm\\n\\nTable 9.1 Insurance company life table; at each age, n D number of\\npolicy holders, y D number of deaths, Oh D hazard rate y=n, OS D\\nsurvival probability estimate (9.6).\\n\\nAge\\n\\nn\\n\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n48\\n49\\n50\\n51\\n52\\n53\\n54\\n55\\n56\\n57\\n58\\n59\\n\\n116\\n44\\n95\\n97\\n120\\n71\\n125\\n122\\n82\\n113\\n79\\n90\\n154\\n103\\n144\\n192\\n153\\n179\\n210\\n259\\n225\\n346\\n370\\n568\\n1081\\n1042\\n1094\\n597\\n359\\n312\\n\\ny\\n\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n2\\n1\\n1\\n0\\n2\\n2\\n1\\n2\\n4\\n8\\n2\\n10\\n4\\n1\\n5\\n\\nOh\\n\\n.000\\n.000\\n.000\\n.000\\n.000\\n.014\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.010\\n.007\\n.006\\n.000\\n.008\\n.009\\n.003\\n.005\\n.007\\n.007\\n.002\\n.009\\n.007\\n.003\\n.016\\n\\nOS\\n\\n1.000\\n1.000\\n1.000\\n1.000\\n1.000\\n.986\\n.986\\n.986\\n.986\\n.986\\n.986\\n.986\\n.986\\n.986\\n.986\\n.976\\n.969\\n.964\\n.964\\n.956\\n.948\\n.945\\n.940\\n.933\\n.927\\n.925\\n.916\\n.910\\n.908\\n.893\\n\\nAge\\n\\nn\\n\\n60\\n61\\n62\\n63\\n64\\n65\\n66\\n67\\n68\\n69\\n70\\n71\\n72\\n73\\n74\\n75\\n76\\n77\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85\\n86\\n87\\n88\\n89\\n\\n231\\n245\\n196\\n180\\n170\\n114\\n185\\n127\\n127\\n158\\n100\\n155\\n92\\n90\\n110\\n122\\n138\\n46\\n75\\n69\\n95\\n124\\n67\\n112\\n113\\n116\\n124\\n110\\n63\\n79\\n\\ny\\n\\n1\\n5\\n5\\n4\\n2\\n0\\n5\\n2\\n5\\n2\\n3\\n4\\n1\\n1\\n2\\n5\\n8\\n0\\n4\\n6\\n4\\n6\\n7\\n12\\n8\\n12\\n17\\n21\\n9\\n10\\n\\nOh\\n\\n.004\\n.020\\n.026\\n.022\\n.012\\n.000\\n.027\\n.016\\n.039\\n.013\\n.030\\n.026\\n.011\\n.011\\n.018\\n.041\\n.058\\n.000\\n.053\\n.087\\n.042\\n.048\\n.104\\n.107\\n.071\\n.103\\n.137\\n.191\\n.143\\n.127\\n\\nOS\\n\\n.889\\n.871\\n.849\\n.830\\n.820\\n.820\\n.798\\n.785\\n.755\\n.745\\n.723\\n.704\\n.696\\n.689\\n.676\\n.648\\n.611\\n.611\\n.578\\n.528\\n.506\\n.481\\n.431\\n.385\\n.358\\n.321\\n.277\\n.224\\n.192\\n.168\\n\\nis the probability of dying at age i, and\\n\\nSi D X\\n\\nfj D PrfX (cid:21) ig\\n\\nj (cid:21)i\\n\\n(9.2)\\n\\nis the probability of surviving past age i (cid:0) 1. The hazard rate at age i is by\\n\\n\\x0cdeÔ¨Ånition\\n\\n9.1 Life Tables and Hazard Rates\\n\\nhi D fi =Si D PrfX D ijX (cid:21) ig;\\n\\n133\\n\\n(9.3)\\n\\nthe probability of dying at age i given survival past age i (cid:0) 1.\\n\\nA crucial observation is that the probability Sij of surviving past age j\\ngiven survival past age i (cid:0) 1 is the product of surviving each intermediate\\nyear,\\n\\nSij D\\n\\nj\\nY\\n\\nkDi\\n\\n.1 (cid:0) hk/ D PrfX > j jX (cid:21) igI\\n\\n(9.4)\\n\\nÔ¨Årst you have to survive year i, probability 1 (cid:0) hi ; then year i C 1, proba-\\nbility 1 (cid:0) hiC1, etc., up to year j , probability 1 (cid:0) hj . Notice that Si (9.2)\\nequals S1;i (cid:0)1.\\n\\nOS in Table 9.1 is an estimate of Sij for i D 30. First, each hi was\\nestimated as the binomial proportion of the number of deaths yi among the\\nni clients,\\n\\nand then we set\\n\\nOhi D yi =ni ;\\n\\nOS30;j D\\n\\nj\\nY\\n\\nkD30\\n\\n(cid:16)\\n1 (cid:0) Ohk\\n\\n(cid:17)\\n\\n:\\n\\n(9.5)\\n\\n(9.6)\\n\\nThe insurance company doesn‚Äôt have to wait 50 years to learn the proba-\\nbility of a 30-year-old living past 80 (estimated to be 0.506 in the table).\\nOne year‚Äôs data sufÔ¨Åces.3\\n\\nHazard rates are more often described in terms of a continuous positive\\nrandom variable T (often called ‚Äútime‚Äù), having density function f .t/ and\\n‚Äúreverse cdf,‚Äù or survival function,\\n\\nS.t / D\\n\\nZ 1\\n\\nt\\n\\nf .x/ dx D PrfT (cid:21) tg:\\n\\nh.t/ D f .t/=S.t/\\n\\nThe hazard rate\\n\\nsatisÔ¨Åes\\n\\n:D Pr fT 2 .t; t C dt/jT (cid:21) t g\\nfor dt ! 0, in analogy with (9.3). The analog of (9.4) is(cid:142)\\n\\nh.t /dt\\n\\n3 Of course the estimates can go badly wrong if the hazard rates change over time.\\n\\n(9.7)\\n\\n(9.8)\\n\\n(9.9)\\n\\n(cid:142)1\\n\\n\\x0c134\\n\\nSurvival Analysis and the EM Algorithm\\n\\nPrfT (cid:21) t1jT (cid:21) t0g D exp\\n\\nZ t1\\n\\n(cid:26)\\n\\n(cid:0)\\n\\nt0\\n\\n(cid:27)\\n\\nh.x/ dx\\n\\nso in particular the reverse cdf (9.7) is given by\\n\\nS.t/ D exp\\n\\nZ t\\n\\n(cid:26)\\n\\n(cid:0)\\n\\n0\\n\\n(cid:27)\\n\\nh.x/ dx\\n\\n:\\n\\nA one-sided exponential density\\n\\nf .t/ D .1=c/e(cid:0)t=c\\n\\nfor t (cid:21) 0\\n\\nhas S.t/ D expf(cid:0)t=cg and constant hazard rate\\n\\nh.t/ D 1=c:\\n\\n(9.10)\\n\\n(9.11)\\n\\n(9.12)\\n\\n(9.13)\\n\\nThe name ‚Äúmemoryless‚Äù is quite appropriate for density (9.12): having\\nsurvived to any time t, the probability of surviving dt units more is always\\nthe same, about 1 (cid:0) dt=c, no matter what t is. If human lifetimes were\\nexponential there wouldn‚Äôt be old or young people, only lucky or unlucky\\nones.\\n\\n9.2 Censored Data and the Kaplan‚ÄìMeier Estimate\\n\\nTable 9.2 reports the survival data from a randomized clinical trial run by\\nNCOG (the Northern California Oncology Group) comparing two treat-\\nments for head and neck cancer: Arm A, chemotherapy, versus Arm B,\\nchemotherapy plus radiation. The response for each patient is survival time\\nin days. The C sign following some entries indicates censored data, that is,\\nsurvival times known only to exceed the reported value. These are patients\\n‚Äúlost to followup,‚Äù mostly because the NCOG experiment ended with some\\nof the patients still alive.\\n\\nThis is what the experimenters hoped to see of course, but it compli-\\ncates the comparison. Notice that there is more censoring in Arm B. In\\nthe absence of censoring we could run a simple two-sample test, maybe\\nWilcoxon‚Äôs test, to see whether the more aggressive treatment of Arm B\\nwas increasing the survival times. Kaplan‚ÄìMeier curves provide a graph-\\nical comparison that takes proper account of censoring. (The next section\\ndescribes an appropriate censored data two-sample test.) Kaplan‚ÄìMeier\\ncurves have become familiar friends to medical researchers, a lingua franca\\nfor reporting clinical trial results.\\n\\nLife table methods are appropriate for censored data. Table 9.3 puts the\\nArm A results into the same form as the insurance study of Table 9.1, now\\n\\n\\x0c9.2 Censored data and Kaplan‚ÄìMeier\\n\\n135\\n\\nTable 9.2 Censored survival times in days, from two arms of the NCOG\\nstudy of head/neck cancer.\\n\\n7\\n108\\n149\\n218\\n405\\n1116+\\n\\n37\\n133\\n195\\n528+\\n1331+\\n\\n34\\n112\\n154\\n225\\n417\\n1146\\n\\n84\\n140\\n209\\n547+\\n1557\\n\\nArm A: Chemotherapy\\n\\n42\\n129\\n157\\n241\\n420\\n1226+\\n\\n63\\n133\\n160\\n248\\n440\\n1349+\\n\\n64\\n133\\n160\\n273\\n523\\n1412+\\n\\n74+\\n139\\n165\\n277\\n523+\\n1417\\n\\n83\\n140\\n173\\n279+\\n583\\n\\n84\\n140\\n176\\n297\\n594\\n\\n91\\n146\\n185+\\n319+\\n1101\\n\\nArm B: ChemotherapyCRadiation\\n\\n92\\n146\\n249\\n613+\\n1642+\\n\\n94\\n155\\n281\\n633\\n1771+\\n\\n110\\n159\\n319\\n725\\n1776\\n\\n112\\n169+\\n339\\n759+\\n1897+\\n\\n119\\n173\\n432\\n817\\n2023+\\n\\n127\\n179\\n469\\n1092+\\n2146+\\n\\n130\\n194\\n519\\n1245+\\n2297+\\n\\nwith the time unit being months. Of the 51 patients enrolled4 in Arm A,\\ny1 D 1 was observed to die in the Ô¨Årst month after treatment; this left 50 at\\nrisk, y2 D 2 of whom died in the second month; y3 D 5 of the remaining\\n48 died in their third month after treatment, and one was lost to followup,\\nthis being noted in the l column of the table, leaving n4 D 40 patients ‚Äúat\\nrisk‚Äù at the beginning of month 5, etc.\\n\\nOS here is calculated as in (9.6) except starting at time 1 instead of 30.\\nThere is nothing wrong with this estimate, but binning the NCOG survival\\ndata by months is arbitrary. Why not go down to days, as the data was\\noriginally presented in Table 9.2? A Kaplan‚ÄìMeier survival curve is the\\nlimit of life table survival estimates as the time unit goes to zero.\\nObservations zi for censored data problems are of the form\\n\\nzi D .ti ; di /;\\n\\n(9.14)\\n\\nwhere ti equals the observed survival time while di indicates whether or\\nnot there was censoring,\\n\\ndi D\\n\\n(\\n\\n1 if death observed\\n0 if death not observed\\n\\n(9.15)\\n\\n4 The patients were enrolled at different calendar times, as they entered the study, but for\\neach patient ‚Äútime zero‚Äù in the table is set at the beginning of his or her treatment.\\n\\n\\x0c136\\n\\nSurvival Analysis and the EM Algorithm\\n\\nTable 9.3 Arm A of the NCOG head/neck cancer study, binned by month;\\nn D number at risk, y D number of deaths, l D lost to followup, h D\\nhazard rate y=n; OS D life table survival estimate.\\n\\nMonth\\n\\nn\\n\\ny\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n21\\n22\\n23\\n24\\n\\n51\\n50\\n48\\n42\\n40\\n32\\n25\\n24\\n21\\n19\\n16\\n15\\n15\\n15\\n12\\n11\\n11\\n11\\n9\\n9\\n7\\n7\\n7\\n7\\n\\n1\\n2\\n5\\n2\\n8\\n7\\n0\\n3\\n2\\n2\\n0\\n0\\n0\\n3\\n1\\n0\\n0\\n1\\n0\\n2\\n0\\n0\\n0\\n0\\n\\nl\\n\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n\\nh\\n\\n.020\\n.040\\n.104\\n.048\\n.200\\n.219\\n.000\\n.125\\n.095\\n.105\\n.000\\n.000\\n.000\\n.200\\n.083\\n.000\\n.000\\n.091\\n.000\\n.222\\n.000\\n.000\\n.000\\n.000\\n\\nOS\\n\\n.980\\n.941\\n.843\\n.803\\n.642\\n.502\\n.502\\n.439\\n.397\\n.355\\n.355\\n.355\\n.355\\n.284\\n.261\\n.261\\n.261\\n.237\\n.237\\n.184\\n.184\\n.184\\n.184\\n.184\\n\\nMonth\\n\\nn\\n\\ny\\n\\n25\\n26\\n27\\n28\\n29\\n30\\n31\\n32\\n33\\n34\\n35\\n36\\n37\\n38\\n39\\n40\\n41\\n42\\n43\\n44\\n45\\n46\\n47\\n\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n7\\n5\\n4\\n4\\n4\\n3\\n3\\n3\\n3\\n2\\n2\\n\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n\\nl\\n\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n1\\n0\\n1\\n\\nh\\n\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.143\\n.200\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.000\\n.500\\n\\nOS\\n\\n.184\\n.184\\n.184\\n.184\\n.184\\n.184\\n.184\\n.184\\n.184\\n.184\\n.184\\n.184\\n.158\\n.126\\n.126\\n.126\\n.126\\n.126\\n.126\\n.126\\n.126\\n.126\\n.063\\n\\n(so di D 0 corresponds to a C in Table 9.2). Let\\n\\nt.1/ < t.2/ < t.3/ < : : : < t.n/\\n\\n(9.16)\\n\\ndenote the ordered survival times,5 censored or not, with corresponding\\nindicator d.k/ for t.k/. The Kaplan‚ÄìMeier estimate for survival probability\\nS.j / D PrfX > t.j /g is then(cid:142) the life table estimate\\n\\n(cid:142)2\\n\\nOS.j / D Y\\n\\nk(cid:20)j\\n\\n(cid:18) n (cid:0) k\\n\\n(cid:19)d.k/\\n\\nn (cid:0) k C 1\\n\\n:\\n\\n(9.17)\\n\\n5 Assuming no ties among the survival times, which is convenient but not crucial for what\\n\\nfollows.\\n\\n\\x0c9.2 Censored data and Kaplan‚ÄìMeier\\n137\\nOS jumps downward at death times tj , and is constant between observed\\ndeaths.\\n\\nFigure 9.1 NCOG Kaplan‚ÄìMeier survival curves; lower Arm A\\n(chemotherapy only); upper Arm B (chemotherapyCradiation).\\nVertical lines indicate approximate 95% conÔ¨Ådence intervals.\\n\\nThe Kaplan‚ÄìMeier curves for both arms of the NCOG study are shown\\nin Figure 9.1. Arm B, the more aggressive treatment, looks better: its 50%\\nsurvival estimate occurs at 324 days, compared with 182 days for Arm A.\\nThe answer to the inferential question‚Äîis B really better than A or is this\\njust random variability?‚Äîis less clear-cut.\\n\\nThe accuracy of OS.j / can be estimated from Greenwood‚Äôs formula (cid:142) for (cid:142)3\\n\\nits standard deviation (now back in life table notation),\\n\\n(cid:17)\\n\\n(cid:16) OS.j /\\n\\nsd\\n\\nD OS.j /\\n\\n2\\n\\n4\\n\\nX\\n\\nk(cid:20)j\\n\\nyk\\nnk.nk (cid:0) yk/\\n\\n3\\n\\n1=2\\n\\n5\\n\\n:\\n\\n(9.18)\\n\\nThe vertical bars in Figure 9.1 are approximate 95% conÔ¨Ådence limits for\\nthe two curves based on Greenwood‚Äôs formula. They overlap enough to cast\\ndoubt on the superiority of Arm B at any one choice of ‚Äúdays,‚Äù but the two-\\nsample test of the next section, which compares survival at all timepoints,\\nwill provide more deÔ¨Ånitive evidence.\\n\\nLife tables and the Kaplan‚ÄìMeier estimate seem like a textbook example\\nof frequentist inference as described in Chapter 2: a useful probabilistic\\n\\n02004006008001000120014000.00.20.40.60.81.0DaysSurvivalArm A: chemotherapy onlyArm B: chemotherapy + radiation\\x0c138\\n\\nSurvival Analysis and the EM Algorithm\\n\\n(cid:142)4\\n\\nresult is derived (9.4), and then implemented by the plug-in principle (9.6).\\nThere is more to the story though, as discussed below.\\n\\nLife table curves are nonparametric, in the sense that no particular re-\\nlationship is assumed between the hazard rates hi . A parametric approach\\ncan greatly improve the curves‚Äô accuracy. (cid:142) Reverting to the life table form\\nof Table 9.3, we assume that the death counts yk are independent binomi-\\nals,\\n\\nind(cid:24) Bi.nk; hk/;\\n(9.19)\\nand that the logits (cid:21)k D logfhk=.1 (cid:0) hk/g satisfy some sort of regression\\nequation\\n\\nyk\\n\\n(cid:21) D X Àõ;\\n(9.20)\\nas in (8.22). A cubic regression for instance would set xk D .1; k; k2; k3/0\\nfor the kth row of X , with X 47 (cid:2) 4 for Table 9.3.\\n\\nFigure 9.2 Parametric hazard rate estimates for the NCOG study.\\nArm A, black curve, has about 2.5 times higher hazard than\\nArm B for all times more than a year after treatment. Standard\\nerrors shown at 15 and 30 months.\\n\\nThe parametric hazard-rate estimates in Figure 9.2 were instead based\\n\\non a ‚Äúcubic-linear spline,‚Äù\\n\\n(9.21)\\nwhere .k (cid:0) 11/(cid:0) equals k (cid:0) 11 for k (cid:20) 11, and 0 for k (cid:21) 11. The vector\\n\\nxk D (cid:0)1; k; .k (cid:0) 11/2\\n\\n(cid:0); .k (cid:0) 11/3\\n(cid:0)\\n\\n;\\n\\n(cid:1)0\\n\\n0102030400.000.050.100.15MonthsDeaths per MonthArm A: chemotherapy onlyArm B: chemotherapy + radiation\\x0c9.3 The Log-Rank Test\\n\\n139\\n\\n(cid:21) D X Àõ describes a curve that is cubic for k (cid:20) 11, linear for k (cid:21) 11,\\nand joined smoothly at 11. The logistic regression maximum likelihood\\nestimate OÀõ produced hazard rate curves\\n\\nOhk D 1\\n\\n. (cid:16)\\n\\n1 C e(cid:0)x0\\n\\nk\\n\\nOÀõ(cid:17)\\n\\n(9.22)\\n\\nas in (8.8). The black curve in Figure 9.2 traces Ohk for Arm A, while the\\nred curve is that for Arm B, Ô¨Åt separately.\\n\\nComparison in terms of hazard rates is more informative than the sur-\\nvival curves of Figure 9.1. Both arms show high initial hazards, peaking at\\nÔ¨Åve months, and then a long slow decline.6 Arm B hazard is always below\\nArm A, in a ratio of about 2.5 to 1 after the Ô¨Årst year. Approximate 95%\\nconÔ¨Ådence limits, obtained as in (8.30), don‚Äôt overlap, indicating superior-\\nity of Arm B at 15 and 30 months after treatment.\\n\\nIn addition to its frequentist justiÔ¨Åcation, survival analysis takes us into\\nthe Fisherian realm of conditional inference, Section 4.3. The yk‚Äôs in model\\n(9.19) are considered conditionally on the nk‚Äôs, effectively treating the nk\\nvalues in Table 9.3 as ancillaries, that is as Ô¨Åxed constants, by themselves\\ncontaining no statistical information about the unknown hazard rates. We\\nwill examine this tactic more carefully in the next two sections.\\n\\n9.3 The Log-Rank Test\\n\\nA randomized clinical trial, interpreted by a two-sample test, remains the\\ngold standard of medical experimentation. Interpretation usually involves\\nStudent‚Äôs two-sample t-test or its nonparametric cousin Wilcoxon‚Äôs test,\\nbut neither of these is suitable for censored data. The log-rank test (cid:142)\\nemploys an ingenious extension of life tables for the nonparametric two-\\nsample comparison of censored survival data.\\n\\nTable 9.4 compares the results of the NCOG study for the Ô¨Årst six months7\\nafter treatment. At the beginning8 of month 1 there were 45 patients ‚Äúat\\nrisk‚Äù in Arm B, none of whom died, compared with 51 at risk and 1 death\\nin Arm A. This left 45 at risk in Arm B at the beginning of month 2, and\\n50 in Arm A, with 1 and 2 deaths during the month respectively. (Losses\\n\\n(cid:142)5\\n\\n6 The cubic‚Äìlinear spline (9.21) is designed to show more detail in the early months,\\n\\nwhere there is more available patient data and where hazard rates usually change more\\nquickly.\\n\\n7 A month is deÔ¨Åned here as 365/12=30.4 days.\\n8 The ‚Äúbeginning of month 1‚Äù is each patient‚Äôs initial treatment time, at which all 45\\npatients ever enrolled in Arm B were at risk, that is, available for observation.\\n\\n\\x0c140\\n\\nSurvival Analysis and the EM Algorithm\\n\\nTable 9.4 Life table comparison for the Ô¨Årst six months of the NCOG\\nstudy. For example, at the beginning of the sixth month after treatment,\\nthere were 33 remaining Arm B patients, of whom 4 died during the\\nmonth, compared with 32 at risk and 7 dying in Arm A. The conditional\\nexpected number of deaths in Arm A, assuming the null hypothesis of\\nequal hazard rates in both arms, was 5.42, using expression (9.24).\\n\\nMonth\\n\\nArm B\\n\\nArm A\\n\\nAt risk Died\\n\\nAt risk Died\\n\\nExpected number\\nArm A deaths\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n\\n45\\n45\\n44\\n43\\n38\\n33\\n\\n0\\n1\\n1\\n5\\n5\\n4\\n\\n51\\n50\\n48\\n42\\n40\\n32\\n\\n1\\n2\\n5\\n2\\n8\\n7\\n\\n.53\\n1.56\\n3.13\\n3.46\\n6.67\\n5.42\\n\\nto followup were assumed to occur at the end of each month; there was 1\\nsuch at the end of month 3, reducing the number at risk in Arm A to 42 for\\nmonth 4.)\\n\\nThe month 6 data is displayed in two-by-two tabular form in Table 9.5,\\nshowing the notation used in what follows: nA for the number at risk in\\nArm A, nd for the number of deaths, etc.; y indicates the number of Arm A\\ndeaths. If the marginal totals nA; nB ; nd ; and ns are given, then y deter-\\nmines the other three table entries by subtraction, so we are not losing any\\ninformation by focusing on y.\\n\\nTable 9.5 Two-by-two display of month-6 data for the NCOG study. E is\\nthe expected number of Arm A deaths assuming the null hypothesis of\\nequal hazard rates (last column of Table 9.4).\\n\\nDied\\n\\nSurvived\\n\\nArm A\\n\\ny D 7\\nE D 5:42\\n\\nArm B\\n\\n4\\n\\n25\\n\\n29\\n\\nnA D 32\\n\\nnB D 33\\n\\nnd D 11\\n\\nns D 54\\n\\nn D 65\\n\\nConsider the null hypothesis that the hazard rates (9.3) for month 6 are\\n\\n\\x0c9.3 The Log-Rank Test\\n\\nthe same in Arm A and Arm B,\\n\\nH0.6/ W hA6 D hB6:\\n\\nUnder H0.6/, y has mean E and variance V ,\\n\\nE D nAnd =n\\nV D nAnB nd ns\\n\\nƒ± (cid:2)n2.n (cid:0) 1/(cid:3) ;\\n\\n141\\n\\n(9.23)\\n\\n(9.24)\\n\\nas calculated according to the hypergeometric distribution.(cid:142) E D 5:42 and (cid:142)6\\nV D 2:28 in Table 9.5.\\n\\nWe can form a two-by-two table for each of the N D 47 months of the\\nNCOG study, calculating yi ; Ei , and Vi for month i. The log-rank statistic\\nZ is then deÔ¨Åned to be(cid:142)\\n\\n(cid:142)7\\n\\nN\\nX\\n\\n.yi (cid:0) Ei /\\n\\n,   N\\nX\\n\\n!1=2\\n\\nVi\\n\\n:\\n\\nZ D\\n\\ni D1\\n\\ni D1\\n\\n(9.25)\\n\\nThe idea here is simple but clever. Each month we test the null hypothesis\\nof equal hazard rates\\n\\nH0.i/ W hAi D hBi :\\nThe numerator yi (cid:0) Ei has expectation 0 under H0.i/, but, if hAi is greater\\nthan hBi , that is, if treatment B is superior, then the numerator has a pos-\\nitive expectation. Adding up the numerators gives us power to detect a\\ngeneral superiority of treatment B over A, against the null hypothesis of\\nequal hazard rates, hAi D hBi for all i.\\n\\n(9.26)\\n\\nFor the NCOG study, binned by months,\\n\\nN\\nX\\n\\ni D1\\n\\nyi D 42;\\n\\nN\\nX\\n\\ni D1\\n\\nEi D 32:9;\\n\\nN\\nX\\n\\niD1\\n\\nVi D 16:0;\\n\\n(9.27)\\n\\ngiving log-rank test statistic\\n\\nZ D 2:27:\\n\\n(9.28)\\n\\nAsymptotic calculations based on the central limit theorem suggest\\n\\nZ P(cid:24)\\n\\n.0; 1/\\n\\n(9.29)\\n\\nN\\nunder the null hypothesis that the two treatments are equally effective, i.e.,\\nthat hAi D hBi for i D 1; 2; : : : ; N . In the usual interpretation, Z D\\n2:27 is signiÔ¨Åcant at the one-sided 0.012 level, providing moderately strong\\nevidence in favor of treatment B.\\n\\nAn impressive amount of inferential guile goes into the log-rank test.\\n\\n\\x0c142\\n\\nSurvival Analysis and the EM Algorithm\\n\\n1 Working with hazard rates instead of densities or cdfs is essential for\\n\\nsurvival data.\\n\\n2 Conditioning at each period on the numbers at risk, nA and nB in Ta-\\nble 9.5, Ô¨Ånesses the difÔ¨Åculties of censored data; censoring only changes\\nthe at-risk numbers in future periods.\\n\\n3 Also conditioning on the number of deaths and survivals, nd and ns\\nin Table 9.5, leaves only the univariate statistic y to interpret at each\\nperiod, which is easily done through the null hypothesis of equal hazard\\nrates (9.26).\\n\\n4 Adding the discrepancies yi (cid:0) Ei in the numerator of (9.25) (rather than\\n, or adding the\\ni values) accrues power for the natural alternative hypothesis ‚ÄúhAi >\\n\\nsay, adding the individual Z values Zi D .yi (cid:0) Ei /=V 1=2\\nZ2\\nhBi for all i,‚Äù while avoiding destabilization from small values of Vi .\\n\\ni\\n\\nEach of the four tactics had been used separately in classical applica-\\ntions. Putting them together into the log-rank test was a major inferential\\naccomplishment, foreshadowing a still bigger step forward, the propor-\\ntional hazards model, our subject in the next section.\\n\\nConditional inference takes on an aggressive form in the log-rank test.\\nLet Di indicate all the data except yi available at the end of the ith period.\\nFor month 6 in the NCOG study, D6 includes all data for months 1‚Äì5 in\\nTable 9.4, and the marginals nA; nB ; nd ; and ns in Table 9.5, but not the y\\nvalue for month 6. The key assumption is that, under the null hypothesis of\\nequal hazard rates (9.26),\\n\\nyi jDi\\n\\nind(cid:24) .Ei ; Vi /;\\n\\n(9.30)\\n\\n‚Äúind‚Äù here meaning that the yi ‚Äôs can be treated as independent quantities\\nwith means and variances (9.24). In particular, we can add the variances\\nVi to get the denominator of (9.25). (A ‚Äúpartial likelihood‚Äù argument, de-\\nscribed in the endnotes, justiÔ¨Åes adding the variances.)\\n\\nThe purpose of all this Fisherian conditioning is to simplify the infer-\\nence: the conditional distribution yi jDi depends only on the hazard rates\\nhAi and hBi ; ‚Äúnuisance parameters,‚Äù relating to the survival times and cen-\\nsoring mechanism of the data in Table 9.2, are hidden away. There is a price\\nto pay in testing power, though usually a small one. The lost-to-followup\\nvalues l in Table 9.3 have been ignored, even though they might contain\\nuseful information, say if all the early losses occurred in one arm.\\n\\n\\x0c9.4 The Proportional Hazards Model\\n\\n143\\n\\n9.4 The Proportional Hazards Model\\n\\nThe Kaplan‚ÄìMeier estimator is a one-sample device, dealing with data\\ncoming from a single distribution. The log-rank test makes two-sample\\ncomparisons. Proportional hazards ups the ante to allow for a full regres-\\nsion analysis of censored data. Now the individual data points zi are of the\\nform\\n\\nzi D .ci ; ti ; di /;\\n\\n(9.31)\\n\\nwhere ti and di are observed survival time and censoring indicator, as in\\n(9.14)‚Äì(9.15), and ci is a known 1 (cid:2) p vector of covariates whose effect\\non survival we wish to assess. Both of the previous methods are included\\nhere: for the log-rank test, ci indicates treatment, say ci equals 0 or 1 for\\nArm A or Arm B, while ci is absent for Kaplan‚ÄìMeier.\\n\\nTable 9.6 Pediatric cancer data, Ô¨Årst 20 of 1620 children. Sex 1 D male,\\n2 D female; race 1 D white, 2 D nonwhite; age in years; entry D\\ncalendar date of entry in days since July 1, 2001; far D home distance\\nfrom treatment center in miles; t D survival time in days; d D 1 if death\\nobserved, 0 if not.\\n\\nsex\\n\\nrace\\n\\nage\\n\\nentry\\n\\nfar\\n\\nt\\n\\nd\\n\\n1\\n2\\n2\\n2\\n1\\n2\\n2\\n2\\n1\\n2\\n1\\n1\\n1\\n1\\n2\\n1\\n2\\n1\\n1\\n2\\n\\n1\\n1\\n2\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n1\\n2\\n2\\n\\n2.50\\n10.00\\n18.17\\n3.92\\n11.83\\n11.17\\n5.17\\n10.58\\n1.17\\n6.83\\n13.92\\n5.17\\n2.50\\n.83\\n15.50\\n17.83\\n3.25\\n10.75\\n18.08\\n5.83\\n\\n710\\n1866\\n2531\\n2210\\n875\\n1419\\n1264\\n670\\n1518\\n2101\\n1239\\n518\\n1849\\n2758\\n2004\\n986\\n1443\\n2807\\n1229\\n2727\\n\\n108\\n38\\n100\\n100\\n78\\n0\\n28\\n120\\n73\\n104\\n0\\n117\\n99\\n38\\n12\\n65\\n58\\n42\\n23\\n23\\n\\n325\\n1451\\n221\\n2158\\n760\\n168\\n2976\\n1833\\n131\\n2405\\n969\\n1894\\n193\\n1756\\n682\\n1835\\n2993\\n1616\\n1302\\n174\\n\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n0\\n0\\n0\\n0\\n1\\n\\n\\x0c144\\n\\nSurvival Analysis and the EM Algorithm\\n\\nMedical studies regularly produce data of form (9.31). An example, the\\npediatric cancer data, is partially listed in Table 9.6. The Ô¨Årst 20 of n D\\n1620 cases are shown. There are Ô¨Åve explanatory covariates (deÔ¨Åned in the\\ntable‚Äôs caption): sex, race, age at entry, calendar date of entry into\\nthe study, and far, the distance of the child‚Äôs home from the treatment\\ncenter. The response variable t is survival in days from time of treatment\\nuntil death. Happily, only 160 of the children were observed to die (d D\\n1). Some left the study for various reasons, but most of the d D 0 cases\\nwere those children still alive at the end of the study period. Of particular\\ninterest was the effect of far on survival. We wish to carry out a regression\\nanalysis of this heavily censored data set.\\n\\nThe proportional hazards model assumes that the hazard rate hi .t/ for\\n\\nthe ith individual (9.8) is\\n\\nhi .t/ D h0.t/ec0\\n\\ni Àá :\\n\\n(9.32)\\n\\nHere h0.t / is a baseline hazard (which we need not specify) and Àá is an\\nunknown p-parameter vector we want to estimate. For concise notation,\\nlet\\n\\n(cid:18)i D ec0\\n\\ni Àá I\\n\\n(9.33)\\n\\nmodel (9.32) says that individual i‚Äôs hazard is a constant nonnegative factor\\n(cid:18)i times the baseline hazard. Equivalently, from (9.11), the ith survival\\nfunction Si .t / is a power of the baseline survival function S0.t/,\\n\\nSi .t/ D S0.t/(cid:18)i :\\n\\n(9.34)\\n\\nLarger values of (cid:18)i lead to more quickly declining survival curves, i.e., to\\nworse survival (as in (9.11)).\\n\\nLet J be the number of observed deaths, J D 160 here, occurring at\\n\\ntimes\\n\\nT.1/ < T.2/ < : : : < T.J /;\\n\\n(9.35)\\n\\nagain for convenience assuming no ties.9 Just before time T.j / there is a\\nrisk set of individuals still under observation, whose indices we denote by\\n\\nj ,\\n\\nR\\n\\nj D fi W ti (cid:21) T.j /g:\\n\\nR\\n\\n(9.36)\\n\\nLet ij be the index of the individual observed to die at time T.j /. The\\n\\nkey to proportional hazards regression is the following result.\\n\\n9 More precisely, assuming only one event, a death, occurred at T.j /, with none of the\\n\\nother individuals being lost to followup at exact time T.j /.\\n\\n\\x0c9.4 The Proportional Hazards Model\\n\\n145\\n\\nLemma (cid:142) Under the proportional hazards model (9.32), the conditional (cid:142)8\\nprobability, given the risk set\\nserved to die at time T.j / is\\n\\nj , that individual i in\\n\\nj is the one ob-\\n\\nR\\n\\nR\\n\\nPrfij D ij\\n\\nj g D ec0\\ni Àá\\n\\nR\\n\\n(cid:30) X\\n\\nec0\\n\\nk Àá :\\n\\nk2\\n\\nR\\n\\nj\\n\\n(9.37)\\n\\nTo put it in words, given that one person dies at time T.j /, the probability\\nit is individual i is proportional to exp.c0\\ni Àá/, among the set of individuals\\nat risk.\\n\\nFor the purpose of estimating the parameter vector Àá in model (9.32),\\n\\nwe multiply factors (9.37) to form the partial likelihood\\n\\nL.Àá/ D\\n\\n0\\n@ec0\\n\\nij\\n\\nJ\\nY\\n\\nj D1\\n\\n(cid:30) X\\n\\nÀá\\n\\nk2\\n\\nR\\n\\nj\\n\\n1\\n\\nec0\\nk Àá\\n\\nA :\\n\\n(9.38)\\n\\nL.Àá/ is then treated as an ordinary likelihood function, yielding an approx-\\nimately unbiased MLE-like estimate\\n\\nOÀá D arg max\\n\\nÀá\\n\\nfL.Àá/g ;\\n\\n(9.39)\\n\\nwith an approximate covariance obtained from the second-derivative ma-\\ntrix of l.Àá/ D log L.Àá/,(cid:142) as in Section 4.3,\\n(cid:16) OÀá\\n\\n(cid:17)i(cid:0)1(cid:19)\\n\\nOÀá P(cid:24)\\n\\n(9.40)\\n\\n(cid:0) Rl\\n\\nÀá;\\n\\n(cid:18)\\n\\nh\\n\\n:\\n\\n(cid:142)9\\n\\nTable 9.7 shows the proportional hazards analysis of the pediatric can-\\ncer data, with the covariates age, entry, and far standardized to have\\nmean 0 and standard deviation 1 for the 1620 cases.10 Neither sex nor\\nrace seems to make much difference. We see that age is a mildly signif-\\nicant factor, with older children doing better (i.e., the estimated regression\\ncoefÔ¨Åcient is negative). However, the dramatic effects are date of entry\\nand far. Individuals who entered the study later survived longer‚Äîperhaps\\nthe treatment protocol was being improved‚Äîwhile children living farther\\naway from the treatment center did worse.\\n\\nJustiÔ¨Åcation of the partial likelihood calculations is similar to that for\\nthe log-rank test, but there are some important differences, too: the pro-\\nportional hazards model is semiparametric (‚Äúsemi‚Äù because we don‚Äôt have\\nto specify h0.t / in (9.32)), rather than nonparametric as before; and the\\n\\n10 Table 9.7 was obtained using the R program coxph.\\n\\n\\x0c146\\n\\nSurvival Analysis and the EM Algorithm\\n\\nTable 9.7 Proportional hazards analysis of pediatric cancer data (age,\\nentry and far standardized). Age signiÔ¨Åcantly negative, older children\\ndoing better; entry very signiÔ¨Åcantly negative, showing hazard rate\\ndeclining with calendar date of entry; far very signiÔ¨Åcantly positive,\\nindicating worse results for children living farther away from the\\ntreatment center. Last two columns show limits of approximate 95%\\nconÔ¨Ådence intervals for exp.Àá/.\\n\\nÀá\\n\\nsd\\n\\nz-value\\n\\np-value\\n\\nexp.Àá/\\n\\nLower Upper\\n\\n(cid:0).023\\nsex\\nrace\\n.282\\n(cid:0).235\\nage\\nentry (cid:0).460\\nfar\\n.296\\n\\n(cid:0).142\\n.160\\n.169\\n1.669\\n.088 (cid:0)2.664\\n.079 (cid:0)5.855\\n4.117\\n.072\\n\\n.887\\n.095\\n.008\\n.000\\n.000\\n\\n.98\\n1.33\\n.79\\n.63\\n1.34\\n\\n.71\\n.95\\n.67\\n.54\\n1.17\\n\\n1.34\\n1.85\\n.94\\n.74\\n1.55\\n\\nemphasis on likelihood has increased the Fisherian nature of the inference,\\nmoving it further away from pure frequentism. Still more Fisherian is the\\nemphasis on likelihood inference in (9.38)‚Äì(9.40), rather than the direct\\nfrequentist calculations of (9.24)‚Äì(9.25).\\n\\nThe conditioning argument here is less obvious than that for the Kaplan‚Äì\\nMeier estimate or the log-rank test. Has its convenience possibly come at\\ntoo high a price? In fact it can be shown that inference based on the partial\\nlikelihood is highly efÔ¨Åcient, assuming of course the correctness of the\\nproportional hazards model (9.32).\\n\\n9.5 Missing Data and the EM Algorithm\\n\\nCensored data, the motivating factor for survival analysis, can be thought\\nof as a special case of a more general statistical topic, missing data. What‚Äôs\\nmissing, in Table 9.2 for example, are the actual survival times for the\\nC cases, which are known only to exceed the tabled values. If the data\\nwere not missing, we could use standard statistical methods, for instance\\nWilcoxon‚Äôs test, to compare the two arms of the NCOG study. The EM algo-\\nrithm is an iterative technique for solving missing-data inferential problems\\nusing only standard methods.\\n\\nA missing-data situation is shown in Figure 9.3: n D 40 points have\\nbeen independently sampled from a bivariate normal distribution (5.12),\\n\\n\\x0c9.5 Missing Data and the EM Algorithm\\n\\n147\\n\\nFigure 9.3 Forty points from a bivariate normal distribution, the\\nlast 20 with x2 missing (circled).\\n\\nmeans .(cid:22)1; (cid:22)2/, variances .(cid:27) 2\\n\\n1 ; (cid:27) 2\\n\\n2 /, and correlation (cid:26),\\n\\n!\\n\\nind(cid:24)\\n\\nx1i\\nx2i\\n\\n2\\n\\nN\\n\\n!\\n;\\n\\n(cid:22)1\\n(cid:22)2\\n\\n(cid:18) (cid:27) 2\\n1\\n(cid:27)1(cid:27)2(cid:26)\\n\\n(cid:19)!\\n\\n:\\n\\n(cid:27)1(cid:27)2(cid:26)\\n(cid:27) 2\\n2\\n\\n(9.41)\\n\\nHowever, the second coordinates of the last 20 points have been lost. These\\nare represented by the circled points in Figure 9.3, with their x2 values\\narbitrarily set to 0.\\n\\nWe wish to Ô¨Ånd the maximum likelihood estimate of the parameter vec-\\n\\ntor (cid:18) D .(cid:22)1; (cid:22)2, (cid:27)1; (cid:27)2; (cid:26)/. The standard maximum likelihood estimates\\n\\nO(cid:22)1 D\\n\\n40\\nX\\n\\niD1\\n\\nx1i =40;\\n\\nO(cid:22)2 D\\n\\n40\\nX\\n\\nx2i =40;\\n\\nO(cid:27)1 D\\n\\n\" 40\\nX\\n\\ni D1\\n\\n#1=2\\n\\n.x1i (cid:0) O(cid:22)1/2 =40\\n\\n;\\n\\nO(cid:27)2 D\\n\\niD1\\n\" 40\\nX\\n\\ni D1\\n\\n#1=2\\n\\n.x2i (cid:0) O(cid:22)2/2 =40\\n\\n;\\n\\nO(cid:26) D\\n\\n\" 40\\nX\\n\\ni D1\\n\\n.x1i (cid:0) O(cid:22)1/ .x2i (cid:0) O(cid:22)2/ =40\\n\\n. O(cid:27)1 O(cid:27)2/ ;\\n\\n# ,\\n\\n(9.42)\\n\\nllllllllllllllllllllllllllllllllllllllll012345‚àí0.50.00.51.01.52.0x1x2llllllllllllllllllll \\n  \\n\\x0c148\\n\\nSurvival Analysis and the EM Algorithm\\n\\nare unavailable for (cid:22)2, (cid:27)2, and (cid:26) because of the missing data.\\n\\nThe EM algorithm begins by Ô¨Ålling in the missing data in some way, say\\nby setting x2i D 0 for the 20 missing values, giving an artiÔ¨Åcially complete\\ndata set data.0/. Then it proceeds as follows.\\n\\n2 ; O(cid:27) .0/\\n\\n1 ; O(cid:22).0/\\n\\nO(cid:18) .0/ D . O(cid:22).0/\\n\\n(cid:15) The standard method (9.42) is applied to the Ô¨Ålled-in data.0/ to produce\\n2 ; O(cid:26).0//; this is the M (‚Äúmaximizing‚Äù) step.11\\n(cid:15) Each of the missing values is replaced by its conditional expectation\\n(assuming (cid:18) D O(cid:18) .0/) given the nonmissing data; this is the E (‚Äúexpecta-\\ntion‚Äù) step. In our case the missing values x2i are replaced by\\n\\n1 ; O(cid:27) .0/\\n\\nO(cid:22).0/\\n2\\n\\nC O(cid:26).0/\\n\\nO(cid:27) .0/\\n2\\nO(cid:27) .0/\\n1\\n(cid:15) The E and M steps are repeated, at the j th stage giving a new artiÔ¨Åcially\\ncomplete data set data.j / and an updated estimate O(cid:18) .j /. The iteration\\nstops when k O(cid:18) .j C1/ (cid:0) O(cid:18) .j /k is suitably small.\\n\\n(cid:16)\\nx1i (cid:0) O(cid:22).0/\\n1\\n\\n(9.43)\\n\\n(cid:17)\\n\\n:\\n\\nTable 9.8 shows the EM algorithm at work on the bivariate normal ex-\\nample of Figure 9.3. In exponential families the algorithm is guaranteed to\\nconverge to the MLE O(cid:18) based on just the observed data o; moreover, the\\nlikelihood f O(cid:18) .j /.o/ increases with every step j . (The convergence can be\\nsluggish, as it is here for O(cid:27)2 and O(cid:26).)\\n\\nThe EM algorithm ultimately derives from the fake-data principle, a\\nproperty of maximum likelihood estimation going back to Fisher that can\\nonly brieÔ¨Çy be summarized here. (cid:142) Let x D .o; u/ represent the ‚Äúcomplete\\ndata,‚Äù of which o is observed while u is unobserved or missing. Write the\\ndensity for x as\\n\\n(cid:142)10\\n\\nf(cid:18) .x/ D f(cid:18) .o/f(cid:18) .ujo/;\\n\\n(9.44)\\n\\nand let O(cid:18).o/ be the MLE of (cid:18) based just on o.\\n\\nSuppose we now generate simulations of u by sampling from the condi-\\n\\ntional distribution f O(cid:18).o/.ujo/,\\n\\nu(cid:3)k (cid:24) f O(cid:18).o/.ujo/\\n\\nfor k D 1; 2; : : : ; K\\n\\n(9.45)\\n\\n(the stars indicating creation by the statistician and not by observation),\\ngiving fake complete-data values x(cid:3)k D .o; u(cid:3)k/. Let\\ndata(cid:3) D fx(cid:3)1; x(cid:3)2; : : : ; x(cid:3)Kg;\\n\\n(9.46)\\n\\n11 In this example, O(cid:22).0/\\n1\\n\\nand O(cid:27) .0/\\n\\n1\\n\\nare available as the complete-data estimates in (9.42),\\n\\nand, as in Table 9.8, stay the same in subsequent steps of the algorithm.\\n\\n\\x0c9.5 Missing Data and the EM Algorithm\\n\\n149\\n\\nTable 9.8 EM algorithm for estimating means, standard deviations, and\\nthe correlation of the bivariate normal distribution that gave the data in\\nFigure 9.3.\\n\\nStep\\n\\n(cid:22)1\\n\\n(cid:22)2\\n\\n(cid:27)1\\n\\n(cid:27)2\\n\\n(cid:26)\\n\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n11\\n12\\n13\\n14\\n15\\n16\\n17\\n18\\n19\\n20\\n\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n1.86\\n\\n.463\\n.707\\n.843\\n.923\\n.971\\n1.002\\n1.023\\n1.036\\n1.045\\n1.051\\n1.055\\n1.058\\n1.060\\n1.061\\n1.062\\n1.063\\n1.064\\n1.064\\n1.064\\n1.064\\n\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n1.08\\n\\n.738\\n.622\\n.611\\n.636\\n.667\\n.694\\n.716\\n.731\\n.743\\n.751\\n.756\\n.760\\n.763\\n.765\\n.766\\n.767\\n.768\\n.768\\n.769\\n.769\\n\\n.162\\n.394\\n.574\\n.679\\n.736\\n.769\\n.789\\n.801\\n.808\\n.813\\n.816\\n.819\\n.820\\n.821\\n.822\\n.822\\n.823\\n.823\\n.823\\n.823\\n\\n1 f(cid:18) .x(cid:3)k/ yields MLE O(cid:18) (cid:3). It then turns out\\nwhose notional likelihood QK\\nthat O(cid:18) (cid:3) goes to O(cid:18).o/ as K goes to inÔ¨Ånity. In other words, maximum likeli-\\nhood estimation is self-consistent: generating artiÔ¨Åcial data from the MLE\\ndensity f O(cid:18).o/.ujo/ doesn‚Äôt change the MLE. Moreover, any value O(cid:18) .0/ not\\nequal to the MLE O(cid:18).o/ cannot be self-consistent: carrying through (9.45)‚Äì\\n(9.46) using f O(cid:18) .0/.ujo/ leads to hypothetical MLE O(cid:18) .1/ having f O(cid:18) .1/.o/ >\\nf O(cid:18) .0/.o/, etc., a more general version of the EM algorithm.12\\n\\nModern technology allows social scientists to collect huge data sets,\\nperhaps hundreds of responses for each of thousands or even millions of\\nindividuals. Inevitably, some entries of the individual responses will be\\nmissing. Imputation amounts to employing some version of the fake-data\\nprinciple to Ô¨Åll in the missing values. Imputation‚Äôs goal goes beyond Ô¨Ånd-\\n\\n12 Simulation (9.45) is unnecessary in exponential families, where at each stage data(cid:3) can\\nbe replaced by .o; E .j /.ujo//, with E .j / indicating expectation with respect to O(cid:18) .j /,\\nas in (9.43).\\n\\n\\x0c150\\n\\nSurvival Analysis and the EM Algorithm\\n\\n(cid:142)11\\n\\ning the MLE, to the creation of graphs, conÔ¨Ådence intervals, histograms,\\nand more, using only convenient, standard complete-data methods.\\n\\nFinally, returning to survival analysis, the Kaplan‚ÄìMeier estimate (9.17)\\nis itself self-consistent. (cid:142) Consider the Arm A censored observation 74C\\nin Table 9.2. We know that that patient‚Äôs survival time exceeded 74. Sup-\\npose we distribute his probability mass (1=51 of the Arm A sample) to the\\nright, in accordance with the conditional distribution for x > 74 deÔ¨Åned\\nby the Arm A Kaplan‚ÄìMeier survival curve. It turns out that redistributing\\nall the censored cases does not change the original Kaplan‚ÄìMeier survival\\ncurve; Kaplan‚ÄìMeier is self-consistent, leading to its identiÔ¨Åcation as the\\n‚Äúnonparametric MLE‚Äù of a survival function.\\n\\n9.6 Notes and Details\\n\\nThe progression from life tables, Kaplan‚ÄìMeier curves, and the log-rank\\ntest to proportional hazards regression was modest in its computational\\ndemands, until the Ô¨Ånal step. Kaplan‚ÄìMeier curves lie within the capabil-\\nities of mechanical calculators. Not so for proportional hazards, which is\\nemphatically a child of the computer age. As the algorithms grew more in-\\ntricate, their inferential justiÔ¨Åcation deepened in scope and sophistication.\\nThis is a pattern we also saw in Chapter 8, in the progression from bioassay\\nto logistic regression to generalized linear models, and will reappear as we\\nmove from the jackknife to the bootstrap in Chapter 10.\\n\\nCensoring is not the same as truncation. For the truncated galaxy data\\nof Section 8.3, we learn of the existence of a galaxy only if it falls into\\nthe observation region (8.38). The censored individuals in Table 9.2 are\\nknown to exist, but with imperfect knowledge of their lifetimes. There is a\\nversion of the Kaplan‚ÄìMeier curve applying to truncated data, which was\\ndeveloped in the astronomy literature by Lynden-Bell (1971).\\n\\nThe methods of this chapter apply to data that is left-truncated as well\\nas right-censored. In a survival time study of a new HIV drug, for instance,\\nsubject i might not enter the study until some time (cid:28)i after his or her initial\\ndiagnosis, in which case ti would be left-truncated at (cid:28)i , as well as possibly\\nlater right-censored. This only modiÔ¨Åes the composition of the various risk\\nsets. However, other missing-data situations, e.g., left- and right-censoring,\\nrequire more elaborate, less elegant, treatments.\\n\\n(cid:142)1 [p. 133] Formula (9.10). Let the interval ≈ít0; t1(cid:141) be partitioned into a large\\nnumber of subintervals of length dt , with tk the midpoint of subinterval k.\\n\\n\\x0c9.6 Notes and Details\\n\\n151\\n\\nAs in (9.4), using (9.9),\\n\\nPrfT (cid:21) t1jT (cid:21) t0g :D Y\\nD exp\\n:D exp\\n\\n.1 (cid:0) h.ti / dt/\\nnX\\n\\no\\nlog.1 (cid:0) h.ti / dt/\\n\\nn\\n(cid:0) X\\n\\nh.ti / dt\\n\\no\\n\\n;\\n\\n(9.47)\\n\\nwhich, as dt ! 0, goes to (9.10).\\n\\n(cid:142)2 [p. 136] Kaplan‚ÄìMeier estimate. In the life table formula (9.6) (with k D\\n1), let the time unit be small enough to make each bin contain at most one\\nvalue t.k/ (9.16). Then at t.k/,\\n\\nOh.k/ D d.k/\\n\\nn (cid:0) k C 1\\n\\n;\\n\\n(9.48)\\n\\ngiving expression (9.17).\\n\\n(cid:142)3 [p. 137] Greenwood‚Äôs formula (9.18). In the life table formulation of Sec-\\n\\ntion 9.1, (9.6) gives\\n\\nlog OSj D\\n\\nj\\nX\\n\\n1\\n\\n(cid:16)\\n1 (cid:0) Ohk\\n\\n(cid:17)\\n\\n:\\n\\nlog\\n\\nFrom nk\\n\\nOhk\\n\\nind(cid:24) Bi.nk; hk/ we get\\n\\nn\\nlog OSj\\n\\no\\n\\nD\\n\\nvar\\n\\nD\\n\\nj\\nX\\n\\n1\\n\\nj\\nX\\n\\n1\\n\\nn\\n\\nlog\\n\\n(cid:16)\\n1 (cid:0) Ohk\\n\\n(cid:17)o :D\\n\\nvar\\n\\nj\\nX\\n\\n1\\n\\nvar Ohk\\n.1 (cid:0) hk/2\\n\\nhk\\n1 (cid:0) hk\\n\\n1\\nnk\\n\\n;\\n\\n(9.49)\\n\\n(9.50)\\n\\nwhere we have used the delta-method approximation varflog X g :D varfX g=\\nEfX g2. Plugging in hk D yk=nk yields\\n\\nn\\nlog OSj\\n\\no :D\\n\\nvar\\n\\nj\\nX\\n\\nyk\\nnk.nk (cid:0) yk/\\n\\n:\\n\\n(9.51)\\n\\n1\\nThen the inverse approximation varfX g D EfXg2 varflog Xg gives Green-\\nwood‚Äôs formula (9.18).\\n\\nThe censored data situation of Section 9.2 does not enjoy independence\\nbetween the Ohk values. However, successive conditional independence, given\\nthe nk values, is enough to verify the result, as in the partial likelihood cal-\\nculations below. Note: the conÔ¨Ådence intervals in Figure 9.1 were obtained\\n\\n\\x0c152\\n\\nSurvival Analysis and the EM Algorithm\\n\\nby exponentiating the intervals,\\n\\nlog OSj Àô 1:96\\n\\nn\\n\\nh\\nvar\\n\\nlog OSj\\n\\noi1=2\\n\\n:\\n\\n(9.52)\\n\\n(cid:142)4 [p. 138] Parametric life tables analysis. Figure 9.2 and the analysis behind\\nit is developed in Efron (1988), where it is called ‚Äúpartial logistic regres-\\nsion‚Äù in analogy with partial likelihood.\\n\\n(cid:142)5 [p. 139] The log-rank test. This chapter featured an all-star cast, includ-\\ning four of the most referenced papers of the post-war era: Kaplan and\\nMeier (1958), Cox (1972) on proportional hazards, Dempster et al. (1977)\\ncodifying and naming the EM algorithm, and Mantel and Haenszel (1959)\\non the log-rank test. (Cox (1958) gives a careful, and early, analysis of\\nthe Mantel‚ÄìHaenszel idea.) The not very helpful name ‚Äúlog-rank‚Äù does\\nat least remind us that the test depends only on the ranks of the survival\\ntimes, and will give the same result if all the observed survival times ti are\\nmonotonically transformed, say to exp.ti / or t 1=2\\n. It is often referred to as\\nthe Mantel‚ÄìHaenszel or Cochran‚ÄìMantel‚ÄìHaenszel test in older literature.\\nKaplan‚ÄìMeier and proportional hazards are also rank-based procedures.\\n(cid:142)6 [p. 141] Hypergeometric distribution. Hypergeometric calculations, as for\\nTable 9.5, are often stated as follows: n marbles are placed in an urn, nA\\nlabeled A and nB labeled B; nd marbles are drawn out at random; y is\\nthe number of these labeled A. Elementary (but not simple) calculations\\nthen produce the conditional distribution of y given the table‚Äôs marginals\\nnA; nB ; n; nd ; and ns,\\n\\ni\\n\\nPrfyjmarginalsg D\\n\\n! \\n\\nnA\\ny\\n\\nnB\\nnd (cid:0) y\\n\\n!(cid:30) \\n\\n!\\n\\nn\\nnd\\n\\n(9.53)\\n\\nfor\\n\\nmax.nA (cid:0) ns; 0/ (cid:20) y (cid:20) min.nd ; nA/;\\n\\nand expressions (9.24) for the mean and variance. If nA and nB go to inÔ¨Ån-\\nity such that nA=n ! pA and nB =n ! 1 (cid:0) pA, then V ! nd pA.1 (cid:0) pA/,\\nthe variance of y (cid:24) Bi.nd ; pA/.\\n\\n(cid:142)7 [p. 141] Log-rank statistic Z (9.25). Why is .PN\\n\\nnominator for Z? Let ui D yi (cid:0) Ei in (9.30), so Z‚Äôs numerator is PN\\nwith\\n\\n1 Vi /1=2 the correct de-\\n1 ui ,\\n\\nui jDi (cid:24) .0; Vi /\\n\\n(9.54)\\n\\nunder the null hypothesis of equal hazard rates. This implies that, uncon-\\nditionally, Efui g D 0. For j < i , uj is a function of Di (since yj and\\n\\n \\n\\x0c9.6 Notes and Details\\n\\n153\\n\\nEj are), so Efuj ui jDi g D 0, and, again unconditionally, Efuj ui g D 0.\\nTherefore, assuming equal hazard rates,\\n\\n  N\\nX\\n\\nE\\n\\n1\\n\\n!2\\n\\nui\\n\\nD E\\n\\n  N\\nX\\n\\n!\\n\\nN\\nX\\n\\nD\\n\\nu2\\ni\\n\\nvarfui g\\n\\n1\\n\\n(9.55)\\n\\n1\\n\\nVi :\\n\\nN\\nX\\n\\n:D\\n\\n1\\n\\nThe last approximation, replacing unconditional variances varfui g with\\nconditional variances Vi , is justiÔ¨Åed in Crowley (1974), as is the asymp-\\ntotic normality (9.29).\\n\\n(cid:142)8 [p. 145] Lemma (9.37). For i 2\\n\\nj , the probability pi that death occurs in\\n\\nthe inÔ¨Ånitesimal interval .T.j /; T.j / C d T / is hi .T.j // d T , so\\n\\nR\\n\\npi D h0.T.j //ec0\\n\\ni Àá d T;\\n\\n(9.56)\\n\\nand the probability of event Ai that individual i dies while the others don‚Äôt\\nis\\n\\nPi D pi\\n\\nY\\n\\n.1 (cid:0) pk/:\\n\\n(9.57)\\n\\nj (cid:0)i\\nR\\nBut the Ai are disjoint events, so, given that [Ai has occurred, the proba-\\nbility that it is individual i who died is\\n\\nk2\\n\\n(cid:30) X\\n\\nPi\\n\\n:D eci Àá\\n\\nPj\\n\\n(cid:30) X\\n\\neck Àá ;\\n\\nj\\n\\nR\\n\\nk2\\n\\nR\\n\\nj\\n\\n(9.58)\\n\\nthis becoming exactly (9.37) as d T ! 0.\\n\\n(cid:142)9 [p. 145] Partial likelihood (9.40). Cox (1975) introduced partial likelihood\\nas inferential justiÔ¨Åcation for the proportional hazards model, which had\\nbeen questioned in the literature. Let Dj indicate all the observable infor-\\nmation available just before time T.j /(9.35), including all the death or loss\\ntimes for individuals having ti < T.j /. (Notice that Dj determines the risk\\nj .) By successive conditioning we write the full likelihood f(cid:18) .data/\\nset\\nas\\n\\nR\\n\\nf(cid:18) .data/ D f(cid:18) .D1/f(cid:18) .i1j\\nJ\\nY\\n\\nR\\n\\nD\\n\\nf(cid:18) .Dj jDj (cid:0)1/\\n\\n1/f(cid:18) .D2jD1/f(cid:18) .i2j\\n\\nJ\\nY\\n\\nj D1\\n\\nf(cid:18) .ij j\\n\\nj /:\\n\\nR\\n\\nj D1\\n\\n2/ : : :\\n\\nR\\n\\n(9.59)\\n\\nLetting (cid:18) D .Àõ; Àá/, where Àõ is a nuisance parameter vector having to do\\n\\n\\x0c154\\n\\nSurvival Analysis and the EM Algorithm\\n\\nwith the occurrence and timing of events between observed deaths,\\n\\nfÀõ;Àá .data/ D\\n\\n2\\n\\n4\\n\\nJ\\nY\\n\\nj D1\\n\\n3\\n\\nfÀõ;Àá .Dj jDj (cid:0)1/\\n\\n5 L.Àá/;\\n\\n(9.60)\\n\\nwhere L.Àá/ is the partial likelihood (9.38).\\n\\nThe proportional hazards model simply ignores the bracketed factor in\\n(9.60); l.Àá/ D log L.Àá/ is treated as a genuine likelihood, maximized to\\ngive OÀá, and assigned covariance matrix .(cid:0) Rl. OÀá//(cid:0)1 as in Section 4.3. Efron\\n(1977) shows this tactic is highly efÔ¨Åcient for the estimation of Àá.\\n\\n(cid:142)10 [p. 148] Fake-data principle. For any two values of the parameters (cid:18)1 and\\n\\n(cid:18)2 deÔ¨Åne\\n\\nZ\\n\\nl(cid:18)1.(cid:18)2/ D\\n\\n≈ílog f(cid:18)2.o; u/(cid:141) f(cid:18)1.ujo/ d u;\\n\\n(9.61)\\n\\nthis being the limit as K ! 1 of\\n\\nl(cid:18)1.(cid:18)2/ D lim\\nK!1\\n\\n1\\nK\\n\\nK\\nX\\n\\nkD1\\n\\nlog f(cid:18)2.o; u(cid:3)k/;\\n\\n(9.62)\\n\\nthe fake-data log likelihood (9.46) under (cid:18)2, if (cid:18)1 were the true value of (cid:18).\\nUsing f(cid:18) .o; u/ D f(cid:18) .o/f(cid:18) .ujo/, deÔ¨Ånition (9.61) gives\\n(cid:18) f(cid:18)2.ujo/\\n(cid:18) f(cid:18)2.o/\\nf(cid:18)1.ujo/\\nf(cid:18)1.o/\\n(cid:18) f(cid:18)2.o/\\nf(cid:18)1.o/\\n\\nD .f(cid:18)1.ujo/; f(cid:18)2.ujo// ;\\n\\nl(cid:18)1.(cid:18)2/ (cid:0) l(cid:18)1.(cid:18)1/ D log\\n\\nf(cid:18)1.ujo/\\n\\n(cid:0) 1\\n2\\n\\nD log\\n\\n(9.63)\\n\\nlog\\n\\nC\\n\\n(cid:19)\\n\\n(cid:19)\\n\\n(cid:19)\\n\\nZ\\n\\nwith D the deviance (8.31), which is always positive unless ujo has the\\nsame distribution under (cid:18)1 and (cid:18)2, which we will assume doesn‚Äôt happen.\\nSuppose we begin the EM algorithm at (cid:18) D (cid:18)1 and Ô¨Ånd the value (cid:18)2\\nmaximizing l(cid:18)1.(cid:18) /. Then l(cid:18)1.(cid:18)2/ > l(cid:18)1.(cid:18)1/ and D > 0 implies f(cid:18)2.o/ >\\nf(cid:18)1.o/ in (9.63); that is, we have increased the likelihood of the observed\\ndata. Now take (cid:18)1 D O(cid:18) D arg max(cid:18) f(cid:18) .o/. Then the right side of (9.63) is\\nnegative, implying l O(cid:18) . O(cid:18)/ > l O(cid:18) .(cid:18)2/ for any (cid:18)2 not equaling (cid:18)1 D O(cid:18). Putting\\nthis together,13 successively computing (cid:18)1; (cid:18)2; (cid:18)3; : : : by fake-data MLE\\ncalculations increases f(cid:18) .o/ at every step, and the only stable point of the\\nalgorithm is at (cid:18) D O(cid:18).o/.\\n\\n(cid:142)11 [p. 150] Kaplan‚ÄìMeier self-consistency. This property was veriÔ¨Åed in Efron\\n\\n(1967), where the name was coined.\\n\\n13 Generating the fake data is equivalent to the E step of the algorithm, the M step being\\n\\nthe maximization of l(cid:18)j .(cid:18)/.\\n\\n\\x0c10\\n\\nThe Jackknife and the Bootstrap\\n\\nA central element of frequentist inference is the standard error. An algo-\\nrithm has produced an estimate of a parameter of interest, for instance the\\nmean Nx D 0:752 for the 47 ALL scores in the top panel of Figure 1.4.\\nHow accurate is the estimate? In this case, formula (1.2) for the standard\\ndeviation1 of a sample mean gives estimated standard error\\n\\nbse D 0:040;\\n\\n(10.1)\\n\\nso one can‚Äôt take the third digit of Nx D 0:752 very seriously, and even the\\n5 is dubious.\\n\\nDirect standard error formulas like (1.2) exist for various forms of aver-\\naging, such as linear regression (7.34), and for hardly anything else. Tay-\\nlor series approximations (‚Äúdevice 2‚Äù of Section 2.1) extend the formulas\\nto smooth functions of averages, as in (8.30). Before computers, applied\\nstatisticians needed to be Taylor series experts in laboriously pursuing the\\naccuracy of even moderately complicated statistics.\\n\\nThe jackknife (1957) was a Ô¨Årst step toward a computation-based, non-\\nformulaic approach to standard errors. The bootstrap (1979) went further\\ntoward automating a wide variety of inferential calculations, including stan-\\ndard errors. Besides sparing statisticians the exhaustion of tedious routine\\ncalculations the jackknife and bootstrap opened the door for more com-\\nplicated estimation algorithms, which could be pursued with the assurance\\nthat their accuracy would be easily assessed. This chapter focuses on stan-\\ndard errors, with more adventurous bootstrap ideas deferred to Chapter 11.\\nWe end with a brief discussion of accuracy estimation for robust statistics.\\n\\n1 We will use the terms ‚Äústandard error‚Äù and ‚Äústandard deviation‚Äù interchangeably.\\n\\n155\\n\\n\\x0c156\\n\\nThe Jackknife and the Bootstrap\\n\\n10.1 The Jackknife Estimate of Standard Error\\n\\nThe basic applications of the jackknife apply to one-sample problems, where\\nthe statistician has observed an independent and identically distributed (iid)\\nsample x D .x1; x2; : : : ; xn/0 from an unknown probability distribution F\\non some space\\n\\n,\\n\\nX\\n\\nxi\\n\\niid(cid:24) F\\n\\nfor i D 1; 2; : : : ; n:\\n\\n(10.2)\\n\\ncan be anything: the real line, the plane, a function space.2 A real-valued\\n\\nX\\nstatistic O(cid:18) has been computed by applying some algorithm s.(cid:1)/ to x,\\n\\nO(cid:18) D s.x/;\\n(10.3)\\nand we wish to assign a standard error to O(cid:18) . That is, we wish to estimate\\nthe standard deviation of O(cid:18) D s.x/ under sampling model (10.2).\\n\\nLet x.i/ be the sample with xi removed,\\n\\nx.i/ D .x1; x2; : : : ; xi(cid:0)1; xiC1; : : : ; xn/0;\\n\\nand denote the corresponding value of the statistic of interest as\\n\\nO(cid:18).i/ D s.x.i//:\\n\\n(10.4)\\n\\n(10.5)\\n\\nThen the jackknife estimate of standard error for O(cid:18) is\\n\\nbsejack D\\n\\n\"\\n\\nn (cid:0) 1\\nn\\n\\nn\\nX\\n\\n1\\n\\n#1=2\\n\\n(cid:16) O(cid:18).i/ (cid:0) O(cid:18).(cid:1)/\\n\\n(cid:17)2\\n\\n; with O(cid:18).(cid:1)/ D\\n\\nn\\nX\\n\\n1\\n\\nO(cid:18).i/=n:\\n\\n(10.6)\\n\\nIn the case where O(cid:18) is the mean Nx of real values x1; x2; : : : ; xn (i.e.,\\nX\\nis an interval of the real line), O(cid:18).i/ is their average excluding xi , which can\\nbe expressed as\\n\\nO(cid:18).i/ D .n Nx (cid:0) xi /=.n (cid:0) 1/:\\n\\nEquation (10.7) gives O(cid:18).(cid:1)/ D Nx, O(cid:18).i/ (cid:0) O(cid:18).(cid:1)/ D . Nx (cid:0) xi /=.n (cid:0) 1/, and\\n\\nbsejack D\\n\\n\" n\\nX\\n\\n.xi (cid:0) Nx/2= .n.n (cid:0) 1//\\n\\n#1=2\\n\\n;\\n\\ni D1\\n\\n(10.7)\\n\\n(10.8)\\n\\nexactly the same as the classic formula (1.2). This is no coincidence. The\\nfudge factor .n (cid:0) 1/=n in deÔ¨Ånition (10.6) was inserted to make bsejack agree\\nwith (1.2) when O(cid:18) is Nx.\\n2 If X is an interval of the real line we might take F to be the usual cumulative\\n\\ndistribution function, but here we will just think of F as any full description of the\\nprobability distribution for an xi on X .\\n\\n\\x0c10.1 The Jackknife Estimate of Standard Error\\n\\n157\\n\\nThe advantage of bsejack is that deÔ¨Ånition (10.6) can be applied in an au-\\ntomatic way to any statistic O(cid:18) D s.x/. All that is needed is an algorithm\\nthat computes s.(cid:1)/ for the deleted data sets x.i/. Computer power is being\\nsubstituted for theoretical Taylor series calculations. Later we will see that\\nthe underlying inferential ideas‚Äîplug-in estimation of frequentist standard\\nerrors‚Äîhaven‚Äôt changed, only their implementation.\\n\\nAs an example, consider the kidney function data set of Section 1.1. Here\\nthe data consists of n D 157 points .xi ; yi /, with x D age and y D tot in\\nFigure 1.1. (So the generic xi in (10.2) now represents the pair .xi ; yi /, and\\nF describes a distribution in the plane.) Suppose we are interested in the\\ncorrelation between age and tot, estimated by the usual sample correlation\\nO(cid:18) D s.x/,\\n\\ns.x/ D\\n\\n.xi (cid:0) Nx/.yi (cid:0) Ny/\\n\\n.xi (cid:0) Nx/2\\n\\n, \" n\\nX\\n\\nn\\nX\\n\\n.yi (cid:0) Ny/2\\n\\n#1=2\\n\\n; (10.9)\\n\\nn\\nX\\n\\niD1\\n\\n1\\n\\n1\\n\\ncomputed to be O(cid:18) D (cid:0)0:572 for the kidney data.\\n\\nApplying (10.6) gave bsejack D 0:058 for the accuracy of O(cid:18). Nonpara-\\nmetric bootstrap computations, Section 10.2, also gave estimated standard\\nerror 0.058. The classic Taylor series formula looks quite formidable in this\\ncase,\\n\\nbsetaylor D\\n\\n( O(cid:18) 2\\n4n\\n\\n(cid:20) O(cid:22)40\\nO(cid:22)2\\n20\\n\\nC\\n\\nO(cid:22)04\\nO(cid:22)2\\n02\\n\\nC 2 O(cid:22)22\\nO(cid:22)20 O(cid:22)02\\n\\nC 4 O(cid:22)22\\nO(cid:22)2\\n11\\n\\n(cid:0) 4 O(cid:22)31\\nO(cid:22)11 O(cid:22)20\\n\\n(cid:0) 4 O(cid:22)13\\nO(cid:22)11 O(cid:22)02\\n\\n(cid:21))1=2\\n\\nwhere\\n\\nO(cid:22)hk D\\n\\nn\\nX\\n\\n.xi (cid:0) Nx/h.yi (cid:0) Ny/k=n:\\n\\ni D1\\n\\n(10.10)\\n\\n(10.11)\\n\\nIt gave bse D 0:057.\\n\\nIt is worth emphasizing some features of the jackknife formula (10.6).\\n\\n(cid:15) It is nonparametric; no special form of the underlying distribution F\\n\\nneed be assumed.\\n\\n(cid:15) It is completely automatic: a single master algorithm can be written that\\n\\ninputs the data set x and the function s.x/, and outputs bsejack.\\n\\n(cid:15) The algorithm works with data sets of size n(cid:0)1, not n. There is a hidden\\nassumption of smooth behavior across sample sizes. This can be worri-\\nsome for statistics like the sample median that have a different deÔ¨Ånition\\nfor odd and even sample size.\\n\\n\\x0c158\\n\\nThe Jackknife and the Bootstrap\\n\\n(cid:15) The jackknife standard error is upwardly biased as an estimate of the\\n\\n(cid:142)1\\n\\ntrue standard error.(cid:142)\\n\\n(cid:15) The connection of the jackknife formula (10.6) with Taylor series meth-\\n\\nods is closer than it appears. We can write\\n\\nbsejack D\\n\\n(cid:20) Pn\\n\\n1 D2\\ni\\nn2\\n\\n(cid:21)1=2\\n\\n;\\n\\nwhere Di D\\n\\nO(cid:18).i/ (cid:0) O(cid:18).(cid:1)/\\n1=pn.n (cid:0) 1/\\n\\n:\\n\\n(10.12)\\n\\nAs discussed in Section 10.3, the Di are approximate directional deriva-\\ntives, measures of how fast the statistic s.x/ is changing as we decrease\\nthe weight on data point xi . So se2\\njack is proportional to the sum of\\nsquared derivatives of s.x/ in the n component directions. Taylor series\\nexpressions such as (10.10) amount to doing the derivatives by formula\\nrather than numerically.\\n\\nFigure 10.1 The lowess curve for the kidney data of\\nFigure 1.2. Vertical bars indicate Àô2 standard errors: jackknife\\n(10.6) blue dashed; bootstrap (10.16) red solid. The jackknife\\ngreatly overestimates variability at age 25.\\n\\nThe principal weakness of the jackknife is its dependence on local deriva-\\ntives. Unsmooth statistics s.x/, such as the kidney data lowess curve in\\nFigure 1.2, can result in erratic behavior for bsejack. Figure 10.1 illustrates\\nthe point. The dashed blue vertical bars indicate Àô2 jackknife standard er-\\n\\n‚àí4‚àí202Agetot202530354045505560657075808525\\x0c10.2 The Nonparametric Bootstrap\\n\\n159\\n\\nrors for the lowess curve evaluated at ages 20; 25; : : : ; 85. For the most\\npart these agree with the dependable bootstrap standard errors, solid red\\nbars, described in Section 10.2. But things go awry at age 25, where the\\nlocal derivatives greatly overstate the sensitivity of the lowess curve to\\nglobal changes in the sample x.\\n\\n10.2 The Nonparametric Bootstrap\\n\\nFrom the point of view of the bootstrap, the jackknife was a halfway house\\nbetween classical methodology and a full-throated use of electronic com-\\nputation. (The term ‚Äúcomputer-intensive statistics‚Äù was coined to describe\\nthe bootstrap.) The frequentist standard error of an estimate O(cid:18) D s.x/ is,\\nideally, the standard deviation we would observe by repeatedly sampling\\nnew versions of x from F . This is impossible since F is unknown. Instead,\\nthe bootstrap (‚Äúingenious device‚Äù number 4 in Section 2.1) substitutes an\\nestimate OF for F and then estimates the frequentist standard error by direct\\nsimulation, a feasible tactic only since the advent of electronic computa-\\ntion.\\n\\nThe bootstrap estimate of standard error for a statistic O(cid:18) D s.x/ com-\\nputed from a random sample x D .x1; x2; : : : ; xn/ (10.2) begins with the\\nnotion of a bootstrap sample\\n\\nx(cid:3) D .x(cid:3)\\n\\n1 ; x(cid:3)\\n\\n2 ; : : : ; x(cid:3)\\nn/;\\n\\n(10.13)\\n\\nwhere each x(cid:3)\\ni is drawn randomly with equal probability and with replace-\\nment from fx1; x2; : : : ; xng. Each bootstrap sample provides a bootstrap\\nreplication of the statistic of interest,3\\n\\nO(cid:18) (cid:3) D s.x(cid:3)/:\\n\\n(10.14)\\n\\nSome large number B of bootstrap samples are independently drawn\\n(B D 500 in Figure 10.1). The corresponding bootstrap replications are\\ncalculated, say\\n\\nO(cid:18) (cid:3)b D s.x(cid:3)b/\\n\\nfor b D 1; 2; : : : ; B:\\n\\n(10.15)\\n\\nThe resulting bootstrap estimate of standard error for O(cid:18) is the empirical\\n\\n3 The star notation x(cid:3) is intended to avoid confusion with the original data x, which stays\\n\\nÔ¨Åxed in bootstrap computations, and likewise O(cid:18) (cid:3) vis-a-vis O(cid:18).\\n\\n\\x0c160\\nstandard deviation of the O(cid:18) (cid:3)b values,\\n\\nThe Jackknife and the Bootstrap\\n\\nbseboot D\\n\\nbD1\\n\\n\" B\\nX\\n\\n(cid:16) O(cid:18) (cid:3)b (cid:0) O(cid:18) (cid:3)(cid:1)(cid:17)2 .\\n\\n.B (cid:0) 1/\\n\\n#1=2\\n\\n; with O(cid:18) (cid:3)(cid:1) D\\n\\nB\\nX\\n\\nO(cid:18) (cid:3)bƒ±B:\\n\\n(10.16)\\nMotivation for bseboot begins by noting that O(cid:18) is obtained in two steps:\\nÔ¨Årst x is generated by iid sampling from probability distribution F , and\\nthen O(cid:18) is calculated from x according to algorithm s.(cid:1)/,\\n\\nbD1\\n\\niid(cid:0)! x\\n\\ns(cid:0)! O(cid:18):\\n\\nF\\n\\n(10.17)\\n\\nWe don‚Äôt know F , but we can estimate it by the empirical probability dis-\\ntribution OF that puts probability 1=n on each point xi (e.g., weight 1=157\\non each point .xi ; yi / in Figure 1.2). Notice that a bootstrap sample x(cid:3)\\n(10.13) is an iid sample drawn from OF , since then each x(cid:3) independently\\nhas equal probability of being any member of fx1; x2; : : : ; xng. It can be\\nshown that OF maximizes the probability of obtaining the observed sample\\nx under all possible choices of F in (10.2), i.e., it is the nonparametric\\nMLE of F .\\n\\nBootstrap replications O(cid:18) (cid:3) are obtained by a process analogous to (10.17),\\n\\nOF\\n\\niid(cid:0)! x(cid:3)\\n\\ns(cid:0)! O(cid:18) (cid:3):\\n\\n(10.18)\\n\\nIn the real world (10.17) we only get to see the single value O(cid:18), but the boot-\\nstrap world (10.18) is more generous: we can generate as many bootstrap\\nreplications O(cid:18) (cid:3)b as we want, or have time for, and directly estimate their\\nOF approaches F as n grows large\\nvariability as in (10.16). The fact that\\nsuggests, correctly in most cases, that bseboot approaches the true standard\\nerror of O(cid:18) .\\nThe true standard deviation of O(cid:18), i.e., its standard error, can be thought\\nof as a function of the probability distribution F that generates the data,\\nsay Sd.F /. Hypothetically, Sd.F / inputs F and outputs the standard devi-\\nation of O(cid:18) , which we can imagine being evaluated by independently run-\\nning (10.17) some enormous number of times N , and then computing the\\nempirical standard deviation of the resulting O(cid:18) values,\\n\\nSd.F / D\\n\\n2\\n\\n4\\n\\nN\\nX\\n\\nj D1\\n\\n(cid:16) O(cid:18) .j / (cid:0) O(cid:18) .(cid:1)/(cid:17)2 .\\n\\n.N (cid:0) 1/\\n\\n3\\n\\n1=2\\n\\n5\\n\\n; with O(cid:18) .(cid:1)/ D\\n\\nN\\nX\\n\\n1\\n\\nO(cid:18) .j /ƒ±N:\\n\\n(10.19)\\n\\n\\x0c10.2 The Nonparametric Bootstrap\\n\\n161\\n\\nThe bootstrap standard error of O(cid:18) is the plug-in estimate\\n\\nbseboot D Sd. OF /:\\n(10.20)\\nMore exactly, Sd. OF / is the ideal bootstrap estimate of standard error, what\\nwe would get by letting the number of bootstrap replications B go to in-\\nÔ¨Ånity. In practice we have to stop at some Ô¨Ånite value of B, as discussed in\\nwhat follows.\\n\\nAs with the jackknife, there are several important points worth empha-\\n\\nsizing about bseboot.\\n(cid:15) It is completely automatic. Once again, a master algorithm can be writ-\\nten that inputs the data x and the function s.(cid:1)/, and outputs bseboot.\\n(cid:15) We have described the one-sample nonparametric bootstrap. Parametric\\n\\nand multisample versions will be taken up later.\\n\\n(cid:15) Bootstrapping ‚Äúshakes‚Äù the original data more violently than jackknif-\\ning, producing nonlocal deviations of x(cid:3) from x. The bootstrap is more\\ndependable than the jackknife for unsmooth statistics since it doesn‚Äôt\\ndepend on local derivatives.\\n\\n(cid:15) B D 200 is usually sufÔ¨Åcient (cid:142) for evaluating bseboot. Larger values, 1000 (cid:142)2\\nor 2000, will be required for the bootstrap conÔ¨Ådence intervals of Chap-\\nter 11.\\n\\n(cid:15) There is nothing special about standard errors. We could just as well\\nuse the bootstrap replications to estimate the expected absolute error\\nEfj O(cid:18) (cid:0) (cid:18)jg, or any other accuracy measure.\\n\\n(cid:15) Fisher‚Äôs MLE formula (4.27) is applied in practice via\\n\\nO(cid:18) /(cid:0)1=2;\\nI\\n\\nbseÔ¨Åsher D .n\\n\\n(10.21)\\nthat is, by plugging in O(cid:18) for (cid:18) after a theoretical calculation of se. The\\nbootstrap operates in the same way at (10.20), though the plugging in is\\ndone before rather than after the calculation. The connection with Fishe-\\nrian theory is more obvious for the parametric bootstrap of Section 10.4.\\n\\nThe jackknife is a completely frequentist device, both in its assumptions\\nand in its applications (standard errors and biases). The bootstrap is also\\nbasically frequentist, but with a touch of the Fisherian as in the relation\\nwith (10.21). Its versatility has led to applications in a variety of estima-\\ntion and prediction problems, with even some Bayesian connections. (cid:142)\\nUnusual applications can also pop up for the jackknife; see the jackknife-\\nafter-bootstrap comment in the chapter endnotes.(cid:142)\\n\\nFrom a classical point of view, the bootstrap is an incredible computa-\\ntional spendthrift. Classical statistics was fashioned to minimize the hard\\n\\n(cid:142)3\\n\\n(cid:142)4\\n\\n\\x0c162\\n\\nThe Jackknife and the Bootstrap\\n\\nlabor of mechanical computation. The bootstrap seems to go out of its way\\nto multiply it, by factors of B D 200 or 2000 or more. It is nice to re-\\nport that all this computational largesse can have surprising data analytic\\npayoffs.\\n\\nTable 10.1 Correlation matrix for the student score data. The eigenvalues\\nare 3.463, 0.660, 0.447, 0.234, and 0.197. The eigenratio statistic\\nO(cid:18) D 0:693, and its bootstrap standard error estimate is 0.075\\n(B D 2000).\\n\\nmechanics vectors algebra analytics statistics\\n\\nmechanics\\nvectors\\nalgebra\\nanalysis\\nstatistics\\n\\n1.00\\n.50\\n.76\\n.65\\n.54\\n\\n.50\\n1.00\\n.59\\n.51\\n.38\\n\\n.76\\n.59\\n1.00\\n.76\\n.67\\n\\n.65\\n.51\\n.76\\n1.00\\n.74\\n\\n.54\\n.38\\n.67\\n.74\\n1.00\\n\\nThe 22 students of Table 3.1 actually each took Ô¨Åve tests, mechanics,\\nvectors, algebra, analytics, and statistics. Table 10.1 shows\\nthe sample correlation matrix and also its eigenvalues. The ‚Äúeigenratio‚Äù\\nstatistic,\\n\\nO(cid:18) D largest eigenvalue=sum eigenvalues;\\n\\n(10.22)\\n\\nmeasures how closely the Ô¨Åve scores can be predicted by a single linear\\ncombination, essentially an IQ score for each student: O(cid:18) D 0:693 here,\\nindicating strong predictive power for the IQ score. How accurate is 0.693?\\nB D 2000 bootstrap replications (10.15) yielded bootstrap standard er-\\nror estimate (10.16) bseboot D 0:075. (This was 10 times more bootstraps\\nthan necessary for bseboot, but will be needed for Chapter 11‚Äôs bootstrap con-\\nÔ¨Ådence interval calculations.) The jackknife (10.6) gave a bigger estimate,\\nbsejack D 0:083.\\nStandard errors are usually used to suggest approximate conÔ¨Ådence in-\\ntervals, often O(cid:18) Àô 1:96bse for 95% coverage. These are based on an assump-\\ntion of normality for O(cid:18) . The histogram of the 2000 bootstrap replications of\\nO(cid:18), as seen in Figure 10.2, disabuses belief in even approximate normality.\\nCompared with classical methods, a massive amount of computation has\\ngone into the histogram, but this will pay off in Chapter 11 with more ac-\\ncurate conÔ¨Ådence limits. We can claim a double reward here for bootstrap\\nmethods: much wider applicability and improved inferences. The bootstrap\\n\\n\\x0c10.3 Resampling Plans\\n\\n163\\n\\nFigure 10.2 Histogram of B D 2000 bootstrap replications O(cid:18) (cid:3)\\nfor the eigenratio statistic (10.22) for the student score data. The\\nvertical black line is at O(cid:18) D :693. The long left tail shows that\\nnormality is a dangerous assumption in this case.\\n\\nhistogram‚Äîinvisible to classical statisticians‚Äînicely illustrates the advan-\\ntages of computer-age statistical inference.\\n\\n10.3 Resampling Plans\\n\\nThere is a second way to think about the jackknife and the bootstrap:\\nas algorithms that reweight, or resample, the original data vector x D\\n.x1; x2; : : : ; xn/0. At the price of a little more abstraction, resampling con-\\nnects the two algorithms and suggests a class of other possibilities.\\n\\nA resampling vector P D .P1; P2; : : : ; Pn/0 is by deÔ¨Ånition a vector of\\n\\nnonnegative weights summing to 1,\\n\\nP D .P1; P2; : : : ; Pn/0\\n\\nwith Pi (cid:21) 0 and\\n\\nn\\nX\\n\\ni D1\\n\\nPi D 1:\\n\\n(10.23)\\n\\nThat is, P is a member of the simplex\\nn (5.39). Resampling plans operate\\nby holding the original data set x Ô¨Åxed, and seeing how the statistic of\\ninterest O(cid:18) changes as the weight vector P varies across\\n\\nS\\n\\nn.\\n\\nS\\n\\n q^*Frequency0.40.50.60.70.80.9050100150Standard ErrorBootstrap .075Jackknife .083\\x0c164\\n\\nThe Jackknife and the Bootstrap\\n\\nWe denote the value of O(cid:18) for a vector putting weight Pi on xi as\\n\\nO(cid:18) (cid:3) D S.P/;\\n\\n(10.24)\\n\\nthe star notation now indicating any reweighting, not necessarily from boot-\\nstrapping; O(cid:18) D s.x/ describes the behavior of O(cid:18) in the real world (10.17),\\nwhile O(cid:18) (cid:3) D S.P/ describes it in the resampling world. For the sample\\nmean s.x/ D Nx, we have S.P/ D Pn\\n1 Pi xi . The unbiased estimate of\\nvariance s.x/ D Pn\\n\\ni .xi (cid:0) Nx/2=.n (cid:0) 1/ can be seen to have\\n\\nS.P/ D n\\nn (cid:0) 1\\n\\n2\\n\\n4\\n\\nn\\nX\\n\\niD1\\n\\nPi x2\\ni\\n\\n(cid:0)\\n\\n!23\\n5 :\\n\\nPi xi\\n\\n  n\\nX\\n\\niD1\\n\\n(10.25)\\n\\nFigure 10.3 Resampling simplex for sample size n D 3. The\\ncenter point is P0 (10.26); the green circles are the jackknife\\npoints P.i/ (10.28); triples indicate bootstrap resampling numbers\\n.N1; N2; N3/ (10.29). The bootstrap probabilities are 6=27 for\\nP0, 1=27 for each corner point, and 3=27 for each of the six\\nstarred points.\\n\\nLetting\\n\\nP0 D .1; 1; : : : ; 1/0=n;\\n\\n(10.26)\\n\\nP0 P(1) P(2) P(3) (3,0,0) (2,1,0) (1,2,0) (0,3,0) (1,1,1) (2,0,1) (0,2,1) (1,0,2) (0,1,2) (0,0,3) 0.5 1.0 1.5 0.0 ‚Äì 0.5 ‚Äì 1.0 ‚Äì 1.5 0.5 1.0 1.5 0.0 ‚Äì 0.5 2.0 \\x0c10.3 Resampling Plans\\n\\n165\\n\\nthe resampling vector putting equal weight on each value xi , we require in\\nthe deÔ¨Ånition of S.(cid:1)/ that\\n\\nS.P0/ D s.x/ D O(cid:18);\\n(10.27)\\nthe original estimate. The ith jackknife value O(cid:18).i/ (10.5) corresponds to\\nresampling vector\\n\\nP.i/ D .1; 1; : : : ; 1; 0; 1; : : : ; 1/0=.n (cid:0) 1/;\\n\\n(10.28)\\n\\nwith 0 in the ith place. Figure 10.3 illustrates the resampling simplex\\n3\\nS\\napplying to sample size n D 3, with the center point being P0 and the open\\ncircles the three possible jackknife vectors P.i/.\\n\\nWith n D 3 sample points fx1; x2; x3g there are only 10 distinct boot-\\n\\nstrap vectors (10.13), also shown in Figure 10.3. Let\\n\\nNi D #fx(cid:3)\\n\\nD xi g;\\n\\n(10.29)\\n\\nj\\nthe number of bootstrap draws in x(cid:3) equaling xi . The triples in the Ô¨Åg-\\nure are .N1; N2; N3/, for example .1; 0; 2/ for x(cid:3) having x1 once and x3\\ntwice.4 The bootstrap resampling vectors are of the form\\n\\nP (cid:3) D .N1; N2; : : : ; Nn/0=n;\\n\\n(10.30)\\n\\nwhere the Ni are nonnegative integers summing to n. According to deÔ¨Å-\\nnition (10.13) of bootstrap sampling, the vector N D .N1; N2; : : : ; Nn/0\\nfollows a multinomial distribution (5.38) with n draws on n equally likely\\ncategories,\\n\\nN (cid:24) Multn.n; P0/:\\n\\nThis gives bootstrap probability (5.37)\\n\\nn≈†\\nN1≈†N2≈† : : : Nn≈†\\n\\n1\\nnn\\n\\n(10.31)\\n\\n(10.32)\\n\\non P (cid:3) (10.30).\\n\\nFigure 10.3 is misleading in that the jackknife vectors P.i/ appear only\\nslightly closer to P0 than are the bootstrap vectors P (cid:3). As n grows large\\nthey are, in fact, an order of magnitude closer. Subtracting (10.26) from\\n(10.28) gives Euclidean distance\\n\\nkP.i/ (cid:0) P0k D 1\\n\\n.p\\n\\nn.n (cid:0) 1/:\\n\\n(10.33)\\n\\n4 A hidden assumption of deÔ¨Ånition (10.24) is that O(cid:18) D s.x/ has the same value for any\\npermutation of x, so for instance s.x1; x3; x3/ D s.x3; x1; x3/ D S.1=3; 0; 2=3/.\\n\\n\\x0c166\\n\\nThe Jackknife and the Bootstrap\\n\\nFor the bootstrap, notice that Ni in (10.29) has a binomial distribution,\\n\\nNi (cid:24) Bi\\n\\n(cid:18)\\n\\nn;\\n\\n(cid:19)\\n\\n;\\n\\n1\\nn\\n\\n(10.34)\\n\\nwith mean 1 and variance .n (cid:0) 1/=n. Then P (cid:3)\\nD Ni =n has mean and vari-\\ni\\nance .1=n; .n (cid:0) 1/=n3/. Adding over the n coordinates gives the expected\\nroot mean square distance for bootstrap vector P (cid:3),\\n\\n(cid:0)EkP (cid:3) (cid:0) P0k2(cid:1)1=2 D p\\n\\n.n (cid:0) 1/=n2;\\n\\np\\n\\nan order of magnitude\\n\\nn times further than (10.33).\\n\\nThe function S.P/ has approximate directional derivative\\n\\nDi D S.P.i// (cid:0) S.P0/\\nkP.i/ (cid:0) P0k\\n\\n(10.35)\\n\\n(10.36)\\n\\nin the direction from P0 toward P.i/ (measured along the dashed lines\\nin Figure 10.3). Di measures the slope of function S.P/ at P0, in the\\ndirection of P.i/. Formula (10.12) shows bsejack as proportional to the root\\nmean square of the slopes.\\n\\nIf S.P/ is a linear function of P, as it is for the sample mean, it turns\\nout that bsejack equals bseboot (except for the fudge factor .n (cid:0) 1/=n in (10.6)).\\nMost statistics are not linear, and then the local jackknife resamples may\\nprovide a poor approximation to the full resampling behavior of S.P/. This\\nwas the case at one point in Figure 10.1.\\n\\nWith only 10 possible resampling points P (cid:3), we can easily evaluate the\\n\\nideal bootstrap standard error estimate\\n\\nbseboot D\\n\\n\" 10\\nX\\n\\nkD1\\n\\n(cid:16) O(cid:18) (cid:3)k (cid:0) O(cid:18) (cid:3)(cid:1)(cid:17)2\\n\\npk\\n\\n#1=2\\n\\n;\\n\\nO(cid:18) (cid:3)(cid:1) D\\n\\nO(cid:18) (cid:3)k;\\n\\npk\\n\\n(10.37)\\n\\n10\\nX\\n\\nkD1\\n\\nwith O(cid:18) (cid:3)k D S.P k/ and pk the probability from (10.32) (listed in Fig-\\nure 10.3). This rapidly becomes impractical. The number of distinct boot-\\nstrap samples for n points turns out to be\\n!\\n:\\n\\n(10.38)\\n\\n2n (cid:0) 1\\nn\\n\\nFor n D 10 this is already 92,378, while n D 20 gives 6:9 (cid:2) 1010 distinct\\npossible resamples. Choosing B vectors P (cid:3) at random, which is what al-\\ngorithm (10.13)‚Äì(10.15) effectively is doing, makes the un-ideal bootstrap\\nstandard error estimate (10.16) almost as accurate as (10.37) for B as small\\nas 200 or even less.\\n\\n \\n\\x0c10.3 Resampling Plans\\n\\n167\\n\\nThe luxury of examining the resampling surface provides a major advan-\\ntage to modern statisticians, both in inference and methodology. A variety\\nof other resampling schemes have been proposed, a few of which follow.\\n\\nThe InÔ¨Ånitesimal Jackknife\\n\\nLooking at Figure 10.3 again, the vector\\n\\nPi .(cid:15)/ D .1 (cid:0) (cid:15)/P0 C (cid:15)P.i/ D P0 C (cid:15).P.i/ (cid:0) P0/\\n\\n(10.39)\\n\\nlies proportion (cid:15) of the way from P0 to P.i/. Then\\n\\nQDi D lim\\n(cid:15)!0\\n\\nS .Pi .(cid:15)// (cid:0) S.P0/\\n(cid:15)kP.i/ (cid:0) P0k\\n\\n(10.40)\\n\\nexactly deÔ¨Ånes the direction derivative at P0 in the direction of P.i/. The\\ninÔ¨Ånitesimal jackknife estimate of standard error is\\n\\nbseIJ D\\n\\n!1=2\\n\\n;\\n\\nQD2\\n\\ni\\n\\nƒ±n2\\n\\n  n\\nX\\n\\ni D1\\n\\n(10.41)\\n\\nusually evaluated numerically by setting (cid:15) to some small value in (10.40)‚Äì\\n(10.41) (rather than (cid:15) D 1 in (10.12)). We will meet the inÔ¨Ånitesimal jack-\\nknife again in Chapters 17 and 20.\\n\\nMultisample Bootstrap\\n\\nThe median difference between the AML and the ALL scores in Figure 1.4\\nis\\n\\nmediff D 0:968 (cid:0) 0:733 D 0:235:\\n\\n(10.42)\\n\\nHow accurate is 0.235? An appropriate form of bootstrapping draws 25\\ntimes with replacement from the 25 AML patients, 47 times with replace-\\nment from the 47 ALL patients, and computes mediff(cid:3) as the difference\\nbetween the medians of the two bootstrap samples. (Drawing one boot-\\nstrap sample of size 72 from all the patients would result in random sample\\nsizes for the AML(cid:3)=ALL(cid:3) groups, adding inappropriate variability to the\\nfrequentist standard error estimate.)\\n\\nA histogram of B D 500 mediff(cid:3) values appears in Figure 10.4. They\\ngive bseboot D 0:074. The estimate (10.42) is 3.18 bse units above zero, agree-\\ning surprisingly well with the usual two-sample t-statistic 3.01 (based on\\nmean differences), and its permutation histogram Figure 4.3. Permutation\\ntesting can be considered another form of resampling.\\n\\n\\x0c168\\n\\nThe Jackknife and the Bootstrap\\n\\nFigure 10.4 B D 500 bootstrap replications for the median\\ndifference between the AML and ALL scores in Figure 1.4, giving\\nbseboot D 0:074. The observed value mediff D 0:235 (vertical\\nblack line) is more than 3 standard errors above zero.\\n\\nMoving Blocks Bootstrap\\nSuppose x D .x1; x2; : : : ; xn/, instead of being an iid sample (10.2), is a\\ntime series. That is, the x values occur in a meaningful order, perhaps with\\nnearby observations highly correlated with each other. Let\\nm be the set of\\ncontiguous blocks of length m, for example\\n\\nB\\n\\n3 D f.x1; x2; x3/; .x2; x3; x4/; : : : ; .xn(cid:0)2; xn(cid:0)1; xn/g :\\n\\nB\\n\\n(10.43)\\n\\nPresumably, m is chosen large enough that correlations between xi and xj ,\\njj (cid:0) ij > m, are neglible. The moving block bootstrap Ô¨Årst selects n=m\\nblocks from\\nm, and assembles them in random order to construct a boot-\\nstrap sample x(cid:3). Having constructed B such samples, bseboot is calculated\\nas in (10.15)‚Äì(10.16).\\n\\nB\\n\\nThe Bayesian Bootstrap\\n\\nLet G1; G2; : : : ; Gn be independent one-sided exponential variates (de-\\nnoted Gam(1,1) in Table 5.1), each having density exp.(cid:0)x/ for x > 0.\\n\\n mediff*Frequency0.00.10.20.30.40.5010203040506070\\x0c10.4 The Parametric Bootstrap\\n\\n169\\n\\nThe Bayesian bootstrap uses resampling vectors\\n\\nP (cid:3) D .G1; G2; : : : ; Gn/\\n\\n, n\\nX\\n\\nGi :\\n\\n1\\n\\n(10.44)\\n\\nIt can be shown that P (cid:3) is then uniformly distributed over the resampling\\nn; for n D 3, uniformly distributed over the triangle in Fig-\\nsimplex\\nS\\nure 10.3. Prescription (10.44) is motivated by assuming a Jeffreys-style\\nuninformative prior distribution (Section 3.2) on the unknown distribution\\nF (10.2).\\n\\nDistribution (10.44) for P (cid:3) has mean vector and covariance matrix\\n\\nP (cid:3) (cid:24)\\n\\n(cid:20)\\n\\nP0;\\n\\n1\\nn C 1\\n\\n(cid:0)diag.P0/ (cid:0) P0P 0\\n\\n0\\n\\n(cid:21)\\n(cid:1)\\n\\n:\\n\\n(10.45)\\n\\nThis is almost identical to the mean and covariance of bootstrap resamples\\nP (cid:3) (cid:24) Multn.n, P0/=n,\\n\\nP (cid:3) (cid:24)\\n\\n(cid:20)\\n\\nP0;\\n\\n1\\nn\\n\\n(cid:0)diag.P0/ (cid:0) P0P 0\\n\\n0\\n\\n(cid:21)\\n\\n(cid:1)\\n\\n;\\n\\n(10.46)\\n\\n(5.40). The Bayesian bootstrap and the ordinary bootstrap tend to agree, at\\nleast for smoothly deÔ¨Åned statistics O(cid:18) (cid:3) D S.P (cid:3)/.\\n\\nThere was some Bayesian disparagement of the bootstrap when it Ô¨Årst\\nappeared because of its blatantly frequentist take on estimation accuracy.\\nAnd yet connections like (10.45)‚Äì(10.46) have continued to pop up, as we\\nwill see in Chapter 13.\\n\\n10.4 The Parametric Bootstrap\\n\\nIn our description (10.18) of bootstrap resampling,\\n\\nOF\\n\\niid(cid:0)! x(cid:3) (cid:0)! O(cid:18) (cid:3);\\n(10.47)\\nthere is no need to insist that OF be the nonparametric MLE of F . Suppose\\nwe are willing to assume that the observed data vector x comes from a\\nparametric family\\n\\nas in (5.1),\\n\\nF\\n\\nF\\n\\nD Àöf(cid:22).x/; (cid:22) 2 (cid:127)(cid:9) :\\n\\n(10.48)\\n\\nLet O(cid:22) be the MLE of (cid:22). The bootstrap parametric resamples from f O(cid:22).(cid:1)/,\\nf O(cid:22) (cid:0)! x(cid:3) (cid:0)! O(cid:18) (cid:3);\\n\\n(10.49)\\n\\nand proceeds as in (10.14)‚Äì(10.16) to calculate bseboot.\\n\\n\\x0c170\\n\\nThe Jackknife and the Bootstrap\\n\\nAs an example, suppose that x D .x1; x2; : : : ; xn/ is an iid sample of\\n\\nsize n from a normal distribution,\\n\\n(10.50)\\n2 ; : : : ; x(cid:3)\\nn/,\\n\\niid(cid:24)\\n\\nxi\\n\\nN\\n\\n.(cid:22); 1/;\\n\\ni D 1; 2; : : : ; n:\\n\\nThen O(cid:22) D Nx, and a parametric bootstrap sample is x(cid:3) D .x(cid:3)\\nwhere\\n\\n1 ; x(cid:3)\\n\\nx(cid:3)\\ni\\n\\niid(cid:24)\\n\\n. Nx; 1/;\\n\\ni D 1; 2; : : : ; n:\\n\\n(10.51)\\n\\nN\\nMore adventurously, if\\n\\nF\\n\\nwere a family of time series models for x,\\nalgorithm (10.49) would still apply (now without any iid structure): x(cid:3)\\nwould be a time series sampled from model f O(cid:22).(cid:1)/, and O(cid:18) (cid:3) D s.x(cid:3)/ the\\nresampled statistic of interest. B independent realizations x(cid:3)b would give\\nO(cid:18) (cid:3)b, b D 1; 2; : : : ; B, and bseboot from (10.16).\\n\\nFigure 10.5 The gfr data of Figure 5.7 (histogram). Curves\\nshow the MLE Ô¨Åts from polynomial Poisson models, for degrees\\nof freedom df D 2; 3; : : : ; 7. The points on the curves show the\\nÔ¨Åts computed at the centers x.j / of the bins, with the responses\\nbeing the counts in the bins. The dashes at the base of the plot\\nshow the nine gfr values appearing in Table 10.2.\\n\\nAs an example of parametric bootstrapping, Figure 10.5 expands the\\ngfr investigation of Figure 5.7. In addition to the seventh-degree polyno-\\nmial Ô¨Åt (5.62), we now show lower-degree polynomial Ô¨Åts for 2, 3, 4, 5,\\n\\n gfrCounts20406080100051015202530lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllldf = 2df = 3,4,5,6df = 7\\x0c10.4 The Parametric Bootstrap\\n\\n171\\n\\nand 6 degrees of freedom; df D 2 obviously gives a poor Ô¨Åt; df D 3; 4; 5; 6\\ngive nearly identical curves; df D 7 gives only a slightly better Ô¨Åt to the\\nraw data.\\n\\nThe plotted curves were obtained from the Poisson regression method\\n\\nused in Section 8.3, which we refer to as ‚ÄúLindsey‚Äôs method‚Äù.\\n\\n(cid:15) The x-axis was partitioned into K D 32 bins, with endpoints 13; 16; 19,\\n\\n: : : ; 109, and centerpoints, say,\\n\\nx. / D .x.1/; x.2/; : : : ; x.K//;\\n\\n(10.52)\\n\\nx.1/ D 14:5, x.2/ D 17:5, etc.\\n\\n(cid:15) Count vector y D .y1; y2; : : : ; yK/ was computed\\n\\nyk D #fxi in binkg\\n\\n(10.53)\\n\\n(so y gives the heights of the bars in Figure 10.5).\\n\\n(cid:15) An independent Poisson model was assumed for the counts,\\n\\nyk\\n\\nind(cid:24) Poi.(cid:22)k/\\n\\n(10.54)\\n(cid:15) The parametric model of degree ‚Äúdf‚Äù assumed that the (cid:22)k values were\\ndescribed by an exponential polynomial of degree df in the x.k/ values,\\n\\nfor k D 1; 2; : : : ; K:\\n\\nlog.(cid:22)k/ D\\n\\ndf\\nX\\n\\nj D0\\n\\nÀáj xj\\n\\n.k/:\\n\\n(10.55)\\n\\n(cid:15) The MLE OÀá D . OÀá0; OÀá1; : : : ; OÀádf/ in model (10.54)‚Äì(10.55) was found.5\\n(cid:15) The plotted curves in Figure 10.5 trace the MLE values O(cid:22)k,\\n\\nlog. O(cid:22)k/ D\\n\\ndf\\nX\\n\\nj D0\\n\\nOÀáj xj\\n\\n.k/:\\n\\n(10.56)\\n\\nHow accurate are the curves? Parametric bootstraps were used to assess\\ntheir standard errors. That is, Poisson resamples were generated according\\nto\\n\\ny(cid:3)\\nk\\n\\nind(cid:24) Poi. O(cid:22)k/\\n\\nfor k D 1; 2; : : : ; K;\\n\\n(10.57)\\n\\nand bootstrap MLE values O(cid:22)(cid:3)\\nk calculated as above, but now based on count\\nvector y (cid:3) rather than y. All of this was done B D 200 times, yielding\\nbootstrap standard errors (10.16).\\n\\nThe results appear in Table 10.2, showing bseboot for df D 2; 3; : : : ; 7\\n\\n5 A single R command, glm(y(cid:24)poly(x,df),family=poisson) accomplishes\\n\\nthis.\\n\\n\\x0c172\\n\\nThe Jackknife and the Bootstrap\\n\\nTable 10.2 Bootstrap estimates of standard error for the gfr density.\\nPoisson regression models (10.54)‚Äì(10.55), df D 2; 3; : : : ; 7, as in\\nFigure 10.5; each B D 200 bootstrap replications; nonparametric\\nstandard errors based on binomial bin counts.\\n\\nDegrees of freedom\\n\\ngfr\\n\\n20.5\\n29.5\\n38.5\\n47.5\\n56.5\\n65.5\\n74.5\\n83.5\\n92.5\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\n7\\n\\n.28\\n.65\\n1.05\\n1.47\\n1.57\\n1.15\\n.76\\n.40\\n.13\\n\\n.07\\n.57\\n1.39\\n1.91\\n1.60\\n1.10\\n.61\\n.30\\n.20\\n\\n.13\\n.57\\n1.33\\n2.12\\n1.79\\n1.07\\n.62\\n.40\\n.29\\n\\n.13\\n.66\\n1.52\\n1.93\\n1.93\\n1.31\\n.68\\n.38\\n.29\\n\\n.12\\n.74\\n1.72\\n2.15\\n1.87\\n1.34\\n.81\\n.49\\n.34\\n\\n.05\\n1.11\\n1.73\\n2.39\\n2.28\\n1.27\\n.71\\n.68\\n.46\\n\\nNonparametric\\nstandard error\\n\\n.00\\n1.72\\n2.77\\n4.25\\n4.35\\n1.72\\n1.72\\n1.72\\n.00\\n\\ndegrees of freedom evaluated at nine values of gfr. Variability generally\\nincreases with increasing df, as expected. Choosing a ‚Äúbest‚Äù model is a\\ncompromise between standard error and possible deÔ¨Ånitional bias as sug-\\ngested by Figure 10.5, with perhaps df D 3 or 4, the winner.\\n\\nIf we kept increasing the degrees of freedom, eventually (at df D 32)\\nwe would exactly match the bar heights yk in the histogram. At this point\\nthe parametric bootstrap would merge into the nonparametric bootstrap.\\n‚ÄúNonparametric‚Äù is another name for ‚Äúvery highly parameterized.‚Äù The\\nhuge sample sizes associated with modern applications have encouraged\\nnonparametric methods, on the sometimes mistaken ground that estimation\\nefÔ¨Åciency is no longer of concern. It is costly here, as the ‚Äúnonparametric‚Äù\\ncolumn of Table 10.2 shows.6\\n\\nFigure 10.6 returns to the student score eigenratio calculations of Fig-\\nure 10.2. The solid histogram shows 2000 parametric bootstrap replica-\\ntions (10.49), with f O(cid:22) the Ô¨Åve-dimensional bivariate normal distribution\\n5. Nx; O‚Ä†/. Here Nx and O‚Ä† are the usual MLE estimates for the expectation\\nN\\nvector and covariance matrix based on the 22 Ô¨Åve-component student score\\nvectors. It is narrower than the corresponding nonparametric bootstrap his-\\ntogram, with bseboot D 0:070 compared with the nonparametric estimate\\n\\n6 These are the binomial standard errors ≈íyk.n (cid:0) yk/=n(cid:141)1=2, n D 211. The\\n\\nnonparametric results look much more competitive when estimating cdf‚Äôs rather than\\ndensities.\\n\\n\\x0c10.4 The Parametric Bootstrap\\n\\n173\\n\\nFigure 10.6 Eigenratio example, student score data. Solid\\nhistogram B D 2000 parametric bootstrap replications O(cid:18) (cid:3) from\\nthe Ô¨Åve-dimensional normal MLE; line histogram the 2000\\nnonparametric replications of Figure 10.2. MLE O(cid:18) D :693 is\\nvertical red line.\\n\\n0.075. (Note the different histogram bin limits from Figure 10.2, changing\\nthe details of the nonparametric histogram.)\\n\\nParametric families act as regularizers, smoothing out the raw data and\\nde-emphasizing outliers. In fact the student score data is not a good can-\\ndidate for normal modeling, having at least one notable outlier,7 casting\\ndoubt on the smaller estimate of standard error.\\n\\nThe classical statistician could only imagine a mathematical device that\\ngiven any statistic O(cid:18) D s.x/ would produce a formula for its standard er-\\nror, as formula (1.2) does for Nx. The electronic computer is such a device.\\nAs harnessed by the bootstrap, it automatically produces a numerical esti-\\nmate of standard error (though not a formula), with no further cleverness\\nrequired. Chapter 11 discusses a more ambitious substitution of computer\\npower for mathematical analysis: the bootstrap computation of conÔ¨Ådence\\nintervals.\\n\\n7 As revealed by examining scatterplots of the Ô¨Åve variates taken two at a time. Fast and\\n\\npainless plotting is another advantage for twenty-Ô¨Årst-century data analysts.\\n\\n eigenratio*Frequency0.40.50.60.70.80.9020406080100120Bootstrap Standard ErrorsNonparametric .075Parametric .070\\x0c174\\n\\nThe Jackknife and the Bootstrap\\n\\n10.5 InÔ¨Çuence Functions and Robust Estimation\\n\\nThe sample mean played a dominant role in classical statistics for reasons\\nheavily weighted toward mathematical tractibility. Beginning in the 1960s,\\nan important counter-movement, robust estimation, aimed to improve upon\\nthe statistical properties of the mean. A central element of that theory, the\\ninÔ¨Çuence function, is closely related to the jackknife and inÔ¨Ånitesimal jack-\\nknife estimates of standard error.\\n\\nX\\n\\nWe will only consider the case where\\n\\n, the sample space, is an interval\\nof the real line. The unknown probability distribution F yielding the iid\\nsample x D .x1; x2; : : : ; xn/ in (10.2) is now the cdf of a density function\\nf .x/ on\\n. A parameter of interest, i.e., a function of F , is to be estimated\\nby the plug-in principle, O(cid:18) D T . OF /, where, as in Section 10.2,\\nOF is the\\nempirical probability distribution putting probability 1=n on each sample\\npoint xi . For the mean,\\n\\nX\\n\\n(cid:18) D T .F / D\\n\\nZ\\n\\nX\\n\\nxf .x/ dx and\\n\\nO(cid:18) D T\\n\\n(cid:17)\\n\\n(cid:16) OF\\n\\nD 1\\nn\\n\\nn\\nX\\n\\ni D1\\n\\nxi :\\n\\n(10.58)\\n\\n(In Riemann‚ÄìStieltjes notation, (cid:18) D R xdF .x/ and O(cid:18) D R xd OF .x/.)\\n\\nThe inÔ¨Çuence function of T .F /, evaluated at point x in\\n\\n, is deÔ¨Åned to\\n\\nX\\n\\nbe\\n\\nIF.x/ D lim\\n(cid:15)!0\\n\\nT ..1 (cid:0) (cid:15)/F C (cid:15)ƒ±x/ (cid:0) T .F /\\n(cid:15)\\n\\n;\\n\\n(10.59)\\n\\nwhere ƒ±x is the ‚Äúone-point probability distribution‚Äù putting probability 1\\non x. In words, IF.x/ measures the differential effect of modifying F by\\nputting additional probability on x. For the mean (cid:18) D R xf .x/dx we cal-\\nculate that\\n\\nIF.x/ D x (cid:0) (cid:18):\\n\\n(10.60)\\n\\n(cid:142)5\\n\\nA fundamental theorem(cid:142) says that O(cid:18) D T . OF / is approximately\\n\\nO(cid:18)\\n\\n:D (cid:18) C 1\\nn\\n\\nn\\nX\\n\\niD1\\n\\nIF.xi /;\\n\\n(10.61)\\n\\nwith the approximation becoming exact as n goes to inÔ¨Ånity. This implies\\nthat O(cid:18) (cid:0) (cid:18) is, approximately, the mean of the n iid variates IF.xi /, and that\\nthe variance of O(cid:18) is approximately\\n\\nvar\\n\\nn O(cid:18)\\n\\no :D 1\\nn\\n\\nvar fIF.x/g ;\\n\\n(10.62)\\n\\n\\x0c10.5 InÔ¨Çuence Functions and Robust Estimation\\n\\n175\\n\\nvarfxg:\\n\\nvarfIF.x/g being the variance of IF.x/ for any one draw of x from F . For\\nthe sample mean, using (10.60) in (10.62) gives the familiar equality\\nvarf Nxg D 1\\nn\\nThe sample mean suffers from an unbounded inÔ¨Çuence function (10.60),\\nwhich grows ever larger as x moves farther from (cid:18). This makes Nx unstable\\nagainst heavy-tailed densities such as the Cauchy (4.39). Robust estimation\\ntheory seeks estimators O(cid:18) of bounded inÔ¨Çuence, that do well against heavy-\\ntailed densities without giving up too much efÔ¨Åciency against light-tailed\\ndensities such as the normal. Of particular interest have been the trimmed\\nmean and its close cousin the winsorized mean.\\n\\n(10.63)\\n\\nLet x.Àõ/ denote the 100Àõth percentile of distribution F , satisfying F .x.Àõ//\\n\\nD Àõ or equivalently\\n\\nÀõ D\\n\\nZ x.Àõ/\\n\\n(cid:0)1\\n\\nf .x/ dx:\\n\\nThe Àõth trimmed mean of F , (cid:18)trim.Àõ/, is deÔ¨Åned as\\n(cid:18)trim.Àõ/ D 1\\n\\nZ x.1(cid:0)Àõ/\\n\\nxf .x/ dx;\\n\\n1 (cid:0) 2Àõ\\n\\nx.Àõ/\\n\\n(10.64)\\n\\n(10.65)\\n\\nthe mean of the central 1 (cid:0) 2Àõ portion of F , trimming off the lower and\\nupper Àõ portions. This is not the same as the Àõth winsorized mean (cid:18)wins.Àõ/,\\n\\nwhere\\n\\n(cid:18)wins.Àõ/ D\\n\\nZ\\n\\nX\\n\\nW .x/f .x/ dx;\\n\\n(10.66)\\n\\nW .x/ D\\n\\n8\\nx.Àõ/\\nÀÜ<\\nx\\nÀÜ:\\nx.1(cid:0)Àõ/\\n\\nif x (cid:20) x.Àõ/\\nif x.Àõ/ (cid:20) x (cid:20) x.1(cid:0)Àõ/\\nif x (cid:21) x.1(cid:0)Àõ/I\\n\\n(10.67)\\n\\n(cid:18)trim.Àõ/ removes the outer portions of F , while (cid:18)wins.Àõ/ moves them into\\nx.Àõ/ or x.1(cid:0)Àõ/. In practice, empirical versions O(cid:18)trim.Àõ/ and O(cid:18)wins.Àõ/ are used,\\nsubstituting the empirical density Of , with probability 1=n at each xi , for\\nf .\\n\\nThere turns out to be an interesting relationship between the two: the\\n\\ninÔ¨Çuence function of (cid:18)trim.Àõ/ is a function of (cid:18)wins.Àõ/,\\n\\nIFÀõ.x/ D W .x/ (cid:0) (cid:18)wins.Àõ/\\n\\n1 (cid:0) 2Àõ\\n\\n:\\n\\n(10.68)\\n\\nThis is pictured in Figure 10.7, where we have plotted empirical inÔ¨Çuence\\n\\n\\x0c176\\n\\nThe Jackknife and the Bootstrap\\n\\nFigure 10.7 Empirical inÔ¨Çuence functions for the 47 leukemia\\nALL scores of Figure 1.4. The two dashed curves are IFÀõ.x/ for\\nthe trimmed means (10.68), for Àõ D 0:2 and Àõ D 0:4. The solid\\ncurve is IF.x/ for the sample mean Nx (10.60).\\n\\nfunctions (plugging in OF for F in deÔ¨Ånition (10.59)) relating to the 47\\nleukemia ALL scores of Figure 1.4: IF0:2.x/ and IF0:4.x/ are plotted, along\\nwith IF0.x/ (10.60), that is, for the mean.\\n\\nTable 10.3 Trimmed means and their bootstrap standard deviations for\\nthe 47 leukemia ALL scores of Figure 1.4; B D 1000 bootstrap\\nreplications for each trim value. The last column gives empirical inÔ¨Çuence\\nfunction estimates of the standard error, which are also the inÔ¨Ånitesimal\\njackknife estimates (10.41). These fail for the median.\\n\\nTrimmed Bootstrap\\n\\nTrim\\n\\nmean\\n\\nMean\\n\\nMedian\\n\\n.0\\n.1\\n.2\\n.3\\n.4\\n.5\\n\\n.752\\n.729\\n.720\\n.725\\n.734\\n.733\\n\\nsd\\n\\n.040\\n.038\\n.035\\n.044\\n.047\\n.053\\n\\n(IFse)\\n\\n(.040)\\n(.034)\\n(.034)\\n(.044)\\n(.054)\\n\\n0.20.40.60.81.01.21.41.6‚àí0.50.00.51.0ALLInfluence Functionmeantrimmed mean a = 0. 2trimmed mean a = 0. 4\\x0c10.6 Notes and Details\\n\\n177\\n\\nThe upper panel of Figure 1.4 shows a moderately heavy right tail for\\nthe ALL distribution. Would it be more efÔ¨Åcient to estimate the center of\\nthe distribution with a trimmed mean rather than Nx? The bootstrap pro-\\nvides an answer: bseboot (10.16) was calculated for Nx and O(cid:18)trim.Àõ/, Àõ D\\n0:1; 0:2; 0:3; 0:4, and 0.5, the last being the sample median. It appears that\\nO(cid:18)trim.0:2/ is moderately better than Nx. This brings up an important question\\ndiscussed in Chapter 20: if we use something like Table 10.3 to select an\\nestimator, how does the selection process affect the accuracy of the result-\\ning estimate?\\n\\nWe might also use the square root of formula (10.62) to estimate the\\nstandard errors of the various estimators, plugging in the empirical inÔ¨Çu-\\nence function for IF.x/. This turns out to be the same as using the inÔ¨Ånites-\\nimal jackknife (10.41). These appear in the last column of Table 10.3. Pre-\\ndictably, this approach fails for the sample median, whose inÔ¨Çuence func-\\ntion is a square wave, sharply discontinuous at the median (cid:18),\\n\\nIF.x/ D Àô1ƒ± .2f .(cid:18)// :\\n\\n(10.69)\\n\\nRobust estimation offers a nice illustration of statistical progress in the\\ncomputer age. Trimmed means go far back into the classical era. InÔ¨Çuence\\nfunctions are an insightful inferential tool for understanding the tradeoffs in\\ntrimmed mean estimation. And Ô¨Ånally the bootstrap allows easy assessment\\nof the accuracy of robust estimation, including some more elaborate ones\\nnot discussed here.\\n\\n10.6 Notes and Details\\n\\nQuenouille (1956) introduced what is now called the jackknife estimate\\nof bias. Tukey (1958) realized that Quenouille-type calculations could be\\nrepurposed for nonparametric standard-error estimation, inventing formula\\n(10.6) and naming it ‚Äúthe jackknife,‚Äù as a rough and ready tool. Miller‚Äôs im-\\nportant 1964 paper, ‚ÄúA trustworthy jackknife,‚Äù asked when formula (10.6)\\ncould be trusted. (Not for the median.)\\n\\nThe bootstrap (Efron, 1979) began as an attempt to better understand\\nthe jackknife‚Äôs successes and failures. Its name celebrates Baron Mun-\\nchausen‚Äôs success in pulling himself up by his own bootstraps from the\\nbottom of a lake. Burgeoning computer power soon overcame the boot-\\nstrap‚Äôs main drawback, prodigous amounts of calculation, propelling it into\\ngeneral use. Meanwhile, 1000C theoretical papers were published asking\\nwhen the bootstrap itself could be trusted. (Most but not all of the time in\\ncommon practice.)\\n\\n\\x0c178\\n\\nThe Jackknife and the Bootstrap\\n\\nA main reference for the chapter is Efron‚Äôs 1982 monograph The Jack-\\nknife, the Bootstrap and Other Resampling Plans. Its Chapter 6 shows the\\nequality of three nonparametric standard error estimates: Jaeckel‚Äôs (1972)\\ninÔ¨Ånitesimal jackknife (10.41); the empirical inÔ¨Çuence function estimate,\\nbased on (10.62); and what is known as the nonparametric delta method.\\n\\nBootstrap Packages\\n\\nVarious bootstrap packages in R are available on the CRAN contributed-\\npackages web site, bootstrap being an ambitious one. Algorithm 10.1\\nshows a simple R program for nonparametric bootstrapping. Aside from\\nbookkeeping, it‚Äôs only a few lines long.\\n\\nAlgorithm 10.1 R PROGRAM FOR THE NONPARAMETRIC BOOTSTRAP.\\n\\nBoot <- function (x, B, func, ...){\\n\\n# x is data vector or matrix (with each row a case)\\n# B is number of bootstrap replications\\n# func is R function that inputs a data vector or\\n# matrix and returns a numeric number or vector\\n# ... other arguments for func\\n\\nx <- as.matrix(x)\\nn <- nrow(x)\\nf0=func(x,...) # get size of output\\nfmat <- matrix(0,length(f0),B)\\nfor (b in 1:B) {\\n\\ni=sample(1:n, n, replace = TRUE)\\nfmat[,b] <- func(x[i, ],...)\\n\\n}\\n\\ndrop(fmat)\\n\\n}\\n\\n(cid:142)1 [p. 158] The jackknife standard error. The 1982 monograph also contains\\nEfron and Stein‚Äôs (1981) result on the bias of the jackknife variance esti-\\nmate, the square of formula (10.6): modulo certain sample size considera-\\ntions, the expectation of the jackknife variance estimate is biased upward\\nfor the true variance.\\n\\nFor the sample mean Nx, the jackknife yields exactly the usual variance\\ni .xi (cid:0) Nx/2=.n.n (cid:0) 1//, while the ideal bootstrap estimate\\n\\nestimate (1.2), P\\n(B ! 1) gives\\n\\nn\\nX\\n\\n.xi (cid:0) Nx/2=n2:\\n\\ni D1\\n\\n(10.70)\\n\\n\\x0c10.6 Notes and Details\\n\\n179\\n\\nAs with the jackknife, we could append a fudge factor to get perfect agree-\\nment with (1.2), but there is no real gain in doing so.\\n\\n(cid:142)2 [p. 161] Bootstrap sample sizes. Let bseB indicate the bootstrap standard er-\\nror estimate (10.16) based on B replications, and bse1 the ‚Äúideal bootstrap,‚Äù\\nB ! 1. In any actual application, there are diminishing returns from in-\\ncreasing B past a certain point, because bse1 is itself a statistic whose value\\nvaries with the observed sample x (as in (10.70)), leaving an irreducible re-\\nmainder of randomness in any standard error estimate. Section 6.4 of Efron\\nand Tibshirani (1993) shows that B D 200 will almost always be plenty\\n(for standard errors, but not for bootstrap conÔ¨Ådence intervals, Chapter 11).\\nSmaller numbers, 25 or even less, can still be quite useful in complicated\\nsituations where resampling is expensive. An early complaint, ‚ÄúBootstrap\\nestimates are random,‚Äù is less often heard in an era of frequent and massive\\nsimulations.\\n\\n(cid:142)3 [p. 161] The Bayesian bootstrap. Rubin (1981) suggested the Bayesian\\nbootstrap (10.44). Section 10.6 of Efron (1982) used (10.45)‚Äì(10.46) as\\nan objective Bayes justiÔ¨Åcation for what we will call the percentile-method\\nbootstrap conÔ¨Ådence intervals in Chapter 12.\\n\\n(cid:142)4 [p. 161] Jackknife-after-bootstrap. For the eigenratio example displayed in\\nFigure 10.2, B D 2000 nonparametric bootstrap replications gave bseboot D\\n0:075. How accurate is this value? Bootstrapping the bootstrap seems like\\ntoo much work, perhaps 200 times 2000 resamples. It turns out, though,\\nthat we can use the jackknife to estimate the variability of bseboot based on\\njust the original 2000 replications.\\n\\nNow the deleted sample estimate in (10.6) is bseboot.i/. The key idea is\\nto consider those bootstrap samples x(cid:3) (10.13), among the original 2000,\\nthat do not include the point xi . About 37% of the original B samples will\\nbe in this subset. Section 19.4 of Efron and Tibshirani (1993) shows that\\napplying deÔ¨Ånition (10.16) to this subset gives bseboot.i/. For the estimate of\\nFigure 10.2, the jackknife-after-bootstrap calculations gave bsejack D 0:022\\nfor bseboot D 0:075. In other words, 0.075 isn‚Äôt very accurate, which is\\nto be expected for the standard error of a complicated statistic estimated\\nfrom only n D 22 observations. An inÔ¨Ånitesimal jackknife version of this\\ntechnique will play a major role in Chapter 20.\\n\\n(cid:142)5 [p. 174] A fundamental theorem. Tukey can justly be considered the found-\\ning father of robust statistics, his 1960 paper being especially inÔ¨Çuential.\\nHuber‚Äôs celebrated 1964 paper brought the subject into the realm of high-\\nconcept mathematical statistics. Robust Statistics: The Approach Based on\\nInÔ¨Çuence Functions, the 1986 book by Hampel et al., conveys the breadth\\nof a subject only lightly scratched in our Section 10.5. Hampel (1974)\\n\\n\\x0c180\\n\\nThe Jackknife and the Bootstrap\\n\\nintroduced the inÔ¨Çuence function as a statistical tool. Boos and SerÔ¨Çing\\n(1980) veriÔ¨Åed expression (10.62). Qualitative notions of robustness, more\\nthan speciÔ¨Åc theoretical results, have had a continuing inÔ¨Çuence on modern\\ndata analysis.\\n\\n\\x0c11\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nThe jackknife and the bootstrap represent a different use of modern com-\\nputer power: rather than extending classical methodology‚Äîfrom ordinary\\nleast squares to generalized linear models, for example‚Äîthey extend the\\nreach of classical inference.\\n\\nChapter 10 focused on standard errors. Here we will take up a more am-\\nbitious inferential goal, the bootstrap automation of conÔ¨Ådence intervals.\\nThe familiar standard intervals\\n\\nO(cid:18) Àô 1:96 bse;\\nfor approximate 95% coverage, are immensely useful in practice but often\\nnot very accurate. If we observe O(cid:18) D 10 from a Poisson model O(cid:18) (cid:24) Poi.(cid:18)/,\\nthe standard 95% interval .3:8; 16:2/ (using bse D O(cid:18) 1=2) is a mediocre ap-\\nproximation to the exact interval1\\n\\n(11.1)\\n\\n.5:1; 17:8/:\\n(11.2)\\nStandard intervals (11.1) are symmetric around O(cid:18), this being their main\\nweakness. Poisson distributions grow more variable as (cid:18) increases, which\\nis why interval (11.2) extends farther to the right of O(cid:18) D 10 than to the\\nleft. Correctly capturing such effects in an automatic way is the goal of\\nbootstrap conÔ¨Ådence interval theory.\\n\\n11.1 Neyman‚Äôs Construction for One-Parameter\\nProblems\\n\\nThe student score data of Table 3.1 comprised n D 22 pairs,\\n\\nxi D .mi ; vi /;\\n\\ni D 1; 2; : : : ; 22;\\n\\n(11.3)\\n\\n1 Using the Neyman construction of Section 11.1, as explained there; see also Table 11.2\\n\\nin Section 11.4.\\n\\n181\\n\\n\\x0c182\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nwhere mi and vi were student i‚Äôs scores on the ‚Äúmechanics‚Äù and ‚Äúvectors‚Äù\\ntests. The sample correlation coefÔ¨Åcient O(cid:18) between mi and vi was com-\\nputed to be\\n\\nO(cid:18) D 0:498:\\n\\n(11.4)\\n\\nQuestion: What can we infer about the true correlation (cid:18) between m and v?\\nFigure 3.2 displayed three possible Bayesian answers. ConÔ¨Ådence intervals\\nprovide the frequentist solution, by far the most popular in applied practice.\\n\\nFigure 11.1 The solid curve is the normal correlation coefÔ¨Åcient\\ndensity f O(cid:18) .r/ (3.11) for O(cid:18) D 0:498, the MLE estimate for the\\nstudent score data; O(cid:18).lo/ D 0:093 and O(cid:18).up/ D 0:751 are the\\nendpoints of the 95% conÔ¨Ådence interval for (cid:18), with\\ncorresponding densities shown by dashed curves. These yield tail\\nareas 0.025 at O(cid:18) (11.6).\\n\\nSuppose, Ô¨Årst, that we assume a bivariate normal model (5.12) for the\\npairs .mi ; vi /. In that case the probability density f(cid:18) . O(cid:18)/ for sample corre-\\nlation O(cid:18) given true correlation (cid:18) has known form (3.11). The solid curve in\\nFigure 11.1 graphs f for (cid:18) D 0:498, that is, for (cid:18) set equal to the observed\\nvalue O(cid:18) . In more careful notation, the curve graphs f O(cid:18) .r/ as a function of\\nthe dummy variable2 r taking values in ≈í(cid:0)1; 1(cid:141).\\n\\n2 This is an example of a parametric bootstrap distribution (10.49), here with O(cid:22) being O(cid:18).\\n\\n‚àí0.4‚àí0.20.00.20.40.60.81.001234CorrelationDensity0.0930.7510.498l0.0250.025\\x0c11.1 Neyman‚Äôs construction\\n\\n183\\n\\nTwo other curves f(cid:18) .r/ appear in Figure 11.1: for (cid:18) equaling\\n\\nO(cid:18).lo/ D 0:093 and\\n\\nO(cid:18).up/ D 0:751:\\n\\n(11.5)\\n\\nThese were numerically calculated as the solutions to\\n\\nZ 1\\n\\nO(cid:18)\\n\\nf O(cid:18).lo/.r/ dr D 0:025 and\\n\\nZ O(cid:18)\\n\\n(cid:0)1\\n\\nf O(cid:18).up/.r/ dr D 0:025:\\n\\n(11.6)\\n\\nIn words, O(cid:18).lo/ is the smallest value of (cid:18) putting probability at least 0.025\\nabove O(cid:18) D 0:498, while O(cid:18).up/ is the largest value with probability at least\\n0.025 below O(cid:18);\\n\\n(cid:18) 2\\n\\ni\\nh O(cid:18).lo/; O(cid:18).up/\\n\\n(11.7)\\n\\nis a 95% conÔ¨Ådence interval for the true correlation, statement (11.7) hold-\\ning true with probability 0.95, for every possible value of (cid:18).\\n\\nWe have just described Neyman‚Äôs construction of conÔ¨Ådence intervals\\nfor one-parameter problems f(cid:18) . O(cid:18)/. (Later we will consider the more difÔ¨Å-\\ncult situation where there are ‚Äúnuisance parameters‚Äù in addition to the pa-\\nrameter of interest (cid:18).) One of the jewels of classical frequentist inference,\\nit depends on a pivotal argument‚Äî‚Äúingenious device‚Äù number 5 of Sec-\\ntion 2.1‚Äîto show that it produces genuine conÔ¨Ådence intervals, i.e., ones\\nthat contain the true parameter value (cid:18) at the claimed probability level,\\n0.95 in Figure 11.1. The argument appears in the chapter endnotes.(cid:142)\\n\\nFor the Poisson calculation (11.2) it was necessary to deÔ¨Åne exactly what\\n‚Äúthe smallest value of (cid:18) putting probability at least 0.025 above O(cid:18) ‚Äù meant.\\nThis was done assuming that, for any (cid:18), half of the probability f(cid:18) . O(cid:18)/ at\\nO(cid:18) D 10 counted as ‚Äúabove,‚Äù and similarly for calculating the upper limit.\\n\\n(cid:142)1\\n\\nTransformation Invariance\\n\\nConÔ¨Ådence intervals enjoy the important and useful property of transfor-\\nmation invariance. In the Poisson example (11.2), suppose our interest\\nshifts from parameter (cid:18) to parameter (cid:30) D log (cid:18). The 95% exact inter-\\nval (11.2) for (cid:18) then transforms to the exact 95% interval for (cid:30) simply by\\ntaking logs of the endpoints,\\n\\n.log.5:1/; log.17:8// D .1:63; 2:88/:\\n(11.8)\\nTo state things generally, suppose we observe O(cid:18) from a family of densi-\\n. O(cid:18)/ for (cid:18) of coverage level\\n\\nties f(cid:18) . O(cid:18)/ and construct a conÔ¨Ådence interval\\n\\nC\\n\\n\\x0c184\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nÀõ (Àõ D 0:95 in our examples). Now let parameter (cid:30) be a monotonic in-\\ncreasing function of (cid:18), say\\n\\n(cid:30) D m.(cid:18)/\\n(11.9)\\n(m.(cid:18)/ D log (cid:18) in (11.8)), and likewise O(cid:30) D m. O(cid:18)/ for the point estimate.\\n(cid:30). O(cid:30)/, a level-Àõ conÔ¨Ådence interval\\nThen\\nfor (cid:30),\\n\\n. O(cid:18)/ maps point by point into\\n\\nC\\n\\nC\\n\\n(cid:30). O(cid:30)/ D\\n\\nn\\n(cid:30) D m.(cid:18)/ for (cid:18) 2\\n\\n(cid:17)o\\n\\n(cid:16) O(cid:18)\\n\\n:\\n\\n(11.10)\\n\\nC\\n. O(cid:18)/g is the same as the event f(cid:30) 2\\nThis just says that the event f(cid:18) 2\\nC\\n(cid:30). O(cid:30)/g, so if the former always occurs with probability Àõ then so must the\\nC\\nlatter.\\n\\nC\\n\\nFigure 11.2 The situation in Figure 11.1 after transformation to\\n(cid:30) D m.(cid:18)/ according to (11.11). The curves are nearly N.(cid:30); (cid:27) 2/\\np\\nwith standard deviation (cid:27) D 1=\\n\\n19 D 0:229.\\n\\nTransformation invariance has an historical resonance with the normal\\ncorrelation coefÔ¨Åcient. Fisher‚Äôs derivation of f(cid:18) . O(cid:18)/ (3.11) in 1915 was a\\nmathematical triumph, but a difÔ¨Åcult one to exploit in an era of mechanical\\ncomputation. Most ingeniously, Fisher suggested instead working with the\\ntransformed parameter (cid:30) D m.(cid:18)/ where\\n\\n(cid:30) D m.(cid:18)/ D 1\\n2\\n\\nlog\\n\\n(cid:18) 1 C (cid:18)\\n1 (cid:0) (cid:18)\\n\\n(cid:19)\\n\\n;\\n\\n(11.11)\\n\\n‚àí0.50.00.51.01.50.00.51.01.5Fisher¬¢s f transformDensity0.0930.9750.546l0.0250.025\\x0c11.2 The Percentile Method\\n185\\nand likewise with statistic O(cid:30) D m. O(cid:18)/. Then, to a surprisingly good approx-\\nimation,\\n\\nO(cid:30) P(cid:24)\\n\\n(cid:18)\\n\\n(cid:30);\\n\\n(cid:19)\\n\\n:\\n\\n1\\nn (cid:0) 3\\n\\nN\\n\\n(11.12)\\n\\nSee Figure 11.2, which shows Neyman‚Äôs construction on the (cid:30) scale.\\n\\nIn other words, we are back in Fisher‚Äôs favored situation (4.31), the sim-\\n\\nple normal translation problem, where\\n\\n(cid:17)\\n\\n(cid:30) (cid:16) O(cid:30)\\n\\nD O(cid:30) Àô 1:96\\n\\n1p\\n\\nn (cid:0) 3\\n\\nC\\n\\n(11.13)\\n\\nis the ‚Äúobviously correct‚Äù 95% conÔ¨Ådence interval3 for (cid:30), closely approx-\\nimating Neyman‚Äôs construction. The endpoints of (11.13) are then trans-\\nformed back to the (cid:18) scale according to the inverse transformation\\n\\n(cid:18) D e2(cid:30) (cid:0) 1\\ne2(cid:30) C 1\\n\\n;\\n\\n(11.14)\\n\\ngiving (almost) the interval\\nvolved computations.\\n\\nC\\n\\n. O(cid:18)/ seen in Figure 11.1, but without the in-\\n\\nBayesian conÔ¨Ådence statements are inherently transformation invariant.\\nThe fact that the Neyman intervals are also invariant, unlike the standard\\nintervals (11.1), has made them more palatable to Bayesian statisticians.\\nTransformation invariance will play a major role in justifying the bootstrap\\nconÔ¨Ådence intervals introduced next.\\n\\n11.2 The Percentile Method\\n\\nOur goal is to automate the calculation of conÔ¨Ådence intervals: given the\\nbootstrap distribution of a statistical estimator O(cid:18), we want to automatically\\nproduce an appropriate conÔ¨Ådence interval for the unseen parameter (cid:18) . To\\nthis end, a series of four increasingly accurate bootstrap conÔ¨Ådence interval\\nalgorithms will be described.\\n\\nThe Ô¨Årst and simplest method is to use the standard interval (11.1),\\nO(cid:18) Àô 1:96bse for 95% coverage, with bse taken to be the bootstrap standard\\nerror bseboot (10.16). The limitations of this approach become obvious in\\nFigure 11.3, where the histogram shows B D 2000 nonparametric boot-\\nstrap replications O(cid:18) (cid:3) of the sample correlation coefÔ¨Åcient for the student\\n\\n3 This is an anachronism. Fisher hated the term ‚ÄúconÔ¨Ådence interval‚Äù after it was later\\n\\ncoined by Neyman for his comprehensive theory. He thought of (11.13) as an example\\nof the logic of inductive inference.\\n\\n\\x0c186\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nFigure 11.3 Histogram of B D 2000 nonparametric bootstrap\\nreplications O(cid:18) (cid:3) for the student score sample correlation; the solid\\ncurve is the ideal parametric bootstrap distribution f O(cid:18) .r/ as in\\nFigure 11.1. Observed correlation O(cid:18) D 0:498. Small triangles\\nshow histogram‚Äôs 0.025 and 0.975 quantiles.\\n\\nscore data, obtained as in Section 10.2. The standard intervals are justiÔ¨Åed\\nby taking literally the asymptotic normality of O(cid:18) ,\\n\\nO(cid:18) P(cid:24)\\n\\nN\\n\\n.(cid:18); (cid:27) 2/;\\n\\n(11.15)\\n\\n(cid:27) the true standard error.\\n\\nRelation (11.15) will generally hold for large enough sample size n, but\\nwe can see that for the student score data asymptotic normality has not\\nyet set in, with the histogram being notably long-tailed to the left. We can‚Äôt\\nexpect good performance from the standard method in this case. (The para-\\nmetric bootstrap distribution is just as nonnormal, as shown by the smooth\\ncurve.)\\n\\nThe percentile method uses the shape of the bootstrap distribution to\\nimprove upon the standard intervals (11.1). Having generated B bootstrap\\nreplications O(cid:18) (cid:3)1; O(cid:18) (cid:3)2; : : : ; O(cid:18) (cid:3)B , either nonparametrically as in Section 10.2\\nor parametrically as in Section 10.4, we use the obvious percentiles of their\\ndistribution to deÔ¨Åne the percentile conÔ¨Ådence limits. The histogram in\\nFigure 11.3 has its 0.025 and 0.975 percentiles equal to 0.118 and 0.758,\\n\\n Bootstrap CorrelationsFrequency‚àí0.4‚àí0.20.00.20.40.60.81.00204060801000.4980.1180.758l\\x0c11.2 The Percentile Method\\n\\n187\\n\\nand these are the endpoints of the central 95% nonparametric percentile\\ninterval.\\n\\nFigure 11.4 A 95% central conÔ¨Ådence interval via the percentile\\nmethod, based on the 2000 nonparametric replications O(cid:18) (cid:3) of\\nFigure 11.3.\\n\\nWe can state things more precisely in terms of the bootstrap cdf\\n\\nOG.t/,\\n\\nthe proportion of bootstrap samples less than t,\\no .\\n\\nOG.t/ D #\\n\\nn O(cid:18) (cid:3)b (cid:20) t\\n\\nB:\\n\\n(11.16)\\n\\nThe Àõth percentile point O(cid:18) (cid:3).Àõ/ of the bootstrap distribution is given by the\\ninverse function of OG,\\n\\nO(cid:18) (cid:3).Àõ/ D OG(cid:0)1.Àõ/I\\n(11.17)\\nO(cid:18) (cid:3).Àõ/ is the value putting proportion Àõ of the bootstrap sample to its left.\\nThe level-Àõ upper endpoint of the percentile interval, say O(cid:18)%ile≈íÀõ(cid:141), is by\\ndeÔ¨Ånition\\n\\nO(cid:18)%ile≈íÀõ(cid:141) D O(cid:18) (cid:3).Àõ/ D OG(cid:0)1.Àõ/:\\n\\nIn this notation, the 95% central percentile interval is\\n\\n(cid:17)\\n(cid:16) O(cid:18)%ile≈í:025(cid:141); O(cid:18)%ile≈í:975(cid:141)\\n\\n:\\n\\n(11.18)\\n\\n(11.19)\\n\\n0.00.20.40.60.81.00.00.20.40.60.81.0q^*allllll.118.758.025.975G^\\x0c188\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nThe construction is illustrated in Figure 11.4.\\n\\nThe percentile intervals are transformation invariant. Let (cid:30) D m.(cid:18)/ as\\nin (11.9), and likewise O(cid:30) D m. O(cid:18)/ (m.(cid:1)/ monotonically increasing), with\\nbootstrap replications O(cid:30)(cid:3)b D m. O(cid:18) (cid:3)b/ for b D 1; 2; : : : ; B. The bootstrap\\npercentiles transform in the same way,\\n\\nso that, as in (11.18),\\n\\nO(cid:30)(cid:3).Àõ/ D m\\n\\n(cid:16) O(cid:18) (cid:3).Àõ/(cid:17)\\n\\n;\\n\\nO(cid:30)%ile≈íÀõ(cid:141) D m\\n\\n(cid:16) O(cid:18)%ile≈íÀõ(cid:141)\\n\\n(cid:17)\\n\\n;\\n\\n(11.20)\\n\\n(11.21)\\n\\nverifying transformation invariance.\\n\\nIn what sense does the percentile method improve upon the standard\\nintervals? One answer involves transformation invariance. Suppose there\\nexists a monotone transformation (cid:30) D m.(cid:18)/ and O(cid:30) D m. O(cid:18)/ such that\\n\\nO(cid:30) (cid:24)\\n\\n.(cid:30); (cid:27) 2/\\n\\n(11.22)\\n\\nN\\nfor every (cid:18), with (cid:27) 2 constant. Fisher‚Äôs transformation (11.11)‚Äì(11.12) al-\\nmost accomplishes this for the normal correlation coefÔ¨Åcient.\\n\\nIt would then be true that parametric bootstrap replications would also\\n\\nfollow (11.22),\\n\\nO(cid:30)(cid:3) (cid:24)\\n\\nN\\n\\n(cid:16) O(cid:30); (cid:27) 2(cid:17)\\n\\n:\\n\\n(11.23)\\n\\nThat is, the bootstrap cdf OG(cid:30) would be normal with mean O(cid:30) and variance\\n(cid:27) 2. The Àõth percentile of OG(cid:30) would equal\\n\\nO(cid:30)%ile≈íÀõ(cid:141) D O(cid:30)(cid:3).Àõ/ D O(cid:30) C z.Àõ/(cid:27);\\n\\n(11.24)\\n\\nwhere z.Àõ/ denotes the Àõth percentile of a standard normal distribution,\\n\\nz.Àõ/ D ÀÜ(cid:0)1.Àõ/\\n\\n(11.25)\\n\\n(z.:975/ D 1:96, z.:025/ D (cid:0)1:96, etc.).\\n\\nIn other words, the percentile method would provide Fisher‚Äôs ‚Äúobviously\\n\\ncorrect‚Äù intervals for (cid:30),\\n\\nO(cid:30) Àô 1:96(cid:27)\\n\\n(11.26)\\n\\nfor 95% coverage for example. But, because of transformation invariance,\\nthe percentile intervals for our original parameter (cid:18) would also be exactly\\ncorrect.\\n\\nSome comments concerning the percentile method are pertinent.\\n\\n\\x0c11.2 The Percentile Method\\n\\n189\\n\\n(cid:15) The method does not require actually knowing the transformation to nor-\\n\\nmality O(cid:30) D m. O(cid:18)/, it only assumes its existence.\\n\\n(cid:15) If a transformation to form (11.22) exists, then the percentile intervals\\nare not only accurate, but also correct in the Fisherian sense of giving\\nthe logically appropriate inference.(cid:142)\\n\\n(cid:15) The justifying assumption for the standard intervals (11.15), O(cid:18) P(cid:24)\\n\\n.(cid:18),\\n(cid:27) 2/, becomes more accurate as the sample size n increases (usually\\nwith (cid:27) decreasing as 1=\\nn), but the convergence can be slow in cases\\nlike that of the normal correlation coefÔ¨Åcient. The broader assumption\\n(11.22), that m. O(cid:18)/ P(cid:24)\\n.m.(cid:18)/; (cid:27) 2/ for some transformation m.(cid:1)/, speeds\\nup convergence, irrespective of whether or not it holds exactly. Sec-\\ntion 11.4 makes this point explicit, in terms of asymptotic rates of con-\\nvergence.\\n\\np\\n\\nN\\n\\nN\\n\\n(cid:142)2\\n\\n(cid:15) The standard method works Ô¨Åne once it is applied on an appropriate\\nscale, as in Figure 11.2. The trouble is that the method is not transforma-\\ntion invariant, leaving the statistician the job of Ô¨Ånding the correct scale.\\nThe percentile method can be thought of as a transformation-invariant\\nversion of the standard intervals, an ‚Äúautomatic Fisher‚Äù that substitutes\\nmassive computations for mathematical ingenuity.\\n\\n(cid:15) The method requires bootstrap sample sizes(cid:142) on the order of B D 2000. (cid:142)3\\n(cid:15) The percentile method is not the last word in bootstrap conÔ¨Ådence in-\\ntervals. Two improvements, the ‚ÄúBC‚Äù and ‚ÄúBCa‚Äù methods, will be dis-\\ncussed in the next section. Table 11.1 compares the various intervals as\\napplied to the student score correlation, O(cid:18) D 0:498.\\n\\nTable 11.1 Bootstrap conÔ¨Ådence limits for student score correlation,\\nO(cid:18) D 0:498, n D 22. Parametric exact limits from Neyman‚Äôs construction\\nas in Figure 11.1. The BC and BCa methods are discussed in the next two\\nsections; .z0; a/, two constants required for BCa, are .(cid:0)0:055; 0:005/\\nparametric, and .0:000; 0:006/ nonparametric.\\n\\nParametric\\n.975\\n.025\\n\\nNonparametric\\n.025\\n\\n.975\\n\\n1. Standard\\n2. Percentile\\n3. BC\\n4. BCa\\n\\nExact\\n\\n.17\\n.11\\n.08\\n.08\\n\\n.09\\n\\n.83\\n.77\\n.75\\n.75\\n\\n.75\\n\\n.18\\n.13\\n.13\\n.12\\n\\n.82\\n.76\\n.76\\n.76\\n\\nThe label ‚Äúcomputer-intensive inference‚Äù seems especially apt as ap-\\n\\n\\x0c190\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nplied to bootstrap conÔ¨Ådence intervals. Neyman and Fisher‚Äôs constructions\\nare expanded from a few special theoretically tractable cases to almost any\\nsituation where the statistician has a repeatable algorithm. Automation, the\\nreplacement of mathematical formulas with wide-ranging computer algo-\\nrithms, will be a major theme of succeeding chapters.\\n\\n11.3 Bias-Corrected ConÔ¨Ådence Intervals\\n\\n. O(cid:30); (cid:27) 2/, says\\nThe ideal form (11.23) for the percentile method, O(cid:30)(cid:3) (cid:24)\\nthat the transformation O(cid:30) D m. O(cid:18)/ yields an unbiased estimator of con-\\nstant variance. The improved methods of this section and the next take into\\naccount the possibility of bias and changing variance. We begin with bias.\\n.(cid:30); (cid:27) 2/ for all (cid:30) D m.(cid:18)/, as hypothesized in (11.22), then\\n\\nIf O(cid:30) (cid:24)\\n\\nN\\n\\nO(cid:30)(cid:3) (cid:24)\\n\\nN\\n\\n. O(cid:30); (cid:27) 2/ and\\n\\nN\\n\\nn O(cid:30)(cid:3) (cid:20) O(cid:30)\\n\\no\\n\\nPr(cid:3)\\n\\nD 0:50\\n\\n(11.27)\\n\\n(Pr(cid:3) indicating bootstrap probability), in which case the monotonicity of\\nm.(cid:1)/ gives\\n\\nn O(cid:18) (cid:3) (cid:20) O(cid:18)\\n\\no\\n\\nPr(cid:3)\\n\\nD 0:50:\\n\\n(11.28)\\n\\nThat is, O(cid:18) (cid:3) is median unbiased4 for O(cid:18), and likewise O(cid:18) for (cid:18) .\\n\\nWe can check that. For a parametric family of densities f(cid:18) . O(cid:18)/, (11.28)\\n\\nimplies\\n\\nZ O(cid:18)\\n\\n(cid:0)1\\n\\n(cid:16) O(cid:18) (cid:3)(cid:17)\\n\\nf O(cid:18)\\n\\nd O(cid:18) (cid:3) D 0:50:\\n\\n(11.29)\\n\\nFor the normal correlation coefÔ¨Åcient density (3.11), n D 22, numerical\\nintegration gives\\n\\nZ :498\\n\\n(cid:0)1\\n\\n(cid:16) O(cid:18) (cid:3)(cid:17)\\n\\nf:498\\n\\nd O(cid:18) (cid:3) D 0:478;\\n\\n(11.30)\\n\\nwhich is not far removed from 0.50, but far enough to have a small impact\\non proper inference. It suggests that O(cid:18) (cid:3) is biased upward relative to O(cid:18) ‚Äî\\nthat‚Äôs why less than half of the bootstrap probability lies below O(cid:18)‚Äîand\\nby implication that O(cid:18) is upwardly biased for estimating (cid:18). Accordingly,\\nconÔ¨Ådence intervals should be adjusted a little bit downward. The bias-\\ncorrected percentile method (BC for short) is a data-based algorithm for\\nmaking such adjustments.\\n\\n4 Median unbiasedness, unlike the usual mean unbiasedness deÔ¨Ånition, has the advantage\\n\\nof being transformation invariant.\\n\\n\\x0c11.3 Bias-Corrected ConÔ¨Ådence Intervals\\n191\\nHaving simulated B bootstrap replications O(cid:18) (cid:3)1; O(cid:18) (cid:3)2; : : : ; O(cid:18) (cid:3)B , paramet-\\n\\nric or nonparametric, let p0 be the proportion of replications less than O(cid:18) ,\\nn O(cid:18) (cid:3)b (cid:20) O(cid:18)\\n\\np0 D #\\n\\no .\\n\\n(11.31)\\n\\nB\\n\\n(an estimate of (11.29)), and deÔ¨Åne the bias-correction value\\n\\nz0 D ÀÜ(cid:0)1.p0/;\\n\\n(11.32)\\n\\nwhere ÀÜ(cid:0)1 is the inverse function of the standard normal cdf. The BC\\nlevel-Àõ conÔ¨Ådence interval endpoint is deÔ¨Åned to be\\n2z0 C z.Àõ/(cid:17)i\\nO(cid:18)BC≈íÀõ(cid:141) D OG(cid:0)1 h\\nÀÜ\\n\\n(11.33)\\n\\n(cid:16)\\n\\n;\\n\\nwhere OG is the bootstrap cdf (11.16) and z.Àõ/ D ÀÜ(cid:0)1.Àõ/ (11.25).\\nIf p0 D 0:50, the median unbiased situation, then z0 D 0 and\\nz.Àõ/(cid:17)i\\n(cid:16)\\n\\nD OG(cid:0)1.Àõ/ D O(cid:18)%ile≈íÀõ(cid:141);\\n\\nO(cid:18)BC≈íÀõ(cid:141) D OG(cid:0)1 h\\nÀÜ\\n\\n(11.34)\\n\\nthe percentile limit (11.18). Otherwise, a bias correction is made. Taking\\np0 D 0:478 for the normal correlation example (the value we would get\\nfrom an inÔ¨Ånite number of parametric bootstrap replications) gives bias\\ncorrection value (cid:0)0:055. Notice that the BC limits are indeed shifted down-\\nward from the parametric percentile limits in Table 11.1. Nonparametric\\nbootstrapping gave p0 about 0.50 in this case, making the BC limits nearly\\nthe same as the percentile limits.\\n\\nA more general transformation argument motivates the BC deÔ¨Ånition\\n(11.33). Suppose there exists a monotone transformation (cid:30) D m.(cid:18)/ and\\nO(cid:30) D m. O(cid:18)/ such that for any (cid:18)\\nO(cid:30) (cid:24)\\n\\n.(cid:30) (cid:0) z0(cid:27); (cid:27) 2/;\\n\\n(11.35)\\n\\nN\\n\\nwith z0 and (cid:27) Ô¨Åxed constants. Then the BC endpoints are accurate, i.e.,\\nhave the claimed coverage probabilities, and are also ‚Äúobviously correct‚Äù\\nin the Fisherian sense. See the chapter endnotes(cid:142) for proof and discussion. (cid:142)4\\nAs before, the statistican does not need to know the transformation m.(cid:1)/\\nthat leads to O(cid:30) (cid:24)\\n.(cid:30) (cid:0) z0(cid:27); (cid:27) 2/, only that it exists. It is a broader target\\nthan O(cid:30) (cid:24)\\n.(cid:30); (cid:27) 2/ (11.22), making the BC method better justiÔ¨Åed than\\nthe percentile method, irrespective of whether or not such a transformation\\nexists. There is no extra computational burden: the bootstrap replications\\nf O(cid:18) (cid:3)b; b D 1; 2; : : : ; Bg, parametric or nonparametric, provide OG (11.16)\\nand z0 (11.31)‚Äì(11.32), giving O(cid:18)BC≈íÀõ(cid:141) from (11.33).\\n\\nN\\n\\nN\\n\\n\\x0c192\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\n11.4 Second-Order Accuracy\\n\\np\\n\\nCoverage errors of the standard conÔ¨Ådence intervals typically decrease at\\nn/ in the sample size n: having calculated O(cid:18)stan≈íÀõ(cid:141) D O(cid:18) Cz.Àõ/ O(cid:27)\\norder O.1=\\nfor an iid sample x D .x1; x2; : : : ; xn/, we can expect the actual coverage\\nprobability to be\\n\\nn\\n\\n(cid:18) (cid:20) O(cid:18)stan≈íÀõ(cid:141)\\n\\no :D Àõ C c1\\n\\nPr(cid:18)\\n\\np\\n\\nƒ±\\n\\nn;\\n\\n(11.36)\\n\\nwhere c1 depends on the problem at hand; (11.36) deÔ¨Ånes ‚ÄúÔ¨Årst-order accu-\\nracy.‚Äù It can connote painfully slow convergence to the nominal coverage\\nlevel Àõ, requiring sample size 4n to cut the error in half.\\n\\nA second-order accurate method, say O(cid:18)2nd≈íÀõ(cid:141), makes errors of order only\\n\\nO.1=n/,\\n\\nn\\n(cid:18) (cid:20) O(cid:18)2nd≈íÀõ(cid:141)\\n\\no :D Àõ C c2=n:\\n\\nPr(cid:18)\\n\\n(11.37)\\n\\nThe improvement is more than theoretical. In practical problems like that\\nof Table 11.1, second-order accurate methods‚ÄîBCa, deÔ¨Åned in the follow-\\ning, is one such‚Äîoften provide nearly the claimed coverage probabilities,\\neven in small-size samples.\\n\\nNeither the percentile method nor the BC method is second-order ac-\\ncurate (although, as in Table 11.1, they tend to be more accurate than the\\nstandard intervals). The difÔ¨Åculty for O(cid:18)BC≈íÀõ(cid:141) lies in the ideal form (11.35),\\n.(cid:30) (cid:0) z0(cid:27); (cid:27) 2/, where it is assumed O(cid:30) D m. O(cid:18)/ has constant standard\\nO(cid:30) (cid:24)\\nerror (cid:27). Instead, we now postulate the existence of a monotone transforma-\\ntion (cid:30) D m.(cid:18) / and O(cid:30) D m. O(cid:18)/ less restrictive than (11.35),\\n\\nN\\n\\nO(cid:30) (cid:24)\\n\\nN\\n\\n.(cid:30) (cid:0) z0(cid:27)(cid:30); (cid:27) 2\\n\\n(cid:30) /;\\n\\n(cid:27)(cid:30) D 1 C a(cid:30):\\n\\n(11.38)\\n\\n(cid:142)5\\n\\nHere the ‚Äúacceleration‚Äù (cid:142) a is a small constant describing how the standard\\ndeviation of O(cid:30) varies with (cid:30). If a D 0 we are back in situation (11.34)5,\\nbut if not, an amendment to the BC formula (11.33) is required.\\n\\nThe BCa method (‚Äúbias-corrected and accelerated‚Äù) takes its level-Àõ\\n\\nconÔ¨Ådence limit to be\\n\\nO(cid:18)BCa≈íÀõ(cid:141) D OG(cid:0)1\\n\\n(cid:20)\\n\\nÀÜ\\n\\n(cid:18)\\n\\nz0 C\\n\\nz0 C z.Àõ/\\n1 (cid:0) a.z0 C z.Àõ//\\n\\n(cid:19)(cid:21)\\n\\n:\\n\\n(11.39)\\n\\nA still more elaborate transformation argument shows that, if there exists\\na monotone transformation (cid:30) D m.(cid:18)/ and constants z0 and a yielding\\n\\n5 This assumes (cid:27)0 D 1 on the right side of (11.38), which can always be achieved by\\n\\nfurther transforming (cid:30) to (cid:30)=(cid:27).\\n\\n\\x0c11.4 Second-Order Accuracy\\n\\n193\\n\\n(11.38), then the BCa limits have their claimed coverage probabilities and,\\nmoreover, are correct in the Fisherian sense.\\n\\nBCa makes three corrections to the standard intervals (11.1): for non-\\nnormality of O(cid:18) (through using the bootstrap percentiles rather than just the\\nbootstrap standard error); for bias (through the bias correction value z0);\\nand for nonconstant standard error (through a). Notice that if a D 0 then\\nBCa (11.39) reduces to BC (11.33). If z0 D 0 then BC reduces to the\\npercentile method (11.18); and if OG, the bootstrap histogram, is normal,\\nthen (11.18) reduces to the standard interval (11.1). All three of the correc-\\ntions, for nonnormality, bias, and acceleration, can have substantial effects\\nin practice and are necessary to achieve second-order accuracy. A great\\ndeal of theoretical effort was devoted to verifying the second-order accu-\\nracy and BCa intervals under reasonably general assumptions.6\\n\\nTable 11.2 Nominal 95% central conÔ¨Ådence intervals for Poisson\\nparameter (cid:18) having observed O(cid:18) D 10; actual tail areas above and below\\nO(cid:18) D 10 deÔ¨Åned as in Figure 11.1 (atom of probability split at 10). For\\ninstance, lower standard limit 3.80 actually puts probability 0.004 above\\n10, rather than nominal value 0.025. Bias correction value z0 (11.32) and\\nacceleration a (11.38) both equal 0.050.\\n\\nNominal limits\\n.025\\n\\n.975\\n\\nTail areas\\nAbove Below\\n\\n1. Standard\\n2. %ile\\n3. BC\\n4. BCa\\n\\nExact\\n\\n3.80\\n4.18\\n4.41\\n5.02\\n\\n5.08\\n\\n16.20\\n16.73\\n17.10\\n17.96\\n\\n17.82\\n\\n.004\\n.007\\n.010\\n.023\\n\\n.025\\n\\n.055\\n.042\\n.036\\n.023\\n\\n.025\\n\\nThe advantages of increased accuracy are not limited to large sample\\nsizes. Table 11.2 returns to our original example of observing O(cid:18) D 10\\nfrom Poisson model O(cid:18) (cid:24) Poi.(cid:18)/. According to Neyman‚Äôs construction,\\nthe 0.95 exact limits give tail areas 0.025 in both the above and below\\ndirections, as in Figure 11.1, and this is nearly matched by the BCa limits.\\nHowever the standard limits are much too conservative at the left end and\\nanti-conservative at the right.\\n\\n6 The mathematical side of statistics has also been affected by electronic computation,\\n\\nwhere it is called upon to establish the properties of general-purpose computer\\nalgorithms such as the bootstrap. Asymptotic analysis in particular has been aggressively\\ndeveloped, the veriÔ¨Åcation of second-order accuracy being a nice success story.\\n\\n\\x0c194\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nTable 11.3 95% nominal conÔ¨Ådence intervals for the parametric and\\nnonparametric eigenratio examples of Figures 10.2 and 10.6.\\n\\nParametric\\n.975\\n\\n.025\\n\\nNonparametric\\n\\n.025\\n\\n.975\\n\\n1. Standard\\n2. %ile\\n3. BC\\n4. BCa\\n\\n.829\\n.815\\n.828\\n.820\\n\\n.556\\n.542\\n.523\\n.555\\n.z0 D (cid:0):029; a D :058/\\n\\n.840\\n.818\\n.813\\n.828\\n\\n.545\\n.517\\n.507\\n.523\\n.z0 D (cid:0):049; a D :051/\\n\\nBootstrap conÔ¨Ådence limits continue to provide better inferences in the\\nvast majority of situations too complicated for exact analysis. One such\\nsituation is examined in Table 11.3. It relates to the eigenratio example\\nillustrated in Figures 10.2‚Äì10.6. In this case the nonnormality and bias cor-\\nrections stretch the bootstrap intervals to the left, but the acceleration effect\\npulls right, partially canceling out the net change from the standard inter-\\nvals.\\n\\nThe percentile and BC methods are completely automatic, and can be\\napplied whenever a sufÔ¨Åciently large number of bootstrap replications are\\navailable. The same cannot be said of BCa. A drawback of the BCa method\\nis that the acceleration a is not a function of the bootstrap distribution and\\nmust be computed separately. Often this is straightforward:\\n\\n(cid:15) For one-parameter exponential families such as the Poisson, a equals z0.\\n(cid:15) In one-sample nonparametric problems, a can be estimated from the\\n\\njackknife resamples O(cid:18).i/ (10.5),\\n\\nOa D 1\\n6\\n\\nPn\\n\\niD1\\n\\n(cid:20)\\n\\nPn\\n\\niD1\\n\\n(cid:16) O(cid:18).i/ (cid:0) O(cid:18).(cid:1)/\\n(cid:16) O(cid:18).i/ (cid:0) O(cid:18).(cid:1)/\\n\\n(cid:17)3\\n(cid:17)2(cid:21)1:5 :\\n\\n(11.40)\\n\\n(cid:15) The abc method computes a in multiparameter exponential families (5.54),\\n\\nas does the resampling-based R algorithm accel.\\n\\nConÔ¨Ådence intervals require the number of bootstrap replications B to\\nbe on the order of 2000, rather than the 200 or fewer needed for standard\\nerrors; the corrections made to the standard intervals are more delicate than\\nstandard errors and require greater accuracy.\\n\\nThere is one more cautionary note to sound concerning nuisance param-\\neters: biases can easily get out of hand when the parameter vector (cid:22) is\\n\\n\\x0c11.5 Bootstrap-t Intervals\\n\\n195\\n\\nhigh-dimensional. Suppose we observe\\n\\nind(cid:24)\\n\\nxi\\n\\n.(cid:22)i ; 1/\\n\\nfor i D 1; 2; : : : ; n;\\n\\n(11.41)\\n\\nN\\nand wish to set a conÔ¨Ådence interval for (cid:18) D Pn\\n1 x2\\ni\\nwill be sharply biased upward if n is at all large. To be speciÔ¨Åc, if n D 10\\nand O(cid:18) D 20, we compute(cid:142)\\n\\ni . The MLE O(cid:18) D Pn\\n\\n1 (cid:22)2\\n\\n(cid:142)6\\n\\nz0 D ÀÜ(cid:0)1.0:156/ D (cid:0)1:01:\\n(11.42)\\nThis makes7 O(cid:18)BC≈í:025(cid:141) (11.33) equal a ludicrously small bootstrap per-\\ncentile,\\n\\nOG(cid:0)1.0:000034/;\\n\\n(11.43)\\n\\na warning sign against the BC or BCa intervals, which work most depend-\\nably for jz0j and jaj small, say (cid:20) 0:2.\\n\\nA more general warning would be against blind trust in maximum likeli-\\nhood estimates in high dimensions. Computing z0 is a wise precaution even\\nif it is not used for BC or BCa purposes, in case it alerts one to dangerous\\nbiases.\\n\\nConÔ¨Ådence intervals for classical applications were most often based on\\nthe standard method (11.1) (with bse estimated by the delta method) except\\nin a few especially simple situations such as the Poisson. Second-order ac-\\ncurate intervals are very much a computer-age development, with both the\\nalgorithms and the inferential theory presupposing high-speed electronic\\ncomputation.\\n\\n11.5 Bootstrap-t Intervals\\n\\nThe initial breakthrough on exact conÔ¨Ådence intervals came in the form of\\nStudent‚Äôs t distribution in 1908. Suppose we independently observe data\\nfrom two possibly different normal distributions, x D .x1; x2; : : : ; xnx /\\nand y D .y1; y2; : : : ; yny /,\\n\\niid(cid:24)\\n\\nxi\\n\\nN\\n\\n.(cid:22)x; (cid:27) 2/\\n\\nand yi\\n\\niid(cid:24)\\n\\n.(cid:22)y; (cid:27) 2/;\\n\\nN\\n\\nand wish to form a 0.95 central conÔ¨Ådence interval for\\n\\nThe obvious estimate is\\n\\n(cid:18) D (cid:22)y (cid:0) (cid:22)x:\\n\\nO(cid:18) D Ny (cid:0) Nx;\\n\\n7 Also O(cid:18)BCa≈í:025(cid:141), a is zero in this model.\\n\\n(11.44)\\n\\n(11.45)\\n\\n(11.46)\\n\\n\\x0c196\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nbut its distribution depends on the nuisance parameter (cid:27) 2.\\n\\nStudent‚Äôs masterstroke was to base inference about (cid:18) on the pivotal\\n\\nquantity\\n\\nt D\\n\\nO(cid:18) (cid:0) (cid:18)\\nbse\\nwhere bse2 is an unbiased estimate of (cid:27) 2,\\nC 1\\nny\\n\\n(cid:18) 1\\nnx\\n\\nbse2 D\\n\\n(cid:19) Pnx\\n\\n1 .xi (cid:0) Nx/2 C Pny\\nnx C ny (cid:0) 2\\n\\n1 .yi (cid:0) Ny/2\\n\\n(11.47)\\n\\nI\\n\\n(11.48)\\n\\nt then has the ‚ÄúStudent‚Äôs t distribution‚Äù with df D nx C ny (cid:0) 2 degrees of\\nfreedom if (cid:22)x D (cid:22)y, no matter what (cid:27) 2 may be.\\n\\nLetting t .Àõ/\\ndf\\n\\nrepresent the 100Àõth percentile of a tdf distribution yields\\nbse (cid:1) t .1(cid:0)Àõ/\\n\\nO(cid:18)t ≈íÀõ(cid:141) D O(cid:18) (cid:0)\\n\\ndf\\n\\n(11.49)\\n\\nas the upper level-Àõ interval of a Student‚Äôs t conÔ¨Ådence limit. Applied to\\nthe difference between the AML and ALL scores in Figure 1.4, the central\\n0.95 Student‚Äôs t interval for (cid:18) D EfAMLg (cid:0) EfALLg was calculated to be\\n(cid:17)\\n\\nD .:062; :314/:\\n\\n(11.50)\\n\\n(cid:16) O(cid:18)t ≈í:025(cid:141); O(cid:18)t ≈í:975(cid:141)\\nHere nx D 47, ny D 25, and df D 70.\\n\\nStudent‚Äôs theory depends on the normality assumptions of (11.44). The\\nbootstrap-t approach is to accept (or pretend) that t in (11.47) is pivotal,\\nbut to estimate its distribution via bootstrap resampling. Nonparametric\\nbootstrap samples are drawn separately from x and y,\\n\\nx(cid:3) D .x(cid:3)\\n\\n2 ; : : : ; x(cid:3)\\n\\n1 ; x(cid:3)\\nfrom which we calculate O(cid:18) (cid:3) and bse\\n\\nnx /\\n\\n(cid:3)\\n\\nand y (cid:3) D .y(cid:3)\\n\\n1 ; y(cid:3)\\n\\n2 ; : : : ; y(cid:3)\\n\\nny /;\\n\\n, (11.46) and (11.48), giving\\nO(cid:18) (cid:3) (cid:0) O(cid:18)\\nbse\\n\\n(cid:3)\\n\\n;\\n\\nt (cid:3) D\\n\\n(11.51)\\n\\n(11.52)\\n\\nwith O(cid:18) playing the role of (cid:18), as appropriate in the bootstrap world. Repli-\\ncations ft (cid:3)b; b D 1; 2; : : : ; Bg provide estimated percentiles t (cid:3).Àõ/ and cor-\\nresponding conÔ¨Ådence limits\\nt ≈íÀõ(cid:141) D O(cid:18) (cid:0)\\nO(cid:18) (cid:3)\\n\\n(11.53)\\n\\nbse (cid:1) t (cid:3).1(cid:0)Àõ/:\\n\\nFor the AML‚ÄìALL example, the t (cid:3) distribution differed only slightly\\nfrom a t70 distribution; the resulting 0.95 interval was .0:072; 0:323/, nearly\\n\\n\\x0c11.5 Bootstrap-t Intervals\\n\\n197\\n\\nthe same as (11.50), lending credence to the original normality assump-\\ntions.\\n\\nFigure 11.5 B D 2000 nonparametric replications of bootstrap-t\\nstatistic for the student score correlation; small triangles show\\n0.025 and 0.975 percentile points. The histogram is sharply\\nskewed to the right; the solid curve is Student‚Äôs t density for 21\\ndegrees of freedom.\\n\\nReturning to the student score correlation example of Table 11.1, we can\\napply bootstrap-t methods by still taking t D . O(cid:18) (cid:0) (cid:18)/=bse to be notionally\\npivotal, but now with (cid:18) the true correlation, O(cid:18) the sample correlation, and\\nbse the approximate standard error .1 (cid:0) O(cid:18) 2/=\\n19. Figure 11.5 shows the\\nhistogram of B D 2000 nonparametric bootstrap replications t (cid:3) D . O(cid:18) (cid:3) (cid:0)\\nO(cid:18) /=bse\\n\\n. These gave bootstrap percentiles\\n\\np\\n\\n(cid:3)\\n\\nt (cid:3).:025/; t (cid:3).:975/(cid:17)\\n(cid:16)\\n\\nD .(cid:0)1:64; 2:59/\\n\\n(11.54)\\n\\n(which might be compared with .(cid:0)2:08; 2:08/ for a standard t21 distribu-\\ntion), and 0.95 interval .0:051; 0:781/ from (11.53), somewhat out of place\\ncompared with the other entries in the right panel of Table 11.1.\\n\\nBootstrap-t intervals are not transformation invariant. This means they\\ncan perform poorly or well depending on the scale of application. If per-\\nformed on Fisher‚Äôs scale (11.11) they agree well with exact intervals for\\n\\n t* valuesFrequency050100150200t distributiondf = 21‚àí3‚àí2‚àí101234‚àí1.642.59\\x0c198\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nthe correlation coefÔ¨Åcient. A practical difÔ¨Åculty is the requirement of a for-\\nmula for bse.\\n\\nNevertheless, the idea of estimating the actual distribution of a proposed\\npivotal quantity has great appeal to the modern statistical spirit. Calculat-\\ning the percentiles of the original Student t distribution was a multi-year\\nproject in the early twentieth century. Now we can afford to calculate our\\nown special ‚Äút table‚Äù for each new application. Spending such computa-\\ntional wealth wisely, while not losing one‚Äôs inferential footing, is the cen-\\ntral task and goal of twenty-Ô¨Årst-century statisticians.\\n\\n11.6 Objective Bayes Intervals and the ConÔ¨Ådence\\nDistribution\\n\\nInterval estimates are ubiquitous. They play a major role in the scientiÔ¨Åc\\ndiscourse of a hundred disciplines, from physics, astronomy, and biology\\nto medicine and the social sciences. Neyman-style frequentist conÔ¨Ådence\\nintervals dominate the literature, but there have been inÔ¨Çuential Bayesian\\nand Fisherian developments as well, as discussed next.\\n\\nGiven a one-parameter family of densities f(cid:18) . O(cid:18)/ and a prior density\\n\\ng.(cid:18)/, Bayes‚Äô rule (3.5) produces the posterior density of (cid:18),\\n\\ng.(cid:18) j O(cid:18)/ D g.(cid:18)/f(cid:18) . O(cid:18)/=f . O(cid:18)/;\\n(11.55)\\nwhere f . O(cid:18)/ is the marginal density R f(cid:18) . O(cid:18)/g.(cid:18)/d(cid:18). The Bayes 0.95 cred-\\nible interval\\n\\n.(cid:18)j O(cid:18)/ spans the central 0.95 region of g.(cid:18) j O(cid:18)/, say\\n.(cid:18)j O(cid:18)/ D .a. O(cid:18)/; b. O(cid:18)//;\\n\\nC\\n\\nC\\n\\nwith\\n\\nZ b. O(cid:18)/\\n\\na. O(cid:18)/\\n\\ng.(cid:18) j O(cid:18)/ d(cid:18) D 0:95;\\n\\n(11.56)\\n\\n(11.57)\\n\\nand with posterior probability 0.025 in each tail region.\\n\\nConÔ¨Ådence intervals, of course, require no prior information, making\\nthem eminently useful in day-to-day applied practice. The Bayesian equiv-\\nalents are credible intervals based on uninformative priors, Section 3.2.\\n‚ÄúMatching priors,‚Äù those whose credible intervals nearly match Neyman\\nconÔ¨Ådence intervals, have been of particular interest. Jeffreys‚Äô prior (3.17),\\n\\ng.(cid:18) / D\\n\\n(cid:18) D\\nI\\n\\n1=2\\n;\\n(cid:18)\\nI\\nZ (cid:20) @\\n@(cid:18)\\n\\n(cid:21)2\\n\\nlog f(cid:18) . O(cid:18)/\\n\\nf(cid:18) . O(cid:18)/ d O(cid:18);\\n\\n(11.58)\\n\\n\\x0c11.6 Objective Bayes intervals\\n\\n199\\n\\nprovides a generally accurate matching prior for one-parameter problems.\\nFigure 3.2 illustrates this for the student score correlation, where the credi-\\nble interval .0:093; 0:750/ is a near-exact match to the Neyman 0.95 inter-\\nval of Figure 11.1.\\n\\nDifÔ¨Åculties begin with multiparameter families f(cid:22).x/ (5.1): we wish to\\nconstruct an interval estimate for a one-dimensional function (cid:18) D t.(cid:22)/ of\\nthe p-dimensional parameter vector (cid:22), and must somehow remove the ef-\\nfects of the p (cid:0) 1 ‚Äúnuisance parameters.‚Äù In a few rare situations, including\\nthe normal theory correlation coefÔ¨Åcient, this can be done exactly. Pivotal\\nmethods do the job for Student‚Äôs t construction. Bootstrap conÔ¨Ådence inter-\\nvals greatly extend the reach of such methods, at a cost of greatly increased\\ncomputation.\\n\\nBayesians get rid of nuisance parameters by integrating them out of the\\nposterior density g.(cid:22)jx/ D g.(cid:22)/f(cid:22).x/=f .x/ (3.6) (x now representing\\nall the data, ‚Äúx‚Äù equaling .x; y/ for the Student t setup (11.44)). That is,\\nwe calculate8 the marginal density of (cid:18) D t.(cid:22)/ given x, and call it h.(cid:18) jx/.\\n.(cid:18)jx/, is then constructed as in (11.56)‚Äì(11.57),\\nA credible interval for (cid:18),\\nwith h.(cid:18) jx/ playing the role of g.(cid:18)j O(cid:18)/. This leaves us the knotty problem\\nof choosing an uninformative multidimensional prior g.(cid:22)/. We will return\\nto the question after Ô¨Årst discussing Ô¨Åducial methods, a uniquely Fisherian\\ndevice.\\n\\nC\\n\\nFiducial constructions begin with what seems like an obviously incorrect\\ninterpretation of pivotality. We rewrite the Student t pivotal t D . O(cid:18) (cid:0) (cid:18)/=bse\\n(11.47) as\\n\\n(cid:18) D O(cid:18) (cid:0)\\n\\nbse (cid:1) t;\\n\\n(11.59)\\n\\nwhere t has a Student‚Äôs t distribution with df degrees of freedom, t (cid:24)\\ntdf. Having observed the data .x; y/ (11.44), Ô¨Åducial theory assigns (cid:18) the\\ndistribution implied by (11.59), as if O(cid:18) and bse were Ô¨Åxed at their calculated\\nvalues while t was distributed as tdf. Then O(cid:18)t ≈íÀõ(cid:141) (11.49), the Student t level-\\nÀõ conÔ¨Ådence limit, is the 100Àõth percentile of (cid:18) ‚Äôs Ô¨Åducial distribution.\\n\\nWe seem to have achieved a Bayesian posterior conclusion without any\\nprior assumptions.9 The historical development here is confused by Fisher‚Äôs\\nrefusal to accept Neyman‚Äôs conÔ¨Ådence interval theory, as well as his dispar-\\nagement of Bayesian ideas. As events worked out, all of Fisher‚Äôs immense\\nprestige was not enough to save Ô¨Åducial theory from the scrapheap of failed\\nstatistical methods.\\n\\n8 Often a difÔ¨Åcult calculation, as discussed in Chapter 13.\\n9 ‚ÄúEnjoying the Bayesian omelette without breaking the Bayesian eggs,‚Äù in L. J. Savage‚Äôs\\n\\nwords.\\n\\n\\x0c200\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nAnd yet, in Arthur Koestler‚Äôs words, ‚ÄúThe history of ideas is Ô¨Ålled with\\nbarren truths and fertile errors.‚Äù Fisher‚Äôs underlying rationale went some-\\nthing like this: O(cid:18) and bse exhaust the information about (cid:18) available from the\\ndata, after which there remains an irreducible component of randomness\\ndescribed by t . This is an idea of substantial inferential appeal, and one\\nthat can be rephrased in more general terms discussed next that bear on the\\nquestion of uninformative priors.\\n\\nBy deÔ¨Ånition, an upper conÔ¨Ådence limit O(cid:18)x≈íÀõ(cid:141) satisÔ¨Åes\\nn\\n(cid:18) (cid:20) O(cid:18)x≈íÀõ(cid:141)\\n\\nD Àõ\\n\\nPr\\n\\no\\n\\n(11.60)\\n\\n(where now we have indicated the observed data x in the notation), and so\\nn O(cid:18)x≈íÀõ(cid:141) (cid:20) (cid:18) (cid:20) O(cid:18)x≈íÀõ C (cid:15)(cid:141)\\n\\n(11.61)\\n\\nD (cid:15):\\n\\nPr\\n\\no\\n\\nWe can consider O(cid:18)x≈íÀõ(cid:141) as a one-to-one function between Àõ in .0; 1/ and (cid:18) a\\npoint in its parameter space ‚Äö (assuming that O(cid:18)x≈íÀõ(cid:141) is smoothly increasing\\nin Àõ). Letting (cid:15) go to zero in (11.61) determines the conÔ¨Ådence density of\\n(cid:18), say Qgx.(cid:18) /,\\n\\nQgx.(cid:18)/ D dÀõ=d(cid:18);\\n\\n(11.62)\\n\\nthe local derivative of probability at location (cid:18) for the unknown parameter,\\nthe derivative being taken at (cid:18) D O(cid:18)x≈íÀõ(cid:141).\\n\\nIntegrating Qgx.(cid:18) / recovers Àõ as a function of (cid:18). Let (cid:18)1 D O(cid:18)x≈íÀõ1(cid:141) and\\n\\n(cid:18)2 D O(cid:18)x≈íÀõ2(cid:141) for any two values Àõ1 < Àõ2 in .0; 1/. Then\\n\\nZ (cid:18)2\\n\\n(cid:18)1\\n\\nQgx.(cid:18) / d(cid:18) D\\n\\nZ (cid:18)2\\n\\ndÀõ\\nd(cid:18)\\nD Prf(cid:18)1 (cid:20) (cid:18) (cid:20) (cid:18)2g;\\n\\n(cid:18)1\\n\\nd(cid:18) D Àõ2 (cid:0) Àõ1\\n\\n(11.63)\\n\\nas in (11.60). There is nothing controversial about (11.63) as long as we\\nremember that the random quantity in Prf(cid:18)1 (cid:20) (cid:18) (cid:20) (cid:18)2g is not (cid:18) but rather\\nthe interval .(cid:18)1; (cid:18)2/, which varies as a function of x. Forgetting this leads to\\nthe textbook error of attributing Bayesian properties to frequentist results:\\n‚ÄúThere is 0.95 probability that (cid:18) is in its 0.95 conÔ¨Ådence interval,‚Äù etc.\\n\\nThis is exactly what the Ô¨Åducial argument does.10 Whether or not one\\naccepts (11.63), there is an immediate connection with matching priors.\\n\\n10 Fiducial and conÔ¨Ådence densities agree, as can be seen in the Student t situation (11.59),\\nat least in the somewhat limited catalog of cases Fisher thought appropriate for Ô¨Åducial\\ncalculations.\\n\\n\\x0c11.6 Objective Bayes intervals\\n\\n201\\n\\nSuppose prior g.(cid:22)/ gives a perfect match to the conÔ¨Ådence interval system\\nO(cid:18)x≈íÀõ(cid:141). Then, by deÔ¨Ånition, its posterior density h.(cid:18) jx/ must satisfy\\n\\nZ O(cid:18)x ≈íÀõ(cid:141)\\n\\n(cid:0)1\\n\\nh.(cid:18)jx/ d(cid:18) D Àõ D\\n\\nZ O(cid:18)x ≈íÀõ(cid:141)\\n\\n(cid:0)1\\n\\nQgx.(cid:18)/ d(cid:18)\\n\\n(11.64)\\n\\nfor 0 < Àõ < 1. But this implies h.(cid:18)jx/ equals Qgx.(cid:18)/ for all (cid:18) . That is,\\nthe conÔ¨Ådence density Qgx.(cid:18)/ is the posterior density of (cid:18) given x for any\\nmatching prior.\\n\\nFigure 11.6 ConÔ¨Ådence density (11.62) for Poisson parameter (cid:18)\\nhaving observed O(cid:18) D 10. There is area 0.95 under the curve\\nbetween 5.08 and 17.82, as in Table 11.2, and areas 0.025 in each\\ntail.\\n\\nFigure 11.6 graphs the conÔ¨Ådence density for O(cid:18) (cid:24) Poi.(cid:18)/ having ob-\\nserved O(cid:18) D 10. This was obtained by numerically differentiating Àõ as a\\nfunction of (cid:18) (11.62),\\n\\nÀõ D Pr f10 (cid:20) Poi.(cid:18)/g ;\\n\\n(11.65)\\n\\n‚Äú(cid:20)‚Äù including splitting the atom of probability at 10. According to Ta-\\nble 11.2, Qg10.(cid:18) / has area 0.95 between 5.08 and 17.82, and area 0.025 in\\neach tail. Whatever its provenance, the graph delivers a striking picture of\\nthe uncertainty in the unknown value of (cid:18).\\n\\n05101520250.000.020.040.060.080.100.120.14qConfidence Density5.0817.82l10\\x0c202\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\nBootstrap conÔ¨Ådence intervals provide easily computable conÔ¨Ådence den-\\nOG.(cid:18)/ be the bootstrap cdf and Og.(cid:18)/ its density function (ob-\\nsities. Let\\ntained by differentiating a smoothed version of OG.(cid:18)/ when OG is based on\\nB bootstrap replications). The percentile conÔ¨Ådence limits O(cid:18)≈íÀõ(cid:141) D OG(cid:0)1.Àõ/\\n(11.17) have Àõ D OG.(cid:18)/, giving\\n\\nQgx.(cid:18)/ D Og.(cid:18)/:\\n\\n(11.66)\\n\\n(It is helpful to picture this in Figure 11.4.) For the percentile method, the\\nbootstrap density is the conÔ¨Ådence density.\\n\\nFor the BCa intervals (11.39), the conÔ¨Ådence density is obtained by\\n\\nreweighting Og.(cid:18)/,\\n\\n(cid:142)7\\n\\nwhere(cid:142)\\n\\nQgx.(cid:18)/ D cw.(cid:18)/ Og.(cid:18)/;\\n\\n(11.67)\\n\\nw.(cid:18)/ D \\' ≈íz(cid:18) =.1 C az(cid:18) / (cid:0) z0(cid:141)\\n.1 C az(cid:18) /2\\'.z(cid:18) C z0/\\n\\n; with z(cid:18) D ÀÜ(cid:0)1 OG.(cid:18)/ (cid:0) z0:\\n\\n(11.68)\\n\\nHere \\' is the standard normal density, ÀÜ its cdf, and c the constant that\\nmakes Qgx.(cid:18) / integrate to 1. In the usual case where the bootstrap cdf is es-\\ntimated from replications O(cid:18) (cid:3)b, b D 1; 2; : : : ; B (either parametric or non-\\nparametric), the BCa conÔ¨Ådence density is a reweighted version of Og.(cid:18)/.\\nDeÔ¨Åne\\n\\nWb D w\\n\\n(cid:16) O(cid:18) (cid:3)b(cid:17)\\n\\n, B\\nX\\n\\n(cid:16) O(cid:18) (cid:3)i (cid:17)\\n\\n:\\n\\nw\\n\\n(11.69)\\n\\ni D1\\n\\nThen the BCa conÔ¨Ådence density is the discrete density putting weight Wb\\non O(cid:18) (cid:3)b.\\n\\nFigure 11.7 returns to the student score data, n D 22 students, Ô¨Åve scores\\n\\neach, modeled normally as in Figure 10.6,\\n\\niid(cid:24)\\n\\nxi\\n\\n(11.70)\\nThis is a p D 20-dimensional parametric family: 5 expectations, 5 vari-\\nances, 10 covariances. The parameter of interest was taken to be\\n\\nfor i D 1; 2; : : : ; 22:\\n\\n5.(cid:21); ‚Ä†/\\n\\nN\\n\\n(cid:18) D maximum eigenvalue of ‚Ä†:\\n(11.71)\\nIt had MLE O(cid:18) D 683, this being the maximum eigenvalue of the MLE\\nsample covariance matrix O‚Ä† (dividing each sum of squares by 22 rather\\nthan 21).\\n\\nB D 8000 parametric bootstrap replications11 O(cid:18) (cid:3)b gave percentile and\\n11 B D 2000 would have been enough for most purposes, but B D 8000 gave a sharper\\n\\npicture of the different curves.\\n\\n\\x0c11.6 Objective Bayes intervals\\n\\n203\\n\\nFigure 11.7 ConÔ¨Ådence densities for the maximum eigenvalue\\nparameter (11.71), using a multivariate normal model (11.70) for\\nthe student score data. The dashed red curve is the percentile\\nmethod, solid black the BCa (with .z0; a/ D .0:178; 0:093/). The\\ndotted blue curve is the Bayes posterior density for (cid:18), using\\nJeffreys‚Äô prior (11.72).\\n\\nBCa conÔ¨Ådence densities as shown. In this case the weights Wb (11.69)\\nincreased with O(cid:18) (cid:3)b, pushing the BCa density to the right. Also shown is the\\nBayes posterior density(cid:142) for (cid:18) starting from Jeffreys‚Äô multiparameter prior (cid:142)8\\ndensity\\n\\ngJeff.(cid:22)/ D jI (cid:22)j1=2;\\n\\n(11.72)\\n\\nwhere I (cid:22) is the Fisher information matrix (5.26). It isn‚Äôt truly uninforma-\\ntive here, moving its credible limits upward from the second-order accurate\\nBCa conÔ¨Ådence limits. Formula (11.72) is discussed further in Chapter 13.\\nBayesian data analysis has the attractive property that, after examin-\\ning the data, we can express our remaining uncertainty in the language\\nof probability. Fiducial and conÔ¨Ådence densities provide something similar\\nfor conÔ¨Ådence intervals, at least partially freeing the frequentist from the\\ninterpretive limitations of Neyman‚Äôs intervals.\\n\\n500100015000200400600800q: maximum eigenvaluePosterior DensityBCaJeffreysPercentile683l\\x0c204\\n\\nBootstrap ConÔ¨Ådence Intervals\\n\\n11.7 Notes and Details\\n\\nFisher‚Äôs theory of Ô¨Åducial inference (1930) preceded Neyman‚Äôs approach,\\nformalized in (1937), which was presented as an attempt to put interval es-\\ntimation on a Ô¨Årm probabilistic basis, as opposed to the mysteries of Ô¨Ådu-\\ncialism. The result was an elegant theory of exact and optimal intervals,\\nphrased in hard-edged frequentistic terms. Readers familiar with the the-\\nory will know that Neyman‚Äôs construction‚Äîa favorite name in the physics\\nliterature‚Äîas pictured in Figure 11.1, requires some conditions on the fam-\\nily of densities f(cid:18) . O(cid:18)/ to yield optimal intervals, a sufÔ¨Åcient condition being\\nmonotone likelihood ratios.\\n\\nBootstrap conÔ¨Ådence intervals, Efron (1979, 1987), are neither exact nor\\noptimal, but aim instead for wide applicability combined with near-exact\\naccuracy. Second-order acuracy of BCa intervals was established by Hall\\n(1988). BCa is emphatically a child of the computer age, routinely requir-\\ning B D 2000 or more bootstrap replications per use. Shortcut methods are\\navailable. The ‚Äúabc method‚Äù (DiCiccio and Efron, 1992) needs only 1% as\\nmuch computation, at the expense of requiring smoothness properties for\\n(cid:18) D t .(cid:22)/, and a less automatic coding of the exponential family setting for\\nindividual situations. In other words, it is less convenient.\\n\\n(cid:142)1 [p. 183] Neyman‚Äôs construction. For any given value of (cid:18) , let .(cid:18) .:025/; (cid:18) .:975//\\n\\ndenote the central 95% interval of density f(cid:18) . O(cid:18)/, satisfying\\n\\nZ (cid:18) .:025/\\n\\n(cid:0)1\\n\\n(cid:17)\\n\\n(cid:16) O(cid:18)\\n\\nf(cid:18)\\n\\nd O(cid:18) D 0:025 and\\n\\nZ (cid:18) .:975/\\n\\n(cid:0)1\\n\\n(cid:17)\\n\\n(cid:16) O(cid:18)\\n\\nf(cid:18)\\n\\nd O(cid:18) D 0:975I\\n\\nand let I(cid:18) . O(cid:18)/ be the indicator function for O(cid:18) 2 .(cid:18) .:025/; (cid:18) .:975//,\\n\\n(cid:17)\\n\\n(cid:16) O(cid:18)\\n\\nD\\n\\nI(cid:18)\\n\\n(\\n1 if (cid:18) .:025/ < O(cid:18) < (cid:18) .:975/\\n0 otherwise.\\n\\nBy deÔ¨Ånition, I(cid:18) . O(cid:18)/ has a two-point probability distribution,\\n(\\n\\n(cid:17)\\n\\n(cid:16) O(cid:18)\\n\\nD\\n\\nI(cid:18)\\n\\n1 probability 0:95\\n0 probability 0:05:\\n\\n(11.73)\\n\\n(11.74)\\n\\n(11.75)\\n\\nThis makes I(cid:18) . O(cid:18)/ a pivotal statistic, one whose distribution does not de-\\npend upon (cid:18).\\n\\nNeyman‚Äôs construction takes the conÔ¨Ådence interval\\n\\nto observed value O(cid:18) to be\\n\\nC\\n\\n. O(cid:18)/ corresponding\\n\\n(cid:17)\\n\\n(cid:16) O(cid:18)\\n\\nD\\n\\nn\\n(cid:18) W I(cid:18)\\n\\n(cid:17)\\n\\n(cid:16) O(cid:18)\\n\\no\\nD 1\\n\\n:\\n\\nC\\n\\n(11.76)\\n\\n\\x0c11.7 Notes and Details\\n\\n205\\n\\nThen\\n\\nC\\n\\n. O(cid:18)/ has the desired coverage property\\n(cid:17)o\\n\\n(cid:17)\\n\\nn\\n\\nD Pr(cid:18)\\n\\nn\\nI(cid:18)\\n\\n(cid:16) O(cid:18)\\n\\nPr(cid:18)\\n\\n(cid:18) 2\\n\\n(cid:16) O(cid:18)\\n\\nC\\n\\nD 1\\n\\no\\n\\nD 0:95\\n\\n(11.77)\\n\\nfor any choice of the true parameter (cid:18). (For the normal theory correlation\\ndensity of f(cid:18) . O(cid:18)/, O(cid:18).:025/ and O(cid:18).:975/ are increasing functions of (cid:18). This\\nmakes our previous construction (11.6) agree with (11.76).) The construc-\\ntion applies quite generally, as long as we are able to deÔ¨Åne acceptance\\nregions of the sample space having the desired target probability content\\nfor every choice of (cid:18). This can be challenging in multiparameter families.\\n(cid:142)2 [p. 189] Fisherian correctness. Fisher, arguing against the Neyman para-\\ndigm, pointed out that conÔ¨Ådence intervals could be accurate without being\\n.(cid:18); 1/ for i D 1; 2; : : : ; 20, the standard\\ncorrect: having observed xi\\n0.95 interval based on just the Ô¨Årst 10 observations would provide exact\\n0.95 coverage while giving obviously incorrect inferences for (cid:18) . If we can\\nreduce the situation to form (11.22), the percentile method intervals satisfy\\nFisher‚Äôs ‚Äúlogic of inductive inference‚Äù for correctness, as at (4.31).\\n\\nN\\n\\niid(cid:24)\\n\\n(cid:142)3 [p. 189] Bootstrap sample sizes. Why we need bootstrap sample sizes on\\nthe order of B D 2000 for conÔ¨Ådence interval construction can be seen\\nin the estimation of the bias correction value z0 (11.32). The delta-method\\nstandard error of z0 D ÀÜ(cid:0)1.p0/ is calculated to be\\n\\n1\\n\\'.z0/\\n\\n(cid:20) p0.1 (cid:0) p0/\\nB\\n\\n(cid:21)1=2\\n\\n;\\n\\n(11.78)\\n\\n:D 0 this is\\nwith \\'.z/ the standard normal density. With p0\\nabout 1:25=B 1=2, equaling 0.028 at B D 2000, a none-too-small error for\\nuse in the BC formula (11.33) or the BCa formula (11.39).\\n\\n:D 0:5 and z0\\n\\n(cid:142)4 [p. 191] BCa accuracy and correctness. The BCa conÔ¨Ådence limit O(cid:18)BCa≈íÀõ(cid:141)\\n\\n(11.39) is transformation invariant. DeÔ¨Åne\\n\\nz≈íÀõ(cid:141) D z0 C\\n\\nz0 C z.Àõ/\\n1 (cid:0) a.z0 C z.Àõ//\\n\\n;\\n\\n(11.79)\\n\\nso O(cid:18)BCa≈íÀõ(cid:141) D OG(cid:0)1fÀÜ≈íz≈íÀõ(cid:141)(cid:141)g. For a monotone increasing transformation\\n(cid:30) D m.(cid:18) /, O(cid:30) D m. O(cid:18)/, and O(cid:30)(cid:3) D m.r/, the bootstrap cdf\\nOH of O(cid:30)(cid:3) sat-\\nOH (cid:0)1.Àõ/ D m≈í OG(cid:0)1.Àõ/(cid:141) since O(cid:30)(cid:3).Àõ/ D m. O(cid:18) (cid:3).Àõ// for the bootstrap\\nisÔ¨Åes\\npercentiles. Therefore\\n\\nO(cid:30)BCa≈íÀõ(cid:141) D OH (cid:0)1 fÀÜ .z≈íÀõ(cid:141)/g D m\\n\\n;\\n(11.80)\\nverifying transformation invariance. (Notice that z0 D ÀÜ(cid:0)1≈í OG. O(cid:18)/(cid:141) equals\\n\\n(cid:17)\\n(cid:16) OG(cid:0)1 fÀÜ .z≈íÀõ(cid:141)/g\\n\\n(cid:16) O(cid:18)BCa≈íÀõ(cid:141)\\n\\nD m\\n\\n(cid:17)\\n\\n\\x0cBootstrap ConÔ¨Ådence Intervals\\n206\\nÀÜ(cid:0)1≈í OH . O(cid:30)/(cid:141) and is also transformation invariant, as is a, as discussed pre-\\nviously.)\\n\\nExact conÔ¨Ådence intervals are transformation invariant, adding consider-\\nably to their inferential appeal. For approximate intervals, transformation\\ninvariance means that if we can demonstrate good behavior on any one\\nscale then it remains good on all scales. The model (11.38) to the (cid:30) scale\\ncan be re-expressed as\\n\\nn\\n1 C a O(cid:30)\\n\\no\\n\\nD f1 C a(cid:30)g f1 C a.Z (cid:0) z0/g ;\\n\\n(11.81)\\n\\nwhere Z is a standard normal variate, Z (cid:24)\\n\\nTaking logarithms,\\n\\n.0; 1/.\\n\\nN\\n\\nO(cid:13) D (cid:13) C U;\\n(11.82)\\nwhere O(cid:13) D logf1 C a O(cid:30)g, (cid:13) D logf1 C a(cid:30)g, and U is the random variable\\nlogf1 C a.Z (cid:0) z0/g; (11.82) represents the simplest kind of translation\\nmodel, where the unknown value of (cid:13) rigidly shifts the distribution of U .\\nThe obvious conÔ¨Ådence limit for (cid:13),\\n\\nO(cid:13)≈íÀõ(cid:141) D O(cid:13) (cid:0) U .1(cid:0)Àõ/;\\n\\n(11.83)\\n\\nwhere U .1(cid:0)Àõ/ is the 100.1 (cid:0) Àõ/th percentile of U , is then accurate, and\\nalso ‚Äúcorrect,‚Äù according to Fisher‚Äôs (admittedly vague) logic of inductive\\ninference. It is an algebraic exercise, given in Section 3 of Efron (1987),\\nto reverse the transformations (cid:18) ! (cid:30) ! (cid:13) and recover O(cid:18)BCa≈íÀõ(cid:141) (11.39).\\nSetting a D 0 shows the accuracy and correctness of O(cid:18)BC≈íÀõ(cid:141) (11.33).\\n(cid:142)5 [p. 192] The acceleration a. This a appears in (11.38) as d(cid:27)(cid:30)=d(cid:30), the\\nrate of change of O(cid:30)‚Äôs standard deviation as a function of its expectation.\\nIn one-parameter exponential families it turns out that this is one-third of\\nd(cid:27)(cid:18) =d(cid:18) ; that is, the transformation to normality (cid:30) D m.(cid:18)/ also decreases\\nthe instability of the standard deviation, though not to zero.\\n\\nThe variance of the score function Plx.(cid:18)/ determines the standard de-\\nviation of the MLE O(cid:18) (4.17)‚Äì(4.18). In one-parameter exponential fami-\\nlies, one-sixth the skewness of Plx.(cid:18)/ gives a. The skewness connection can\\nbe seen at work in estimate (11.40). In multivariate exponential families\\n(5.50), the skewness must be evaluated in the ‚Äúleast favorable‚Äù direction,\\ndiscussed further in Chapter 13. The R algorithm accel (book web site)\\nuses B parametric bootstrap replications . OÀá(cid:3)b; O(cid:18) (cid:3)b/ to estimate a. The per-\\ncentile and BC intervals require only the replications O(cid:18) (cid:3)b, while BCa also\\nrequires knowledge of the underlying exponential family. See Sections 4,\\n6, and 7 of Efron (1987).\\n\\n\\x0c11.7 Notes and Details\\n\\n207\\n\\n(cid:142)6 [p. 195] Equation (11.42). Model (11.41) makes O(cid:18) D P x2\\n\\ntral chi-square variable with noncentrality parameter (cid:18) D P (cid:22)2\\ndegrees of freedom, written as O(cid:18) (cid:24) (cid:31)2\\nparametric bootstrap distribution is r (cid:24) (cid:31)2\\nPrf(cid:31)2\\n\\ni a noncen-\\ni and n\\n(cid:18);n. With O(cid:18) D 20 and n D 10, the\\n20;10. Numerical evaluation gives\\n\\n(cid:20) 20g D 0:156, leading to (11.42).\\n\\n20;10\\n\\nEfron (1985) concerns conÔ¨Ådence intervals for parameters (cid:18) D t.(cid:22)/ in\\nmodel (11.41), where third-order accurate conÔ¨Ådence intervals can be cal-\\nculated. The acceleration a equals zero for such problems, making the BC\\nintervals second-order accurate. In practice, the BC intervals usually per-\\nform well, and are a reasonable choice if the accleration a is unavailable.\\n\\n(cid:142)7 [p. 202] BCa conÔ¨Ådence density (11.68). DeÔ¨Åne\\n\\ni\\nz(cid:18) D ÀÜ(cid:0)1 h OG.(cid:18)/\\n\\n(cid:0) z0 D\\n\\nz0 C z.Àõ/\\n1 (cid:0) a (cid:0)z0 C z.Àõ/(cid:1) ;\\n\\nso that\\n\\nz.Àõ/ D z(cid:18)\\n\\n1 C az(cid:18)\\n\\n(cid:0) z0\\n\\nand Àõ D ÀÜ\\n\\n(cid:18) z(cid:18)\\n\\n1 C az(cid:18)\\n\\n(cid:19)\\n\\n:\\n\\n(cid:0) z0\\n\\n(11.84)\\n\\n(11.85)\\n\\nHere we are thinking of Àõ and (cid:18) as functionally related by (cid:18) D O(cid:18)BCa≈íÀõ(cid:141).\\nDifferentiation yields\\n\\n(cid:17)\\n\\n;\\n\\n(cid:16) z(cid:18)\\n\\n(cid:0) z0\\n1Caz(cid:18)\\n.1 C az(cid:18) /2\\n(cid:16) z(cid:18)\\n\\n\\'\\n\\n1Caz(cid:18)\\n\\n(cid:0) z0\\n\\n\\'\\n\\nD\\n\\nD\\n\\ndÀõ\\ndz(cid:18)\\n\\ndz(cid:18)\\nd(cid:18)\\n\\n.1 C az(cid:18) /2\\'.z(cid:18) C z0/\\n\\nOg.(cid:18)/;\\n\\n(cid:17)\\n\\n(11.86)\\n\\nwhich together give dÀõ=d(cid:18) , verifying (11.68).\\n\\nThe name ‚ÄúconÔ¨Ådence density‚Äù seems to appear Ô¨Årst in Efron (1993),\\nthough the idea is familiar in the Ô¨Åducial literature. An ambitious frequen-\\ntist theory of conÔ¨Ådence distributions is developed in Xie and Singh (2013).\\n(cid:142)8 [p. 203] Jeffreys‚Äô prior. Formula (11.72) is discussed further in Chapter 13,\\nin the more general context of uninformative prior distributions. The the-\\nory of matching priors was initiated by Welch and Peers (1963), another\\nimportant reference being Tibshirani (1989).\\n\\n\\x0c12\\n\\nCross-Validation and Cp Estimates of\\nPrediction Error\\n\\nPrediction has become a major branch of twenty-Ô¨Årst-century commerce.\\nQuestions of prediction arise naturally: how credit-worthy is a loan appli-\\ncant? Is a new email message spam? How healthy is the kidney of a poten-\\ntial donor? Two problems present themselves: how to construct an effective\\nprediction rule, and how to estimate the accuracy of its predictions. In the\\nlanguage of Chapter 1, the Ô¨Årst problem is more algorithmic, the second\\nmore inferential. Chapters 16‚Äì19, on machine learning, concern predic-\\ntion rule construction. Here we will focus on the second question: having\\nchosen a particular rule, how do we estimate its predictive accuracy?\\n\\nTwo quite distinct approaches to prediction error assessment developed\\nin the 1970s. The Ô¨Årst, depending on the classical technique of cross-\\nvalidation, was fully general and nonparametric. A narrower (but more\\nefÔ¨Åcient) model-based approach was the second, emerging in the form of\\nMallows‚Äô Cp estimate and the Akaike information criterion (AIC). Both\\ntheories will be discussed here, beginning with cross-validation, after a\\nbrief overview of prediction rules.\\n\\n12.1 Prediction Rules\\n\\nPrediction problems typically begin with a training set d consisting of N\\npairs .xi ; yi /,\\n\\nd D f.xi ; yi /; i D 1; 2; : : : ; N g ;\\n\\n(12.1)\\n\\nwhere xi is a vector of p predictors and yi a real-valued response. On the\\nbasis of the training set, a prediction rule rd .x/ is constructed such that a\\nprediction Oy is produced for any point x in the predictor‚Äôs sample space\\n,\\n\\nX\\n\\nOy D rd .x/\\n\\nfor x 2\\n\\n:\\n\\nX\\n\\n(12.2)\\n\\n208\\n\\n\\x0c12.1 Prediction Rules\\n\\n209\\n\\nThe inferential task is to assess the accuracy of the rule‚Äôs predictions. (In\\npractice there are usually several competing rules under consideration and\\nthe main question is determining which is best.)\\n\\nIn the spam data of Section 8.1, xi comprised p D 57 keyword counts,\\nwhile yi (8.18) indicated whether or not message i was spam. The rule\\nrd .x/ in Table 8.3 was an MLE logistic regression Ô¨Åt. Given a new mes-\\nsage‚Äôs count vector, say x0, rd .x0/ provided an estimated probability O(cid:25)0 of\\nit being spam, which could be converted into a prediction Oy0 according to\\n\\nOy0 D\\n\\n(\\n1\\n0\\n\\nif O(cid:25)0 (cid:21) 0:5\\nif O(cid:25)0 < 0:5:\\n\\n(12.3)\\n\\nThe diabetes data of Table 7.2, Section 7.3, involved the p D 10 pre-\\ndictors x D (age, sex, . . . , glu), obtained at baseline, and a response y\\nmeasuring disease progression one year later. Given a new patient‚Äôs base-\\nline measurements x0, we would like to predict his or her progression y0.\\nTable 7.3 suggests two possible prediction rules, ordinary least squares and\\nridge regression using ridge parameter (cid:21) D 0:1, either of which will pro-\\nduce a prediction Oy0. In this case we might assess prediction error in terms\\nof squared error, .y0 (cid:0) Oy0/2.\\n\\nIn both of these examples, rd .x/ was a regression estimator suggested\\nby a probability model. One of the charms of prediction is that the rule\\nrd .x/ need not be based on an explicit model. Regression trees, as pictured\\nin Figure 8.7, are widely used1 prediction algorithms that do not require\\nmodel speciÔ¨Åcations. Prediction, perhaps because of its model-free nature,\\nis an area where algorithmic developments have run far ahead of their in-\\nferential justiÔ¨Åcation.\\n\\nQuantifying the prediction error of a rule rd .x/ requires speciÔ¨Åcation of\\nthe discrepancy D.y; Oy/ between a prediction Oy and the actual response y.\\nThe two most common choices are squared error\\n\\nand classiÔ¨Åcation error\\n\\nD.y; Oy/ D .y (cid:0) Oy/2;\\n\\nD.y; Oy/ D\\n\\n(\\n\\n1 if y ¬§ Oy\\n0 if y D Oy;\\n\\n(12.4)\\n\\n(12.5)\\n\\nwhen, as with the spam data, the response y is dichotomous. (Prediction\\nof a dichotomous response is often called ‚ÄúclassiÔ¨Åcation.‚Äù)\\n\\n1 Random forests, one of the most popular machine learning prediction algorithms, is an\\n\\nelaboration of regression trees. See Chapter 17.\\n\\n\\x0c210\\n\\nCross-Validation and Cp Estimates\\n\\nFor the purpose of error estimation, we suppose that the pairs .xi ; yi / in\\nthe training set d of (12.1) have been obtained by random sampling from\\nsome probability distribution F on .p C 1/-dimensional space\\n\\npC1,\\n\\nR\\n\\n.xi ; yi / iid(cid:24) F\\n\\nfor i D 1; 2; : : : ; N:\\n\\n(12.6)\\n\\nThe true error rate Errd of rule rd .x/ is the expected discrepancy of Oy0 D\\nrd .x0/ from y0 given a new pair .x0; y0/ drawn from F independently of\\nd,\\n\\nErrd D EF fD.y0; Oy0/g I\\n\\n(12.7)\\n\\nd (and rd .(cid:1)/) is held Ô¨Åxed in expectation (12.7), only .x0; y0/ varying.\\n\\n(cid:142)1\\n\\nFigure 12.1 concerns the supernova data, an example we will return to in\\nthe next section.(cid:142)Absolute magnitudes yi have been measured for N D 39\\nrelatively nearby Type Ia supernovas, with the data scaled such that\\n\\nind(cid:24)\\n\\nyi\\n\\nN\\n\\n.(cid:22)i ; 1/;\\n\\ni D 1; 2; : : : ; 39;\\n\\n(12.8)\\n\\nis a reasonable model. For each supernova, a vector xi of p D 10 spectral\\nenergies has been observed,\\n\\nxi D .xi1; xi2; : : : ; xi10/;\\n\\ni D 1; 2; : : : ; 39:\\n\\n(12.9)\\n\\nTable 12.1 shows .xi ; yi / for i D 1; 2; : : : ; 5. (The frequency measure-\\nments have been standardized to have mean 0 and variance 1, while y has\\nbeen adjusted to have mean 0.)\\n\\nOn the basis of the training set d D f.xi ; yi /; i D 1; 2; : : : ; 39g, we\\nwish to construct a rule rd .x/ that, given the frequency vector x0 for a\\nnewly observed Type Ia supernova, accurately predicts2 its absolute mag-\\nnitude y0. To this end, a lasso estimate QÀá.(cid:21)/ was Ô¨Åt, with y in (7.42) the\\nvector .y1; y2; : : : ; y39/ and x the 39 (cid:2) 10 matrix having ith row xi ; (cid:21)\\nwas selected to minimize a Cp estimate of prediction error, Section 12.3,\\nyielding prediction rule\\n\\nOy0 D x0\\n\\n0\\n\\nQÀá.(cid:21)/:\\n\\n(12.10)\\n\\n(So in this case constructing rd .x/ itself involves error rate estimation.)\\n\\n2 Type Ia supernovas were used as ‚Äústandard candles‚Äù in the discovery of dark energy and\\nthe cosmological expansion of the Universe, on the grounds that they have constant\\nabsolute magnitude. This isn‚Äôt exactly true. Our training set is unusual in that the 39\\nsupernovas are close enough to Earth to have y ascertained directly. This allows the\\nconstruction of a prediction rule based on the frequency vector x, which is observable\\nfor distant supernovas, leading to improved calibration of the cosmological expansion.\\n\\n\\x0c12.1 Prediction Rules\\n\\n211\\n\\nFigure 12.1 The supernova data; observed absolute magnitudes\\nyi (on log scale) plotted versus predictions Oyi obtained from lasso\\nrule (12.10), for N D 39 nearby Type Ia supernovas. Predictions\\nbased on 10 spectral power measurements, 7 of which had\\nnonzero coefÔ¨Åcients in QÀá.(cid:21)/.\\n\\nThe plotted points in Figure 12.1 are . Oyi ; yi / for i D 1; 2; : : : ; N D 39.\\n\\nThese gave apparent error\\n\\nerr D 1\\nN\\n\\nN\\nX\\n\\n.yi (cid:0) Oyi /2 D 0:720:\\n\\ni D1\\n\\n(12.11)\\n\\nComparing this with P.yi (cid:0) Ny/2=N D 3:91 yields an impressive-looking\\n‚ÄúR squared‚Äù value\\n\\nR2 D 1 (cid:0) 0:720=3:91 D 0:816:\\n\\n(12.12)\\n\\nThings aren‚Äôt really that good (see (12.23)). Cross-validation and Cp meth-\\nods allow us to correct apparent errors for the fact that rd .x/ was chosen\\nto make the predictions Oyi Ô¨Åt the data yi .\\n\\nPrediction and estimation are close cousins but they are not twins. As\\ndiscussed earlier, prediction is less model-dependent, which partly accounts\\nfor the distinctions made in Section 8.4. The prediction criterion Err (12.7)\\n\\nlllllllllllllllllllllllllllllllllllllll‚àí4‚àí20246‚àí4‚àí20246Predicted magnitude y^iAbsolute magnitude yiapparent mean squarederror = 0.72\\x0c212\\n\\nCross-Validation and Cp Estimates\\n\\nTable 12.1 Supernova data; 10 frequency measurements and response\\nvariable ‚Äúabsolute magnitude‚Äù for the Ô¨Årst 5 of N D 39 Type Ia\\nsupernovas. In terms of notation (12.1), frequency measurements are x\\nand magnitude y.\\n\\nSN1\\n\\nSN2\\n\\nSN3\\n\\nSN4\\n\\nSN5\\n\\nx1\\nx2\\nx3\\nx4\\nx5\\nx6\\nx7\\nx8\\nx9\\nx10\\nmag\\n\\n(cid:0).84 (cid:0)1.89\\n(cid:0).46\\n(cid:0).93\\n2.41\\n.32\\n.77\\n.18\\n(cid:0).68\\n(cid:0).94\\n(cid:0)1.27 (cid:0)1.53\\n\\n(cid:0).08\\n.41\\n.26\\n(cid:0).81\\n(cid:0).80\\n1.02\\n(cid:0).13\\n(cid:0).21\\n1.14\\n(cid:0).86 (cid:0)1.12\\n1.31\\n(cid:0).65\\n(cid:0).86\\n.30\\n.72\\n(cid:0).82\\n.62\\n.56 (cid:0)1.53\\n.62 (cid:0)1.49\\n(cid:0).49 (cid:0)1.09\\n:95 (cid:0)3:75\\n\\n.68\\n(cid:0).35\\n.09 (cid:0)1.04\\n.26 (cid:0)1.10\\n.18 (cid:0)1.32\\n(cid:0).54 (cid:0)1.70\\n(cid:0):22\\n2:12\\n\\n.34\\n(cid:0).43\\n(cid:0).02\\n(cid:0).3\\n(cid:0):54\\n\\nis an expectation over the .x; y/ space. This emphasizes good overall per-\\nformance, without much concern for behavior at individual points x in\\n.\\nShrinkage usually improves prediction. Consider a Bayesian model like\\n\\nX\\n\\nthat of Section 7.1,\\n\\n(cid:22)i (cid:24)\\n\\nN\\n\\n.0; A/\\n\\nand xi j(cid:22)i (cid:24)\\n\\n.(cid:22)i ; 1/\\n\\nN\\n\\nfor i D 1; 2; : : : ; N:\\n\\nThe Bayes shrinkage estimator, which is ideal for estimation,\\n\\nO(cid:22)i D Bxi ;\\n\\nB D A=.A C 1/;\\n\\n(12.13)\\n\\n(12.14)\\n\\nis also ideal for prediction. Suppose that in addition to the observations\\nxi there are independent unobserved replicates, one for each of the N xi\\nvalues,\\n\\nyi (cid:24)\\n\\nN\\n\\n.(cid:22)i ; 1/\\n\\nfor i D 1; 2; : : : ; N;\\n\\n(12.15)\\n\\nthat we wish to predict. The Bayes predictor\\n\\nOyi D Bxi\\n\\nhas overall Bayes prediction error\\n\\n(\\n\\n1\\nN\\n\\nE\\n\\nN\\nX\\n\\n.yi (cid:0) Oyi /2\\n\\n)\\n\\nD B C 1;\\n\\ni D1\\n\\n(12.16)\\n\\n(12.17)\\n\\n\\x0c12.2 Cross-Validation\\n\\n213\\n\\nwhich cannot be improved upon. The MLE rule Oyi D xi has Bayes predic-\\ntion error 2, which is always worse than (12.17).\\n\\nAs far as prediction is concerned it pays to overshrink, as illustrated in\\nFigure 7.1 for the James‚ÄìStein version of situation (12.13). This is Ô¨Åne for\\nprediction, but less Ô¨Åne for estimation if we are concerned about extreme\\ncases; see Table 7.4. Prediction rules sacriÔ¨Åce the extremes for the sake of\\nthe middle, a particularly effective tactic in dichotomous situations (12.5),\\nwhere the cost of individual errors is bounded. The most successful ma-\\nchine learning prediction algorithms, discussed in Chapters 16‚Äì19, carry\\nout a version of local Bayesian shrinkage in selected regions of\\n\\n.\\n\\nX\\n\\n12.2 Cross-Validation\\n\\nHaving constructed a prediction rule rd .x/ on the basis of training set d,\\nwe wish to know its prediction error Err D EF fD.y0; Oy0/g (12.7) for a\\nnew case obtained independently of d. A Ô¨Årst guess is the apparent error\\n\\nerr D 1\\nN\\n\\nN\\nX\\n\\ni D1\\n\\nD.yi ; Oyi /;\\n\\n(12.18)\\n\\nthe average discrepancy in the training set between yi and its prediction\\nOyi D rd .xi /; err usually underestimates Err since rd .x/ has been adjusted3\\nto Ô¨Åt the observed responses yi .\\n\\nThe ideal remedy, discussed in Section 12.4, would be to have an inde-\\n\\npendent validation set (or test set) dval of Nval additional cases,\\n\\ndval D Àö.x0j ; y0j /; j D 1; 2; : : : ; Nval\\n\\n(cid:9) :\\n\\n(12.19)\\n\\nThis would provide an unbiased estimate of Err,\\n\\ncErrval D 1\\nNval\\n\\nNvalX\\n\\nj D1\\n\\nD.y0j ; Oy0j /;\\n\\nOy0j D rd .x0j /:\\n\\n(12.20)\\n\\nCross-validation attempts to mimic cErrval without the need for a valida-\\ntion set. DeÔ¨Åne d.i / to be the reduced training set in which pair .xi ; yi /\\nhas been omitted, and let rd.i/.(cid:1)/ indicate the rule constructed on the basis\\n\\n3 Linear regression using ordinary least squares Ô¨Åtting provides a classical illustration:\\ni .yi (cid:0) Oyi /2=.N (cid:0) p/, where p is\\n\\nerr D P\\nthe degrees of freedom, to obtain an unbiased estimate of the noise variance (cid:27) 2.\\n\\ni .yi (cid:0) Oyi /2=N must be increased to P\\n\\n\\x0c214\\n\\nCross-Validation and Cp Estimates\\n\\nof d.i/. The cross-validation estimate of prediction error is\\n\\ncErrcv D 1\\nN\\n\\nN\\nX\\n\\ni D1\\n\\nD.yi ; Oy.i//;\\n\\nOy.i/ D rd.i/.xi /:\\n\\n(12.21)\\n\\nNow .xi ; yi / is not involved in the construction of the prediction rule for\\nyi .\\n\\ncErrcv (12.21) is the ‚Äúleave one out‚Äù version of cross-validation. A more\\ncommon tactic is to leave out several pairs at a time: d is randomly parti-\\ntioned into J groups of size about N=J each; d.j /, the training set with\\ngroup j omitted, provides rule rd.j /.x/, which is used to provide predic-\\ntions for the yi in group j . Then cErrcv is evaluated as in (12.21). Besides\\nreducing the number of rule constructions necessary, from N to J , group-\\ning induces larger changes among the J training sets, improving the predic-\\ntive performance on rules rd .x/ that include discontinuities. (The argument\\nhere is similar to that for the jackknife, Section 10.1.)\\n\\nCross-validation was applied to the supernova data pictured in Figure 12.1.\\n\\nThe 39 cases were split, randomly, into J D 13 groups of three cases each.\\nThis gave\\n\\ncErrcv D 1:17;\\n(12.21), 62% larger than err D 0:72 (12.11). The R2 calculation (12.12)\\nnow yields the smaller value\\n\\n(12.22)\\n\\nR2 D 1 (cid:0) 1:17=3:91 D 0:701:\\n\\n(12.23)\\n\\nWe can apply cross-validation to the spam data of Section 8.1, having\\nN D 4061 cases, p D 57 predictors, and dichotomous response y. For\\nthis example, each of the 57 predictors was itself dichotomized to be either\\n0 or 1 depending on whether the original value xij equaled zero or not.\\nA logistic regression, Section 8.1, regressing yi on the 57 dichotomized\\npredictors, gave apparent classiÔ¨Åcation error (12.5)\\n\\nerr D 0:064;\\n\\n(12.24)\\n\\ni.e., 295 wrong predictions among the 4061 cases. Cross-validation, with\\nJ D 10 groups of size 460 or 461 each, increased this to\\n\\ncErrcv D 0:069;\\n\\n(12.25)\\n\\nan increase of 8%.\\n\\nGlmnet is an automatic model building program that, among other\\nthings, constructs a lasso sequence of logistic regression models, adding\\n\\n\\x0c12.2 Cross-Validation\\n\\n215\\n\\nFigure 12.2 Spam data. Apparent error rate err (blue) and\\ncross-validated estimate (red) for a sequence of prediction rules\\ngenerated by glmnet. The degrees of freedom are the number of\\nnonzero regression coefÔ¨Åcients: df D 57 corresponds to ordinary\\nlogistic regression, which gave apparent err 0.064, cross-validated\\nrate 0.069. The minimum cross-validated error rate is 0.067.\\n\\nvariables one at a time in their order of apparent predictive power; see\\nChapter 16. The blue curve in Figure 12.2 tracks the apparent error err\\n(12.18) as a function of the number of predictors employed. Aside from nu-\\nmerical artifacts, err is monotonically decreasing, declining to err D 0:064\\nfor the full model that employs all 57 predictors, i.e., for the usual logistic\\nregression model, as in (12.24).\\n\\nGlmnet produced prediction error estimates cErrcv for each of the suc-\\ncessive models, shown by the red curve. These are a little noisy themselves,\\nbut settle down between 4% and 8% above the corresponding err estimates.\\nThe minimum value\\n\\ncErrcv D 0:067\\n\\n(12.26)\\n\\noccurred for the model using 47 predictors.\\n\\nThe difference between (12.26) and (12.25) is too small to take seriously\\ngiven the noise in the cErrcv estimates. There is a more subtle objection:\\nthe choice of ‚Äúbest‚Äù prediction rule based on comparative cErrcv estimates\\nis not itself cross-validated. Each case .xi ; yi / is involved in choosing its\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.000.050.100.150.200.25Degrees of freedomMisclassification error ratelllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllogisticregression\\x0c216\\n\\nCross-Validation and Cp Estimates\\n\\nown best prediction, so cErrcv at the apparently optimum choice cannot be\\ntaken entirely at face value.\\n\\nNevertheless, perhaps the principal use of cross-validation lies in choos-\\ning among competing prediction rules. Whether or not this is fully justiÔ¨Åed,\\nit is often the only game in town. That being said, minimum predictive er-\\nror, no matter how effectuated, is a notably weaker selection principle than\\nminimum variance of estimation.\\n\\nAs an example, consider an iid normal sample\\n\\niid(cid:24)\\n\\nxi\\n\\nN\\n\\n.(cid:22); 1/;\\n\\ni D 1; 2; : : : ; 25;\\n\\n(12.27)\\n\\nhaving mean Nx and median Mx. Both are unbiased for estimating (cid:22), but Nx is\\nmuch more efÔ¨Åcient,\\n\\nvar. Mx/= var. Nx/\\n\\n:D 1:57:\\n\\n(12.28)\\n\\nSuppose we wish to predict a future observation x0 independently selected\\n.(cid:22); 1/ distribution. In this case there is very little advan-\\nfrom the same\\ntage to Nx,\\n\\nN\\n\\nE Àö.x0 (cid:0) Mx/2(cid:9) ƒ±E Àö.x0 (cid:0) Nx/2(cid:9) D 1:02:\\n\\n(12.29)\\n\\nThe noise in x0 (cid:24)\\n.(cid:22); 1/ dominates its prediction error. Perhaps the\\nproliferation of prediction algorithms to be seen in Part III reÔ¨Çects how\\nweakly changes in strategy affect prediction error.\\n\\nN\\n\\nTable 12.2 Ratio of predictive errors Ef. Nx0 (cid:0) Mx/2g=Ef. Nx0 (cid:0) Nx/2g for Nx0\\n.(cid:22); 1/; Nx and Mx are\\nthe mean of an independent sample of size N0 from\\nthe mean and median from xi (cid:24)\\n\\n.(cid:22); 1/ for i D 1; 2; : : : ; 25.\\n\\nN\\n\\nN0\\nRatio\\n\\n1\\n1.02\\n\\n100\\n1.46\\n\\n1000 1\\n1.57\\n1.56\\n\\nN\\n10\\n1.16\\n\\nIn this last example, suppose that our task was to predict the average\\nNx0 of N0 further draws from the\\n.(cid:22); 1/ distribution. Table 12.2 shows\\nthe ratio of predictive errors as a function of N0. The superiority of the\\nmean compared to the median reveals itself as N0 gets larger. In this super-\\nsimpliÔ¨Åed example, the difference between prediction and estimation lies\\nin predicting the average of one versus an inÔ¨Ånite number of future obser-\\nvations.\\n\\nN\\n\\nDoes cErrcv actually estimate Errd as deÔ¨Åned in (12.7)? It seems like the\\nanswer must be yes, but there is some doubt expressed in the literature, for\\n\\n\\x0c12.2 Cross-Validation\\n\\n217\\n\\nreasons demonstrated in the following simulation: we take the true distri-\\nbution F in (12.6) to be the discrete distribution OF that puts weight 1=39\\non each of the 39 .xi ; yi / pairs of the supernova data.4 A random sample\\nwith replacement of size 39 from OF gives simulated data set d (cid:3) and pre-\\ndiction rule rd (cid:3).(cid:1)/ based on the lasso/Cp recipe used originally. The same\\n(cid:3)\\ncross-validation procedure as before, applied to d (cid:3), gives cErr\\ncv. Because\\nthis is a simulation, we can also compute the actual mean-squared error\\nrate of rule rd (cid:3).(cid:1)/ applied to the true distribution OF ,\\n\\nErr(cid:3) D 1\\n39\\n\\n39\\nX\\n\\ni D1\\n\\nD .yi ; rd (cid:3).xi // :\\n\\n(12.30)\\n\\nFigure 12.3 Simulation experiment comparing true error Err\\n(cid:3)\\ncv; 500 simulations based on the\\nwith cross-validation estimate cErr\\n(cid:3)\\ncv and Err are negatively correlated.\\nsupernova data. cErr\\n\\nFigure 12.3 plots .Err(cid:3); cErr\\n\\n(cid:3)\\ncv/ for 500 simulations, using squared er-\\nror discrepancy D.y; Oy/ D .y (cid:0) Oy/2. Summary statistics are given in Ta-\\n(cid:3)\\nble 12.3. cErr\\ncv has performed well overall, averaging 1.07, quite near the\\ntrue Err 1.02, both estimates being 80% greater than the average appar-\\nent error 0.57. However, the Ô¨Ågure shows something unsettling: there is a\\n\\n4 Simulation based on OF is the same as nonparametric bootstrap analysis, Chapter 10.\\n\\n********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************1.01.52.02.50.51.01.52.02.53.0Err*Errcv*l\\x0c218\\n\\nCross-Validation and Cp Estimates\\n\\nTable 12.3 True error Err(cid:3), cross-validated error cErr\\nerror err(cid:3); 500 simulations based on supernova data. Correlation (cid:0)0:175\\nbetween Err(cid:3) and cErr\\n\\n(cid:3)\\ncv, and apparent\\n\\n(cid:3)\\ncv.\\n\\nErr(cid:3)\\n\\n1.02\\n.27\\n\\n(cid:3)\\ncErr\\ncv\\n\\n1.07\\n.34\\n\\nerr(cid:3)\\n\\n.57\\n.16\\n\\nMean\\nSt dev\\n\\n(cid:3)\\nnegative correlation between cErr\\nsmaller values of the true prediction error, and vice versa.\\n\\ncv and Err(cid:3). Large values of cErr\\n\\n(cid:3)\\ncv go with\\n\\nOur original deÔ¨Ånition of Err,\\n\\nErrd D EF fD.y0; rd .x0//g ;\\n\\n(12.31)\\n\\n(cid:3)\\n\\ntook rd .(cid:1)/ Ô¨Åxed as constructed from d, only .x0; y0/ (cid:24) F random. In other\\nwords, Errd was the expected prediction error for the speciÔ¨Åc rule rd .(cid:1)/, as\\ncv is tracking Err(cid:3) we would expect to see a positive\\nis Err(cid:3) for rd (cid:3).(cid:1)/. If cErr\\ncorrelation in Figure 12.3.\\n\\n(cid:3)\\ncv is estimating the expected predictive\\nAs it is, all we can say is that cErr\\nerror, where d as well as .x0; y0/ is random in deÔ¨Ånition (12.31). This\\nmakes cross-validation a strongly frequentist device: cErrcv is estimating the\\naverage prediction error of the algorithm producing rd .(cid:1)/, not of rd .(cid:1)/ itself.\\n\\n12.3 Covariance Penalties\\n\\nCross-validation does its work nonparametrically and without the need for\\nprobabilistic modeling. Covariance penalty procedures require probability\\nmodels, but within their ambit they provide less noisy estimates of predic-\\ntion error. Some of the most prominent covariance penalty techniques will\\nbe examined here, including Mallows‚Äô Cp, Akaike‚Äôs information criterion\\n(AIC), and Stein‚Äôs unbiased risk estimate (SURE).\\n\\nThe covariance penalty approach treats prediction error estimation in\\na regression framework: the predictor vectors xi in the training set d D\\nf.xi ; yi /; i D 1; 2; : : : ; N g (12.1) are considered Ô¨Åxed at their observed\\nvalues, not random as in (12.6). An unknown vector (cid:22) of expectations\\n(cid:22)i D Efyi g has yielded the observed vector of responses y according to\\nsome given probability model, which to begin with we assume to have the\\n\\n\\x0c12.3 Covariance Penalties\\n\\n219\\n\\nsimple form\\n\\ny (cid:24) .(cid:22); (cid:27) 2I/I\\n\\n(12.32)\\n\\nthat is, the yi are uncorrelated, with yi having unknown mean (cid:22)i and vari-\\nance (cid:27) 2. We take (cid:27) 2 as known, though in practice it must usually be esti-\\nmated.\\n\\nA regression rule r.(cid:1)/ has been used to produce an estimate of vector (cid:22),\\n\\nO(cid:22) D r.y/:\\n\\n(12.33)\\n\\n(Only y is included in the notation since the predictors xi are considered\\nÔ¨Åxed and known.) For instance we might take\\n\\nO(cid:22) D r.y/ D X .X 0X /(cid:0)1X 0y;\\n\\n(12.34)\\n\\nwhere X is the N (cid:2) p matrix having xi as the ith row, as suggested by the\\nlinear regression model (cid:22) D X Àá.\\n\\nIn covariance penalty calculations, the estimator O(cid:22) also functions as a\\npredictor. We wonder how accurate O(cid:22) D r.y/ will be in predicting a new\\nvector of observations y0 from model (12.32),\\n\\ny0 (cid:24) .(cid:22); (cid:27) 2I/;\\n\\nindependent of y:\\n\\n(12.35)\\n\\nTo begin with, prediction error will be assessed in terms of squared dis-\\ncrepancy,\\n\\nErri D E0\\n\\nÀö.y0i (cid:0) O(cid:22)i /2(cid:9)\\n\\n(12.36)\\n\\nfor component i, where E0 indicates expectation with y0i random but O(cid:22)i\\nheld Ô¨Åxed. Overall prediction error is the average5\\n\\nErr(cid:1) D 1\\nN\\n\\nN\\nX\\n\\ni D1\\n\\nErri :\\n\\nThe apparent error for component i is\\n\\nerri D .yi (cid:0) O(cid:22)i /2:\\n\\n(12.37)\\n\\n(12.38)\\n\\nA simple but powerful lemma underlies the theory of covariance penalties.\\n\\nLemma Let E indicate expectation over both y in (12.32) and y0 in\\n(12.35). Then\\n\\nEfErri g D Eferri g C 2 cov. O(cid:22)i ; yi /;\\n\\n(12.39)\\n\\n5 Err(cid:1) is sometimes called ‚Äúinsample error,‚Äù as opposed to ‚Äúoutsample error‚Äù Err (12.7),\\n\\nthough in practice the two tend to behave similarly.\\n\\n\\x0c220\\n\\nCross-Validation and Cp Estimates\\n\\nwhere the last term is the covariance between the ith components of O(cid:22) and\\ny,\\n\\ncov. O(cid:22)i ; yi / D E f. O(cid:22)i (cid:0) (cid:22)i /.yi (cid:0) (cid:22)i /g :\\n\\n(12.40)\\n\\n(Note: (12.40) does not require Ef O(cid:22)i g D (cid:22)i .)\\nProof Letting (cid:15)i D yi (cid:0) (cid:22)i and ƒ±i D . O(cid:22)i (cid:0) (cid:22)i /, the elementary equality\\n.(cid:15)i (cid:0) ƒ±i /2 D (cid:15)2\\ni\\n\\ni becomes\\n.yi (cid:0) O(cid:22)i /2 D .yi (cid:0) (cid:22)i /2 (cid:0) 2. O(cid:22)i (cid:0) (cid:22)i /.yi (cid:0) (cid:22)i / C . O(cid:22)i (cid:0) (cid:22)i /2;\\n\\n(cid:0) 2(cid:15)i ƒ±i C ƒ±2\\n\\n(12.41)\\n\\nand likewise\\n\\n.y0i (cid:0) O(cid:22)i /2 D .y0i (cid:0) (cid:22)i /2 (cid:0) 2. O(cid:22)i (cid:0) (cid:22)i /.y0i (cid:0) (cid:22)i / C . O(cid:22)i (cid:0) (cid:22)i /2: (12.42)\\n\\nTaking expectations, (12.41) gives\\n\\nEferri g D (cid:27) 2 (cid:0) 2 cov. O(cid:22)i ; yi / C E. O(cid:22)i (cid:0) (cid:22)i /2;\\n\\n(12.43)\\n\\nwhile (12.42) gives\\n\\nEfErri g D (cid:27) 2 C E. O(cid:22)i (cid:0) (cid:22)i /2;\\n\\n(12.44)\\n\\nthe middle term on the right side of (12.42) equaling zero because of the\\nindependence of y0i and O(cid:22)i . Taking the difference between (12.44) and\\n(cid:4)\\n(12.43) veriÔ¨Åes the lemma.\\n\\nNote: The lemma remains valid if (cid:27) 2 varies with i.\\n\\nThe lemma says that, on average, the apparent error erri understimates\\nthe true prediction error Erri by the covariance penalty 2 cov. O(cid:22)i ; yi /. (This\\nmakes intuitive sense since cov.(cid:22)i ; yi / measures the amount by which yi\\ninÔ¨Çuences its own prediction O(cid:22)i .) Covariance penalty estimates of predic-\\ntion error take the form\\n\\ncErri D erri C2dcov. O(cid:22)i ; yi /;\\n(12.45)\\nwhere dcov. O(cid:22)i ; yi / approximates cov.(cid:22)i ; yi /; overall prediction error (12.37)\\nis estimated by\\n\\ncErr(cid:1) D err C 2\\nN\\n\\nN\\nX\\n\\ni D1\\n\\ndcov. O(cid:22)i ; yi /;\\n\\n(12.46)\\n\\nwhere err D P erri =N as before.\\n\\nThe form of dcov. O(cid:22)i ; yi / in (12.45) depends on the context assumed for\\n\\nthe prediction problem.\\n\\n\\x0c12.3 Covariance Penalties\\n\\n221\\n\\n(1)\\n\\nSuppose that O(cid:22) D r.y/ in (12.32)‚Äì(12.33) is linear,\\n\\nO(cid:22) D c C My;\\n\\n(12.47)\\n\\nwhere c is a known N -vector and M a known N (cid:2) N matrix. Then the\\ncovariance matrix between O(cid:22) and y is\\n\\ncov. O(cid:22); y/ D (cid:27) 2M ;\\n\\ngiving cov. O(cid:22)i ; yi / D (cid:27) 2Mi i , Mi i the ith diagonal element of M ,\\n\\ncErri D erri C2(cid:27) 2Mi i ;\\n\\nand, since err D P\\n\\ni .yi (cid:0) O(cid:22)i /2=N ,\\n\\n(12.48)\\n\\n(12.49)\\n\\ncErr(cid:1) D 1\\nN\\n\\nN\\nX\\n\\niD1\\n\\n.yi (cid:0) O(cid:22)i /2 C 2(cid:27) 2\\nN\\n\\ntr.M /:\\n\\n(12.50)\\n\\nFormula (12.50) is Mallows‚Äô Cp estimate of prediction error. For OLS\\nestimation (12.34), M D X .X 0X /(cid:0)1X 0 has tr.M / D p, the number of\\npredictors, so\\n\\ncErr(cid:1) D 1\\nN\\n\\nN\\nX\\n\\ni D1\\n\\n.yi (cid:0) O(cid:22)i /2 C 2\\nN\\n\\n(cid:27) 2p:\\n\\n(12.51)\\n\\nFor the supernova data (12.8)‚Äì(12.9), the OLS predictor O(cid:22) D X .X 0X /(cid:0)1\\nX 0y yielded err D P.yi (cid:0) O(cid:22)i /2=39 D 0:719. The covariance penalty, with\\nN D 39, (cid:27) 2 D 1, and6 p D 10, was 0.513, giving Cp estimate of predic-\\ntion error\\n\\ncErr(cid:1) D 0:719 C 0:513 D 1:23:\\nFor OLS regression, the degrees of freedom p, the rank of matrix X in\\n(12.34), determines the covariance penalty .2=N /(cid:27) 2p in (12.51). Compar-\\ning this with (12.46) leads to a general deÔ¨Ånition of degrees of freedom df\\nfor a regression rule O(cid:22) D r.y/,\\n\\n(12.52)\\n\\ndf D .1=(cid:27) 2/\\n\\nN\\nX\\n\\ni D1\\n\\ndcov. O(cid:22)i ; yi /:\\n\\n(12.53)\\n\\nThis deÔ¨Ånition provides common ground for comparing different types of\\nregression rules. Rules with larger df are more Ô¨Çexible and tend toward\\nbetter apparent Ô¨Åts to the data, but require bigger covariance penalties for\\nfair comparison.\\n\\n6 We are not counting the intercept as an 11th predictor since y and all the xi were\\n\\nstandardized to have mean 0, all our models assuming zero intercept.\\n\\n\\x0c222\\n\\nCross-Validation and Cp Estimates\\n\\n(cid:142)2\\n\\n(2)\\nFor lasso estimation (7.42) and (12.10), it can be shown that for-\\nmula (12.51), with p equaling the number of nonzero regression coefÔ¨Å-\\ncients, holds to a good approximation. (cid:142) The lasso rule used in Figure 12.1\\nfor the supernova data had p D 7; err was 0.720 for this rule, almost the\\nsame as for the OLS rule above, but the Cp penalty is less, 2(cid:1)7=39 D 0:359,\\ngiving\\n\\ncErr(cid:1) D 0:720 C 0:359 D 1:08;\\n\\n(12.54)\\n\\ncompared with 1.23 for OLS. This estimate does not account for the data-\\nbased selection of the choice p D 7, see item (4) below.\\n\\n(3)\\n\\nIf we are willing to add multivariate normality to model (12.32),\\n\\ny (cid:24)\\n\\nN\\n\\np.(cid:22); (cid:27) 2I/;\\n\\n(12.55)\\n\\nwe can drop the assumption of linearity (12.47). In this case it can be\\nshown that, for any differentiable estimator O(cid:22) D r.y/, the covariance in\\nformula (12.39) is given by(cid:142)\\n\\n(cid:142)3\\n\\ncov. O(cid:22)i ; yi / D (cid:27) 2Ef@ O(cid:22)i =@yi g;\\n(cid:27) 2 times the partial derivative of O(cid:22)i with respect to yi . (Another measure of\\nyi ‚Äôs inÔ¨Çuence on its own prediction.) The SURE formula (Stein‚Äôs unbiased\\nrisk estimator) is\\n\\n(12.56)\\n\\ncErri D erri C2(cid:27) 2 @ O(cid:22)i\\n@yi\\n\\n;\\n\\nwith corresponding estimate for overall prediction error\\n\\ncErr(cid:1) D err C 2(cid:27) 2\\nN\\n\\nN\\nX\\n\\ni D1\\n\\n@ O(cid:22)i\\n@yi\\n\\n:\\n\\n(12.57)\\n\\n(12.58)\\n\\nSURE was applied to the rule O(cid:22) D lowess(x,y,1/3) for the kid-\\nney Ô¨Åtness data of Figure 1.2. The open circles in Figure 12.4 plot the\\ncomponent-wise degrees of freedom estimates7\\n\\n@ O(cid:22)i\\n@yi\\n\\n;\\n\\ni D 1; 2; : : : ; N D 157;\\n\\n(12.59)\\n\\n(obtained by numerical differentiation) versus agei . Their sum\\n\\nN\\nX\\n\\niD1\\n\\n@ O(cid:22)i\\n@yi\\n\\nD 6:67\\n\\n(12.60)\\n\\n7 Notice that the factor (cid:27) 2 in (12.56) cancels out in (12.53).\\n\\n\\x0c12.3 Covariance Penalties\\n\\n223\\n\\nestimates the total degrees of freedom, as in (12.53), implying that\\nlowess(x,y,1/3) is about as Ô¨Çexible as a sixth-degree polynomial Ô¨Åt,\\nwith df = 7.\\n\\nFigure 12.4 Analysis of the lowess(x,y,1/3) Ô¨Åt to kidney\\ndata of Figure 1.2. Open circles are SURE coordinate-wise df\\nestimates @ O(cid:22)i =@yi , plotted versus agei , giving total degrees of\\nfreedom 6.67. The solid curve tracks bootstrap coordinate-wise\\nestimates (12.65), with their sum giving total df D 6:81.\\n\\nThe parametric bootstrap8 of Section 10.4 can be used to estimate\\n(4)\\nthe covariances cov. O(cid:22)i ; yi / in the lemma (12.39). The data vector y is as-\\nsumed to be generated from a member f(cid:22).y/ of a given parametric family\\n\\nyielding O(cid:22) D r.y/,\\n\\nD Àöf(cid:22).y/; (cid:22) 2 (cid:127)(cid:9) ;\\n\\nF\\n\\nf(cid:22) ! y ! O(cid:22) D r.y/:\\n\\n(12.61)\\n\\n(12.62)\\n\\nParametric bootstrap replications of y and O(cid:22) are obtained by analogy with\\n\\n8 There is also a nonparametric bootstrap competitor to cross-validation, the ‚Äú.632\\n\\nestimate;‚Äù see the chapter endnote (cid:142)4.\\n\\nlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll20304050607080900.00.10.20.30.4agedf estimateDegrees of FreedomSURE = 6.67Bootstrap = 6.81\\x0c224\\n\\n(12.62),9\\n\\nCross-Validation and Cp Estimates\\n\\nf O(cid:22) ! y (cid:3) ! O(cid:22)(cid:3) D r.y (cid:3)/:\\n\\n(12.63)\\n\\nA large number B of replications then yield bootstrap estimates\\n\\ndcov. O(cid:22)i ; yi / D 1\\n\\nB\\n\\nB\\nX\\n\\n. O(cid:22)(cid:3)b\\ni\\n\\nbD1\\n\\n(cid:0) O(cid:22)(cid:3)(cid:1)\\n\\ni /.y(cid:3)b\\n\\ni\\n\\n(cid:0) y(cid:3)(cid:1)\\n\\ni /;\\n\\n(12.64)\\n\\nthe dot notation indicating averages over the B replications.\\n\\nB D 1000 parametric bootstrap replications . O(cid:22)(cid:3); y (cid:3)/ were obtained\\nfrom the normal model (12.55), taking O(cid:22) in (12.63) to be the estimate from\\nlowess(x,y,1/3) as in Figure 1.2. A standard linear regression, of y\\nas a 12th-degree polynomial function of age, gave O(cid:27) 2 D 3:28. Covariances\\nwere computed as in (12.64), yielding coordinate-wise degrees of freedom\\nestimates (12.53),\\n\\ndfi D\\n\\ndcov. O(cid:22)i ; yi /= O(cid:27) 2:\\n\\n(12.65)\\n\\nThe solid curve in Figure 12.4 plots dfi as a function of agei . These are\\nseen to be similar to but less noisy than the SURE estimates. They totaled\\n6.81, nearly the same as (12.60). The overall covariance penalty term in\\n(12.46) equaled 0.284, increasing cErr(cid:1) by about 9% over err D 3:15.\\n\\nThe advantage of parametric bootstrap estimates (12.64) of covariance\\npenalties is their applicability to any prediction rule O(cid:22) D r.y/ no matter\\nhow exotic. Applied to the lasso estimates for the supernova data, B D\\n1000 replications yielded total df D 6:85 for the rule that always used\\np D 7 predictors, compared with the theoretical approximation df D 7.\\nAnother 1000 replications, now letting O(cid:22)(cid:3) D r.y (cid:3)/ choose the apparently\\nbest p(cid:3) each time, increased the df estimate to 7.48, so the adaptive choice\\nof p cost about 0.6 extra degrees of freedom. These calculations exem-\\nplify modern computer-intensive inference, carrying through error estima-\\ntion for complicated adaptive prediction rules on a totally automatic basis.\\n\\n(5) Covariance penalties can apply to measures of prediction error other\\nthan squared error D.yi ; O(cid:22)i / D .yi (cid:0) O(cid:22)i /2. We will discuss two examples\\nof a general theory. First consider classiÔ¨Åcation, where yi equals 0 or 1 and\\n\\n9 It isn‚Äôt necessary for the O(cid:22) in (12.63) to equal O(cid:22) D r.y/. The calculation (12.64) was\\n\\nrerun taking O(cid:22) in (12.63) from lowess(x,y,1/6) (but with r.y/ still from\\nlowess(x,y,1/3)) with almost identical results. In general, one might take O(cid:22) in\\n(12.63) to be from a more Ô¨Çexible, less biased, estimator than r.y/.\\n\\n\\x0c12.3 Covariance Penalties\\n\\n225\\n\\nsimilarly the predictor O(cid:22)i , with dichotomous error\\n\\nD.yi ; O(cid:22)i / D\\n\\n(\\n1\\n0\\n\\nif yi ¬§ O(cid:22)i\\nif yi D O(cid:22)i ;\\n\\n(12.66)\\n\\nas in (12.5).10 In this situation, the apparent error is the observed proportion\\nof prediction mistakes in the training set (12.1),\\n\\nerr D #fyi ¬§ O(cid:22)i g=N:\\n\\n(12.67)\\n\\nNow the true prediction error for case i is\\n\\nErri D Pr0fy0i ¬§ O(cid:22)i g;\\nthe conditional probability given O(cid:22)i that an independent replicate y0i of yi\\nwill be incorrectly predicted. The lemma holds as stated in (12.39), leading\\nto the prediction error estimate\\n\\n(12.68)\\n\\ncErr(cid:1) D #fyi ¬§ O(cid:22)i g\\n\\nN\\n\\nC 2\\nN\\n\\nN\\nX\\n\\niD1\\n\\ncov. O(cid:22)i ; yi /:\\n\\n(12.69)\\n\\nSome algebra yields\\n\\ncov. O(cid:22)i ; yi / D (cid:22)i .1 (cid:0) (cid:22)i / .Prf O(cid:22)i D 1jyi D 1g (cid:0) Prf O(cid:22)1 D 1jyi D 0g/ ;\\n\\n(12.70)\\nwith (cid:22)i D Prfyi D 1g, showing again the covariance penalty measuring\\nthe self-inÔ¨Çuence of yi on its own prediction.\\n\\nAs a second example, suppose that the observations yi are obtained\\nfrom different members of a one-parameter exponential family f(cid:22).y/ D\\nexpf(cid:21)y (cid:0) (cid:13).(cid:21)/gf0.y/ (8.32),\\n\\nyi (cid:24) f(cid:22)i .yi /\\n\\nfor i D 1; 2; : : : ; N;\\n\\n(12.71)\\n\\nand that error is measured by the deviance (8.31),\\n\\nD.y; O(cid:22)/ D 2\\n\\nZ\\n\\nfy.Y / log\\n\\n(cid:19)\\n\\n(cid:18) fy.Y /\\nf O(cid:22).Y /\\n\\nd Y:\\n\\n(12.72)\\n\\nAccording to (8.33), the apparent error P D.yi ; O(cid:22)i / is then\\n\\nY\\n\\nerr D 2\\nN\\n\\nN\\nX\\n\\niD1\\n\\nlog\\n\\n(cid:19)\\n\\n(cid:18) fyi .yi /\\nf O(cid:22)i .yi /\\n\\nD 2\\nN\\n\\nÀölog (cid:0)fy .y/(cid:1) (cid:0) log (cid:0)f O(cid:22).y/(cid:1)(cid:9) :\\n\\n(12.73)\\n\\n10 More generally, O(cid:25)i is some predictor of Prfyi D 1g, and O(cid:22)i is the indicator function\\n\\nI. O(cid:25)i (cid:21) 0:5/.\\n\\n\\x0c226\\n\\nCross-Validation and Cp Estimates\\n\\nIn this case the general theory gives overall covariance penalty\\n\\npenalty D 2\\nN\\n\\nN\\nX\\n\\n(cid:16) O(cid:21)i ; yi\\n\\n(cid:17)\\n\\n;\\n\\ncov\\n\\n(12.74)\\n\\niD1\\nwhere O(cid:21)i is the natural parameter in family (8.32) corresponding to O(cid:22)(cid:21)\\n(e.g., O(cid:21)i D log O(cid:22)i for Poisson observations). Moreover, if O(cid:22) is obtained\\nas the MLE of (cid:22) in a generalized linear model with p degrees of freedom\\n(8.22),\\n\\npenalty\\n\\n:D 2p\\nN\\n\\n(12.75)\\n\\nto a good approximation. The corresponding version of cErr(cid:1) (12.46) can\\nthen be written as\\n\\n:D (cid:0) 2\\nN\\nthe constant .2=N / log.fy .y// not depending on O(cid:22).\\n\\nÀölog (cid:0)f O(cid:22).y/(cid:1) (cid:0) p(cid:9) C constant;\\n\\ncErr(cid:1)\\n\\n(12.76)\\n\\nThe term in brackets is the Akaike information criterion (AIC): if the\\nstatistician is comparing possible prediction rules r .j /.y/ for a given data\\nset y, the AIC says to select the rule maximizing the penalized maximum\\nlikelihood\\n\\nlog (cid:0)f O(cid:22).j /.y/(cid:1) (cid:0) p.j /;\\nwhere O(cid:22).j / is rule j ‚Äôs MLE and p.j / its degrees of freedom. Comparison\\nwith (12.76) shows that for GLMs, the AIC amounts to selecting the rule\\n.j /\\nwith the smallest value of cErr\\n(cid:1)\\n\\n(12.77)\\n\\n.\\n\\n(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)\\n\\nCross-validation does not require a probability model, but if such a model\\nis available then the error estimate cErrcv can be improved by bootstrap\\nsmoothing.11 With the predictor vectors xi considered Ô¨Åxed as observed, a\\nparametric model generates the data set d D f.xi ; yi /; i D 1; : : : ; N g as\\nin (12.62), from which we calculate the prediction rule rd .(cid:1)/ and the error\\nestimate cErrcv (12.21),\\n\\nf(cid:22) ! d ! rd .(cid:1)/ ! cErrcv:\\n\\n(12.78)\\n\\nSubstituting the estimated density f O(cid:22) for f(cid:22), as in (12.63), provides\\n\\n11 Perhaps better known as ‚Äúbagging;‚Äù see Chapter 17.\\n\\n\\x0c12.4 Training, Validation, and Ephemeral Predictors\\n\\n227\\n\\nparametric bootstrap replicates of cErrcv,\\n\\nf O(cid:22) ! d (cid:3) ! rd (cid:3).(cid:1)/ ! cErr\\nSome large number B of replications can then be averaged to give the\\nsmoothed estimate\\n\\n(12.79)\\n\\n(cid:3)\\ncv:\\n\\nErr D 1\\nB\\n\\nB\\nX\\n\\nbD1\\n\\n(cid:3)b\\ncv :\\n\\ncErr\\n\\n(12.80)\\n\\nErr averages out the considerable noise in cErrcv, often signiÔ¨Åcantly reduc-\\ning its variability.12\\n\\nA surprising result, referenced in the endnotes, shows that Err approxi-\\nmates the covariance penalty estimate Err(cid:1). Speaking broadly, Err(cid:1) is what‚Äôs\\nleft after excess randomness is squeezed out of cErrcv (an example of ‚ÄúRao‚Äì\\nBlackwellization,‚Äù to use classical terminology). Improvements can be quite\\nsubstantial. (cid:142) Covariance penalty estimates, when believable parametric (cid:142)4\\nmodels are available, should be preferred to cross-validation.\\n\\n12.4 Training, Validation, and Ephemeral Predictors\\n\\nGood Practice suggests splitting the full set of observed predictor‚Äìresponse\\npairs .x; y/ into a training set d of size N (12.1), and a validation set\\ndval, of size Nval (12.19). The validation set is put into a vault while the\\ntraining set is used to develop an effective prediction rule rd .x/. Finally,\\ndval is removed from the vault and used to calculate cErrval (12.20), an honest\\nestimate of the predictive error rate of rd .\\n\\nThis is a good idea, and seems foolproof, at least if one has enough data\\nto afford setting aside a substantial portion for a validation set during the\\ntraining process. Nevertheless, there remains some peril of underestimating\\nthe true error rate, arising from ephemeral predictors, those whose predic-\\ntive powers fade away over time. A contrived, but not completely fanciful,\\nexample illustrates the danger.\\n\\nThe example takes the form of an imaginary microarray study involving\\n\\n360 subjects, 180 patients and 180 healthy controls, coded\\n\\nyi D\\n\\n(\\n\\n1 patient\\n0 control;\\n\\ni D 1; 2; : : : ; 360:\\n\\n(12.81)\\n\\n12 A related tactic pertaining to grouped cross-validation is to repeat calculation (12.21) for\\nseveral different randomly selected splits into J groups, and then average the resulting\\ncErrcv estimates.\\n\\n\\x0c228\\n\\nCross-Validation and Cp Estimates\\n\\nEach subject is assessed on a microarray measuring the genetic activity of\\np D 100 genes, these being the predictors\\n\\nxi D .xi1; xi2; xi3; : : : ; xi100/0:\\n\\n(12.82)\\n\\nOne subject per day is assessed, alternating patients and controls.\\n\\nFigure 12.5 Orange bars indicate transient episodes, (12.84) and\\nthe reverse, for imaginary medical study (12.81)‚Äì(12.82).\\n\\nThe measurements xij are independent of each other and of the yi ‚Äôs,\\n\\nind(cid:24)\\n\\nxij\\n\\nN\\n\\n.(cid:22)ij ; 1/\\n\\nfor i D 1; 2; : : : ; 360 and j D 1; 2; : : : ; 100:\\n\\n(12.83)\\nMost of the (cid:22)ij equal zero, but each gene‚Äôs measurements can experience\\n‚Äútransient episodes‚Äù of two possible types: in type 1,\\n\\n(cid:22)ij D\\n\\n(\\nif yi D 1\\n2\\n(cid:0)2 if yi D 0;\\n\\n(12.84)\\n\\nwhile type 2 reverses signs. The episodes are about 30 days long, randomly\\nand independently located between days 1 and 360, with an average of two\\nepisodes per gene. The orange bars in Figure 12.5 indicate the episodes.\\n\\nFor the purpose of future diagnoses we wish to construct a prediction\\nrule Oy D rd .x/. To this end we randomly divide the 360 subjects into a\\n\\n050100150200250300350020406080100 ***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************subjectsgenes\\x0c12.4 Ephemeral Predictors\\n\\n229\\n\\ntraining set d of size N D 300 and a validation set dval of size Nval D\\n60. The popular ‚Äúmachine learning‚Äù prediction program Random Forests,\\nChapter 17, is applied. Random Forests forms rd .x/ by averaging the pre-\\ndictions of a large number of randomly subsampled regression trees (Sec-\\ntion 8.4).\\n\\nFigure 12.6 Test error (blue) and cross-validated training error\\n(black), for Random Forest prediction rules using the imaginary\\nmedical study (12.81)‚Äì(12.82). Top panel: training set randomly\\nselected 300 days, test set the remaining 60 days. Bottom panel:\\ntraining set the Ô¨Årst 300 days, test set the last 60 days.\\n\\nThe top panel of Figure 12.6 shows the results, with blue points indi-\\ncating test-set error and black the (cross-validated) training-set error. Both\\nconverge to 15% as the number of Random Forest trees grows large. This\\nseems to conÔ¨Årm an 85% success rate for prediction rule rd .x/.\\n\\nOne change has been made for the bottom panel: now the training set\\nis the data for days 1 through 300, and the test set days 301 through 360.\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.10.20.30.40.5Training set random 300 days, test set the remainder# treesPrediction errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll15%llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01002003004000.00.10.20.30.40.5Training days 1‚àí300, test days 301‚àí360# treesPrediction errorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll27%15%\\x0c230\\n\\nCross-Validation and Cp Estimates\\n\\nThe cross-validated training-set prediction error still converges to 15%, but\\ncErrval is now 27%, nearly double.\\n\\nThe reason isn‚Äôt hard to see. Any predictive power must come from the\\ntransient episodes, which lose efÔ¨Åcacy outside of their limited span. In the\\nÔ¨Årst example the test days are located among the training days, and inherit\\ntheir predictive accuracy from them. This mostly fails in the second setup,\\nwhere the test days are farther removed from the training days. (Only the\\norange bars crossing the 300-day line can help lower cErrval in this situa-\\ntion.)\\n\\nAn obvious, but often ignored, dictum is that cErrval is more believable if\\nthe test set is further separated from the training set. ‚ÄúFurther‚Äù has a clear\\nmeaning in studies with a time or location factor, but not necessarily in\\ngeneral. For J -fold cross-validation, separation is improved by removing\\ncontiguous blocks of N=J cases for each group, rather than by random\\nselection, but the amount of separation is still limited, making cErrcv less\\nbelievable than a suitably constructed cErrval.\\n\\nThe distinction between transient, ephemeral predictors and dependable\\nones is sometimes phrased as the difference between correlation and cau-\\nsation. For prediction purposes, if not for scientiÔ¨Åc exegesis, we may be\\nhappy to settle for correlations as long as they are persistent enough for\\nour purposes. We return to this question in Chapter 15 in the discussion of\\nlarge-scale hypothesis testing.\\n\\nA notorious cautionary tale of fading correlations concerns Google Flu\\nTrends, (cid:142) a machine-learning algorithm for predicting inÔ¨Çuenza outbreaks.\\nIntroduced in 2008, the algorithm, based on counts of internet search terms,\\noutperformed traditional medical surveys in terms of speed and predictive\\naccuracy. Four years later, however, the algorithm failed, badly overesti-\\nmating what turned out to be a nonexistent Ô¨Çu epidemic. Perhaps one les-\\nson here is that the Google algorithmists needed a validation set years‚Äînot\\nweeks or months‚Äîremoved from the training data.\\n\\nError rate estimation is mainly frequentist in nature, but the very large\\ndata sets available from the internet have encouraged a disregard for infer-\\nential justiÔ¨Åcation of any type. This can be dangerous. The heterogeneous\\nnature of ‚Äúfound‚Äù data makes statistical principles of analysis more, not\\nless, relevant.\\n\\n12.5 Notes and Details\\n\\nThe evolution of prediction algorithms and their error estimates nicely il-\\nlustrates the inÔ¨Çuence of electronic computation on statistical theory and\\n\\n(cid:142)5\\n\\n\\x0c12.5 Notes and Details\\n\\n231\\n\\npractice. The classical recipe for cross-validation recommended splitting\\nthe full data set in two, doing variable selection, model choice, and data Ô¨Åt-\\nting on the Ô¨Årst half, and then testing the resulting procedure on the second\\nhalf. Interest revived in 1974 with the independent publication of papers\\nby Geisser and by Stone, featuring leave-one-out cross-validation of pre-\\ndictive error rates.\\n\\nA question of bias versus variance arises here. A rule based on only N=2\\ncases is less accurate than the actual rule based on all N . Leave-one-out\\ncross-validation minimizes this type of bias, at the expense of increased\\nvariability of error rate estimates for ‚Äújumpy‚Äù rules of a discontinuous\\nnature. Current best practice is described in Section 7.10 of Hastie et al.\\n(2009), where J -fold cross-validation with J perhaps 10 is recommended,\\npossibly averaged over several random data splits.\\n\\nNineteen seventy-three was another good year for error estimation, fea-\\nturing Mallows‚Äô Cp estimator and Akaike‚Äôs information criterion. Efron\\n(1986) extended Cp methods to a general class of situations (see below),\\nestablished the connection with AIC, and suggested bootstrapping methods\\nfor covariance penalties. The connection between cross-validation and co-\\nvariance penalties was examined in Efron (2004), where the Rao‚ÄìBlackwell-\\ntype relationship mentioned at the end of Section 12.3 was demonstrated.\\nThe SURE criterion appeared in Charles Stein‚Äôs 1981 paper. Ye (1998)\\nsuggested the general degrees of freedom deÔ¨Ånition (12.53).\\n\\n(cid:142)1 [p. 210] Standard candles and dark energy. Adam Riess, Saul Perlmutter,\\nand Brian Schmidt won the 2011 Nobel Prize in physics for discovering\\nincreasing rates of expansion of the Universe, attributed to an Einsteinian\\nconcept of dark energy. They measured cosmic distances using Type Ia\\nsupernovas as ‚Äústandard candles.‚Äù The type of analysis suggested by Fig-\\nure 12.1 is intended to improve the cosmological distance scale.\\n\\n(cid:142)2 [p. 222] Data-based choice of a lasso estimate. The regularization param-\\neter (cid:21) for a lasso estimator (7.42) controls the number of nonzero coefÔ¨Å-\\ncients of QÀá.(cid:21)/, with larger (cid:21) yielding fewer nonzeros. Efron et al. (2004)\\nand Zou et al. (2007) showed that a good approximation for the degrees of\\nfreedom df (12.53) of a lasso estimate is the number of its nonzero coefÔ¨Å-\\ncients. Substituting this for p in (12.51) provides a quick version of cErr(cid:1).\\nThis was minimized at df D 7 for the supernova example in Figure 12.1\\n(12.54).\\n\\n(cid:142)3 [p. 222] Stein‚Äôs unbiased risk estimate. The covariance formula (12.56) is\\nobtained directly from integration by parts. The computation is clear from\\n\\n\\x0c232\\n\\nCross-Validation and Cp Estimates\\n\\nthe one-dimensional version of (12.55), N D 1:\\n(cid:20)\\n\\nZ 1\\n\\n1p\\n\\n(cid:0) 1\\n2\\n\\ne\\n\\n.y(cid:0)(cid:22)/2\\n(cid:27)2\\n\\n.y (cid:0) (cid:22)/\\n\\n(cid:21)\\n\\nO(cid:22).y/ dy\\n\\ncov. O(cid:22); y/ D\\n\\n(cid:0)1\\n\\nZ 1\\n\\nD (cid:27) 2\\n\\n2(cid:25)(cid:27) 2\\n(cid:20)\\n1p\\n\\n(cid:0)1\\n(cid:26) @ O(cid:22).y/\\n@y\\n\\n2(cid:25)(cid:27) 2\\n(cid:27)\\n\\n:\\n\\nD (cid:27) 2E\\n\\n(cid:0) 1\\n2\\n\\ne\\n\\n.y(cid:0)(cid:22)/2\\n(cid:27)2\\n\\n(cid:21) @ O(cid:22).y/\\n@y\\n\\ndy\\n\\n(12.85)\\n\\nBroad regularity conditions for SURE are given in Stein (1981).\\n\\n(cid:142)4 [p. 227] The .632 rule. Bootstrap competitors to cross-validation are dis-\\ncussed in Efron (1983) and Efron and Tibshirani (1997). The most success-\\nful of these, the ‚Äú.632 rule‚Äù is generally less variable than leave-one-out\\ncross-validation. We suppose that nonparametric bootstrap data sets d (cid:3)b,\\nb D 1; 2; : : : ; B, have been formed, each by sampling with replacement\\nN times from the original N members of d (12.1). Data set d (cid:3)b produces\\nrule\\n\\nr (cid:3)b.x/ D rd (cid:3)b .x/;\\n\\n(12.86)\\n\\ngiving predictions\\n\\nLet I b\\ni\\n\\ny(cid:3)b\\ni\\n\\nD r (cid:3)b.xi /:\\n\\n(12.87)\\nD 1 if pair .xi ; yi / is not in d (cid:3)b, and 0 if it is. (About e(cid:0)1 D\\ni will equal 1, the remaining 0.632 equaling 0.) The\\n\\n0:368 of the N (cid:1) B I b\\n‚Äúout of bootstrap‚Äù estimate of prediction error is\\n, N\\nX\\n\\nB\\nX\\n\\nN\\nX\\n\\n(cid:17)\\n\\nI b\\ni D\\n\\n(cid:16)\\nyi ; Oy(cid:3)b\\n\\ni\\n\\ncErrout D\\n\\nB\\nX\\n\\nI b\\ni ;\\n\\n(12.88)\\n\\niD1\\n\\nj D1\\n\\niD1\\n\\nj D1\\n\\nthe average discrepancy in the omitted cases.\\n\\ncErrout is similar to a grouped cross-validation estimate that omits about\\n37% of the cases each time. The .632 rule compensates for the upward bias\\nin cErrout by incorporating the downwardly biased apparent error (12.18),\\ncErr:632 D 0:632 cErrout C 0:368 err :\\ncErrout has resurfaced in the popular Random Forests prediction algorithm,\\nChapter 17, where a closely related procedure gives the ‚Äúout of bag‚Äù esti-\\nmate of Err.\\n\\n(12.89)\\n\\n(cid:142)5 [p. 230] Google Flu Trends. Harford‚Äôs 2014 article, ‚ÄúBig data: A big mis-\\ntake?,‚Äù concerns the enormous ‚Äúfound‚Äù data sets available in the internet\\nage, and the dangers of forgetting the principles of statistical inference in\\ntheir analysis. Google Flu Trends is his primary cautionary example.\\n\\n\\x0c13\\n\\nObjective Bayes Inference and Markov Chain\\nMonte Carlo\\n\\nFrom its very beginnings, Bayesian inference exerted a powerful inÔ¨Çuence\\non statistical thinking. The notion of a single coherent methodology em-\\nploying only the rules of probability to go from assumption to conclusion\\nwas and is immensely attractive. For 200 years, however, two impediments\\nstood between Bayesian theory‚Äôs philosophical attraction and its practical\\napplication.\\n\\n1 In the absence of relevant past experience, the choice of a prior distribu-\\ntion introduces an unwanted subjective element into scientiÔ¨Åc inference.\\n2 Bayes‚Äô rule (3.5) looks simple enough, but carrying out the numerical\\ncalculation of a posterior distribution often involves intricate higher-\\ndimensional integrals.\\n\\nThe two impediments Ô¨Åt neatly into the dichotomy of Chapter 1, the Ô¨Årst\\nbeing inferential and the second algorithmic.1\\n\\nA renewed cycle of Bayesian enthusiasm took hold in the 1960s, at Ô¨Årst\\nconcerned mainly with coherent inference. Building on work by Bruno de\\nFinetti and L. J. Savage, a principled theory of subjective probability was\\nconstructed: the Bayesian statistician, by the careful elicitation of prior\\nknowledge, utility, and belief, arrives at the correct subjective prior dis-\\ntribution for the problem at hand. Subjective Bayesianism is particularly\\nappropriate for individual decision making, say for the business executive\\ntrying to choose the best investment in the face of uncertain information.\\n\\nIt is less appropriate for scientiÔ¨Åc inference, where the sometimes skep-\\ntical world of science puts a premium on objectivity. An answer came from\\nthe school of objective Bayes inference. Following the approach of Laplace\\nand Jeffreys, as discussed in Section 3.2, their goal was to fashion objec-\\ntive, or ‚Äúuninformative,‚Äù prior distributions that in some sense were unbi-\\nased in their effects upon the data analysis.\\n\\n1 The exponential family material in this chapter provides technical support, but is not\\n\\nrequired in detail for a general understanding of the main ideas.\\n\\n233\\n\\n\\x0c234\\n\\nObjective Bayes Inference and MCMC\\n\\nIn what came as a surprise to the Bayes community, the objective school\\nhas been the most successful in bringing Bayesian ideas to bear on scien-\\ntiÔ¨Åc data analysis. Of the 24 articles in the December 2014 issue of the\\nAnnals of Applied Statistics, 8 employed Bayesian analysis, predominantly\\nbased on objective priors.\\n\\nThis is where electronic computation enters the story. Commencing in\\nthe 1980s, dramatic steps forward were made in the numerical calculation\\nof high-dimensional Bayes posterior distributions. Markov chain Monte\\nCarlo (MCMC) is the generic name for modern posterior computation al-\\ngorithms. These proved particularly well suited for certain forms of objec-\\ntive Bayes prior distributions.\\n\\nTaken together, objective priors and MCMC computations provide an\\nattractive package for the statistician faced with a complicated data analy-\\nsis situation. Statistical inference becomes almost automatic, at least com-\\npared with the rigors of frequentist analysis. This chapter discusses both\\nparts of the package, the choice of prior and the subsequent computational\\nmethods. Criticisms arise, both from the frequentist viewpoint and that of\\ninformative Bayesian analysis, which are brought up here and also in Chap-\\nter 21.\\n\\n13.1 Objective Prior Distributions\\n\\nA Ô¨Çat, or uniform, distribution over the space of possible parameter values\\nseems like the obvious choice for an uninformative prior distribution, and\\nhas been so ever since Laplace‚Äôs advocacy in the late eighteenth century.\\nFor a Ô¨Ånite parameter space (cid:127), say\\n\\n(cid:127) D f(cid:22).1/; (cid:22).2/; : : : ; (cid:22).K/g;\\n\\n‚ÄúÔ¨Çat‚Äù has the obvious meaning\\n\\ngÔ¨Çat.(cid:22)/ D 1\\nK\\n\\nfor all (cid:22) 2 (cid:127):\\n\\nIf K is inÔ¨Ånite, or if (cid:127) is continuous, we can still take\\n\\ngÔ¨Çat.(cid:22)/ D constant:\\n\\n(13.1)\\n\\n(13.2)\\n\\n(13.3)\\n\\nBayes‚Äô rule (3.5) gives the same posterior distribution for any choice of the\\nconstant,\\n\\ngÔ¨Çat.(cid:22)jx/ D gÔ¨Çat.(cid:22)/f(cid:22).x/=f .x/; with\\n\\nf .x/ D\\n\\nZ\\n\\n(cid:127)\\n\\nf(cid:22).x/gÔ¨Çat.(cid:22)/ d(cid:22):\\n\\n(13.4)\\n\\n\\x0c13.1 Objective Prior Distributions\\n\\n235\\n\\nNotice that gÔ¨Çat.(cid:22)/ cancels out of gÔ¨Çat.(cid:22)jx/. The fact that gÔ¨Çat.(cid:22)/ is ‚Äúim-\\nproper,‚Äù that is, it integrates to inÔ¨Ånity, doesn‚Äôt affect the formal use of\\nBayes‚Äô rule in (13.4) as long as f .x/ is Ô¨Ånite.\\n\\nNotice also that gÔ¨Çat.(cid:22)jx/ amounts to taking the posterior density of\\n(cid:22) to be proportional to the likelihood function Lx.(cid:22)/ D f(cid:22).x/ (with x\\nÔ¨Åxed and (cid:22) varying over (cid:127)). This brings us close to Fisherian inference,\\nwith its emphasis on the direct interpretation of likelihoods, but Fisher was\\nadamant in his insistance that likelihood was not probability.\\n\\nFigure 13.1 The solid curve is Ô¨Çat-prior posterior density (13.4)\\nhaving observed x D 10 from Poisson model x (cid:24) Poi.(cid:22)/; it is\\nshifted about 0.5 units right from the conÔ¨Ådence density (dashed)\\nof Figure 11.6. Jeffreys‚Äô prior gives a posterior density (dotted)\\nnearly the same as the conÔ¨Ådence density.\\n\\nThe solid curve in Figure 13.1 shows gÔ¨Çat.(cid:22)jx/ for the Poisson situation\\n\\nof Table 11.2,\\n\\nx (cid:24) Poi.(cid:22)/;\\n\\n(13.5)\\n\\nwith x D 10 observed; gÔ¨Çat.(cid:22)jx/ is shifted almost exactly 0.5 units right\\nof the conÔ¨Ådence density from Figure 11.6. (‚Äú(cid:18)‚Äù is (cid:22) itself in this case.)2\\nFisher‚Äôs withering criticism of Ô¨Çat-prior Bayes inference focused on its\\n\\n2 The reader may wish to review Chapter 11, particularly Section 11.6, for these\\n\\nconstructions.\\n\\n0510152025300.000.020.040.060.080.100.12qDensitiesconfidencedensityposterior density,flat prior10Jeffreys\\x0c236\\n\\nObjective Bayes Inference and MCMC\\n\\nlack of transformation invariance. If we were interested in (cid:18) D log.(cid:22)/\\nrather than (cid:22), gÔ¨Çat.(cid:18)jx/ would not be the transformation to the log scale of\\ngÔ¨Çat.(cid:22)jx/. Jeffreys‚Äô prior, (3.17) or (11.72), which does transform correctly,\\nis\\n\\ngJeff.(cid:22)/ D 1ƒ±p\\nfor x (cid:24) Poi.(cid:22)/; gJeff.(cid:22)jx D 10/ is then a close match to the conÔ¨Ådence\\ndensity in Figure 13.1.\\n\\n(13.6)\\n\\n(cid:22)\\n\\nCoverage Matching Priors\\n\\nA variety of improvements and variations on Jeffreys‚Äô prior have been\\nsuggested for use as general-purpose uninformative prior distributions, as\\nbrieÔ¨Çy discussed in the chapter endnotes. (cid:142) All share the drawback seen in\\nFigure 11.7: the posterior distribution g.(cid:22)jx/ can have unintended effects\\non the resulting inferences for a real-valued parameter of interest (cid:18) D t.(cid:22)/.\\nThis is unavoidable; it is mathematically impossible for any single prior to\\nbe uninformative for every choice of (cid:18) D t.(cid:22)/.\\n\\nThe label ‚Äúuninformative‚Äù for a prior sometimes means ‚Äúgives Bayes\\nposterior intervals that closely match conÔ¨Ådence intervals.‚Äù Perhaps sur-\\nprisingly, this deÔ¨Ånition has considerable resonance in the Bayes commu-\\nnity. Such priors can be constructed for any given scalar parameter of in-\\nterest (cid:18) D t .(cid:22)/, for instance the maximum eigenvalue parameter of Fig-\\nure 11.7. In brief, the construction proceeds as follows.(cid:142)\\n\\n(cid:142)1\\n\\n(cid:142)2\\n\\n(cid:15) The p-dimensional parameter vector (cid:22) is transformed to a form that\\n\\nmakes (cid:18) the Ô¨Årst coordinate, say\\n\\n(cid:22) ! .(cid:18); (cid:23)/;\\n\\n(13.7)\\n\\nwhere (cid:23) is a .p (cid:0) 1/-dimensioned nuisance parameter.\\n\\n(cid:15) The transformation is chosen so that the Fisher information matrix (11.72)\\n\\nfor .(cid:18); (cid:23)/ has the ‚Äúdiagonal‚Äù form\\n\\n(cid:18)\\n\\n(cid:18)(cid:18)\\nI\\n00\\n\\n(cid:19)\\n\\n:\\n\\n0\\nI (cid:23)(cid:23)\\n\\n(13.8)\\n\\n(This is always possible.)\\n\\n(cid:15) Finally, the prior for .(cid:18); (cid:23)/ is taken proportional to\\n\\n1=2\\n(cid:18)(cid:18) h.(cid:23)/;\\nI\\nwhere h.(cid:23)/ is an arbitrary .p (cid:0) 1/-dimensional density. In other words,\\n\\ng.(cid:18); (cid:23)/ D\\n\\n(13.9)\\n\\n\\x0c13.2 Conjugate Prior Distributions\\n\\n237\\n\\ng.(cid:18); (cid:23)/ combines the one-dimensional Jeffreys‚Äô prior (3.17) for (cid:18) with\\nan arbitrary independent prior for the orthogonal nuisance parameter\\nvector (cid:23).\\n\\nThe main thing to notice about (13.9) is that g.(cid:18); (cid:23)/ represents different\\npriors on the original parameter vector (cid:22) for different functions (cid:18) D t.(cid:22)/.\\nNo single prior g.(cid:22)/ can be uninformative for all choices of the parameter\\nof interest (cid:18) .\\n\\nCalculating g.(cid:18); (cid:23)/ can be difÔ¨Åcult. One alternative is to go directly to\\nthe BCa conÔ¨Ådence density (11.68)‚Äì(11.69), which can be interpreted as\\nthe posterior distribution from an uninformative prior (because its integrals\\nagree closely with conÔ¨Ådence interval endpoints).\\n\\nCoverage matching priors are not much used in practice, and in fact none\\nof the eight Annals of Applied Statistics objective Bayes papers mentioned\\nearlier were of type (13.9). A form of ‚Äúalmost uninformative‚Äù priors, the\\nconjugates, is more popular, mainly because of the simpler computation of\\ntheir posterior distributions.\\n\\n13.2 Conjugate Prior Distributions\\n\\nA mathematically convenient class of prior distributions, the conjugate pri-\\nors, applies to samples from an exponential family,3 Section 5.5,\\n\\nf(cid:22).x/ D eÀõx(cid:0) .Àõ/f0.x/:\\n\\nHere we have indexed the family with the expectation parameter\\n\\n(cid:22) D Ef fxg;\\n\\n(13.10)\\n\\n(13.11)\\n\\nrather than the canonical parameter Àõ. On the right-hand side of (13.10), Àõ\\ncan be thought of as a one-to-one function of (cid:22) (the so-called ‚Äúlink func-\\ntion‚Äù), e.g., Àõ D log.(cid:22)/ for the Poisson family. The observed data is a\\nrandom sample x D .x1; x2; : : : ; xn/ from f(cid:22),\\n\\nx1; x2; : : : ; xn\\n\\niid(cid:24) f(cid:22);\\n\\nhaving density function\\n\\nf(cid:22).x/ D en≈íÀõ Nx(cid:0) .Àõ/(cid:141)f0.x/;\\n\\nthe average Nx D P xi =n being sufÔ¨Åcient.\\n\\n(13.12)\\n\\n(13.13)\\n\\n3 We will concentrate on one-parameter families, though the theory extends to the\\n\\nmultiparameter case. Figure 13.2 relates to a two-parameter situation.\\n\\n\\x0c238\\n\\nObjective Bayes Inference and MCMC\\n\\nThe family of conjugate priors for (cid:22), gn0;x0.(cid:22)/, allows the statistician\\n\\nto choose two parameters, n0 and x0,\\n\\ngn0;x0.(cid:22)/ D cen0≈íx0Àõ(cid:0) .Àõ/(cid:141)ƒ±V .(cid:22)/;\\n\\nV .(cid:22)/ the variance of an x from f(cid:22),\\n\\nV .(cid:22)/ D varf fxgI\\n\\n(13.14)\\n\\n(13.15)\\n\\nc is the constant that makes gn0;x0.(cid:22)/ integrate to 1 with respect to Lebesgue\\nmeasure on the interval of possible (cid:22) values. The interpretation is that x0\\nrepresents the average of n0 hypothetical prior observations from f(cid:22).\\nThe utility of conjugate priors is seen in the following theorem.\\n\\n(cid:142)3\\n\\nTheorem 13.1 (cid:142) DeÔ¨Åne\\n\\nnC D n0 C n and\\n\\nNxC D n0\\nnC\\n\\nx0 C n\\nnC\\n\\nNx:\\n\\nThen the posterior density of (cid:22) given x D .x1; x2; : : : ; xn/ is\\n\\ng.(cid:22)jx/ D gnC; NxC.(cid:22)/I\\n\\nmoreover, the posterior expectation of (cid:22) given x is\\n\\nEf(cid:22)jxg D n0\\nnC\\n\\nx0 C n\\nnC\\n\\nNx:\\n\\n(13.16)\\n\\n(13.17)\\n\\n(13.18)\\n\\nThe intuitive interpretation is quite satisfying: we begin with a hypo-\\nthetical prior sample of size n0, sufÔ¨Åcient statistic x0; observe x, a sam-\\nple of size n; and update our prior distribution gn0;x0.(cid:22)/ to a distribution\\ngnC; NxC.(cid:22)/ of the same form. Moreover, Ef(cid:22)jxg equals the average of a\\nhypothetical sample with n0 copies of x0,\\n\\n.x0; x0; : : : ; x0; x1; x2; : : : ; xn/:\\n\\n(13.19)\\n\\nAs an example, suppose xi\\n\\niid(cid:24) Poi.(cid:22)/, that is we have n i.i.d. observa-\\ntions from a Poisson distribution, Table 5.1. Formula (13.14) gives conju-\\ngate prior (cid:142)\\n\\n(cid:142)4\\n\\ngn0;x0.(cid:22)/ D c(cid:22)n0x0(cid:0)1e(cid:0)n0(cid:22);\\n\\n(13.20)\\n\\nc not depending on (cid:22). So in the notation of Table 5.1, gn0;x0.(cid:22)/ is a gamma\\ndistribution, Gam.n0x0; 1=n0/. The posterior distribution is\\ng.(cid:22)jx/ D gnC; NxC.(cid:22)/ (cid:24) Gam.nC NxC; 1=nC/\\n\\n(cid:24) 1\\nnC\\n\\nGnC NxC;\\n\\n(13.21)\\n\\n\\x0c13.2 Conjugate Prior Distributions\\n\\n239\\n\\n(cid:142)5\\n\\nwhere G(cid:23) indicates a standard gamma distribution,(cid:142)\\n\\nG(cid:23) D Gam.(cid:23); 1/:\\n\\n(13.22)\\n\\nTable 13.1 Conjugate priors (13.14)‚Äì(13.16) for four familiar\\none-parameter exponential families, using notation in Table 5.1; the last\\ncolumn shows the posterior distribution of (cid:22) given n observations xi ,\\nstarting from prior gn0;x0.(cid:22)/. In line 4, G(cid:23) is the standard gamma\\ndistribution Gam.(cid:23); 1/, with (cid:22) the same as gamma parameter (cid:27) in\\nTable 5.1. The chapter endnotes give the density of the inverse gamma\\ndistribution 1=G(cid:23), and corresponding results for chi-squared variates.\\n\\nName\\n\\nxi distribution\\n\\ngn0;x0 .(cid:22)/\\n\\ng.(cid:22)jx/\\n\\n1. Normal\\n\\nN .(cid:22); (cid:27) 2\\n1 /\\n((cid:27) 2\\n1 known)\\n\\nN .x0; (cid:27) 2\\n\\n1 =n0/\\n\\nN . NxC; (cid:27) 2\\n\\n1 =nC/\\n\\n2. Poisson\\n\\nPoi.(cid:22)/\\n\\nGam.n0x0; 1=n0/\\n\\nGam.nC NxC; 1=nC/\\n\\n3. Binomial\\n\\nBi.1; (cid:22)/\\n\\nBe.n0x0; n0.1 (cid:0) x0// Be.nC NxC; nC.1 (cid:0) NxC//\\n\\n4. Gamma\\n\\n(cid:22)G(cid:23)=(cid:23)\\n((cid:23) known)\\n\\nn0x0(cid:23)=Gn0(cid:23)C1\\n\\nnC NxC(cid:23)=GnC(cid:23)C1\\n\\nTable 13.1 describes the conjugate prior and posterior distributions for\\nfour familiar one-parameter families. The binomial case, where (cid:22) is the\\n‚Äúsuccess probability‚Äù (cid:25) in Table 5.1, is particularly evocative: indepen-\\ndent coin Ô¨Çips x1; x2; : : : ; xn give, say, s D P\\ni xi D n Nx successes. Prior\\ngn0;x0.(cid:25)/ amounts to assuming proportion x0 D s0=n0 prior successes in\\nn0 Ô¨Çips. Formula (13.18) becomes\\n\\nEf(cid:25)jxg D s0 C s\\nn0 C n\\n\\n(13.23)\\n\\nfor the posterior expectation of (cid:25). The choice .n0; x0/ D .2; 1=2/ for in-\\nstance gives Bayesian estimate .s C 1/=.n C 2/ for (cid:25), pulling the MLE s=n\\na little bit toward 1/2.\\n\\nThe size of n0, the number of hypothetical prior observations, deter-\\nmines how informative or uninformative the prior gn0;x0.(cid:22)/ is. Recent\\nobjective Bayes literature has favored choosing n0 small, n0 D 1 being\\npopular. The hope here is to employ a proper prior (one that has a Ô¨Ånite\\nintegral), while still not injecting much unwarranted information into the\\nanalysis. The choice of x0 is also by convention. One possibility is to set\\n\\n\\x0c240\\n\\nObjective Bayes Inference and MCMC\\n\\nx0 D Nx, in which case the posterior expectation Ef(cid:22)jxg (13.18) equals\\nthe MLE Nx. Another possibility is choosing x0 equal to a ‚Äúnull‚Äù value, for\\ninstance x0 D 0 for effect size estimation in (3.28).\\n\\nTable 13.2 Vasoconstriction data; volume of air inspired in 39 cases, 19\\nwithout vasoconstriction (y D 0) and 20 with vasoconstriction (y D 1).\\n\\ny D 0\\n\\ny D 1\\n\\n98\\n98\\n104\\n104\\n113\\n118\\n120\\n123\\n137\\n\\n60\\n74\\n78\\n78\\n78\\n88\\n90\\n95\\n95\\n98\\n\\n85\\n88\\n88\\n90\\n90\\n93\\n104\\n108\\n110\\n111\\n\\n115\\n120\\n126\\n126\\n128\\n136\\n143\\n151\\n154\\n157\\n\\n(cid:142)6\\n\\nAs a miniature example of objective Bayes inference, we consider the\\nvasoconstriction data(cid:142)of Table 13.2: n D 39 measurements of lung volume\\nhave been obtained, 19 without vasoconstriction .y D 0/ and 20 with .y D\\n1/. Here we will think of the yi as binomial variates,\\n\\nyi\\n\\nind(cid:24) Bi.1; (cid:25)i /;\\n\\ni D 1; 2; : : : ; 39;\\n\\n(13.24)\\n\\nfollowing logistic regression model (8.5),\\n\\nlog\\n\\n(cid:18) (cid:25)i\\n\\n(cid:19)\\n\\n1 (cid:0) (cid:25)i\\n\\nD Àõ0 C Àõ1xi ;\\n\\n(13.25)\\n\\nwith the xi as Ô¨Åxed covariates (the values in Table 13.2).\\n\\nLetting Xi D .1; xi /0, (13.24)‚Äì(13.25) results in a two-parameter expo-\\n\\nnential family (8.24),\\n\\nh\\n\\nfÀõ.y/ D en\\n\\ni\\nÀõ0 OÀá (cid:0) .Àõ/\\n\\nf0.y/;\\n\\n(13.26)\\n\\nhaving\\n\\nOÀá D 1\\nn\\n\\n  n\\nX\\n\\nn\\nX\\n\\nyi ;\\n\\nxi yi\\n\\n!0\\n\\niD1\\n\\ni D1\\n\\nand  .Àõ/ D 1\\nn\\n\\nn\\nX\\n\\niD1\\n\\nlog.1 C eÀõ0Xi /:\\n\\nThe MLE OÀõ has approximate 2 (cid:2) 2 covariance matrix OV as given in (8.30).\\n\\n\\x0c13.2 Conjugate Prior Distributions\\n\\n241\\n\\nIn Figure 13.2, the posterior distributions are graphed in terms of\\n\\n(cid:13) D OV (cid:0)1=2.Àõ (cid:0) OÀõ/\\n\\n(13.27)\\n\\nrather than Àõ or (cid:22), making the contours of equal density roughly circular\\nand centered at zero.\\n\\nFigure 13.2 Vasoconstriction data; contours of equal posterior\\ndensity of (cid:13) (13.27) from four uninformative priors, as described\\nin the text. Numbers indicate probability content within contours;\\nlight dashed contours from Panel A, Ô¨Çat prior.\\n\\nPanel A of Figure 13.2 illustrates the Ô¨Çat prior posterior density of (cid:13)\\ngiven the data y in model (13.24)‚Äì(13.25). The heavy lines are contours\\nof equal density, with the one labeled ‚Äú0.9‚Äù containing 90% of the pos-\\nterior probability, etc. Panel B shows the corresponding posterior density\\n\\nA. Flat Priorg1g2 0.1  0.3  0.5  0.75  0.9  0.975  0.99  0.997  0.999  0.999 ‚àí4‚àí2024‚àí4‚àí2024lB. Jeffreys\\' Priorg1g2 0.1  0.3  0.5  0.75  0.9  0.975  0.99  0.997  0.999 ‚àí4‚àí2024‚àí4‚àí2024 0.1  0.5  0.9  0.99  0.999  0.999 lC. Conjugate Priorg1g2 0.1  0.3  0.5  0.75  0.9  0.975  0.99  0.997  0.999 ‚àí4‚àí2024‚àí4‚àí2024 0.1  0.5  0.9  0.99  0.999  0.999 lD. Bootstrap Distributiong1g2 0.1  0.3  0.5  0.75  0.9  0.975  0.99  0.997  0.997  0.999 ‚àí4‚àí2024‚àí4‚àí2024l 0.1  0.5  0.9  0.99  0.999  0.999 \\x0c242\\n\\nObjective Bayes Inference and MCMC\\n\\ncontours obtained from Jeffreys‚Äô multiparameter prior (11.72), in this case\\n\\ngJeff.Àõ/ D jVÀõj(cid:0)1=2;\\n\\n(13.28)\\n\\nVÀõ the covariance matrix of OÀõ, as calculated from (8.30). For comparison\\npurposes the light dashed curves show some of the Ô¨Çat prior contours from\\npanel A. The effect of gJeff.Àõ/ is to reduce the Ô¨Çat prior bulge toward the\\nupper left corner.\\n\\nPanel C relates to the conjugate prior4 g1;0.Àõ/. Besides reducing the Ô¨Çat\\n\\nprior bulge, g1;0.Àõ/ pulls the contours slightly downward.\\n\\nPanel D shows the parametric bootstrap distribution: model (13.24)‚Äì\\n(13.25), with OÀõ replacing Àõ, gave resamples y (cid:3) and MLE replications OÀõ(cid:3).\\nThe contours of O(cid:13) (cid:3) D OV (cid:0)1=2. OÀõ(cid:3) (cid:0) OÀõ/ considerably accentuate the bulge\\ntoward the left.\\n\\nFigure 13.3 Posterior densities for (cid:13)1, Ô¨Årst coordinate of (cid:13) in\\n(13.27), for the vasoconstriction data. Dashed red curve: raw\\n(unweighted) distribution of B D 8000 parametric replications\\nfrom model (13.24)‚Äì(13.25); solid black curve: BCa density\\n(11.68) (z0 D 0:123, a D 0:053); dotted blue curve: posterior\\ndensity using Jeffreys multiparameter prior (11.72).\\n\\n4 The role of Nx in (13.13) is taken by OÀá in (13.26), so g1;0 has OÀá D 0, n0 D 1. This\\n\\nmakes g1;0.Àõ/ D expf(cid:0) .Àõ/g. The factor V .(cid:22)/ in (13.14) is absent in the conjugate\\nprior for Àõ (as opposed to (cid:22)).\\n\\n‚àí4‚àí20240.000.010.020.030.04g1Density*********************************************************************************BCaraw bootstrapJeffreys\\x0c13.3 Model Selection and the Bayesian Information Criterion 243\\n\\nThis doesn‚Äôt necessarily imply that a bootstrap analysis would give much\\ndifferent answers than the three (quite similar) objective Bayes results. For\\nany particular real-valued parameter of interest (cid:18), the raw bootstrap distri-\\nbution (equal weight on each replication) would be reweighted according to\\nthe BCa formula (11.68) in order to produce accurate conÔ¨Ådence intervals.\\nFigure 13.3 compares the raw bootstrap distribution, the BCa conÔ¨Ådence\\ndensity, and the posterior density obtained from Jeffreys‚Äô prior, for (cid:18) equal\\nto (cid:13)1, the Ô¨Årst coordinate of (cid:13) in (13.27). The BCa density is shifted to the\\nright of Jeffreys‚Äô.\\n\\nCritique of Objective Bayes Inference\\n\\nDespite its simplicity, or perhaps because of it, objective Bayes procedures\\nare vulnerable to criticism from both ends of the statistical spectrum. From\\nthe subjectivist point of view, objective Bayes is only partially Bayesian: it\\nemploys Bayes‚Äô theorem but without doing the hard work of determining a\\nconvincing prior distribution. This introduces frequentist elements into its\\npractice‚Äîclearly so in the case of Jeffreys‚Äô prior‚Äîalong with frequentist\\nincoherencies.\\n\\nFor the frequentist, objective Bayes analysis can seem dangerously un-\\ntethered from the usual standards of accuracy, having only tenuous large-\\nsample claims to legitimacy. This is more than a theoretical objection. The\\npractical advantages claimed for Bayesian methods depend crucially on the\\nÔ¨Åne structure of the prior. Can we safely ignore stopping rules or selective\\ninference (e.g., choosing the largest of many estimated parameters for spe-\\ncial attention) for a prior not based on some form of genuine experience?\\n\\nIn an era of large, complicated, and difÔ¨Åcult data-analytic problems, ob-\\njective Bayes methods are answering a felt need for relatively straightfor-\\nward paths to solution. Granting their usefulness, it is still reasonable to\\nhope for better justiÔ¨Åcation,5 or at least for more careful comparisons with\\ncompeting methods as in Figure 13.3.\\n\\n13.3 Model Selection and the Bayesian Information Criterion\\n\\nData-based model selection has become a major theme of modern statisti-\\ncal inference. In the problem‚Äôs simplest form, the statistician observes data\\nx and wishes to choose between a smaller model\\n0 and a larger model\\n\\nM\\n\\n5 Chapter 20 discusses the frequentist assessment of Bayes and objective Bayes estimates.\\n\\n\\x0c244\\n\\nObjective Bayes Inference and MCMC\\n\\n1. The classic textbook example takes x D .x1; x2; : : : ; xn/0 as an inde-\\n\\nM\\npendent normal sample,\\n\\niid(cid:24)\\n\\nxi\\n\\n.(cid:22); 1/\\n\\nfor i D 1; 2; : : : ; n;\\n\\n(13.29)\\n\\nN\\n0 the null hypothesis (cid:22) D 0 and\\n\\nwith\\nnative,\\n\\nM\\n\\n1 the general two-sided alter-\\n\\nM\\n\\n0 W (cid:22) D 0;\\n\\nM\\n\\nM\\n\\n1 W (cid:22) ¬§ 0:\\n\\n(13.30)\\n\\n(We can include (cid:22) D 0 in\\n1 with no effect on what follows.) From a\\nfrequentist viewpoint, choosing between\\n1 in (13.29)‚Äì(13.30)\\namounts to running a hypothesis test of H0 W (cid:22) D 0, perhaps augmented\\nwith a conÔ¨Ådence interval for (cid:22).\\n\\n0 and\\n\\nM\\n\\nM\\n\\nM\\n\\nBayesian model selection aims for more: an evaluation of the posterior\\n1 given x. A full Bayesian speciÔ¨Åcation re-\\n\\nprobabilities of\\n0 and\\nquires prior probabilities for the two models,\\n\\nM\\n\\nM\\n\\n(cid:25)0 D Prf\\n\\n0g\\n\\nand (cid:25)1 D 1 (cid:0) (cid:25)0 D Prf\\n\\nM\\nand conditional prior densities for (cid:22) within each model,\\n\\nM\\n\\n1g;\\n\\n(13.31)\\n\\ng0.(cid:22)/ D g.(cid:22)j\\n\\nM\\n\\n0/\\n\\nand g1.(cid:22)/ D g.(cid:22)j\\n\\nM\\n\\n1/:\\n\\n(13.32)\\n\\nLet f(cid:22).x/ be the density of x given (cid:22). Each model induces a marginal\\n\\ndensity for x, say\\n\\nZ\\n\\nf0.x/ D\\n\\n0\\n\\nM\\n\\nf(cid:22).x/g0.(cid:22)/ d(cid:22) and f1.x/ D\\n\\nZ\\n\\n1\\n\\nM\\n\\nf(cid:22).x/g1.(cid:22)/ d(cid:22):\\n\\n(13.33)\\n\\nBayes‚Äô theorem, in its ratio form (3.8), then gives posterior probabilities\\n\\n(cid:25)0.x/ D Prf\\n\\nM\\n\\n0jxg\\n\\nand (cid:25)1.x/ D Prf\\n\\nM\\n\\n1jxg\\n\\n(13.34)\\n\\nsatisfying\\n\\n(cid:25)1.x/\\n(cid:25)0.x/\\n\\nD (cid:25)1\\n(cid:25)0\\n\\nB.x/;\\n\\nwhere B.x/ is the Bayes factor\\n\\nB.x/ D f1.x/\\nf0.x/\\n\\n;\\n\\n(13.35)\\n\\n(13.36)\\n\\nleading to the elegant statement that the posterior odds ratio is the prior\\nodds ratio times the Bayes factor.\\n\\nAll of this is of more theoretical than applied use. Prior speciÔ¨Åcations\\n(13.31)‚Äì(13.32) are usually unavailable in practical settings (which is why\\n\\n\\x0c13.3 Model Selection and the BIC\\n\\n245\\n\\nstandard hypothesis testing is so popular). The objective Bayes school has\\nconcentrated on estimating the Bayes factor B.x/, with the understanding\\nthat the prior odds ratio (cid:25)1=(cid:25)0 in (13.35) would be roughly evaluated de-\\npending on the speciÔ¨Åc circumstances‚Äîperhaps set to the Laplace choice\\n(cid:25)1=(cid:25)0 D 1.\\n\\nTable 13.3 Jeffreys‚Äô scale of evidence for the interpretation of Bayes\\nfactors.\\n\\nBayes factor\\n\\nEvidence for M1\\n\\n< 1\\n1‚Äì3\\n3‚Äì20\\n20‚Äì150\\n> 150\\n\\nnegative\\nbarely worthwhile\\npositive\\nstrong\\nvery strong\\n\\nJeffreys suggested a scale of evidence for interpreting Bayes factors, re-\\nproduced in Table 13.3; (cid:142) B.x/ D 10 for instance constitutes positive but (cid:142)7\\nnot strong evidence in favor of the bigger model. Jeffreys‚Äô scale is a Bayes-\\nian version of Fisher‚Äôs interpretive scale for the outcome of a hypothetic\\ntest, with coverage value (one minus the signiÔ¨Åcance level) 0.95 famously\\nconstituting ‚ÄúsigniÔ¨Åcant‚Äù evidence against the null hypothesis. Table 13.4\\nshows Fisher‚Äôs scale, as commonly interpreted in the biomedical and social\\nsciences.\\n\\nTable 13.4 Fisher‚Äôs scale of evidence against null hypothesis\\nfavor of\\n\\n1, as a function of coverage level (1 minus the p-value).\\n\\nM\\n\\n0 and in\\n\\nM\\n\\nCoverage\\n\\n(p-value)\\n\\nEvidence for M1\\n\\n.80\\n.90\\n.95\\n.975\\n.99\\n.995\\n.999\\n\\n(.20)\\n(.10)\\n(.05)\\n(.025)\\n(.01)\\n(.005)\\n(.001)\\n\\nnull\\nborderline\\nmoderate\\nsubstantial\\nstrong\\nvery strong\\noverwhelming\\n\\nEven if we accept the reduction of model selection to assessing the\\nBayes factor B.x/ in (13.35), and even if we accept Jeffreys‚Äô scale of in-\\nterpretation, this still leaves a crucial question: how to compute B.x/ in\\n\\n\\x0c246\\n\\nObjective Bayes Inference and MCMC\\n\\npractice, without requiring informative choices of the priors g0 and g1 in\\n(13.32).\\n\\nA popular objective Bayes answer is provided by the Bayesian informa-\\n\\n(cid:142)8\\n\\ntion criterion(cid:142) (BIC). For a given model\\n\\nwe deÔ¨Åne\\n\\nM\\n\\nBIC.\\n\\n/ D log Àöf O(cid:22).x/(cid:9) (cid:0) p\\n2\\nwhere O(cid:22) is the MLE, p the degrees of freedom (number of free parameters)\\n, and n the sample size. Then the BIC approximation to Bayes factor\\nin\\nB.x/ (13.36) is\\n\\n(13.37)\\n\\nlog.n/;\\n\\nM\\n\\nM\\n\\nlog BBIC.x/ D BIC.\\n\\n1/ (cid:0) BIC.\\nD log Àöf O(cid:22)1.x/=f O(cid:22)0.x/(cid:9) (cid:0) p1 (cid:0) p0\\n\\nM\\n\\nM\\n\\n0/\\n\\n2\\n\\nlog.n/;\\n\\n(13.38)\\n\\nthe subscripts indexing the MLEs and degrees of freedom in\\n\\n0.\\nThis can be restated in somewhat more familiar terms. Letting W .x/ be\\n\\n1 and\\n\\nM\\n\\nM\\n\\nWilks‚Äô likelihood ratio statistic,\\n\\nwe have\\n\\nW .x/ D 2 log Àöf O(cid:22)1.x/=f O(cid:22)0.x/(cid:9) ;\\n\\n(13.39)\\n\\nlog BBIC.x/ D 1\\n2\\n\\nfW .x/ (cid:0) d log.n/g ;\\n\\n(13.40)\\n\\nwith d D p1 (cid:0) p0: W .x/ approximately follows a (cid:31)2\\nmodel\\none, favoring\\n\\nd distribution under\\n0, E0fW .x/g :D d , implying BBIC.x/ will tend to be less than\\n0 if it is true, ever more strongly as n increases.\\n\\nM\\n\\nWe can apply BIC selection to the vasoconstriction data of Table 13.2,\\ntaking\\n0 to be the submodel\\nhaving Àõ1 D 0. In this case d D 1 in (13.40). Direct calculation gives\\nW D 7:07 and\\n\\n1 to be model (13.24)‚Äì(13.25), and\\n\\nM\\n\\nM\\n\\nM\\n\\nBBIC D 5:49;\\n\\n(13.41)\\n\\npositive but not strong evidence against\\n0 according to Jeffreys‚Äô scale.\\nBy comparison, the usual frequentist z-value for testing Àõ1 D 0 is 2.36,\\ncoverage level 0.982, between substantial and strong evidence against\\n0\\non Fisher‚Äôs scale.\\n\\nM\\n\\nM\\n\\nThe BIC was named in reference to Akaike‚Äôs information criterion (AIC),\\n\\n/ D log Àöf O(cid:22).x/(cid:9) (cid:0) p;\\n\\nAIC.\\n\\nM\\n\\n(13.42)\\n\\nwhich suggests, as in (12.73), basing model selection on the sign of\\n\\nAIC.\\n\\nM\\n\\n1/ (cid:0) AIC.\\n\\n0/ D 1\\n2\\n\\nM\\n\\nfW .x/ (cid:0) 2d g :\\n\\n(13.43)\\n\\n\\x0cM\\n\\n13.3 Model Selection and the BIC\\n\\n247\\n\\nThe BIC penalty d log.n/ in (13.40) grows more severe than the AIC\\npenalty 2d as n gets larger, increasingly favoring selection of\\n0 rather\\nthan\\n1. The distinction is rooted in Bayesian notions of coherent behav-\\nior, as discussed in what follows.\\n\\nM\\n\\nM\\n\\nWhere does the BIC penalty term d log.n/ in (13.40) come from? A Ô¨Årst\\n\\nanswer uses the simple normal model xi (cid:24)\\nhas prior g0.(cid:22)/ D g.(cid:22)j\\ntake g1.(cid:22)/ D g.(cid:22)j\\n\\nM\\n\\n0\\nM\\n0/ equal a delta function at zero. Suppose we\\n\\n.(cid:22); 1/, (13.29)‚Äì(13.30).\\n\\nN\\n\\n1/ in (13.32) to be the Gaussian conjugate prior\\n\\ng1.(cid:22)/ (cid:24)\\n\\n.M; A/:\\n\\n(13.44)\\n\\nN\\nThe discussion following (13.23) in Section 13.2 suggests setting M D 0\\nand A D 1, corresponding to prior information equivalent to one of the n\\nactual observations. In this case we can calculate the actual Bayes factor\\nB.x/,\\n\\nlog B.x/ D 1\\n2\\n\\n(cid:26) n\\nn C 1\\n\\n(cid:27)\\nW .x/ (cid:0) log.n C 1/\\n\\n;\\n\\n(13.45)\\n\\nnearly equaling log BBIC.x/ (d D 1), for large n. JustiÔ¨Åcations of the BIC\\nformula as an approximate Bayes factor follow generalizations of this kind\\nof argument, as discussed in the chapter endnotes.\\n\\nThe difference between BIC and frequentist hypothesis testing grows\\n\\nmore drastic for large n. Suppose\\n0\\nM\\naugmented with one additional covariate (so d D 1). Let z be a standard\\nz-value for testing the hypothesis that\\n\\n0 is a regression model and\\n\\n1 is no improvement over\\n\\n1 is\\n\\nM\\n\\nM\\n\\n0,\\n\\nM\\n\\nz P(cid:24)\\n\\nN\\n\\n.0; 1/ under\\n\\n0:\\n\\nM\\n\\nM\\n(13.46)\\n\\nTable 13.5 shows BBIC.x/ as a function of z and n. At n D 15 Fisher‚Äôs\\nand Jeffreys‚Äô scales give roughly similar assessments of the evidence against\\n0 (though Jeffreys‚Äô nomenclature is more conservative). At the other end\\nM\\nof the table, at n D 10; 000, the inferences are contradictory: z D 3:29,\\nwith p-value 0.001 and coverage level 0.999, is overwhelming evidence\\n1 on Fisher‚Äôs scale, but barely worthwhile for Jeffreys‚Äô. Bayesian\\nfor\\ncoherency, the axiom that inferences should be consistent over related sit-\\nuations, lies behind the contradiction.\\n\\nM\\n\\nSuppose n D 1 in the simple normal model (13.29)‚Äì(13.30). That is, we\\n\\nobserve only the single variable\\n\\nx (cid:24)\\n\\n.(cid:22); 1/;\\n\\n(13.47)\\n\\nN\\n\\nand wish to decide between\\ndenote our\\n\\nM\\n\\n1 prior density (13.32) for this situation.\\n\\nM\\n\\n0 W (cid:22) D 0 and\\n\\n1 W (cid:22) ¬§ 0. Let g.1/\\n\\n1 .(cid:22)/\\n\\nM\\n\\n\\x0c248\\n\\nObjective Bayes Inference and MCMC\\n\\nTable 13.5 BIC Bayes factors corresponding to z-values for testing one\\nadditional covariate; coverage value (1 minus the signiÔ¨Åcance level) of a\\ntwo-sided hypothesis test as interpreted by Fisher‚Äôs scale of evidence,\\nright. Jeffreys‚Äô scale of evidence, Table 13.3, is in rough agreement with\\nFisher for n D 15, but favors the null much more strongly for larger\\nsample sizes.\\n\\nn\\n\\nCover z-value\\n\\n15\\n\\n50\\n\\n250 1000 2500 5000 10000 Fisher\\n\\n.80\\n.90\\n.95\\n.975\\n.99\\n.995\\n.999\\n\\n1.28\\n1.64\\n1.96\\n2.24\\n2.58\\n2.81\\n3.29\\n\\n.14\\n.32\\n.59\\n.24\\n.55\\n1.00\\n.43\\n.97\\n1.76\\n.78\\n1.74\\n3.18\\n1.74\\n3.90\\n7.12\\n13.27\\n3.25\\n7.27\\n57.96 31.75 14.20\\n\\n.07\\n.12\\n.22\\n.39\\n.87\\n1.63\\n7.10\\n\\n.05\\n.08\\n.14\\n.25\\n.55\\n1.03\\n4.49\\n\\n.03\\n.05\\n.10\\n.17\\n.39\\n.73\\n3.17\\n\\n.02 null\\n.04 borderline\\n.07 moderate\\n.12 substantial\\n.28 strong\\n.51 very strong\\n2.24 overwhelming\\n\\np\\n\\nThe case n > 1 in (13.29) is logically identical to (13.47). Letting x.n/ D\\nn.P xi =n/ and (cid:22).n/ D\\n\\np\\n\\nn(cid:22) gives\\n(cid:17)\\n(cid:16)\\n(cid:22).n/; 1\\n\\nx.n/ (cid:24)\\n\\nN\\n0 W (cid:22).n/ D 0 and\\n\\nwith (13.30) becoming\\nrequires that (cid:22).n/ in (13.48) have the same\\n(cid:22) D (cid:22).n/=\\nsatisÔ¨Åes\\n\\nM\\nn, this implies that g.n/\\n\\np\\n\\nM\\n1 .(cid:22)/, the\\n\\n1 .(cid:22)/ D g.1/\\ng.n/\\n\\n1\\n\\n(cid:0)(cid:22)ƒ±\\n\\nthis being ‚Äúsample size coherency.‚Äù\\n\\np\\n\\nM\\nn(cid:1) ƒ±\\n\\np\\n\\nn;\\n\\n;\\n\\n(13.48)\\n\\n1 W (cid:22).n/ ¬§ 0. Coherency\\nM\\n1 prior as (cid:22) in (13.47). Since\\n1 prior for sample size n,\\n\\n(13.49)\\n\\n1 .(cid:22)/ farther\\nThe effect of (13.49) is to spread the\\n0 prior g.n/\\naway from the null value (cid:22) D 0 at rate\\n0 .(cid:22)/\\nstays Ô¨Åxed. For any Ô¨Åxed value of the sufÔ¨Åcient statistic x.n/ (x.n/ being\\n‚Äúz‚Äù in Table 13.5), this results in the Bayes factor B.x.n// decreasing at\\nrate 1=\\nn; the frequentist/Bayesian contradiction seen in Table 13.5 goes\\nbeyond the speciÔ¨Åcs of the BIC algorithm.\\n\\n1 prior density g.n/\\nn, while the\\n\\np\\nM\\n\\nM\\n\\np\\n\\n(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)\\n\\nA general information criterion takes the form\\n\\nGIC.\\n\\nM\\n\\n/ D log f O(cid:22).x/ (cid:0) p cn;\\n\\n(13.50)\\n\\n\\x0c13.3 Model Selection and the BIC\\n\\n249\\n\\nwhere cn is any sequence of positive numbers; cn D log.n/=2 for BIC\\n(13.37) and cn D 1 for AIC (13.42). The difference\\n\\n1/ (cid:0) GIC.\\n\\n(cid:129) (cid:17) GIC.\\n\\n0/ D 1\\n2\\nd D p1(cid:0)p0, will be positive if W .x/ > 2cnd . For d D 1, as in Table 13.5,\\n(cid:129) will favor\\n0 is\\nactually true,\\n\\n1 if W .x/ (cid:21) 2cn, with approximate probability, if\\n\\n.W .x/ (cid:0) 2cnd / ;\\n\\n(13.51)\\n\\nM\\n\\nM\\n\\nM\\n\\nM\\n\\nPrf(cid:31)2\\n1\\n\\n(cid:21) 2cng:\\n\\n(13.52)\\n\\nThis equals 0.157 for the AIC choice cn D 1; for BIC, n D 10; 000, it\\nequals 0.0024. The choice\\n\\ncn D 1:92\\n\\n(13.53)\\n0g :D 0:05, agreeing with the usual frequentist 0.05\\n\\nmakes Prf(cid:129) > 0j\\nrejection level.\\n\\nM\\n\\nThe BIC is consistent: Prf(cid:129) > 0g goes to zero as n ! 1 if\\n\\n0 is true.\\nThis isn‚Äôt true of (13.53) for instance, where we will have Prf(cid:129) > 0g :D\\n0:05 no matter how large n may be, but consistency is seldom compelling\\nas a practical argument.\\n\\nM\\n\\nConÔ¨Ådence intervals help compensate for possible frequentist overÔ¨Åt-\\nting. With z D 3:29 and n D 10; 000, the 95% conÔ¨Ådence interval for (cid:22)\\n1 (13.30) is .0:013; 0:053/. Whether or not such a small effect\\nin model\\nis interesting depends on the scientiÔ¨Åc context. The fact that BIC says ‚Äúnot\\ninteresting‚Äù speaks to its inherent small-model bias.\\n\\nM\\n\\nThe prostate cancer study data of Section 3.3 provides a more challeng-\\ning model selection problem. Figure 3.4 shows the histogram of N D 6033\\nobservations xi , each measuring the effects of one gene. The histogram has\\n49 bins, each of width 0.2, with centers cj ranging from (cid:0)4:4 to 5.2; yj ,\\nthe height of the histogram at cj , is the number of xi in bin j ,\\n\\nyj D #fxi 2 bin j g\\n\\nfor j D 1; 2; : : : ; 49:\\n\\n(13.54)\\n\\nWe assume that the yj follow a Poisson regression model as in Sec-\\n\\ntion 8.3,\\n\\nyj\\n\\nind(cid:24) Poi.(cid:23)j /;\\n\\nj D 1; 2; : : : ; 49;\\n\\n(13.55)\\n\\nand wish to Ô¨Åt a log polynomial GLM model to the (cid:23)j . The model selection\\nquestion is ‚ÄúWhat degree polynomial?‚Äù Degree 2 corresponds to normal\\ndensities, but the long tails seen in Figure 3.4 suggest otherwise.\\n\\nModels of degree 2 through 8 are assessed in Figure 13.4. Four model\\nselection measures are compared: AIC (13.42); BIC (13.37) with n D 49,\\n\\n\\x0c250\\n\\nObjective Bayes Inference and MCMC\\n\\nFigure 13.4 Log polynomial models of degree 2 through 8\\napplied to the prostate study histogram of Figure 3.4. Model\\nselection criteria: AIC (13.42); BIC (13.37) with n D 49, number\\nof bins, or 6033, number of genes; GIC (13.50) using classic\\nFisher hypothesis choice cn D 1:92. All four selected the\\nfourth-degree model as best.\\n\\nthe number of yj values (bins), and also n D 6033, the number of genes;\\nand GIC (13.50), with cn D 1:92 (13.53), the choice based on classic Fish-\\nerian hypothesis testing. (This is almost the same as BIC n D 49, since\\nlog.49/=2 D 1:95.) A fourth-degree polynomial model was the winner\\nunder all four criteria.\\n\\nThe ‚Äúuntethered‚Äù criticism made against objective Bayes methods in\\ngeneral is particularly applicable to BIC. The concept of ‚Äúsample size‚Äù\\nis not well deÔ¨Åned, as the prostate study example shows. Sample size co-\\nherency (13.49), the rationale for BIC‚Äôs strong bias toward smaller models,\\nis less convincing in the absence of priors based on genuine experience (es-\\npecially if there is no prospect of the sample size changing). Whatever its\\nvulnerabilities, BIC model selection has nevertheless become a mainstay\\nof objective Bayes model selection, not least because of its freedom from\\nthe choice of Bayesian priors.\\n\\n2345678‚àí120‚àí100‚àí80‚àí60‚àí40Model polynomial degreeAIC and BICBIC 6033BIC 49 and GICAIC\\x0c13.4 Gibbs Sampling and MCMC\\n\\n251\\n\\n13.4 Gibbs Sampling and MCMC\\n\\nMiraculously blessed with visions of the future, a Bayesian statistician of\\nthe 1970s would certainly be pleased with the prevalence of Bayes method-\\nology in twenty-Ô¨Årst-century applications. But his pleasure might be tinged\\nwith surprise that the applications were mostly of the objective, ‚Äúuninfor-\\nmative‚Äù type, rather than taken from the elegant de Finetti‚ÄìSavage school\\nof subjective inference.\\n\\nThe increase in Bayesian applications, and the change in emphasis from\\nsubjective to objective, had more to do with computation than philoso-\\nphy. Better computers and algorithms facilitated the calculation of formerly\\nintractable Bayes posterior distributions. Technology determines practice,\\nand the powerful new algorithms encouraged Bayesian analyses of large\\nand complicated models where subjective priors (or those based on actual\\npast experience) were hard to come by. Add in the fact that the algorithms\\nworked most easily with simple ‚Äúconvenience‚Äù priors like the conjugates\\nof Section 13.2, and the stage was set for an objective Bayes renaissance.\\nAt Ô¨Årst glance it‚Äôs hard to see why Bayesian computations should be\\ndaunting. From parameter vector (cid:18), data x, density function f(cid:18) .x/, and\\nprior density g.(cid:18)/, Bayes‚Äô rule (3.5)‚Äì(3.6) directly produces the posterior\\ndensity\\n\\ng.(cid:18)jx/ D g.(cid:18)/f(cid:18) .x/=f .x/;\\n\\n(13.56)\\n\\nwhere f .x/ is the marginal density\\n\\nf .x/ D\\n\\nZ\\n\\n(cid:127)\\n\\ng.(cid:18)/f(cid:18) .x/ d (cid:18):\\n\\n(13.57)\\n\\nThe posterior probability of any set A in the parameter space (cid:127) is then\\n\\nP fAjxg D\\n\\nZ\\n\\nA\\n\\ng.(cid:18)/f(cid:18) .x/ d (cid:18)\\n\\n(cid:30) Z\\n\\n(cid:127)\\n\\ng.(cid:18)/f(cid:18) .x/ d (cid:18):\\n\\n(13.58)\\n\\nThis is easy to write down but usually difÔ¨Åcult to evaluate if (cid:18) is multidi-\\nmensional.\\n\\nModern Bayes methods attack the problem through the application of\\ncomputer power. Even if we can‚Äôt integrate g.(cid:18)jx/, perhaps we can sample\\nfrom it. If so, a sufÔ¨Åciently large sample, say\\n\\n(cid:18) .1/; (cid:18) .2/; : : : ; (cid:18) .B/ (cid:24) g.(cid:18)jx/\\n\\n(13.59)\\n\\nwould provide estimates\\n\\nOP fAjxg D #\\n\\nn\\n(cid:18) .j / 2 A\\n\\no .\\n\\nB;\\n\\n(13.60)\\n\\n\\x0c252\\n\\nObjective Bayes Inference and MCMC\\n\\nand similarly for posterior moments, correlations, etc. We would in this\\nway be employing the same general tactic as the bootstrap, applied now\\nfor Bayesian rather than frequentist purposes‚Äîtoward the same goal as the\\nbootstrap, of freeing practical applications from the constraints of mathe-\\nmatical tractability.\\n\\nThe two most popular computational methods,6 Gibbs sampling and\\nMarkov chain Monte Carlo (MCMC), are based on Markov chain algo-\\nrithms; that is, the posterior samples (cid:18) .b/ are produced in sequence, each\\none depending only on (cid:18) .b(cid:0)1/ and not on its more distant predecessors. We\\nbegin with Gibbs sampling.\\n\\nThe central idea of Gibbs sampling is to reduce the generation of mul-\\ntidimensional vectors (cid:18) D .(cid:18)1; (cid:18)2; : : : ; (cid:18)K/ to a series of univariate calcu-\\nlations. Let (cid:18).k/ denote (cid:18) with component k removed, and g.k/ the condi-\\ntional density of (cid:18)k given (cid:18).k/ and the data x,\\n\\n(cid:18)kj(cid:18).k/; x (cid:24) g.k/\\n\\n(cid:16)\\n\\n(cid:18)kj(cid:18).k/; x\\n\\n(cid:17)\\n:\\n\\n(13.61)\\n\\nThe algorithm begins at some arbitrary initial value (cid:18) .0/. Having computed\\n(cid:18) .1/, (cid:18) .2/, : : : , (cid:18) .b(cid:0)1/, the components of (cid:18) .b/ are generated according to\\nconditional distributions (13.61),\\n(cid:17)\\n\\n(cid:16)\\n\\nfor k D 1; 2; : : : ; K:\\n\\n(13.62)\\n\\n(cid:18) .b/\\nk\\n\\n(cid:24) g.k/\\n\\n(cid:18)k\\n\\nÀá\\nÀá(cid:18) .b(cid:0)1/\\n\\n.k/\\n\\n; x\\n\\nAs an example, we take x to be the n D 20 observations for y D 1 in\\nthe vasoconstriction data of Table 13.2, and assume that these are a normal\\nsample,\\n\\niid(cid:24)\\n\\nxi\\n\\nN\\n\\n.(cid:22); (cid:28)/;\\n\\ni D 1; 2; : : : ; n D 20:\\n\\n(13.63)\\n\\nThe sufÔ¨Åcient statistics for estimating the bivariate parameter (cid:18) D .(cid:22); (cid:28)/\\nare the sample mean and variance\\n\\nNx D\\n\\nn\\nX\\n\\n1\\n\\nxi =n and T D\\n\\nn\\nX\\n\\n.xi (cid:0) Nx/2=.n (cid:0) 1/;\\n\\n(13.64)\\n\\n1\\n\\nhaving independent normal and gamma distributions,\\n\\nNx (cid:24)\\n\\nN\\n\\n.(cid:22); (cid:28)=n/\\n\\nand T (cid:24) (cid:28)G(cid:23)=(cid:23);\\n\\n(13.65)\\n\\nwith (cid:23) D n(cid:0)1\\n\\n2 , the latter being Gam.(cid:23); (cid:28)=(cid:23)/ in the notation of Table 5.1.\\n\\n6 The two methods are often referred to collectively as MCMC because of mathematical\\nconnections, with ‚ÄúMetropolis-Hasting algorithm‚Äù referring to the second type of\\nprocedure.\\n\\n\\x0c13.4 Gibbs Sampling and MCMC\\n\\n253\\n\\nFor our Bayes prior distribution we take the conjugates\\n\\n(cid:28) (cid:24) k1(cid:28)1=Gk1C1\\n\\nand (cid:22)j(cid:28) (cid:24)\\nIn terms of Table 13.1, .x0; n0(cid:23)/ D .(cid:28)1; k1/ for the gamma, while .x0; (cid:27) 2\\n.(cid:22)0; (cid:28)/ for the normal. (A simple speciÔ¨Åcation would take (cid:22) (cid:24)\\n\\n(13.66)\\n1 / D\\n.(cid:22)0; (cid:28)1=n0/.)\\n\\n.(cid:22)0; (cid:28)=n0/:\\n\\nN\\n\\nN\\n\\nMultiplying the normal and gamma functional forms in Table 5.1 yields\\n\\ndensity function\\n\\nf(cid:22);(cid:28) . Nx; T / D c(cid:28) (cid:0).(cid:23)C 1\\n\\n2 / exp\\n\\n(cid:26)\\n(cid:0) 1\\n(cid:28)\\n\\nh\\n(cid:23)T C n\\n2\\n\\n. Nx (cid:0) (cid:22)/2i(cid:27)\\n\\n(13.67)\\n\\nand prior density\\n\\ng.(cid:22); (cid:28)/ D c(cid:28) (cid:0).k1C2:5/ exp\\n\\n(cid:26)\\n(cid:0) 1\\n(cid:28)\\n\\nh\\nk1(cid:28)1 C n0\\n2\\n\\n.(cid:22) (cid:0) (cid:22)0/2i(cid:27)\\n\\n;\\n\\n(13.68)\\n\\nc indicating positive constants that do not affect the posterior computations.\\nThe posterior density cg.(cid:22); (cid:28)/f(cid:22);(cid:28) . Nx; T / is then calculated to be\\ng.(cid:22); (cid:28)j Nx; T / D c(cid:28) (cid:0).(cid:23)Ck1C3/ expf(cid:0)Q=(cid:28)g;\\n.(cid:22) (cid:0) N(cid:22)C/2 C n0n\\n2nC\\n\\nwhere Q D .k1(cid:28)1 C T / C nC\\n2\\n\\n.(cid:22)0 (cid:0) Nx/2:\\n\\n(13.69)\\n\\nHere nC D n0 C n and N(cid:22)C D .n0(cid:22)0 C n Nx/=nC.\\n\\nIn order to make use of Gibbs sampling we need to know the full con-\\nditional distributions g.(cid:22)j(cid:28); Nx; T / and g.(cid:28) j(cid:22); Nx; T /, as in (13.62). (In this\\ncase, k D 2, (cid:18)1 D (cid:22), and (cid:18)2 D (cid:28) .) This is where the conjugate expressions\\nin Table 13.1 come into play. Inspection of density (13.69) shows that\\n\\n(cid:22)j(cid:28); Nx; T (cid:24)\\n\\n(cid:18)\\n\\nN(cid:22)C;\\n\\n(cid:19)\\n\\n(cid:28)\\nnC\\n\\nN\\n\\nand (cid:28)j(cid:22); Nx; T (cid:24) Q\\n\\nG(cid:23)Ck1C2\\n\\n:\\n\\n(13.70)\\n\\nB D 10; 000 Gibbs samples (cid:18) .b/ D .(cid:22).b/; (cid:28) .b// were generated starting\\nfrom (cid:18) .0/ D . Nx; T / D .116; 554/. The prior speciÔ¨Åcations were chosen to\\nbe (presumably) uninformative or mildly informative,\\n\\nn0 D 1; (cid:22)0 D Nx;\\n\\nk1 D 1 or 9:5;\\n(13.71)\\n(In which case N(cid:22)C D Nx and Q D .(cid:23) C k1/T C nC.(cid:22) (cid:0) Nx/2. From\\n(cid:23) D .n (cid:0) 1/=2, we see that k1 corresponds to about 2k1 hypothetical prior\\nobservations.) The resulting posterior distributions for (cid:28) are shown by the\\nhistograms in Figure 13.5.\\n\\nand (cid:28)1 D T:\\n\\nAs a point of frequentist comparison, B D 10; 000 parametric bootstrap\\n\\nreplications (which involve no prior assumptions),\\n\\nO(cid:28) (cid:3) (cid:24) O(cid:28)G(cid:23)=(cid:23);\\n\\nO(cid:28) D T;\\n\\n(13.72)\\n\\n\\x0c254\\n\\nObjective Bayes Inference and MCMC\\n\\nFigure 13.5 Posterior distributions for variance parameter (cid:28),\\nmodel (13.63)‚Äì(13.65), volume of air inspired for\\nvasoconstriction group y D 1 from Table 13.2. Solid teal\\nhistogram: B D 10; 000 Gibbs samples with k1 D 1; black line\\nhistogram: B D 10; 000 samples with k1 D 9:5; red line\\nhistogram: 10,000 parametric bootstrap samples (13.72) suggests\\neven the k1 D 1 prior has substantial posterior effect.\\n\\nare seen to be noticeably more dispersed than even the k1 D 1 Bayes\\nposterior distribution, the likely choice for an objective Bayes analysis.\\nBayes techniques, even objective ones, have regularization effects that may\\nor may not be appropriate.\\n\\nA similar, independent Gibbs sample of size 10,000 was obtained for the\\n19 y D 0 vasoconstriction measurements in Table 13.2, with speciÔ¨Åcations\\nas in (13.71), k D 1. Let\\n\\nƒ±.b/ D (cid:22).b/\\n1\\n(cid:16)\\n(cid:28) .b/\\n1\\n\\n(cid:0) (cid:22).b/\\n0\\nC (cid:28) .b/\\n0\\n\\n(cid:17)1=2 ;\\n\\n(13.73)\\n\\nwhere .(cid:22).b/\\n1 ; (cid:28) .b/\\ny D 1 and y D 0 runs.\\n\\n1 / and .(cid:22).b/\\n\\n0 ; (cid:28) .b/\\n\\n0 / denote the bth Gibbs samples from the\\n\\nFigure 13.6 shows the posterior distribution of ƒ±. Twenty-eight of the\\n\\n tFrequency200400600800100012000200400600800100012001400Tk1=1k1=9.5parametricbootstrap\\x0c13.4 Gibbs Sampling and MCMC\\n\\n255\\n\\nFigure 13.6 B D10,000 Gibbs samples for ‚ÄúBayes t -statistic‚Äù\\n(13.73) comparing y D 1 with y D 0 values for vasoconstriction\\ndata.\\n\\nB D10,000 values ƒ±.b/ were less than 0, giving a ‚ÄúBayesian t-test‚Äù estimate\\n\\nP fƒ± < 0j Nx1; Nx0; T1; T0g D 0:0028:\\n\\n(13.74)\\n\\n(The usual t-test yielded one-sided p-value 0.0047 against the null hy-\\npothesis (cid:22)0 D (cid:22)1.) An appealing feature of Gibbs sampling is that having\\nobtained (cid:18) .1/; (cid:18) .2/; : : : ; (cid:18) .B/ (13.59) the posterior distribution of any pa-\\nrameter (cid:13) D t .(cid:18)/ is obtained directly from the B values (cid:13) .b/ D t.(cid:18) .b//.\\n\\nGibbs sampling requires the ability to sample from the full conditional\\ndistributions (13.61). A more general Markov chain Monte Carlo method,\\ncommonly referred to as MCMC, makes clearer the basic idea. Suppose the\\nspace of possible (cid:18) values is Ô¨Ånite, say f(cid:18).1/; (cid:18).2/; : : : , (cid:18).M /g, and we\\nwish to simulate samples from a posterior distribution putting probability\\np.i / on (cid:18).i /,\\n\\np D .p.1/; p.2/; : : : ; p.M // :\\n\\n(13.75)\\n\\nThe MCMC algorithm begins with the choice of a ‚Äúcandidate‚Äù proba-\\nbility distribution q.i; j / for moving from (cid:18).i/ to (cid:18).j /; in theory q.i; j /\\ncan be almost anything, for instance q.i; j / D 1=.M (cid:0) 1/ for j ¬§ i. The\\nsimulated samples (cid:18) .b/ are obtained by a random walk: if (cid:18) .b/ equals (cid:18).i/,\\n\\n dFrequency0.00.51.01.50100200300400500600Pr(d<0)=0.0028\\x0c256\\n\\nObjective Bayes Inference and MCMC\\n\\nthen (cid:18) .bC1/ equals (cid:18).j / with probability7\\n\\nQ.i; j / D q.i; j / (cid:1) min\\n\\n(cid:26) p.j /q.j; i/\\np.i/q.i; j /\\n\\n(cid:27)\\n\\n; 1\\n\\nfor j ¬§ i, while with probability\\n\\nQ.i; i/ D 1 (cid:0) X\\nj ¬§i\\n\\nQ.i; j /\\n\\n(13.76)\\n\\n(13.77)\\n\\n(cid:18) .bC1/ D (cid:18) .b/ D (cid:18).i/. Markov chain theory then says that, under quite\\ngeneral conditions, the empirical distribution of the random walk values\\n(cid:18) .b/ will approach the desired distribution p as b gets large.\\n\\nA heuristic argument for why this happens begins by supposing that (cid:18) .1/\\nwas in fact generated by sampling from the target distribution p, Prf(cid:18) .1/ D\\nig D p.i/, and then (cid:18) .2/ was obtained according to transition probabilities\\n(13.76)‚Äì(13.77). A little algebra shows that (13.76) implies\\n\\np.i /Q.i; j / D p.j /Q.j; i/;\\n\\n(13.78)\\n\\nthe so-called balance equations. This results in\\no\\n\\nn\\n\\nPr\\n\\n(cid:18) .2/ D i\\n\\nD p.i /Q.i; i / C X\\nj ¬§i\\n\\np.j /Q.j; i/\\n\\nD p.i /\\n\\nM\\nX\\n\\nj D1\\n\\nQ.i; j / D p.i/:\\n\\n(13.79)\\n\\nIn other words, if (cid:18) .1/ has distribution p then so will (cid:18) .2/, and like-\\nwise (cid:18) .3/; (cid:18) .4/; : : : ; p is the equilibrium distribution of the Markov chain\\nrandom walk deÔ¨Åned by transition probabilities Q. Under reasonable con-\\nditions,(cid:142) (cid:18) .b/ must asymptotically attain distribution p no matter how (cid:18) .1/\\nis initially selected.\\n\\n(cid:142)9\\n\\n13.5 Example: Modeling Population Admixture\\n\\nMCMC has had a big impact in statistical genetics, where Bayesian mod-\\neling is popular and useful for representing the complex evolutionary pro-\\ncesses. Here we illustrate its use in demography and modeling admixture‚Äî\\nestimating the contributions from ancestral populations in an individual\\n\\n7 In Bayes applications, p.i/ D g.(cid:18).i/jx/ D g.(cid:18).i//f(cid:18).i/.x/=f .x/ (13.56).\\n\\nHowever, f .x/ is not needed since it cancels out of (13.76), a considerable advantage\\nin complicated situations when f .x/ is often unavailable, and a prime reason for the\\npopularity of MCMC.\\n\\n\\x0c13.5 Example: Modeling Population Admixture\\n\\n257\\n\\ngenome. For example, we might consider human ancestry, and for each\\nindividual wish to estimate the proportion of their genome coming from\\nEuropean, African, and Asian origins. The procedure we describe\\nhere is unsupervised‚Äîa type of soft clustering‚Äîbut we will see it can be\\nvery informative with regard to such questions. We have a sample of n in-\\ndividuals, and we assume each arose from possible admixture among J\\nparent populations, each with their own characteristic vector of allele fre-\\nquencies. For us J D 3, and let Qi 2\\n3 denote a probability vector for in-\\nS\\ndividual i representing the proportions of their heritage coming from pop-\\nulations j 2 f1; 2; 3g (see Section 5.4). We have genomic measurements\\nfor each individual, in our case SNPs (single-nucleotide polymorphisms)\\nat each of M well-spaced loci, and hence can assume they are in linkage\\nequilibrium. At each SNP we have a measurement that identiÔ¨Åes the two\\nalleles (one per chromosome), where each can be either the wild-type A or\\nthe mutation a. That is, we have the genotype Gim at SNP m for individual\\ni: a three-level factor with levels fAA; Aa; aag which we code as 0; 1; 2.\\nTable 13.6 shows some examples.\\n\\nTable 13.6 A subset of the genotype data on 197 individuals, each with\\ngenotype measurements at 100 SNPs. In this case the ethnicity is\\nknown for each individual, one of Japanese, African, European,\\nor African American. For example, individual NA12239 has\\ngenotype Aa for SNP1, NA19247 has AA, and NA20126 has aa.\\n\\nSubject\\n\\nSNP1\\n\\nSNP2\\n\\nSNP3\\n\\nNA10852\\nNA12239\\nNA19072\\nNA19247\\nNA20126\\nNA18868\\nNA19257\\nNA19079\\nNA19067\\nNA19904\\n\\n1\\n1\\n0\\n0\\n2\\n0\\n0\\n0\\n0\\n0\\n\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n\\n0\\n0\\n0\\n2\\n0\\n1\\n0\\n0\\n0\\n1\\n\\n(cid:1) (cid:1) (cid:1)\\n\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n(cid:1) (cid:1) (cid:1)\\n\\nSNP97\\n\\nSNP98\\n\\nSNP99\\n\\nSNP100\\n\\n1\\n1\\n0\\n0\\n2\\n0\\n0\\n0\\n0\\n0\\n\\n1\\n1\\n0\\n0\\n0\\n0\\n0\\n1\\n0\\n0\\n\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n0\\n\\n0\\n0\\n0\\n2\\n0\\n1\\n0\\n0\\n0\\n1\\n\\nLet Pj be the (unknown) M -vector of minor allele frequencies (propor-\\ntions actually) in population j . We have available a sample of n individ-\\nuals, and for each sample we have their genomic information measured at\\neach of the M loci. Some of the individuals might appear to have pure an-\\ncestral origins, but many do not. Our goal is to estimate Qi ; i D 1; : : : ; n;\\nand Pj ; j 2 f1; 2; 3g.\\n\\n\\x0c258\\n\\nObjective Bayes Inference and MCMC\\n\\nim ; X .2/\\n\\nFor this purpose it is useful to pose a generative model. We Ô¨Årst create a\\npair of variables Xim D .X .1/\\nim / corresponding to each Gim, to which\\nwe allocate the two alleles (in arbitrary order). For example, if Gim D 1\\n(corresponding to Aa), then we might set X .1/\\nD 1 (or\\nim\\nvice versa). If Gim D 0 they are both 0, and if Gim D 2, they are both 1.\\nLet Zim 2 f1; 2; 3g2 represent the ancestral origin for individual i of each\\nof these allele copies Xim at locus m, again a two-vector with elements\\nZim D .Z.1/\\n\\nim /. Then our generative model goes as follows.\\n\\nD 0 and X .2/\\nim\\n\\nim ; Z.2/\\n\\n1 Z.c/\\nim\\n\\n(cid:24) Mult.1; Qi /, independently at each m, for each copy c D 1; 2.\\nThat is, we select the ancestral origin of each chromosome at locus m\\naccording to the individual‚Äôs mixture proportions Qi .\\n\\n2 X .c/\\nim\\n\\n(cid:24) Bi.1; Pj m/ if Z.c/\\nim\\n\\nD j , for each copy c D 1; 2. What this\\nmeans is that, for each of the two ancestral picks at locus m (one for\\neach arm of the chromosome), we draw a binomial with the appropriate\\nallele frequency.\\n\\nTo complete the Bayesian speciÔ¨Åcation, we need to supply priors for the\\nQi and also for Pj m. Although one can get fancy here, we resort to the\\nrecommended Ô¨Çat priors, which are\\n\\n(cid:15) Qi (cid:24) D.(cid:21); (cid:21); (cid:21)/, a Ô¨Çat three-component Dirichlet, independently for\\n\\n(cid:142)10\\n\\neach subject i (cid:142) and\\n\\n(cid:15) Pj m (cid:24) D.(cid:13); (cid:13)/ independently for each population j , and each locus m\\n\\n(the beta distribution; see (cid:142)10 in the end notes).\\n\\nWe use the least-informative values (cid:21) D (cid:13) D 1. In practice, these could\\nget updated as well, but for the purposes of this demonstration we leave\\nthem Ô¨Åxed at these values.\\n\\nLet X be the n (cid:2) M (cid:2) 2 array of observed alleles for all n samples.\\nWe wish to estimate the posterior distribution Pr.P; QjX /, referring col-\\nlectively to all the elements of P and Q.\\n\\nFor this purpose we use Gibbs sampling, which amounts to the following\\n\\nsequence.\\n\\n0 Initialize Z.0/; P .0/; Q.0/.\\n1 Sample Z.b/ from the conditional distribution Pr.ZjX ; P .b(cid:0)1/; Q.b(cid:0)1//.\\n2 Sample P .b/; Q.b/ from the conditional distribution Pr.P; QjX ; Z.b//.\\n\\nGibbs is effective when one can sample efÔ¨Åciently from these conditional\\ndistributions, which is the case here.\\n\\n\\x0c13.5 Example: Modeling Population Admixture\\n\\n259\\n\\nIn step 2, we can sample P and Q separately. It can be seen that for each\\n\\n.j; m/ we should sample Pj m from\\n\\nPj mjX ; Z (cid:24) D.(cid:21) C n.0/\\n\\nj m; (cid:21) C n.1/\\n\\nj m/;\\n\\nwhere Z D Z.b/ and\\n\\nn.0/\\nj m\\nn.1/\\nj m\\n\\nD #f.i; c/ W X .c/\\nim\\nD #f.i; c/ W X .c/\\nim\\n\\nD 0 and Z.c/\\nim\\nD 1 and Z.c/\\nim\\n\\nD j g;\\nD j g:\\n\\n(13.80)\\n\\n(13.81)\\n\\nThis follows from the conjugacy of the two-component Dirichlet (beta)\\nwith the binomial distribution, Table 13.1.\\nUpdating Qi involves simulating from\\n\\nQi jX ; Z (cid:24) D.(cid:13) C mi1; (cid:13) C mi2; (cid:13) C mi3/;\\n\\n(13.82)\\n\\nwhere mij is the number of allele copies in individual i that originated\\n(according to Z D Z.b/) in population j :\\n\\nmij D #f.c; m/ W Z.c/\\nim\\n\\nD j g:\\n\\n(13.83)\\n\\nFigure 13.7 Barycentric coordinate plot for the estimated\\nposterior means of the Qi based on MCMC sampling.\\n\\nStep 1 can be performed by simulating Z.c/\\n\\nim independently, for each i; m;\\n\\nlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllEuropeanJapaneseAfricanAfrican American\\x0c260\\n\\nObjective Bayes Inference and MCMC\\n\\nand c from\\n\\nPr.Z.c/\\nim\\n\\nD j jX ; P; Q/ D Qij Pr.X .c/\\n\\nim\\n\\nP3\\n\\n`D1 Qi` Pr.X .c/\\n\\nD j /\\n\\njP; Z.c/\\nim\\njP; Z.c/\\nim\\n\\nim\\n\\nD `/\\n\\n:\\n\\n(13.84)\\n\\nThe probabilities on the right refer back to our generative distribution de-\\nscribed earlier.\\n\\nFigure 13.7 shows a triangle plot that summarizes the result of running\\nthe MCMC algorithm on our 197 subjects. We used a burn in of 1000\\ncomplete iterations, and then a further 2000 to estimate the distribution\\nof the parameters of interest, in this case the Qi . Each dot in the Ô¨Ågure\\nrepresents a three-component probability vector, and is the posterior mean\\nof the sampled Qi for each subject. The points are colored according to\\nthe known ethnicity. Although this algorithm is unsupervised, we see that\\nthe ethnic groups cluster nicely in the corners of the simplex, and allow\\nus to identify these clusters. The African American group is spread\\nbetween the African and European clusters (with a little movement\\ntoward the Japanese).\\n\\n(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)\\n\\nMarkov chain methods are versatile tools that have proved their value in\\n\\nBayesian applications. There are some drawbacks.\\n\\n(cid:15) The algorithms are not universal in the sense of maximum likelihood,\\n\\nrequiring some individual ingenuity with each application.\\n\\n(cid:15) As a result, applications, especially of Gibbs sampling, have favored\\na small set of convenient priors, mainly Jeffreys and conjugates, that\\nsimplify the calculations. This can cast doubt on the relevance of the\\nresulting Bayes inferences.\\n\\n(cid:15) Successive realizations (cid:18) .b/ are highly correlated with each other, mak-\\n\\ning the convergence of estimates such as N(cid:18) D P (cid:18) .b/=B slow.\\n\\n(cid:15) The correlation makes it difÔ¨Åcult to assign a standard error to N(cid:18). Actual\\napplications ignore an initial B0 of the (cid:18) .b/ values (as a ‚Äúburn-in‚Äù period)\\nand go on to large enough B such that estimates like N(cid:18) appear to settle\\ndown. However, neither the choice of B0 nor that of B may be clear.\\n\\nObjective Bayes offers a paradigm of our book‚Äôs theme, the effect of\\nelectronic computation on statistical inference: ingenious new algorithms\\nfacilitated Bayesian applications over a wide class of applied problems and,\\nin doing so, inÔ¨Çuenced the dominant philosophy of the whole area.\\n\\n\\x0c13.6 Notes and Details\\n\\n261\\n\\n13.6 Notes and Details\\n\\nThe books by Savage (1954) and de Finetti (1972), summarizing his ear-\\nlier work, served as foundational texts for the subjective Bayesian school\\nof inference. Highly inÔ¨Çuential, they championed a framework for Bayes-\\nian applications based on coherent behavior and the careful elucidation of\\npersonal probabilities. A current leading text on Bayesian methods, Carlin\\nand Louis (2000), does not reference either Savage or de Finetti. Now Jef-\\nfreys (1961), again following earlier works, claims foundational status. The\\nchange of direction has not gone without protest from the subjectivists‚Äî\\nsee Adrian Smith‚Äôs discussion of O‚ÄôHagan (1995)‚Äîbut is nonetheless al-\\nmost a complete rout.\\n\\nMetropolis et al. (1953), as part of nuclear weapons research, devel-\\noped the Ô¨Årst MCMC algorithm. A vigorous line of work on Markov chain\\nmethods for solving difÔ¨Åcult probability problems has continued to Ô¨Çour-\\nish under such names as particle Ô¨Åltering and sequential Monte Carlo; see\\nGerber and Chopin (2015) and its enthusiastic discussion.\\n\\nModeling population admixture (Pritchard et al., 2000) is one of sev-\\neral applications of hierarchical Bayesian models and MCMC in genetics.\\nOther applications include haplotype estimation and motif Ô¨Ånding, as well\\nas estimation of phylogenetic trees. The examples in this section were de-\\nveloped with the kind help of Hua Tang and David Golan, both from the\\nStanford Genetics department. Hua suggested the example and provided\\nhelpful guidance; David provided the data, and ran the MCMC algorithm\\nusing the STRUCTURE program in the Pritchard lab.\\n\\n(cid:142)1 [p. 236] Uninformative priors. A large catalog of possible uninformative\\npriors has been proposed, thoroughly surveyed by Kass and Wasserman\\n(1996). One approach is to use the likelihood from a small part of the data,\\nsay just one or two data points out of n, as the prior, as with the ‚Äúintrin-\\nsic priors‚Äù of Berger and Pericchi (1996), or O‚ÄôHagan‚Äôs (1995) ‚Äúfractional\\nBayes factors.‚Äù Another approach is to minimize some mathematical mea-\\nsure of prior information, as with Bernardo‚Äôs (1979) ‚Äúreference priors‚Äù or\\nJaynes‚Äô (1968) ‚Äúmaximum entropy‚Äù criterion. Kass and Wasserman list a\\ndozen more possibilities.\\n\\n(cid:142)2 [p. 236] Coverage matching priors. Welch and Peers (1963) showed that,\\nfor a multiparameter family f(cid:22).x/ and real-valued parameter of interest\\n(cid:18) D t .(cid:22)/, there exist priors g.(cid:22)/ such that the Bayes credible interval of\\ncoverage Àõ has frequentist coverage Àõ CO.1=n/, with n the sample size. In\\nother words, the credible intervals are ‚Äúsecond-order accurate‚Äù conÔ¨Ådence\\nintervals. Tibshirani (1989), building on Stein‚Äôs (1985) work, produced the\\n\\n\\x0c262\\n\\nObjective Bayes Inference and MCMC\\n\\nnice formulation (13.9). Stein‚Äôs paper developed the least-favorable fam-\\nily, the one-parameter subfamily of f(cid:22).x/ that does not inappropriately in-\\ncrease the amount of Fisher information for estimating (cid:18). Cox and Reid‚Äôs\\n(1987) orthogonal parameters form (13.8) is formally equivalent to the\\nleast favorable family construction.\\n\\nLeast favorable family versions of reference priors and intrinsic priors\\nhave been proposed to avoid the difÔ¨Åculty with general-purpose uninfor-\\nmative priors seen in Figure 11.7. They do so, but at the price of requiring\\na different prior for each choice of (cid:18) D t.(cid:22)/‚Äîwhich begins to sound more\\nfrequentistic than Bayesian.\\n\\n(cid:142)3 [p. 238] Conjugate families theorem. Theorem 13.1, (13.16)‚Äì(13.18), is\\nrigorously derived in Diaconis and Ylvisaker (1979). Families other than\\n(13.14) have conjugate-like properties, but not the neat posterior expecta-\\ntion result (13.18).\\n\\n(cid:142)4 [p. 238] Poisson formula (13.20). This follows immediately from (13.14),\\n\\nusing Àõ D log.(cid:22)/,  .Àõ/ D (cid:22), and V .(cid:22)/ D (cid:22) for the Poisson.\\n\\n(cid:142)5 [p. 239] Inverse gamma and chi-square distributions. A G(cid:23) variate (13.22)\\nhas density (cid:22)(cid:23)(cid:0)1e(cid:0)(cid:22) =(cid:128).(cid:23)/. An inverse gamma variate 1=G(cid:23) has density\\n(cid:22)(cid:0).(cid:23)C1/ e(cid:0)1=(cid:22)=(cid:128).(cid:23)/, so\\n\\ngn0;x0.(cid:22)/ D c(cid:22)(cid:0).n0x0C2/e(cid:0)n0x0(cid:23)=(cid:22)\\n\\n(13.85)\\n\\nis the gamma conjugate density in Table 13.1. The gamma results can be\\nrestated in terms of chi-squared variates:\\n\\nxi (cid:24) (cid:22)\\n\\n(cid:31)2\\nm\\nm\\n\\nD (cid:22)\\n\\nGm=2\\nm=2\\n\\n(13.86)\\n\\nhas conjugate prior\\n\\ngn0;x0.(cid:22)/ (cid:24) n0x0m=(cid:31)2\\n\\nn0mC2;\\n\\n(13.87)\\n\\nan inverse chi-squared distribution.\\n\\n(cid:142)6 [p. 240] Vasoconstriction data. Efron and Gous (2001) use this data to illus-\\ntrate a theory connecting Bayes factors with Fisherian hypothesis testing.\\nIt is part of a larger data set appearing in Finney (1947), also discussed in\\nKass and Raftery (1995).\\n\\n(cid:142)7 [p. 245] Jeffreys‚Äô and Fisher‚Äôs scales of evidence. Jeffreys‚Äô scale as it ap-\\npears in Table 13.3 is taken from the slightly amended form in Kass and\\nRaftery (1995). Efron and Gous (2001) compare it with Fisher‚Äôs scale for\\nthe contradictory results of Table 13.5. Fisher and Jeffreys worked in dif-\\nferent scientiÔ¨Åc contexts‚Äîsmall-sample agricultural experiments versus\\n\\n\\x0c13.6 Notes and Details\\n\\n263\\n\\nhard-science geostatistics‚Äîwhich might explain Jeffreys‚Äô more stringent\\nconception of what constitutes signiÔ¨Åcant evidence.\\n\\n(cid:142)8 [p. 246] The Bayesian information criterion. The BIC was proposed by\\nSchwarz (1978). Kass and Wasserman (1996) provide an extended discus-\\nsion of the BIC and model selection. ‚ÄúProofs‚Äù of (13.37) ultimately depend\\non sample size coherency (13.49), as in Efron and Gous (2001). Quotation\\nmarks are used here to indicate the basically qualitative nature of BIC: if\\nwe think of the data points as being collected in pairs then n becomes n=2\\nin (13.38), etc., so it doesn‚Äôt pay to put too Ô¨Åne a point on the criterion.\\n(cid:142)9 [p. 256] MCMC convergence. Suppose we begin the MCMC random walk\\n(13.76)‚Äì(13.77) by choosing (cid:18) .1/ according to some arbitrary starting dis-\\ntribution p.1/. Let p.b/ be the distribution of (cid:18) .b/, obtained after b steps of\\nthe random walk. Markov chain theory says that, under certain broad con-\\nditions on Q.i; j /, p.b/ will converge to the target distribution p (13.75).\\nMoreover, the convergence is geometric in the L1 norm P jp.b/\\n(cid:0)pkj, suc-\\ncessive discrepancies eventually decreasing by a multiplicative factor. A\\nproof appears in Tanner and Wong (1987). Unfortunately, the factor won‚Äôt\\nbe known in most applications, and the actual convergence may be quite\\nslow.\\n\\nk\\n\\n(cid:142)10 [p. 258] Dirichlet distribution. The Dirichlet is a multivariate generaliza-\\ntion of the beta distribution (Section 5.1), typically used to represent prior\\ndistributions for the multinomial distribution. For x D .x1; x2; : : : ; xk/0,\\nwith xj 2 .0; 1/, P\\n\\nj xj D 1, the D.(cid:23)/ density is deÔ¨Åned as\\n\\nf(cid:23).x/ D 1\\nB.(cid:23)/\\n\\nk\\nY\\n\\nj D1\\n\\nx(cid:23)j (cid:0)1\\n\\nj\\n\\n;\\n\\n(13.88)\\n\\nwhere B.(cid:23)/ D Q\\n\\nj (cid:128).(cid:23)j /= (cid:128).P\\n\\nj (cid:23)j /.\\n\\n\\x0c14\\n\\nStatistical Inference and Methodology in the\\nPostwar Era\\n\\nThe fundamentals of statistical inference‚Äîfrequentist, Bayesian, Fisherian\\n‚Äîwere set in place by the end of the Ô¨Årst half of the twentieth century, as\\ndiscussed in Part I of this book. The postwar era witnessed a massive ex-\\npansion of statistical methodology, responding to the data-driven demands\\nof modern scientiÔ¨Åc technology. We are now at the end of Part II, ‚ÄúEarly\\nComputer-Age Methods,‚Äù having surveyed the march of new statistical\\nalgorithms and their inferential justiÔ¨Åcation from the 1950s through the\\n1990s.\\n\\nThis was a time of opportunity for the discipline of statistics, when the\\nspeed of computation increased by a factor of a thousand, and then another\\nthousand. As we said before, a land bridge had opened to a new continent,\\nbut not everyone was eager to cross. We saw a mixed picture: the computer\\nplayed a minor or negligible role in the development of some inÔ¨Çuential\\ntopics such as empirical Bayes, but was fundamental to others such as the\\nbootstrap.\\n\\nFifteen major topics were examined in Chapters 6 through 13. What fol-\\nlows is a short scorecard of their inferential afÔ¨Ånities, Bayesian, frequentist,\\nor Fisherian, as well as an assessment of the computer‚Äôs role in their devel-\\nopment. None of this is very precise, but the overall picture, illustrated in\\nFigure 14.1, is evocative.\\n\\nEmpirical Bayes\\n\\nRobbins‚Äô original development of formula (6.5) was frequentistic, but most\\nstatistical researchers were frequentists in the postwar era so that could be\\nexpected. The obvious Bayesian component of empirical Bayes arguments\\nis balanced by their frequentist emphasis on (nearly) unbiased estimation\\nof Bayesian estimators, as well as the restriction to using only current data\\nfor inference. Electronic computation played hardly any role in the theory‚Äôs\\ndevelopment (as indicated by blue coloring in the Ô¨Ågure). Of course mod-\\n\\n264\\n\\n\\x0cPostwar Inference and Methodology\\n\\n265\\n\\nFigure 14.1 Bayesian, frequentist, and Fisherian inÔ¨Çuences, as\\ndescribed in the text, on 15 major topics, 1950s through 1990s.\\nColors indicate the importance of electronic computation in their\\ndevelopment: red, crucial; violet, very important; green,\\nimportant; light blue, less important; blue, negligible.\\n\\nern empirical Bayes applications are heavily computational, but that is the\\ncase for most methods now.\\n\\nJames‚ÄìStein and Ridge Regression\\n\\nThe frequentist roots of James‚ÄìStein estimation are more deÔ¨Ånitive, es-\\npecially given the force of the James‚ÄìStein theorem (7.16). Nevertheless,\\nthe empirical Bayes interpretation (7.13) lends James‚ÄìStein some Bayes-\\nian credibility. Electronic computation played no role in its development.\\nThis was less true for ridge regression, colored light blue in the Ô¨Ågure,\\nwhere the matrix calculation (7.36) would have been daunting in the pre-\\nelectronic age. The Bayesian justiÔ¨Åcation (7.37)‚Äì(7.39) of ridge regression\\n\\n lllBayesianFrequentistFisherianlKaplan‚àíMeierllog‚àíranklglmlproportional hazards(partial likelihood)lbootstraplempiricalBayeslobjectiveBayes(mcmc)ljackknifelCVlJames‚àíSteinlregression treeslridgeregressionlBIClmissing data(EM)lAIC‚àíCp\\x0c266\\n\\nPostwar Inference and Methodology\\n\\ncarries more weight than for James‚ÄìStein, given the absence of a strong\\nfrequentist theorem.\\n\\nGeneralized Linear Models\\n\\nGLM development began with a pronounced Fisherian emphasis on like-\\nlihood1 modeling, but settled down to more or less standard frequentist\\nregression theory. A key operational feature, low-dimensional sufÔ¨Åcient\\nstatistics, limited its computational demands, but GLM theory could not\\nhave developed before the age of electronic computers (as indicated by\\ngreen coloring).\\n\\nRegression Trees\\n\\nModel building by means of regression trees is a computationally intensive\\nenterprise, indicated by its red color in Figure 14.1. Its justiÔ¨Åcation has\\nbeen mainly in terms of asymptotic frequentist properties.\\n\\nSurvival Analysis\\n\\nThe Kaplan‚ÄìMeier estimate, log-rank test, and proportional hazards model\\nmove from the frequentist pole of the diagram toward the Fisherian pole\\nas the conditioning arguments in Sections 9.2 through 9.4 become more\\nelaborate. The role of computation in their development increases in the\\nsame order. Kaplan‚ÄìMeier estimates can be done by hand (and were),\\nwhile it is impossible to contemplate proportional hazards analysis with-\\nout the computer. Partial likelihood, the enabling argument for the theory,\\nis a quintessential Fisherian device.\\n\\nMissing Data and the EM Algorithm\\nThe imputation of missing data has a Bayesian Ô¨Çavor of indirect evidence,\\nbut the ‚Äúfake data‚Äù principle (9.44)‚Äì(9.46) has Fisherian roots. Fast compu-\\ntation was important to the method‚Äôs development, particularly so for the\\nEM algorithm.\\n\\nJackknife and Bootstrap\\n\\nThe purpose of the jackknife was to calculate frequentist standard errors\\nand biases. Electronic computation was of only minor importance in its\\n\\n1 More explicitly, quasilikelihoods, an extension to a wider class of exponential family\\n\\nmodels.\\n\\n\\x0cPostwar Inference and Methodology\\n\\n267\\n\\ndevelopment. By contrast, the bootstrap is the archetype for computer-\\nintensive statistical inference. It combines frequentism with Fisherian de-\\nvices: plug-in estimation of accuracy estimates, as in (10.18)‚Äì(10.19), and\\ncorrectness arguments for bootstrap conÔ¨Ådence intervals, (11.79)‚Äì(11.83).\\n\\nCross-Validation\\nThe renaissance of interest in cross-validation required fast computation,\\nespecially for assessing modern computer-intensive prediction algorithms.\\nAs pointed out in the text following Figure (12.3), cross-validation is a\\nstrongly frequentist procedure.\\n\\nBIC, AIC, and Cp\\nThese three algorithms were designed to avoid computation, BIC for Bayes-\\nian model selection, Section (13.3), AIC and Cp for unbiased estimation\\nof frequentist prediction error, (12.76) and (12.50).\\n\\nObjective Bayes and MCMC\\nIn addition to their Bayesian provenance, objective Bayes methods have\\nsome connection with Ô¨Åducial ideas and the bootstrap, as discussed in Sec-\\ntion 11.5. (An argument can be made that they are at least as frequentist as\\nthey are Bayesian‚Äîsee the notes below‚Äîthough that has not been acted\\nupon in coloring the Ô¨Ågure.) Gibbs sampling and MCMC, the enabling al-\\ngorithms, epitomize modern computer-intensive inference.\\n\\nNotes\\n\\nFigure 14.1 is an updated version of Figure 8 in Efron (1998), ‚ÄúR. A. Fisher\\nin the 21st Century.‚Äù There the difÔ¨Åculty of properly placing objective\\nBayes is confessed, with Erich Lehmann arguing for a more frequentist\\n(decision-theoretic) location: ‚ÄúIn fact, the concept of uninformative prior\\nis philosophically close to Wald‚Äôs least favorable distribution, and the two\\noften coincide.‚Äù\\n\\nFigure 14.1 shows a healthy mixture of philosophical and computational\\ntactics at work, with all three edges (but not the center) of the triangle in\\nplay. All new points will be red (computer-intensive) as we move into the\\ntwenty-Ô¨Årst century in Part III. Our triangle will have to struggle to accom-\\nmodate some major developments based on machine learning, a philosoph-\\nically atheistic approach to statistical inference.\\n\\n\\x0c\\x0cPart III\\n\\nTwenty-First-Century Topics\\n\\n\\x0c\\x0c15\\n\\nLarge-Scale Hypothesis Testing and\\nFalse-Discovery Rates\\n\\nBy the Ô¨Ånal decade of the twentieth century, electronic computation fully\\ndominated statistical practice. Almost all applications, classical or other-\\nwise, were now performed on a suite of computer platforms: SAS, SPSS,\\nMinitab, Matlab, S (later R), and others.\\n\\nThe trend accelerates when we enter the twenty-Ô¨Årst century, as statis-\\ntical methodology struggles, most often successfully, to keep up with the\\nvastly expanding pace of scientiÔ¨Åc data production. This has been a two-\\nway game of pursuit, with statistical algorithms chasing ever larger data\\nsets, while inferential analysis labors to rationalize the algorithms.\\n\\nPart III of our book concerns topics in twenty-Ô¨Årst-century1 statistics.\\nThe word ‚Äútopics‚Äù is intended to signal selections made from a wide cat-\\nalog of possibilities. Part II was able to review a large portion (though\\ncertainly not all) of the important developments during the postwar period.\\nNow, deprived of the advantage of hindsight, our survey will be more illus-\\ntrative than deÔ¨Ånitive.\\n\\nFor many statisticians, microarrays provided an introduction to large-\\nscale data analysis. These were revolutionary biomedical devices that en-\\nabled the assessment of individual activity for thousands of genes at once‚Äî\\nand, in doing so, raised the need to carry out thousands of simultaneous\\nhypothesis tests, done with the prospect of Ô¨Ånding only a few interesting\\ngenes among a haystack of null cases. This chapter concerns large-scale\\nhypothesis testing and the false-discovery rate, the breakthrough in statis-\\ntical inference it elicited.\\n\\n1 Actually what historians might call ‚Äúthe long twenty-Ô¨Årst century‚Äù since we will begin\\n\\nin 1995.\\n\\n271\\n\\n\\x0c272\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\n15.1 Large-Scale Testing\\n\\nThe prostate cancer data, Figure 3.4, came from a microarray study of\\nn D 102 men, 52 prostate cancer patients and 50 normal controls. Each\\nman‚Äôs gene expression levels were measured on a panel of N D 6033\\ngenes, yielding a 6033 (cid:2) 102 matrix of measurements xij ,\\nxij D activity of ith gene for j th man:\\n\\n(15.1)\\n\\nFor each gene, a two-sample t statistic (2.17) ti was computed com-\\nparing gene i‚Äôs expression levels for the 52 patients with those for the 50\\ncontrols. Under the null hypothesis H0i that the patients‚Äô and the controls‚Äô\\nresponses come from the same normal distribution of gene i expression\\nlevels, ti will follow a standard Student t distribution with 100 degrees of\\nfreedom, t100. The transformation\\n\\nzi D ÀÜ(cid:0)1 .F100.ti // ;\\nwhere F100 is the cdf of a t100 distribution and ÀÜ(cid:0)1 the inverse function of\\na standard normal cdf, makes zi standard normal under the null hypothesis:\\n\\n(15.2)\\n\\nH0i W zi (cid:24)\\n\\n.0; 1/:\\n\\n(15.3)\\n\\nN\\nOf course the investigators were hoping to spot some non-null genes,\\nones for which the patients and controls respond differently. It can be\\nshown that a reasonable model for both null and non-null genes is2 (cid:142)\\n\\n(cid:142)1\\n\\nzi (cid:24)\\n\\n.(cid:22)i ; 1/;\\n\\n(15.4)\\n\\nN\\n(cid:22)i being the effect size for gene i. Null genes have (cid:22)i D 0, while the\\ninvestigators hoped to Ô¨Ånd genes with large positive or negative (cid:22)i effects.\\nFigure 15.1 shows the histogram of the 6033 zi values. The red curve is\\n.0; 1/ density that would apply if in fact all of the genes were\\nthe scaled\\nnull, that is if all of the (cid:22)i equaled zero.3 We can see that the curve is a\\nlittle too high near the center and too low in the tails. Good! Even though\\nmost of the genes appear null, the discrepancies from the curve suggest that\\nthere are some non-null cases, the kind the investigators hoped to Ô¨Ånd.\\n\\nN\\n\\nLarge-scale testing refers exactly to this situation: having observed a\\nlarge number N of test statistics, how should we decide which if any of the\\nnull hypotheses to reject? Classical testing theory involved only a single\\ncase, N D 1. A theory of multiple testing arose in the 1960s, ‚Äúmultiple‚Äù\\n\\n2 This is model (3.28), with zi now replacing the notation xi .\\n3 It is ce(cid:0)z2=2=\\nthe histogram.\\n\\np\\n\\n2(cid:25) with c chosen to make the area under the curve equal the area of\\n\\n\\x0c15.1 Large-Scale Testing\\n\\n273\\n\\nFigure 15.1 Histogram of N D 6033 z-values, one for each gene\\nin the prostate cancer study. If all genes were null (15.3) the\\nhistogram would track the red curve. For which genes can we\\nreject the null hypothesis?\\n\\nmeaning N between 2 and perhaps 20. The microarray era produced data\\nsets with N in the hundreds, thousands, and now even millions. This sounds\\nlike piling difÔ¨Åculty upon difÔ¨Åculty, but in fact there are some inferential\\nadvantages to the large-N framework, as we will see.\\n\\nThe most troubling fact about large-scale testing is how easy it is to be\\nfooled. Running 100 separate hypothesis tests at signiÔ¨Åcance level 0.05\\nwill produce about Ô¨Åve ‚ÄúsigniÔ¨Åcant‚Äù results even if each case is actually\\nnull. The classical Bonferroni bound avoids this fallacy by strengthening\\nthe threshold of evidence required to declare an individual case signiÔ¨Åcant\\n(i.e., non-null). For an overall signiÔ¨Åcance level Àõ, perhaps Àõ D 0:05, with\\nN simultaneous tests, the Bonferroni bound rejects the ith null hypothesis\\nH0i only if it attains individual signiÔ¨Åcance level Àõ=N . For Àõ D 0:05,\\nN D 6033, and H0i W zi (cid:24)\\n.0; 1/, the one-sided Bonferroni threshold\\nfor signiÔ¨Åcance is (cid:0)ÀÜ(cid:0)1.0:05=N / D 4:31 (compared with 1.645 for N D\\n1). Only four of the prostate study genes surpass this threshold.\\n\\nN\\n\\nClassic hypothesis testing is usually phrased in terms of signiÔ¨Åcance lev-\\nels and p-values. If test statistic z has cdf F0.z/ under the null hypothesis\\n\\n‚àí4‚àí20240100200300400500z‚àívaluesCounts\\x0c274\\n\\nthen4\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\np D 1 (cid:0) F0.z/\\n\\n(15.5)\\n\\nis the right-sided p-value, larger z giving smaller p-value. ‚ÄúSigniÔ¨Åcance\\nlevel‚Äù refers to a prechosen threshold value, e.g., Àõ D 0:05. The null\\nhypothesis is ‚Äúrejected at level Àõ‚Äù if we observe p (cid:20) Àõ. Table 13.4 on\\npage 245 (where ‚Äúcoverage level‚Äù means one minus the signiÔ¨Åcance level)\\nshows Fisher‚Äôs scale for interpreting p-values.\\n\\nA level-Àõ test for a single null hypothesis H0 satisÔ¨Åes, by deÔ¨Ånition,\\n\\nÀõ D Prfreject true H0g:\\n\\n(15.6)\\n\\nFor a collection of N null hypotheses H0i , the family-wise error rate is the\\nprobability of making even one false rejection,\\n\\nFWER D Prfreject any true H0i g:\\n\\n(15.7)\\n\\nBonferroni‚Äôs procedure controls FWER at level Àõ: let I0 be the indices of\\nthe true H0i , having say N0 members. Then\\n8\\n<\\n\\n9\\n=\\n\\n(cid:17)\\n\\no\\n\\nFWER D Pr\\n\\n(cid:16)\\npi (cid:20) Àõ\\nN\\n\\n[\\n\\nI0\\n\\n(cid:20) X\\nI0\\n\\n;\\n\\nPr\\n\\nn\\npi (cid:20) Àõ\\nN\\n\\n(15.8)\\n\\n:\\nÀõ\\nN\\n\\nD N0\\n\\n(cid:20) Àõ;\\n\\nthe top line following from Boole‚Äôs inequality (which doesn‚Äôt require even\\nindependence among the pi ).\\n\\nThe Bonferroni bound is quite conservative: for N D 6033 and Àõ D\\n0:05 we reject only those cases having pi (cid:20) 8:3 (cid:1) 10(cid:0)6. One can do only a\\nlittle better under the FWER constraint. ‚ÄúHolm‚Äôs procedure,‚Äù(cid:142)which offers\\nmodest improvement over Bonferroni, goes as follows.\\n\\n(cid:142)2\\n\\n(cid:15) Order the observed p-values from smallest to largest,\\n\\np.1/ (cid:20) p.2/ (cid:20) p.3/ (cid:20) : : : (cid:20) p.i/ (cid:20) : : : (cid:20) p.N /;\\n\\n(15.9)\\n\\nwith H0.i/ denoting the corresponding null hypotheses.\\n\\n(cid:15) Let i0 be the smallest index i such that\\n\\np.i/ > Àõ=.N (cid:0) i C 1/:\\n\\n(15.10)\\n\\n(cid:15) Reject all null hypotheses H0.i/ for i < i0 and accept all with i (cid:21) i0.\\n\\n4 The left-sided p-value is p D F0.z/. We will avoid two-sided p-values in this\\n\\ndiscussion.\\n\\n\\x0c15.2 False-Discovery Rates\\n\\n275\\n\\nIt can be shown that Holm‚Äôs procedure controls FWER at level Àõ, while\\nbeing slightly more generous than Bonferroni in declaring rejections.\\n\\n15.2 False-Discovery Rates\\n\\nThe FWER criterion aims to control the probability of making even one\\nfalse rejection among N simultaneous hypothesis tests. Originally devel-\\noped for small-scale testing, say N (cid:20) 20, FWER usually proved too con-\\nservative for scientists working with N in the thousands. A quite different\\nand more liberal criterion, false-discovery rate (FDR) control, has become\\nstandard.\\n\\nFigure 15.2 A decision rule\\nD\\nhypotheses; a of these decisions were incorrect, i.e., they were\\n‚Äúfalse discoveries,‚Äù while b of them were ‚Äútrue discoveries.‚Äù The\\nfalse-discovery proportion Fdp equals a=R.\\n\\nhas rejected R out of N null\\n\\nFigure 15.2 diagrams the outcome of a hypothetical decision rule\\n\\nap-\\nplied to the data for N simultaneous hypothesis-testing problems, N0 null\\nand N1 D N (cid:0) N0 non-null. An omniscient oracle has reported the rule‚Äôs\\nresults: R null hypotheses have been rejected; a of these were cases of\\nfalse discovery, i.e., valid null hypotheses, for a ‚Äúfalse-discovery propor-\\ntion‚Äù (Fdp) of\\n\\nD\\n\\nFdp.\\n\\n/ D a=R:\\n\\n(15.11)\\n\\nD\\n\\n(We deÔ¨Åne Fdp D 0 if R D 0.) Fdp is unobservable‚Äîwithout the oracle\\nwe cannot see a‚Äîbut under certain assumptions we can control its expec-\\ntation.\\n\\n4\\t\\n \\xa0Null Actual Non-Null Null N0 - a Non-Null a b N1 - b N - R R N N1 N0 Decision \\x0c276\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\nDeÔ¨Åne\\n\\nA decision rule\\ntween 0 and 1, if\\n\\nD\\n\\nFDR.\\n\\n/ D E fFdp.\\n\\n/g :\\n\\nD\\ncontrols FDR at level q, with q a prechosen value be-\\n\\nD\\n\\n(15.12)\\n\\nFDR.\\n\\n/ (cid:20) q:\\n\\n(15.13)\\n\\nD\\nIt might seem difÔ¨Åcult to Ô¨Ånd such a rule, but in fact a quite simple but in-\\ngenious recipe does the job. Ordering the observed p-values from smallest\\nto largest as in (15.9), deÔ¨Åne imax to be the largest index for which\\n\\np.i/ (cid:20) i\\nN\\n\\nq;\\n\\n(15.14)\\n\\nq be the rule5 that rejects H0.i/ for i (cid:20) imax, accepting otherwise.\\n\\nand let\\nA proof of the following theorem is referenced in the chapter endnotes.(cid:142)\\n\\nD\\n\\n(cid:142)3\\n\\nTheorem (Benjamini‚ÄìHochberg FDR Control)\\ning to valid null hypotheses are independent of each other, then\\n\\nIf the p-values correspond-\\n\\nFDR.\\n\\nq/ D (cid:25)0q (cid:20) q;\\n\\nD\\n\\nwhere (cid:25)0 D N0=N:\\n\\n(15.15)\\n\\nD\\n\\nIn other words\\n\\nq controls FDR at level (cid:25)0q. The null proportion (cid:25)0 is\\nunknown (though estimable), so the usual claim is that\\nq controls FDR at\\nlevel q. Not much is sacriÔ¨Åced: large-scale testing problems are most often\\nÔ¨Åshing expeditions in which most of the cases are null, putting (cid:25)0 near 1,\\nidentiÔ¨Åcation of a few non-null cases being the goal. The choice q D 0:1\\nis typical practice.\\n\\nD\\n\\nThe popularity of FDR control hinges on the fact that it is more generous\\nthan FWER in declaring signiÔ¨Åcance.6 Holm‚Äôs procedure (15.10) rejects\\nnull hypothesis H0.i/ if\\n\\np.i/ (cid:20) Threshold(Holm‚Äôs) D\\n\\nÀõ\\nN (cid:0) i C 1\\n\\n;\\n\\nwhile\\n\\nq (15.13) has threshold\\n\\nD\\n\\np.i/ (cid:20) Threshold(\\n\\nq) D q\\nN\\nD\\n\\ni:\\n\\n(15.16)\\n\\n(15.17)\\n\\n5 Sometimes denoted ‚ÄúBHq‚Äù after its inventors Benjamini and Hochberg; see the chapter\\n\\nendnotes.\\n\\n6 The classic term ‚ÄúsigniÔ¨Åcant‚Äù for a non-null identiÔ¨Åcation doesn‚Äôt seem quite right for\\nFDR control, especially given the Bayesian connections of Section 15.3, and we will\\nsometimes use ‚Äúinteresting‚Äù instead.\\n\\n\\x0c15.2 False-Discovery Rates\\n\\n277\\n\\nIn the usual range of interest, large N and small i, the ratio\\n\\nThreshold(\\n\\nq)\\n\\nD\\n\\nThreshold(Holm‚Äôs)\\n\\nD q\\nÀõ\\n\\n(cid:18)\\n1 (cid:0) i (cid:0) 1\\nN\\n\\n(cid:19)\\n\\ni\\n\\n(15.18)\\n\\nincreases almost linearly with i.\\n\\nFigure 15.3 Ordered p-values p.i/ D 1 (cid:0) ÀÜ.z.i// plotted versus\\ni for the 50 largest z-values from the prostate data in\\nq, q D 0:1)\\nFigure 15.1. The FDR control boundary (algorithm\\nrejects H0.i/ for the 28 smallest values p.i/, while Holm‚Äôs FWER\\nprocedure (Àõ D 0:1) rejects for only the 7 smallest values. (The\\nupward slope of Holm‚Äôs boundary (15.16) is too small to see\\nhere.)\\n\\nD\\n\\nFigure 15.3 illustrates the comparison for the right tail of the prostate\\ndata of Figure 15.1, with pi D 1 (cid:0) ÀÜ.zi / (15.3), (15.5), and Àõ D q D\\n0:1. The FDR procedure rejects H0.i/ for the 28 largest z-values (z.i/ (cid:21)\\n3:33), while FWER control rejects only the 7 most extreme z-values (z.i/ (cid:21)\\n4:14).\\n\\nHypothesis testing has been a traditional stronghold of frequentist deci-\\nsion theory, with ‚ÄúType 1‚Äù error control being strictly enforced, very often\\nat the 0.05 level. It is surprising that a new control criterion, FDR, has\\ntaken hold in large-scale testing situations. A critic, noting FDR‚Äôs relaxed\\nrejection standards in Figure 15.3, might raise some pointed questions.\\n\\n**************************************************010203040500e+005e‚àí041e‚àí03index ip‚àívalueHolm\\'sFDRi = 7i = 28\\x0c278\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\n1 Is controlling a rate (i.e., FDR) as meaningful as controlling a probabil-\\n\\nity (of Type 1 error)?\\n2 How should q be chosen?\\n3 The control theorem depends on independence among the p-values.\\n\\nIsn‚Äôt this unlikely in situations such as the prostate study?\\n\\n4 The FDR signiÔ¨Åcance for gene i0, say one with zi0\\n\\nD 3, depends on the\\nresults of all the other genes: the more ‚Äúother‚Äù zi values exceed 3, the\\nmore interesting gene i0 becomes (since that increases i0‚Äôs index i in\\nthe ordered list (15.9), making it more likely that pi0 lies below the\\nthreshold (15.14)). Does this make inferential sense?\\n\\nq\\nD\\n\\nA Bayes/empirical Bayes restatement of the\\nthese questions, as discussed next.\\n\\nD\\n\\nq algorithm helps answer\\n\\n15.3 Empirical Bayes Large-Scale Testing\\n\\nIn practice, single-case hypothesis testing has been a frequentist preserve.\\nIts methods demand little from the scientist‚Äîonly the choice of a test\\nstatistic and the calculation of its null distribution‚Äîwhile usually deliver-\\ning a clear verdict. By contrast, Bayesian model selection, whatever its in-\\nferential virtues, raises the kinds of difÔ¨Åcult modeling questions discussed\\nin Section 13.3.\\n\\nIt then comes as a pleasant surprise that things are different for large-\\nscale testing: Bayesian methods, at least in their empirical Bayes manifes-\\ntation, no longer demand heroic modeling efforts, and can help untangle\\nthe interpretation of simultaneous test results. This is particularly true for\\nthe FDR control algorithm\\n\\nq of the previous section.\\n\\nA simple Bayesian framework for simultaneous testing is provided by\\nthe two-groups model: each of the N cases (the genes for the prostate\\nstudy) is either null with prior probability (cid:25)0 or non-null with probabil-\\nity (cid:25)1 D 1 (cid:0) (cid:25)0; the resulting observation z then has density either f0.z/\\nor f1.z/,\\n\\nD\\n\\n(cid:25)0 D Prfnullg\\n(cid:25)1 D Prfnon-nullg\\n\\nf0.z/ density if null;\\nf1.z/ density if non-null:\\n\\n(15.19)\\n\\nFor the prostate study, (cid:25)0 is nearly 1, and f0.z/ is the standard normal den-\\nsity (cid:30).z/ D exp.(cid:0)z2=2/=\\n2(cid:25) (15.3), while the non-null density remains\\nto be estimated.\\n\\np\\n\\nLet F0.z/ and F1.z/ be the cdf values corresponding to f0.z/ and f1.z/,\\n\\n\\x0c15.3 Empirical Bayes Large-Scale Testing\\n\\n279\\n\\nwith ‚Äúsurvival curves‚Äù\\n\\nS0.z/ D 1 (cid:0) F0.z/\\n\\nand S1.z/ D 1 (cid:0) F1.z/;\\n\\n(15.20)\\n\\nS0.z0/ being the probability that a null z-value exceeds z0, and similarly\\nfor S1.z/. Finally, deÔ¨Åne S.z/ to be the mixture survival curve\\n\\nThe mixture density\\n\\ndetermines S.z/,\\n\\nS.z/ D (cid:25)0S0.z/ C (cid:25)1S1.z/:\\n\\nf .z/ D (cid:25)0f0.z/ C (cid:25)1f1.z/\\n\\nS.z0/ D\\n\\nZ 1\\n\\nz0\\n\\nf .z/ dz:\\n\\n(15.21)\\n\\n(15.22)\\n\\n(15.23)\\n\\nSuppose now that observation zi for case i is seen to exceed some thresh-\\n\\nold value z0, perhaps z0 D 3. Bayes‚Äô rule gives\\n\\nFdr.z0/ (cid:17) Prfcase i is nulljzi (cid:21) z0g\\nD (cid:25)0S0.z0/=S.z0/;\\nthe correspondence with (3.5) on page 23 being (cid:25)0 D g.(cid:22)/, S0.z0/ D\\nf(cid:22).x/, and S.z0/ D f .x/. Fdr is the ‚ÄúBayes false-discovery rate,‚Äù as con-\\ntrasted with the frequentist quantity FDR (15.12).\\n\\n(15.24)\\n\\nIn typical applications, S0.z0/ is assumed known7 (equaling 1 (cid:0) ÀÜ.z0/\\nin the prostate study), and (cid:25)0 is assumed to be near 1. The denominator\\nS.z0/ in (15.24) is unknown, but‚Äîand this is the crucial point‚Äîit has an\\nobvious estimate in large-scale testing situations, namely\\n\\nOS.z0/ D N.z0/=N;\\n\\nwhere N.z0/ D #fzi (cid:21) z0g:\\n\\n(15.25)\\n\\n(By the deÔ¨Ånition of the two-group model, each zi has marginal density\\nf .z/, making OS.z0/ the usual empirical estimate of S.z0/ (15.23).) Plug-\\nging into (15.24) yields an empirical Bayes estimate of the Bayes false-\\ndiscovery rate\\n\\ncFdr.z0/ D (cid:25)0S0.z0/ƒ± OS.z0/:\\nThe connection with FDR control is almost immediate. First of all, from\\ndeÔ¨Ånitions (15.5) and (15.20) we have pi D S0.zi /; also for the ith from\\nthe largest z-value we have OS.z.i// D i=N (15.25). Putting these together,\\ncondition (15.14), p.i/ (cid:20) .i=N /q, becomes\\n\\n(15.26)\\n\\nS0.z.i// (cid:20) OS.z.i// (cid:1) q;\\n\\n(15.27)\\n\\n7 But see Section 15.5.\\n\\n\\x0cLarge-scale Hypothesis Testing and FDRs\\n\\n280\\nor S0.z.i//= OS.z.i// (cid:20) q, which can be written as\\n\\ncFdr.z.i// (cid:20) (cid:25)0q\\n\\n(15.28)\\n\\nD\\n\\nq algorithm, which rejects those null hy-\\n(15.26). In other words, the\\npotheses having8 p.i/ (cid:20) .i=N /q, is in fact rejecting those cases for which\\nthe empirical Bayes posterior probability of nullness is too small, as de-\\nÔ¨Åned by (15.28). The Bayesian nature of FDR control offers a clear advan-\\ntage to the investigating scientist, who gets a numerical assessment of the\\nprobability that he or she will be wasting time following up any one of the\\nselected cases.\\n\\nWe can now respond to the four questions at the end of the previous\\n\\nsection:\\n\\n1 FDR control does relate to a probability‚Äîthe Bayes posterior probabil-\\n\\nity of nullness.\\n2 The choice of q for\\n\\nq amounts to setting the maximum tolerable amount\\n\\nD\\n\\nof Bayes risk of nullness9 (usually after taking (cid:25)0 D 1 in (15.28)).\\n3 Most often the zi , and hence the pi , will be correlated with each other.\\nEven under correlation, however, OS.z0/ in (15.25) is still unbiased for\\nS.z0/, making cFdr.z0/ (15.26) nearly unbiased for Fdr.z0/ (15.24). There\\nis a price to be paid for correlation, which increases the variance of\\nS0.z0/ and cFdr.z0/.\\n\\n4 In the Bayes two-groups model (15.19), all of the non-null zi are i.i.d.\\nobservations from the non-null density f1.z/, with survival curve S1.z/.\\nThe number of null cases zi exceeding some threshold z0 has Ô¨Åxed ex-\\npectation N (cid:25)0S0.z0/. Therefore an increase in the number of observed\\nvalues zi exceeding z0 must come from a heavier right tail for f1.z/,\\nimplying a greater posterior probability of non-nullness Fdr.z0/ (15.24).\\nThis point is made more clearly in the local false-discovery framework\\nof the next section. It emphasizes the ‚Äúlearning from the experience of\\nothers‚Äù aspect of empirical Bayes inference, Section 7.4. The question\\nof ‚ÄúWhich others?‚Äù is returned to in Section 15.6.\\n\\nFigure 15.4 illustrates the two-group model (15.19). The N cases are\\n\\n8 The algorithm, as stated just before the FDR control theorem (15.15), is actually a little\\n\\nmore liberal in allowing rejections.\\n\\n9 For a case of particular interest, the calculation can be reversed: if the case has ordered\\n\\nindex i then, according to (15.14), the value q D Npi =i puts it exactly on the boundary\\nof rejection, making this its q-value. The 50th largest z-value for the prostate data has\\nzi D 2:99, pi D 0:00139, and q-value 0.168, that being both the frequentist\\nboundary for rejection and the empirical Bayes probability of nullness.\\n\\n\\x0c15.3 Empirical Bayes Large-Scale Testing\\n\\n281\\n\\nFigure 15.4 A diagram of the two-groups model (15.19). Here\\nthe statistician observes values zi from a mixture density\\nf .z/ D (cid:25)0f0.z/ C (cid:25)1f1.z/ and decides to reject or accept the\\nnull hypothesis H0i depending on whether zi exceeds or is less\\nthan the threshold value z0.\\n\\nrandomly dispatched to the two arms in proportions (cid:25)0 and (cid:25)1, at which\\npoint they produce z-values according to either f0.z/ or f1.z/. Suppose\\nthat rejects the ith null hypothesis if\\nwe are using a simple decision rule\\nzi exceeds some threshold z0, and accepts otherwise,\\n\\nD\\n\\n(\\n\\nReject H0i\\nAccept H0i\\n\\nW\\n\\nD\\n\\nif zi > z0\\nif zi (cid:20) z0:\\n\\n(15.29)\\n\\nThe oracle of Figure 15.2 knows that N0.z0/ D a of the null case z-\\nvalues exceeded z0, and similarly N1.z0/ D b of the non-null cases, lead-\\ning to\\n\\nN.z0/ D N0.z0/ C N1.z0/ D R\\n\\n(15.30)\\n\\ntotal rejections. The false-discovery proportion (15.11) is\\n\\nFdp D N0.z0/\\nN.z0/\\n\\n(15.31)\\n\\nbut this is unobservable since we see only N.z0/.\\n\\nThe clever inferential strategy of false-discovery rate theory substitutes\\n\\nthe expectation of N0.z0/,\\n\\nE fN0.z0/g D N (cid:25)0S0.z0/;\\n\\n(15.32)\\n\\n\\x0c282\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\nfor N0.z0/ in (15.31), giving\\n\\ndFdp D N (cid:25)0S0.z0/\\n\\nN.z0/\\n\\nD (cid:25)0S0.z0/\\nOS.z0/\\n\\nD cFdr.z0/;\\n\\n(15.33)\\n\\nusing (15.25) and (15.26). Starting from the two-groups model, cFdr.z0/ is\\nan obvious empirical (i.e., frequentist) estimate of the Bayesian probability\\nFdr.z0/, as well as of Fdp.\\n\\nIf placed in the Bayes‚ÄìFisher‚Äìfrequentist triangle of Figure 14.1, false-\\ndiscovery rates would begin life near the frequentist corner but then mi-\\ngrate at least part of the way toward the Bayes corner. There are remark-\\nable parallels with the James‚ÄìStein estimator of Chapter 7. Both theories\\nbegan with a striking frequentist theorem, which was then inferentially\\nrationalized in empirical Bayes terms. Both rely on the use of indirect\\nevidence‚Äîlearning from the experience of others. The difference is that\\nJames‚ÄìStein estimation always aroused controversy, while FDR control\\nhas been quickly welcomed into the pantheon of widely used methods.\\nThis could reÔ¨Çect a change in twenty-Ô¨Årst-century attitudes or, perhaps,\\nq rule better conceals its Bayesian aspects.\\nonly that the\\n\\nD\\n\\n15.4 Local False-Discovery Rates\\n\\nTail-area statistics (p-values) were synonymous with classic one-at-a-time\\nq algorithm carried over p-value interpreta-\\nhypothesis testing, and the\\ntion to large-scale testing theory. But tail-area calculations are neither nec-\\nessary nor desirable from a Bayesian viewpoint, where, having observed\\ntest statistic zi equal to some value z0, we should be more interested in the\\nprobability of nullness given zi D z0 than given zi (cid:21) z0.\\nTo this end we deÔ¨Åne the local false-discovery rate\\n\\nD\\n\\nfdr.z0/ D Prfcase i is nulljzi D z0g\\n\\n(15.34)\\n\\nas opposed to the tail-area false-discovery rate Fdr.z0/ (15.24). The main\\npoint of what follows is that reasonably accurate empirical Bayes estimates\\nof fdr are available in large-scale testing problems.\\n\\nAs a Ô¨Årst try, suppose that\\n\\n0, a proposed region for rejecting null hy-\\n\\npotheses, is a small interval centered at z0,\\n\\nZ\\n\\n(cid:20)\\nz0 (cid:0) d\\n2\\n\\n; z0 C d\\n2\\n\\n(cid:21)\\n\\n;\\n\\n0 D\\n\\nZ\\n\\nwith d perhaps 0.1. We can redraw Figure 15.4, now with N0.\\n\\n(15.35)\\n\\n0/, N1.\\n\\n0/,\\n\\nZ\\n\\nZ\\n\\n\\x0c15.4 Local False-Discovery Rates\\n\\n283\\n\\nand N.\\nfalse-discovery proportion,\\n\\nZ\\n\\n0/ the null, non-null, and total number of z-values in\\n\\n0. The local\\n\\nZ\\n\\nfdp.z0/ D N0.\\n\\n0/=N.\\n\\n0/\\n\\n(15.36)\\n\\nZ\\nis unobservable, but we can replace N0.\\nimate expectation as in (15.31)‚Äì(15.33), yielding the estimate10\\n\\nZ\\n\\nZ\\n\\n0/ with N (cid:25)0f0.z0/d , its approx-\\n\\ncfdr.z0/ D N (cid:25)0f0.z0/d=N.\\n\\nZ\\n\\n0/:\\n\\n(15.37)\\n\\nEstimate (15.37) would be needlessly noisy in practice; z-value distri-\\nbutions tend to be smooth, allowing the use of regression estimates for\\nfdr.z0/. Bayes‚Äô theorem gives\\n\\nfdr.z/ D (cid:25)0f0.z/=f .z/\\n\\n(15.38)\\n\\nin the two-groups model (15.19) (with (cid:22) in (3.5) now the indicator of null\\nor non-null states, and x now z). Drawing a smooth curve Of .z/ through\\nthe histogram of the z-values yields the more efÔ¨Åcient estimate\\n\\ncfdr.z0/ D (cid:25)0f0.z0/= Of .z0/I\\nthe null proportion (cid:25)0 can be estimated‚Äîsee Section 15.5‚Äîor set equal to\\n1.\\n\\n(15.39)\\n\\nFigure 15.5 shows cfdr.z/ for the prostate study data of Figure 15.1,\\nwhere Of .z/ in (15.39) has been estimated as described below. The curve\\nhovers near 1 for the 93% of the cases having jzi j (cid:20) 2, sensibly suggesting\\nthat there is no involvement with prostate cancer for most genes. It declines\\nquickly for jzi j (cid:21) 3, reaching the conventionally ‚Äúinteresting‚Äù threshold\\ncfdr.z/ (cid:20) 0:2\\n(15.40)\\nfor zi (cid:21) 3:34 and zi (cid:20) (cid:0)3:40. This was attained for 27 genes in the right\\ntail and 25 in the left, these being reasonable candidates to Ô¨Çag for follow-\\nup investigation.\\n\\nThe curve Of .z/ used in (15.39) was obtained from a fourth-degree log\\npolynomial Poisson regression Ô¨Åt to the histogram in Figure 15.1, as in\\nFigure 10.5 (10.52)‚Äì(10.56). Log polynomials of degree 2 through 6 were\\nÔ¨Åt by maximum likelihood, giving total residual deviances (8.35) shown in\\nTable 15.1. An enormous improvement in Ô¨Åt is seen in going from degree\\n3 to 4, but nothing signiÔ¨Åcant after that, with decreases less than the null\\nvalue 2 suggested by (12.75).\\n\\n10 Equation (15.37) makes argument (4) of the previous section clearer: having more\\n\\n‚Äúother‚Äù z-values fall into Z0 increases N.Z0/, decreasing cfdr.z0/ and making it more\\nlikely that zi D z0 represents a non-null case.\\n\\n\\x0c284\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\nFigure 15.5 Local false-discovery rate estimate cfdr.z/ (15.39)\\nfor prostate study of Figure 15.1; 27 genes on the right and 25 on\\nthe left, indicated by dashes, have cfdr.zi / (cid:20) 0:2; light dashed\\ncurves are the left and right tail-area estimates cFdr.z/ (15.26).\\n\\nTable 15.1 Total residual deviances from log polynomial Poisson\\nregressions of the prostate data, for polynomial degrees 2 through 6;\\ndegree 4 is preferred.\\n\\nDegree\\nDeviance\\n\\n2\\n138.6\\n\\n3\\n137.0\\n\\n4\\n65.1\\n\\n5\\n64.1\\n\\n6\\n63.7\\n\\nThe points in Figure 15.6 represent the log bin counts from the histogram\\nin Figure 15.1 (excluding zero counts), with the solid curve showing the\\n4th-degree MLE polynomial Ô¨Åt. Also shown is the standard normal log\\ndensity\\n\\nlog f0.z/ D (cid:0) 1\\n2\\n\\nz2 C constant:\\n\\n(15.41)\\n\\nIt Ô¨Åts reasonably well for jzj < 2, emphasizing the null status of the gene\\nmajority.\\n\\nThe cutoff cfdr.z/ (cid:20) 0:2 for declaring a case interesting is not completely\\narbitrary. DeÔ¨Ånitions (15.38) and (15.22), and a little algebra, show that it\\n\\n‚àí4‚àí20240.00.20.40.60.81.0z‚àívaluefdr and Fdrlocal fdr‚àí3.403.34\\x0c15.4 Local False-Discovery Rates\\n\\n285\\n\\nFigure 15.6 Points are log bin counts for Figure 15.1‚Äôs\\nhistogram. The solid black curve is a fourth-degree\\nlog-polynomial Ô¨Åt used to calculate cfdr.z/ in Figure 15.5. The\\ndashed red curve, the log null density (15.41), provides a\\nreasonable Ô¨Åt for jzj (cid:20) 2.\\n\\nis equivalent to\\n\\nf1.z/\\nf0.z/\\n\\n(cid:21) 4\\n\\n(cid:25)0\\n(cid:25)1\\n\\n:\\n\\n(15.42)\\n\\nIf we assume (cid:25)0 (cid:21) 0:90, as is reasonable in most large-scale testing situa-\\ntions, this makes the Bayes factor f1.z/=f0.z/ quite large,\\n\\nf1.z/\\nf0.z/\\n\\n(cid:21) 36;\\n\\n(15.43)\\n\\n‚Äústrong evidence‚Äù against the null hypothesis in Jeffreys‚Äô scale, Table 13.3.\\nThere is a simple relation between the local and tail-area false-discovery\\n\\nrates:(cid:142)\\n\\n(cid:142)4\\n\\nFdr.z0/ D E ffdr.z/jz (cid:21) z0g I\\n\\n(15.44)\\n\\nso Fdr.z0/ is the average value of fdr.z/ for z greater than z0. In interesting\\nsituations, fdr.z/ will be a decreasing function for large values of z, as on\\nthe right side of Figure 15.5, making Fdr.z0/ < fdr.z0/. This accounts\\n\\n‚àí4‚àí202460123456z‚àívaluelog densityllllllllllllllllllllllllllllllllllllllllllllll4th degree logpolynomialN(0,1)\\x0c286\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\n(cid:142)5\\n\\nfor the conventional signiÔ¨Åcance cutoff cFdr.z/ (cid:20) 0:1 being smaller than\\ncfdr.z/ (cid:20) 0:2 (15.40).(cid:142)\\n\\nThe Bayesian interpretation of local false-discovery rates carries with it\\nthe advantages of Bayesian coherency. We don‚Äôt have to change deÔ¨Ånitions\\nas with left-sided and right-sided tail-area cFdr estimates, since cfdr.z/ ap-\\nplies without change to both tails.11 Also, we don‚Äôt need a separate theory\\nfor ‚Äútrue-discovery rates,‚Äù since\\n\\ntdr.z0/ (cid:17) 1 (cid:0) fdr.z0/ D (cid:25)1f1.z0/=f .z0/\\n\\n(15.45)\\n\\nis the conditional probability that case i is non-null given zi D z0.\\n\\n15.5 Choice of the Null Distribution\\n\\nThe null distribution, f0.z/ in the two-groups model (15.19), plays a cru-\\ncial role in large-scale testing, just as it does in the classic single-case the-\\nory. Something different however happens in large-scale problems: with\\nthousands of z-values to examine at once, it can become clear that the con-\\nventional theoretical null is inappropriate for the situation at hand. Put more\\npositively, large-scale applications may allow us to empirically determine\\na more realistic null distribution.\\n\\nThe police data of Figure 15.7 illustrates what can happen. Possi-\\nble racial bias in pedestrian stops was assessed for N D 2749 New York\\nCity police ofÔ¨Åcers in 2006. Each ofÔ¨Åcer was assigned a score zi , large\\npositive scores suggesting racial bias. The zi values were summary scores\\nfrom a complicated logistic regression model intended to compensate for\\ndifferences in the time of day, location, and context of the stops. Logistic\\nregression theory suggested the theoretical null distribution\\n\\nH0i W zi (cid:24)\\n\\n.0; 1/\\n\\nN\\n\\n(15.46)\\n\\nfor the absence of racial bias.\\n\\nThe trouble is that the center of the z-value histogram in Figure 15.7,\\n.0; 1/ curve applying to the presumably large\\nwhich should track the\\nfraction of null-case ofÔ¨Åcers, is much too wide. (Unlike the situation for the\\nprostate data in Figure 15.1.) An MLE Ô¨Åtting algorithm discussed below\\nproduced the empirical null\\n\\nN\\n\\nH0i W zi (cid:24)\\n\\n.0:10; 1:402/\\n\\n(15.47)\\n\\nN\\n11 Going further, z in the two-groups model could be multidimensional. Then tail-area\\nfalse-discovery rates would be unavailable, but (15.38) would still legitimately deÔ¨Åne\\nfdr.z/.\\n\\n\\x0c15.5 Choice of the Null Distribution\\n\\n287\\n\\nFigure 15.7 Police data; histogram of z scores for N D 2749\\nNew York City police ofÔ¨Åcers, with large zi suggesting racial\\nbias. The center of the histogram is too wide compared with the\\ntheoretical null distribution zi (cid:24)\\n.0; 1/. An MLE Ô¨Åt to central\\ndata gave\\n\\n.0:10; 1:402/ as empirical null.\\n\\nN\\n\\nN\\n\\nas appropriate here. This is reinforced by a QQ plot of the zi values shown\\nin Figure 15.8, where we see most of the cases falling nicely along a\\n\\n.0:09; 1:422/ line, with just a few outliers at both extremes.\\nN\\nThere is a lot at stake here. Based on the empirical null (15.47) only\\nfour ofÔ¨Åcers reached the ‚Äúprobably racially biased‚Äù cutoff cfdr.zi / (cid:20) 0:2,\\nthe four circled points at the far right of Figure 15.8; the Ô¨Åfth point had\\ncfdr D 0:38 while all the others exceeded 0.80. The theoretical\\n.0; 1/\\nN\\nnull was much more severe, assigning cfdr (cid:20) 0:2 to the 125 ofÔ¨Åcers having\\nzi (cid:21) 2:50. One can imagine the difference in newspaper headlines.\\n\\nFrom a classical point of view it seems heretical to question the theo-\\nretical null distribution, especially since there is no substitute available in\\nsingle-case testing. Once alerted by data sets like the police study, however,\\nit is easy to list reasons for doubt:\\n\\n(cid:15) Asymptotics Taylor series approximations go into theoretical null calcu-\\nlations such as (15.46), which can lead to inaccuracies, particularly in the\\ncrucial tails of the null distribution.\\n\\n(cid:15) Correlations False-discovery rate methods are correct on the average,\\n\\nz‚àívaluesFrequency‚àí6‚àí4‚àí20246050100150200N(0,1)N(0.1,1.402)\\x0c288\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\nFigure 15.8 QQ plot of police data z scores; most scores closely\\nfollow the\\nThe circled points are cases having local false-discovery estimate\\ncfdr.zi / (cid:20) 0:2, based on the empirical null. Using the theoretical\\n\\n.0:09; 1:422/ line with a few outliers at either end.\\n\\nN\\n\\n.0; 1/ null gives 216 cases with cfdr.zi / (cid:20) 0:2, 91 on the left and\\n\\nN\\n125 on the right.\\n\\n(cid:142)6\\n\\neven with correlations among the N z-values. However, severe correlation\\ndestabilizes the z-value histogram, which can become randomly wider or\\nnarrower than theoretically predicted, undermining theoretical null results\\nfor the data set at hand.(cid:142)\\n\\n(cid:15) Unobserved covariates The police study was observational: individual\\nencounters were not assigned at random to the various ofÔ¨Åcers but simply\\nobserved as they happened. Observed covariates such as the time of day\\nand the neighborhood were included in the logistic regression model, but\\none can never rule out the possibility of inÔ¨Çuential unobserved covariates.\\n(cid:15) Effect size considerations The hypothesis-testing setup, where a large\\nfraction of the cases are truly null, may not be appropriate. An effect\\nsize model, with (cid:22)i (cid:24) g.(cid:1)/ and zi (cid:24)\\n.(cid:22)i ; 1/, might apply, with the\\nprior g.(cid:22)/ not having an atom at (cid:22) D 0. The nonatomic choice g.(cid:22)/ (cid:24)\\n\\nN\\n\\n.0:10; 0:632/ provides a good Ô¨Åt to the QQ plot in Figure 15.8.\\n\\nN\\n\\n*************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************‚àí3‚àí2‚àí10123‚àí10‚àí505Normal QuantilesSample Quantileslllllllllintercept = 0.089slope = 1.424\\x0c15.5 Choice of the Null Distribution\\n\\n289\\n\\nEmpirical Null Estimation\\n\\nOur point of view here is that the theoretical null (15.46), zi (cid:24)\\n.0; 1/, is\\nnot completely wrong but needs adjustment for the data set at hand. To this\\nend we assume the two-groups model (15.19), with f0.z/ normal but not\\nnecessarily\\n\\n.0; 1/, say\\n\\nN\\n\\nN\\n\\nf0.z/ (cid:24)\\n\\n.ƒ±0; (cid:27) 2\\n\\n0 /:\\n\\n(15.48)\\n\\nN\\nIn order to compute the local false-discovery rate fdr.z/ D (cid:25)0f0.z/=f .z/\\nwe want to estimate the three numerator parameters .ƒ±0; (cid:27)0; (cid:25)0/, the mean\\nand standard deviation of the null density and the proportion of null cases.\\n(The denominator f .z/ is estimated as in Section 15.4.)\\n\\nOur key assumptions (besides (15.48)) are that (cid:25)0 is large, say (cid:25)0 (cid:21)\\n0:90, and that most of the zi near 0 are null cases. The algorithm locfdr\\n(cid:142)begins by selecting a set\\nzi in\\nstated as\\n\\n0 are null; in terms of the two-groups model, the assumption can be\\n\\n0 near z D 0 in which it is assumed that all the (cid:142)7\\n\\nA\\n\\nA\\n\\nf1.z/ D 0 for z 2\\n\\n0:\\n\\nA\\n\\n(15.49)\\n\\nModest violations of (15.49), which are to be expected, produce small bi-\\nases in the empirical null estimates. Maximum likelihood based on the\\nnumber and values of the zi observed in\\n0 yield the empirical null es-\\ntimates(cid:142) . Oƒ±0; O(cid:27)0; O(cid:25)0/.\\n\\nA\\n\\n(cid:142)8\\n\\nApplied to the police data, locfdr chose\\n\\nduced estimates\\n\\nA\\n\\n0 D ≈í(cid:0)1:8; 2:0(cid:141) and pro-\\n\\n(cid:16) Oƒ±0; O(cid:27)0; O(cid:25)0\\n\\n(cid:17)\\n\\nD .0:10; 1:40; 0:989/:\\n\\n(15.50)\\n\\nTwo small simulation studies described in Table 15.2 give some idea of the\\nvariabilities and biases inherent in the locfdr estimation process.\\n\\nThe third method, somewhere between the theoretical and empirical null\\nestimates but closer to the former, relies on permutations. The vector z of\\n6033 z-values for the prostate data of Figure 15.1 was obtained from a\\nstudy of 102 men, 52 cancer patients and 50 controls. Randomly permuting\\nthe men‚Äôs data, that is randomly choosing 50 of the 102 to be ‚Äúcontrols‚Äù and\\nthe remaining 52 to be ‚Äúpatients,‚Äù and then carrying through steps (15.1)‚Äì\\n(15.2) gives a vector z(cid:3) in which any actual cancer/control differences have\\nbeen suppressed. A histogram of the z(cid:3)\\ni values (perhaps combining sev-\\neral permutations) provides the ‚Äúpermutation null.‚Äù Here we are extending\\nFisher‚Äôs original permutation idea, Section 4.4, to large-scale testing.\\n\\nTen permutations of the prostate study data produced an almost perfect\\n\\n\\x0c290\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\nTable 15.2 Means and standard deviations of . Oƒ±0; O(cid:27)0; O(cid:25)0/ for two\\nsimulation studies of empirical null estimation using locfdr. N D 5000\\ncases each trial with .ƒ±0; (cid:27)0; (cid:25)0/ as shown; 250 trials; two-groups model\\n(15.19) with non-null density f1.z/ equal to\\n\\n.3; 1/ (left side) or\\n\\nN\\n\\n.4:2; 1/ (right side).\\n\\nN\\n\\nƒ±0\\n\\n0\\n.015\\n.019\\n\\n(cid:27)0\\n\\n1.0\\n1.017\\n.017\\n\\n(cid:25)0\\n\\n.95\\n.962\\n.005\\n\\nƒ±0\\n\\n.10\\n.114\\n.025\\n\\n(cid:27)0\\n\\n1.40\\n1.418\\n.029\\n\\n(cid:25)0\\n\\n.95\\n.958\\n.006\\n\\ntrue\\nmean\\nst dev\\n\\n(cid:142)9\\n\\n.0; 1/ permutation null. (This is as expected from the classic theory of\\nN\\npermutation t -tests.) Permutation methods reliably overcome objection 1\\nto the theoretical null distribution, over-reliance on asymptotic approxima-\\ntions, but cannot cure objections 2, 3, and 4.(cid:142)\\n\\nWhatever the cause of disparity, the operational difference between the\\ntheoretical and empirical null distribution is clear: with the latter, the sig-\\nniÔ¨Åcance of an outlying case is judged relative to the dispersion of the\\nmajority, not by a theoretical yardstick as with the former. This was per-\\nsuasive for the police data, but the story isn‚Äôt one-sided. Estimating the null\\ndistribution adds substantially to the variability of cfdr or cFdr. For situations\\nsuch as the prostate data, when the theoretical null looks nearly correct,12\\nit is reasonable to stick with it.\\n\\nThe very large data sets of twenty-Ô¨Årst-century applications encourage\\nself-contained methodology that proceeds from just the data at hand using\\na minimum of theoretical constructs. False-discovery rate empirical Bayes\\nanalysis of large-scale testing problems, with data-based estimation of O(cid:25)0,\\nOf0, and Of , comes close to the ideal in this sense.\\n\\n15.6 Relevance\\n\\nFalse-discovery rates return us to the purview of indirect evidence, Sec-\\ntions 6.4 and 7.4. Our interest in any one gene in the prostate cancer study\\ndepends on its own z score of course, but also on the other genes‚Äô scores‚Äî\\n‚Äúlearning from the experience of others,‚Äù in the language used before.\\n\\nThe crucial question we have been avoiding is ‚ÄúWhich others?‚Äù Our tacit\\nanswer has been ‚ÄúAll the cases that arrive in the same data set,‚Äù all the genes\\n\\n12 The locfdr algorithm gave . Oƒ±0; O(cid:27)0; O(cid:25)0/ D .0:00; 1:06; 0:984/ for the prostate data.\\n\\n\\x0c15.6 Relevance\\n\\n291\\n\\nin the prostate study, all the ofÔ¨Åcers in the police study. Why this can be a\\ndangerous tactic is shown in our Ô¨Ånal example.\\n\\nA DTI (diffusion tensor imaging) study compared six dyslexic children\\nwith six normal controls. Each DTI scan recorded Ô¨Çuid Ô¨Çows at N D15,443\\n‚Äúvoxels,‚Äù i.e., at 15,443 three-dimensional brain coordinates. A score zi\\ncomparing dyslexics with normal controls was calculated for each voxel i,\\ncalibrated such that the theoretical null distribution of ‚Äúno difference‚Äù was\\n\\nH0i W zi (cid:24)\\n\\n.0; 1/\\n\\nN\\n\\n(15.51)\\n\\nas at (15.3).\\n\\nFigure 15.9 Histogram of z scores for the DTI study, comparing\\ndyslexic versus normal control children at 15,443 brain locations.\\nA FDR analysis based on the empirical null distribution gave 149\\nvoxels with cfdr.zi / (cid:20) 0:20, those having zi (cid:21) 3:17 (indicated by\\nred dashes).\\n\\nFigure 15.9 shows the histogram of all 15,443 zi values, normal-looking\\nnear the center and with a heavy right tail; locfdr gave empirical null\\nparameters\\n\\n(cid:16) Oƒ±0; O(cid:27)0; O(cid:25)0\\n\\n(cid:17)\\n\\nD .(cid:0)0:12; 1:06; 0:984/;\\n\\n(15.52)\\n\\nthe 149 voxels with zi (cid:21) 3:17 having cfdr values (cid:20) 0:20. Using the the-\\n\\n z‚àíscoreFrequency‚àí4‚àí2024020040060080010003.17\\x0c292\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\noretical null (15.51) yielded only modestly different results, now the 177\\nvoxels with zi (cid:21) 3:07 having cfdri (cid:20) 0:20.\\n\\nFigure 15.10 A plot of 15,443 zi scores from a DTI study\\n(vertical axis) and voxel distances xi from the back of the brain\\n(horizontal axis). The starred points are the 149 voxels with\\ncfdr.zi / (cid:20) 0:20, which occur mostly for xi in the interval ≈í50; 70(cid:141).\\n\\nIn Figure 15.10 the voxel scores zi , graphed vertically, are plotted ver-\\nsus xi , the voxel‚Äôs distance from the back of the brain. Waves of differing\\nresponse are apparent. Larger values occur in the interval 50 (cid:20) x (cid:20) 70,\\nwhere the entire z-value distribution‚Äîlow, medium, and high‚Äîis pushed\\nup. Most of the 149 voxels having cfdri (cid:20) 0:20 occur at the top of this wave.\\nFigure 15.10 raises the problem of fair comparison. Perhaps the 4,653\\nvoxels with xi between 50 and 70 should be compared only with each other,\\nand not with all 15,443 cases. Doing so gave\\n\\n(cid:16) Oƒ±0; O(cid:27)0; O(cid:25)0\\n\\n(cid:17)\\n\\nD .0:23; 1:18; 0:970/;\\n\\n(15.53)\\n\\nonly 66 voxels having cfdri (cid:20) 0:20, those with zi (cid:21) 3:57.\\n\\nAll of this is a question of relevance: which other voxels i are relevant\\nto the assessment of signiÔ¨Åcance for voxel i0? One might argue that this is\\na question for the scientist who gathers the data and not for the statistical\\nanalyst, but that is unlikely to be a fruitful avenue, at least not without\\n\\n20406080‚àí2024Distance xZ scores16%ile84%ilemedian*****************************************************************************************************************************************************\\x0c15.6 Relevance\\n\\n293\\n\\na lot of back-and-forth collaboration. Standard Bayesian analysis solves\\nthe problem by dictate: the assertion of a prior is also an assertion of its\\nrelevance. Empirical Bayes situations expose the dangers lurking in such\\nassertions.\\n\\nRelevance was touched upon in Section 7.4, where the limited transla-\\ntion rule (7.47) was designed to protect extreme cases from being shrunk\\ntoo far toward the bulk of ordinary ones. One could imagine having a ‚Äúrel-\\nevance function‚Äù (cid:26).xi ; zi / that, given the covariate information xi and re-\\nsponse zi for casei , somehow adjusts an ensemble false-discovery rate es-\\ntimate to correctly apply to the case of interest‚Äîbut such a theory barely\\nexists.(cid:142)\\n\\n(cid:142)10\\n\\nSummary\\n\\nLarge-scale testing, particularly in its false-discovery rate implementation,\\nis not at all the same thing as the classic Fisher‚ÄìNeyman‚ÄìPearson theory:\\n\\n(cid:15) Frequentist single-case hypothesis testing depends on the theoretical\\nlong-run behavior of samples from the theoretical null distribution. With\\ndata available from say N D 5000 simultaneous tests, the statistician\\nhas his or her own ‚Äúlong run‚Äù in hand, diminishing the importance of\\ntheoretical modeling. In particular, the data may cast doubt on the the-\\noretical null, providing a more appropriate empirical null distribution in\\nits place.\\n\\n(cid:15) Classic testing theory is purely frequentist, whereas false-discovery rates\\n\\ncombine frequentist and Bayesian thinking.\\n\\n(cid:15) In classic testing, the attained signiÔ¨Åcance level for case i depends only\\non its own score zi , while cfdr.zi / or cFdr.zi / also depends on the ob-\\nserved z-values for other cases.\\n\\n(cid:15) Applications of single-test theory usually hope for rejection of the null\\nhypothesis, a familiar prescription being 0.80 power at size 0.05. The\\nopposite is true for large-scale testing, where the usual goal is to ac-\\ncept most of the null hypotheses, leaving just a few interesting cases for\\nfurther study.\\n\\n(cid:15) Sharp null hypotheses such as (cid:22) D 0 are less important in large-scale\\napplications, where the statistician is happy to accept a hefty proportion\\nof uninterestingly small, but nonzero, effect sizes (cid:22)i .\\n\\n(cid:15) False-discovery rate hypothesis testing involves a substantial amount of\\nestimation, blurring the line beteen the two main branches of statistical\\ninference.\\n\\n\\x0c294\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\n15.7 Notes and Details\\n\\nThe story of false-discovery rates illustrates how developments in scien-\\ntiÔ¨Åc technology (microarrays in this case) can inÔ¨Çuence the progress of\\nstatistical inference. A substantial theory of simultaneous inference was\\ndeveloped between 1955 and 1995, mainly aimed at the frequentist control\\nof family-wise error rates in situations involving a small number of hypoth-\\nesis tests, maybe up to 20. Good references are Miller (1981) and Westfall\\nand Young (1993).\\n\\nBenjamini and Hochberg‚Äôs seminal 1995 paper introduced false-discov-\\nery rates at just the right time to catch the wave of large-scale data sets, now\\ninvolving thousands of simultaneous tests, generated by microarray appli-\\ncations. Most of the material in this chapter is taken from Efron (2010),\\nwhere the empirical Bayes nature of Fdr theory is emphasized. The po-\\nlice data is discussed and analyzed at length in Ridgeway and MacDonald\\n(2009).\\n\\n(cid:142)1 [p. 272] Model (15.4). Section 7.4 of Efron (2010) discusses the following\\nresult for the non-null distribution of z-values: a transformation such as\\n(15.2) that produces a z-value (i.e., a standard normal random variable z (cid:24)\\n.0; 1/) under the null hypothesis gives, to a good approximation, z (cid:24)\\n.(cid:22); (cid:27) 2\\n(cid:22)/ under reasonable alternatives. For the speciÔ¨Åc situation in (15.2),\\n\\nN\\nN\\nStudent‚Äôs t with 100 degrees of freedom, (cid:27) 2\\n(cid:22)\\n\\n:D 1 as in (15.4).\\n\\n(cid:142)2 [p. 274] Holm‚Äôs procedure. Methods of FWER control, including Holm‚Äôs\\nprocedure, are surveyed in Chapter 3 of Efron (2010). They display a large\\namount of mathematical ingenuity, and provided the background against\\nwhich FDR theory developed.\\n\\n(cid:142)3 [p. 276] FDR control theorem. Benjamini and Hochberg‚Äôs striking control\\ntheorem (15.15) was rederived by Storey et al. (2004) using martingale\\ntheory. The basic idea of false discoveries, as displayed in Figure 15.2,\\ngoes back to Soric (1989).\\n\\n(cid:142)4 [p. 285] Formula (15.44). Integrating fdr.z/ D (cid:25)0f0.z/=f .z/ gives\\n\\nE ffdr.z/jz (cid:21) z0g D\\n\\nZ 1\\n\\nz0\\n\\n(cid:25)0f0.z/ dz\\n\\n, Z 1\\n\\nz0\\n\\nf .z/ dz\\n\\n(15.54)\\n\\nD (cid:25)0S0.z0/=S.z0/ D Fdr.z0/:\\n\\n(cid:142)5 [p. 286] Thresholds for Fdr and fdr. Suppose the survival curves S0.z/ and\\n\\nS1.z/ (15.20) satisfy the ‚ÄúLehmann alternative‚Äù relationship\\n\\nlog S1.z/ D (cid:13) log S0.z/\\n\\n(15.55)\\n\\n\\x0c15.7 Notes and Details\\n\\n295\\n\\nfor large values of z, where (cid:13) is a positive constant less than 1. (This is a\\nreasonable condition for the non-null density f1.z/ to produce larger pos-\\nitive values of z than does the null density f0.z/.) Differentiating (15.55)\\ngives\\n\\nD 1\\n(cid:13)\\nafter some rearrangement. But fdr.z/ D (cid:25)0f0.z/=.(cid:25)0f0.z/ C (cid:25)1f1.z// is\\nalgebraically equivalent to\\n\\nS0.z/\\nS1.z/\\n\\nf0.z/\\nf1.z/\\n\\n(15.56)\\n\\n(cid:25)0\\n(cid:25)1\\n\\n(cid:25)0\\n(cid:25)1\\n\\n;\\n\\nf0.z/\\nf1.z/\\nand similarly for Fdr.z/=.1 (cid:0) Fdr.z//, yielding\\n\\nfdr.z/\\n1 (cid:0) fdr.z/\\n\\nD (cid:25)0\\n(cid:25)1\\n\\n;\\n\\n(15.57)\\n\\nfdr.z/\\n1 (cid:0) fdr.z/\\n\\nD 1\\n(cid:13)\\n\\nFdr.z/\\n1 (cid:0) Fdr.z/\\n\\n:\\n\\n(15.58)\\n\\nFor large z, both fdr.z/ and Fdr.z/ go to zero, giving the asymptotic rela-\\ntionship\\n\\nfdr.z/\\n\\n:D Fdr.z/=(cid:13):\\n\\n(15.59)\\n\\nIf (cid:13) D 1=2 for instance, fdr.z/ will be about twice Fdr.z/ where z is large.\\nThis motivates the suggested relative thresholds cfdr.zi / (cid:20) 0:20 compared\\nwith cFdr.zi / (cid:20) 0:10.\\n\\n(cid:142)6 [p. 288] Correlation effects. The Poisson regression method used to esti-\\nmate Of .z/ in Figure 15.5 proceeds as if the components of the N -vector of\\nzi values z are independent. Approximation (10.54), that the kth bin count\\nyk P(cid:24) Poi.(cid:22)k/, requires independence. If not, it can be shown that var.yk/\\nincreases above the Poisson value (cid:22)k as\\n\\nvar.yk/\\n\\n:D (cid:22)k C Àõ2ck:\\n\\n(15.60)\\n\\nHere ck is a Ô¨Åxed constant depending on f .z/, while Àõ2 is the mean square\\ncorrelation between all pairs zi and zj ,\\n\\nÀõ2 D\\n\\n2\\n\\n4\\n\\nN\\nX\\n\\nX\\n\\ni D1\\n\\nj ¬§i\\n\\ncov.zi ; zj /2\\n\\n3\\n\\n5\\n\\n,\\n\\nN.N (cid:0) 1/:\\n\\n(15.61)\\n\\nEstimates like cfdr.z/ in Figure 15.5 remain nearly unbiased under correla-\\ntion, but their sampling variability increases as a function of Àõ. Chapters 7\\nand 8 of Efron (2010) discuss correlation effects in detail.\\n\\nOften, Àõ can be estimated. Let X be the 6033 (cid:2) 50 matrix of gene ex-\\npression levels measured for the control subject in the prostate study. Rows\\n\\n\\x0c296\\n\\nLarge-scale Hypothesis Testing and FDRs\\n\\ni and j provide an unbiased estimate of cor.zi ; zj /2. Modern computation\\nis sufÔ¨Åciently fast to evaluate all N.N (cid:0) 1/=2 pairs (though that isn‚Äôt nec-\\nessary, sampling is faster) from which estimate OÀõ is obtained. It equaled\\n0:016 Àô 0:001 for the control subjects, and 0:015 Àô 0:001 for the 6033 (cid:2) 52\\nmatrix of the cancer patients. Correlation is not much of a worry for the\\nprostate study, but other microarray studies show much larger OÀõ values.\\nSections 6.4 and 8.3 of Efron (2010) discuss how correlations can under-\\ncut inferences based on the theoretical null even when it is correct for all\\nthe null cases.\\n\\n(cid:142)7 [p. 289] The program locfdr. Available from CRAN, this is an R pro-\\ngram that provides fdr and Fdr estimates, using both the theoretical and\\nempirical null distributions.\\n\\n(cid:142)8 [p. 289] ML estimation of the empirical null. Let\\n\\n(15.49), z0 the set of zi observed to be in\\nnumber of zi in\\n\\n0. Also deÔ¨Åne\\n\\n0,\\n\\nA\\n\\nI\\n\\n0 be the ‚Äúzero set‚Äù\\n0 their indices, and N0 the\\n\\nA\\n\\nA\\n(cid:30)ƒ±0;(cid:27)0.z/ D e\\n\\n(cid:0) 1\\n2\\n\\n(cid:16) z(cid:0)ƒ±0\\n(cid:27)0\\n\\n(cid:17)2(cid:30)q\\n\\n2(cid:25)(cid:27) 2\\n0 ;\\n\\nZ\\n\\nP .ƒ±0; (cid:27)0/ D\\n\\n(cid:30)ƒ±0;(cid:27)0.z/ dz\\n\\nand (cid:18) D (cid:25)0P .ƒ±0; (cid:27)0/:\\n\\n(15.62)\\n\\n(So (cid:18) D Prfzi 2\\nand likelihood\\n\\nA\\n\\n0\\n\\nA\\n0g according to (15.48)‚Äì(15.49).) Then z0 has density\\n\\nfƒ±0;(cid:27)0;(cid:25)0.z0/ D\\n\\n!\\n\\n\" \\n\\nN\\nN0\\n\\n(cid:18) N0.1 (cid:0) (cid:18)/N (cid:0)N0\\n\\n# \"\\n\\nY\\n\\n0\\n\\nI\\n\\n#\\n\\n(cid:30)ƒ±0;(cid:27)0.zi /\\nP .ƒ±0; (cid:27)0/\\n\\n;\\n\\n(15.63)\\n\\nthe Ô¨Årst factor being the binomial probability of seeing N0 of the zi in\\n0, and the second the conditional probability of those zi falling within\\nA\\n0. The second factor is numerically maximized to give . Oƒ±0; O(cid:27)0/, while\\nA\\nO(cid:18) D N0=N is obtained from the Ô¨Årst, and then O(cid:25)0 D O(cid:18)=P . Oƒ±0; O(cid:27)0/. This is\\na partial likelihood argument, as in Section 9.4; locfdr centers\\n0 at the\\nmedian of the N zi values, with width about twice the interquartile range\\nestimate of (cid:27)0.\\n\\nA\\n\\n(cid:142)9 [p. 290] The permutation null. An impressive amount of theoretical effort\\nconcerned the ‚Äúpermutation t-test‚Äù: in a single-test two-sample situation,\\npermuting the data and computing the t statistic gives, after a great many\\nrepetitions, a histogram dependably close to that of the standard t distri-\\nbution; see Hoeffding (1952). This was Fisher‚Äôs justiÔ¨Åcation for using the\\nstandard t-test on nonnormal data.\\n\\nThe argument cuts both ways. Permutation methods tend to recreate the\\n\\n\\x0c15.7 Notes and Details\\n\\n297\\n\\ntheoretical null, even in situations like that of Figure 15.7 where it isn‚Äôt\\nappropriate. The difÔ¨Åculties are discussed in Section 6.5 of Efron (2010).\\n(cid:142)10 [p. 293] Relevance theory. Suppose that in the DTI example shown in Fig-\\nure 15.10 we want to consider only voxels with x D 60 as relevant to an\\nobserved zi with xi D 60. Now there may not be enough relevant cases to\\nadequately estimate fdr.zi / or Fdr.zi /. Section 10.1 of Efron (2010) shows\\nhow the complete-data estimates cfdr.zi / or cFdr.zi / can be efÔ¨Åciently mod-\\niÔ¨Åed to conform to this situation.\\n\\n\\x0c16\\n\\nSparse Modeling and the Lasso\\n\\nThe amount of data we are faced with keeps growing. From around the\\nlate 1990s we started to see wide data sets, where the number of variables\\nfar exceeds the number of observations. This was largely due to our in-\\ncreasing ability to measure a large amount of information automatically. In\\ngenomics, for example, we can use a high-throughput experiment to auto-\\nmatically measure the expression of tens of thousands of genes in a sam-\\nple in a short amount of time. Similarly, sequencing equipment allows us\\nto genotype millions of SNPs (single-nucleotide polymorphisms) cheaply\\nand quickly. In document retrieval and modeling, we represent a document\\nby the presence or count of each word in the dictionary. This easily leads to\\na feature vector with 20,000 components, one for each distinct vocabulary\\nword, although most would be zero for a small document. If we move to\\nbi-grams or higher, the feature space gets really large.\\n\\nIn even more modest situations, we can be faced with hundreds of vari-\\nables. If these variables are to be predictors in a regression or logistic re-\\ngression model, we probably do not want to use them all. It is likely that a\\nsubset will do the job well, and including all the redundant variables will\\ndegrade our Ô¨Åt. Hence we are often interested in identifying a good subset\\nof variables. Note also that in these wide-data situations, even linear mod-\\nels are over-parametrized, so some form of reduction or regularization is\\nessential.\\n\\nIn this chapter we will discuss some of the popular methods for model\\nselection, starting with the time-tested and worthy forward-stepwise ap-\\nproach. We then look at the lasso, a popular modern method that does se-\\nlection and shrinkage via convex optimization. The LARs algorithm ties\\nthese two approaches together, and leads to methods that can deliver paths\\nof solutions.\\n\\nFinally, we discuss some connections with other modern big- and wide-\\n\\ndata approaches, and mention some extensions.\\n\\n298\\n\\n\\x0c16.1 Forward Stepwise Regression\\n\\n299\\n\\n16.1 Forward Stepwise Regression\\n\\nStepwise procedures have been around for a very long time. They were\\noriginally devised in times when data sets were quite modest in size, in\\nparticular in terms of the number of variables. Originally thought of as the\\npoor cousins of ‚Äúbest-subset‚Äù selection, they had the advantage of being\\nmuch cheaper to compute (and in fact possible to compute for large p). We\\nwill review best-subset regression Ô¨Årst.\\n\\nSuppose we have a set of n observations on a response yi and a vec-\\ntor of p predictors x0\\nD .xi1; xi2; : : : ; xip/, and we plan to Ô¨Åt a linear\\ni\\nregression model. The response could be quantitative, so we can think of\\nÔ¨Åtting a linear model by least squares. It could also be binary, leading to a\\nlinear logistic regression model Ô¨Åt by maximum likelihood. Although we\\nwill focus on these two cases, the same ideas transfer exactly to other gen-\\neralized linear models, the Cox model, and so on. The idea is to build a\\nmodel using a subset of the variables; in fact the smallest subset that ade-\\nquately explains the variation in the response is what we are after, both for\\ninference and for prediction purposes. Suppose our loss function for Ô¨Åtting\\nthe linear model is L (e.g. sum of squares, negative log-likelihood). The\\nmethod of best-subset regression is simple to describe, and is given in Al-\\ngorithm 16.1. Step 3 is easy to state, but requires a lot of computation. For\\n\\nAlgorithm 16.1 BEST-SUBSET REGRESSION.\\n1 Start with m D 0 and the null model O(cid:17)0.x/ D OÀá0, estimated by the mean\\n\\nof the yi .\\n\\n2 At step m D 1, pick the single variable j that Ô¨Åts the response best,\\nin terms of the loss L evaluated on the training data, in a univariate\\nregression O(cid:17)1.x/ D OÀá0 C x0\\n\\n1 D fj g.\\n\\nOÀáj . Set\\n\\n3 For each subset size m 2 f2; 3; : : : ; M g (with M (cid:20) min.n (cid:0) 1; p/)\\nm of size m when Ô¨Åtting a linear model\\nA\\nm with m of the p variables, in terms of the\\n\\nA\\n\\nidentify the best subset\\nOÀá\\nO(cid:17)m.x/ D OÀá0 C x0\\nloss L.\\n\\nA\\n\\nA\\n\\nm\\n\\nj\\n\\n4 Use some external data or other means to select the ‚Äúbest‚Äù amongst these\\n\\nM models.\\n\\np much larger than about 40 it becomes prohibitively expensive to perform\\nexactly‚Äîa so-called ‚ÄúN-P complete‚Äù problem because of its combinatorial\\ncomplexity (there are 2p subsets). Note that the subsets need not be nested:\\n\\n\\x0c300\\n\\nSparse Modeling and the Lasso\\n\\nthe best subset of size m D 3, say, need not include both or any of the\\nvariables in the best subset of size m D 2.\\n\\nIn step 4 there are a number of methods for selecting m. Originally the\\nCp criterion of Chapter 12 was proposed for this purpose. Here we will\\nfavor K-fold cross-validation, since it is applicable to all the methods dis-\\ncussed in this chapter.\\n\\nIt is interesting to digress for a moment on how cross-validation works\\nhere. We are using it to select the subset size m on the basis of prediction\\nperformance (on future data). With K D 10, we divide the n training obser-\\nvations randomly into 10 equal size groups. Leaving out say group k D 1,\\nwe perform steps 1‚Äì3 on the 9=10ths, and for each of the chosen models,\\nwe summarize the prediction performance on the group-1 data. We do this\\nK D 10 times, each time with group k left out. We then average the 10 per-\\nformance measures for each m, and select the value of m corresponding to\\nthe best performance. Notice that for each m, the 10 models O(cid:17)m.x/ might\\ninvolve different subsets of variables! This is not a concern, since we are\\ntrying to Ô¨Ånd a good value of m for the method. Having identiÔ¨Åed Om, we\\nrerun step 3 on the entire training set, and deliver the chosen model O(cid:17) Om.x/.\\nAs hinted above, there are problems with best-subset regression. A pri-\\nmary issue is that it works exactly only for relatively small p. For example,\\nwe cannot run it on the spam data with 57 variables (at least not in 2015 on\\na Macbook Pro!). We may also think that even if we could do the compu-\\ntations, with such a large search space the variance of the procedure might\\nbe too high.\\n\\nm(cid:0)1 (cid:26)\\n\\nAs a result, more manageable stepwise procedures were invented. For-\\nward stepwise regression, Algorithm 16.2, is a simple modiÔ¨Åcation of best-\\nsubset, with the modiÔ¨Åcation occurring in step 3. Forward stepwise re-\\nm (cid:26)\\ngression produces a nested sequence of models ; : : : (cid:26)\\nmC1 : : :. It starts with the null model, here an intercept, and adds vari-\\nA\\nables one at a time. Even with large p, identifying the best variable to add\\nat each step is manageable, and can be distributed if clusters of machines\\nare available. Most importantly, it is feasible for large p. Figure 16.1 shows\\nthe coefÔ¨Åcient proÔ¨Åles for forward-stepwise linear regression on the spam\\ntraining data. Here there are 57 input variables (relative prevalence of par-\\nticular words in the document), and an ‚ÄúofÔ¨Åcial‚Äù (train, test) split of (3065,\\n1536) observations. The response is coded as +1 if the email was spam,\\nelse -1. The Ô¨Ågure caption gives the details. We saw the spam data earlier,\\nin Table 8.3, Figure 8.7 and Figure 12.2.\\n\\nA\\n\\nA\\n\\nFitting the entire forward-stepwise linear regression path as in the Ô¨Ågure\\n(when n > p) has essentially the same cost as a single least squares Ô¨Åt on\\n\\n\\x0c16.1 Forward Stepwise Regression\\n\\n301\\n\\nAlgorithm 16.2 FORWARD STEPWISE REGRESSION.\\n1 Start with m D 0 and the null model O(cid:17)0.x/ D OÀá0, estimated by the mean\\n\\nof the yi .\\n\\n2 At step m D 1, pick the single variable j that Ô¨Åts the response best,\\nin terms of the loss L evaluated on the training data, in a univariate\\nregression O(cid:17)1.x/ D OÀá0 C x0\\n\\n1 D fj g.\\n\\nOÀáj . Set\\n\\n3 For each subset size m 2 f2; 3; : : : ; M g (with M (cid:20) min.n (cid:0) 1; p/)\\nm,\\nm that performs best in terms\\n\\nidentify the variable k that when augmented with\\nleads to the model O(cid:17)m.x/ D OÀá0 C x0\\nof the loss L.\\n\\nm(cid:0)1 to form\\n\\nOÀá\\n\\nA\\n\\nA\\n\\nA\\n\\nA\\n\\nA\\n\\nm\\n\\nj\\n\\n4 Use some external data or other means to select the ‚Äúbest‚Äù amongst these\\n\\nM models.\\n\\nall the variables. This is because the sequence of models can be updated\\neach time a variable is added.(cid:142)However, this is a consequence of the linear (cid:142)1\\nmodel and squared-error loss.\\n\\nSuppose instead we run a forward stepwise logistic regression. Here up-\\ndating does not work, and the entire Ô¨Åt has to be recomputed by maximum\\nlikelihood each time a variable is added. Identifying which variable to add\\nin step 3 in principle requires Ô¨Åtting an .m C 1/-variable model p (cid:0) m\\ntimes, and seeing which one reduces the deviance the most. In practice, we\\ncan use score tests which are much cheaper to evaluate. (cid:142) These amount (cid:142)2\\nto using the quadratic approximation to the log-likelihood from the Ô¨Ånal\\niteratively reweighted least-squares (IRLS) iteration for Ô¨Åtting the model\\nwith m terms. The score test for a variable not in the model is equivalent\\nto testing for the inclusion of this variable in the weighted least-squares Ô¨Åt.\\nHence identifying the next variable is almost back to the previous cases,\\nrequiring p (cid:0) m simple regression updates. (cid:142) Figure 16.2 shows the test (cid:142)3\\nmisclassiÔ¨Åcation error for forward-stepwise linear regression and logistic\\nregression on the spam data, as a function of the number of steps. They\\nboth level off at around 25 steps, and have a similar shape. However, the\\nlogistic regression gives more accurate classiÔ¨Åcations.1\\n\\nAlthough forward-stepwise methods are possible for large p, they get\\ntedious for very large p (in the thousands), especially if the data could sup-\\nport a model with many variables. However, if the ideal active set is fairly\\n\\n1 For this example we can halve the gap between the curves by optimizing the prediction\\n\\nthreshold for linear regression.\\n\\n\\x0c302\\n\\nSparse Modeling and the Lasso\\n\\nFigure 16.1 Forward stepwise linear regression on the spam\\ndata. Each curve corresponds to a particular variable, and shows\\nthe progression of its coefÔ¨Åcient as the model grows. These are\\nplotted against the training R2, and the vertical gray bars\\ncorrespond to each step. Starting at the left at step 1, the Ô¨Årst\\nselected variable explains R2 D 0:16; adding the second increases\\nR2 to 0:25, etc. What we see is that early steps have a big impact\\non the R2, while later steps hardly have any at all. The vertical\\nblack line corresponds to step 25 (see Figure 16.2), and we see\\nthat after that the step-wise improvements in R2 are negligible.\\n\\nsmall, even with many thousands of variables forward-stepwise selection\\nis a viable option.\\n\\nForward-stepwise selection delivers a sequence of models, as seen in\\nthe previous Ô¨Ågures. One would generally want to select a single model,\\nand as discussed earlier, we often use cross-validation for this purpose.\\nFigure 16.3 illustrates using stepwise linear regression on the spam data.\\nHere the sequence of models are Ô¨Åt using squared-error loss on the bi-\\nnary response variable. However, cross-validation scores each model for\\nmisclassiÔ¨Åcation error, the ultimate goal of this modeling exercise. This\\nhighlights one of the advantages of cross-validation in this context. A con-\\nvenient (differentiable and smooth) loss function is used to Ô¨Åt the sequence\\n\\n0.00.10.20.30.40.50.6‚àí0.20.00.20.40.60.8Forward‚àíStepwise RegressionR2 on Training DataCoefficients\\x0c16.2 The Lasso\\n\\n303\\n\\nFigure 16.2 Forward-stepwise regression on the spam data.\\nShown is the misclassiÔ¨Åcation error on the test data, as a function\\nof the number of steps. The brown dots correspond to linear\\nregression, with the response coded as -1 and +1; a prediction\\ngreater than zero is classiÔ¨Åed as +1, one less than zero as -1. The\\nblue dots correspond to logistic regression, which performs better.\\nWe see that both curves essentially reach their minima after 25\\nsteps.\\n\\nof models. However, we can use any performance measure to evaluate the\\nsequence of models; here misclassiÔ¨Åcation error is used. In terms of the\\nparameters of the linear model, misclassiÔ¨Åcation error would be a difÔ¨Åcult\\nand discontinuous loss function to use for parameter estimation. All we\\nneed to use it for here is pick the best model size. There appears to be little\\nbeneÔ¨Åt in going beyond 25‚Äì30 terms.\\n\\n16.2 The Lasso\\n\\nThe stepwise model-selection methods of the previous section are useful\\nif we anticipate a model using a relatively small number of variables, even\\nif the pool of available variables is very large. If we expect a moderate\\nnumber of variables to play a role, these methods become cumbersome.\\nAnother black mark against forward-stepwise methods is that the sequence\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllll01020304050600.00.10.20.30.4Spam DataStepTest Misclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward‚àíStepwise Linear RegressionForward‚àíStepwise Logistic Regression\\x0c304\\n\\nSparse Modeling and the Lasso\\n\\nFigure 16.3 Ten-fold cross-validated misclassiÔ¨Åcation errors\\n(green) for forward-stepwise regression on the spam data, as a\\nfunction of the step number. Since each error is an average of 10\\nnumbers, we can compute a (crude) standard error; included in\\nthe plot are pointwise standard-error bands. The brown curve is\\nthe misclassiÔ¨Åcation error on the test data.\\n\\nof models is derived in a greedy fashion, without any claimed optimality.\\nThe methods we describe here are derived from a more principled proce-\\ndure; indeed they solve a convex optimization, as deÔ¨Åned below.\\n\\nWe will Ô¨Årst present the lasso for squared-error loss, and then the more\\n\\ngeneral case later. Consider the constrained linear regression problem\\n\\nminimize\\nÀá02R; Àá 2Rp\\n\\n1\\nn\\n\\nn\\nX\\n\\n.yi (cid:0) Àá0 (cid:0) x0\\n\\ni Àá/2 subject to kÀák1 (cid:20) t;\\n\\n(16.1)\\n\\ni D1\\n\\nj D1\\n\\nwhere kÀák1 D Pp\\njÀáj j, the `1 norm of the coefÔ¨Åcient vector. Since both\\nthe loss and the constraint are convex in Àá, this is a convex optimization\\nproblem, and it is known as the lasso. The constraint kÀák1 (cid:20) t restricts the\\ncoefÔ¨Åcients of the model by pulling them toward zero; this has the effect\\nof reducing their variance, and prevents overÔ¨Åtting. Ridge regression is an\\nearlier great uncle of the lasso, and solves a similar problem to (16.1), ex-\\ncept the constraint is kÀák2 (cid:20) t; ridge regression bounds the quadratic `2\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.00.10.20.30.4Spam DataStepTest and CV Misclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllTest Error10‚àífold CV Error\\x0c16.2 The Lasso\\n\\n305\\n\\nFigure 16.4 An example with Àá 2 R2 to illustrate the difference\\nbetween ridge regression and the lasso. In both plots, the red\\ncontours correspond to the squared-error loss function, with the\\nunrestricted least-squares estimate OÀá in the center. The blue\\nregions show the constraints, with the lasso on the left and ridge\\non the right. The solution to the constrained problem corresponds\\nto the value of Àá where the expanding loss contours Ô¨Årst touch the\\nconstraint region. Due to the shape of the lasso constraint, this\\nwill often be at a corner (or an edge more generally), as here,\\nwhich means in this case that the minimizing Àá has Àá1 D 0. For\\nthe ridge constraint, this is unlikely to happen.\\n\\nnorm of the coefÔ¨Åcient vector. It also has the effect of pulling the coefÔ¨Å-\\ncients toward zero, in an apparently very similar way. Ridge regression is\\ndiscussed in Section 7.3.2 Both the lasso and ridge regression are shrinkage\\nmethods, in the spirit of the James‚ÄìStein estimator of Chapter 7.\\n\\nA big difference, however, is that for the lasso, the solution typically has\\nmany of the Àáj equal to zero, while for ridge they are all nonzero. Hence\\nthe lasso does variable selection and shrinkage, while ridge only shrinks.\\nFigure 16.4 illustrates this for Àá 2 R2. In higher dimensions, the `1 norm\\nhas sharp edges and corners, which correspond to coefÔ¨Åcient estimates zero\\nin Àá.\\n\\nSince the constraint in the lasso treats all the coefÔ¨Åcients equally, it usu-\\nally makes sense for all the elements of x to be in the same units. If not, we\\n\\n2 Here we use the ‚Äúbound‚Äù form of ridge regression, while in Section 7.3 we use the\\n\\n‚ÄúLagrange‚Äù form. They are equivalent, in that for every ‚ÄúLagrange‚Äù solution, there is a\\ncorresponding bound solution.\\n\\nŒ≤^Œ≤^2..Œ≤1Œ≤2Œ≤1Œ≤\\x0c306\\n\\nSparse Modeling and the Lasso\\n\\ntypically standardize the predictors beforehand so that each has variance\\none.\\n\\nTwo natural boundary values for t in (16.1) are t D 0 and t D 1.\\nThe former corresponds to the constant model (the Ô¨Åt is the mean of the\\nyi ,)3 and the latter corresponds to the unrestricted least-squares Ô¨Åt. In fact,\\nif n > p, and OÀá is the least-squares estimate, then we can replace 1 by\\nk OÀák1, and any value of t (cid:21) k OÀák1 is a non-binding constraint.(cid:142) Figure 16.5\\n\\n(cid:142)4\\n\\nFigure 16.5 The lasso linear regression regularization path on the\\nspam data. Each curve corresponds to a particular variable, and\\nshows the progression of its coefÔ¨Åcient as the regularization\\nbound t grows. These curves are plotted against the training R2\\nrather than t, to make the curves comparable with the\\nforward-stepwise curves in Figure 16.1. Some values of t are\\nindicated at the top. The vertical gray bars indicate changes in the\\nactive set of nonzero coefÔ¨Åcients, typically an inclusion. Here we\\nsee clearly the role of the `1 penalty; as t is relaxed, coefÔ¨Åcients\\nbecome nonzero, but in a smoother fashion than in forward\\nstepwise.\\n\\nshows the regularization path4 for the lasso linear regression problem on\\n\\n3 We typically do not restrict the intercept in the model.\\n4 Also known as the homotopy path.\\n\\n0.00.10.20.30.40.50.6‚àí0.20.00.20.4Lasso RegressionR2 on Training DataCoefficients0.000.060.201.052.453.47\\x0c16.2 The Lasso\\n\\n307\\n\\nthe spam data; that is, the solution path for all values of t. This can be com-\\nputed exactly, as we will see in Section 16.4, because the coefÔ¨Åcient pro-\\nÔ¨Åles are piecewise linear in t. It is natural to compare this coefÔ¨Åcient proÔ¨Åle\\nwith the analogous one in Figure 16.1 for forward-stepwise regression. Be-\\ncause of the control of k OÀá.t/k1, we don‚Äôt see the same range as in forward\\nstepwise, and observe somewhat smoother behavior. Figure 16.6 contrasts\\n\\nFigure 16.6 Lasso versus forward-stepwise regression on the\\nspam data. Shown is the misclassiÔ¨Åcation error on the test data,\\nas a function of the number of variables in the model. Linear\\nregression is coded brown, logistic regression blue; hollow dots\\nforward stepwise, solid dots lasso. In this case it appears stepwise\\nand lasso achieve the same performance, but lasso takes longer to\\nget there, because of the shrinkage.\\n\\nthe prediction performance on the spam data for lasso regularized models\\n(linear regression and logistic regression) versus forward-stepwise models.\\nThe results are rather similar at the end of the path; here forward stepwise\\ncan achieve classiÔ¨Åcation performance similar to that of lasso regularized\\nlogistic regression with about half the terms. Lasso logistic regression (and\\nindeed any likelihood-based linear model) is Ô¨Åt by penalized maximum\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.00.10.20.30.4Spam DataStepTest Misclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward‚àíStepwise Linear RegressionForward‚àíStepwise Logistic RegressionLasso Linear RegressionLasso Logistic Regression\\x0c308\\n\\nSparse Modeling and the Lasso\\n\\nlikelihood:\\n\\nminimize\\nÀá02R; Àá 2Rp\\n\\n1\\nn\\n\\nn\\nX\\n\\ni D1\\n\\nL.yi ; Àá0 C Àá0xi / subject to kÀák1 (cid:20) t:\\n\\n(16.2)\\n\\nHere L is the negative of the log-likelihood function for the response dis-\\ntribution.\\n\\n16.3 Fitting Lasso Models\\n\\nThe lasso objectives (16.1) or (16.2) are differentiable and convex in Àá and\\nÀá0, and the constraint is convex in Àá. Hence solving these problems is a\\nconvex optimization problem, for which standard packages are available.\\nIt turns out these problems have special structure that can be exploited\\nto yield efÔ¨Åcient algorithms for Ô¨Åtting the entire path of solutions as in\\nFigures 16.1 and 16.5. We will start with problem (16.1), which we rewrite\\nin the more convenient Lagrange form:\\n\\nminimize\\nÀá 2Rp\\n\\n1\\n2n\\n\\nky (cid:0) X Àák2 C (cid:21)kÀák1:\\n\\n(16.3)\\n\\nHere we have centered y and the columns of X beforehand, and hence\\nthe intercept has been omitted. The Lagrange and constraint versions are\\nequivalent, in the sense that any solution OÀá.(cid:21)/ to (16.3) with (cid:21) (cid:21) 0 corre-\\nsponds to a solution to (16.1) with t D k OÀá.(cid:21)/k1. Here large values of (cid:21) will\\nencourage solutions with small `1 norm coefÔ¨Åcient vectors, and vice-versa;\\n(cid:21) D 0 corresponds to the ordinary least squares Ô¨Åt.\\n\\nThe solution to (16.3) satisÔ¨Åes the subgradient condition\\n\\n(cid:0) 1\\nn\\n\\nhxj ; y (cid:0) X OÀái C (cid:21)sj D 0;\\n\\nj D 1; : : : ; p;\\n\\n(16.4)\\n\\nwhere sj 2 sign. OÀáj /; j D 1; : : : ; p. This notation means sj D sign. OÀáj /\\nif OÀáj ¬§ 0, and sj 2 ≈í(cid:0)1; 1(cid:141) if OÀáj D 0.) We use the inner-product notation\\nha; bi D a0b in (16.4), which leads to more evocative expressions. These\\nsubgradient conditions are the modern way of characterizing solutions to\\nproblems of this kind, and are equivalent to the Karush‚ÄìKuhn‚ÄìTucker op-\\ntimality conditions. From these conditions we can immediately learn some\\nproperties of a lasso solution.\\n\\njhxj ; y (cid:0) X OÀáij D (cid:21) for all members of the active set; i.e., each of the\\n(cid:15) 1\\nn\\nvariables in the model (with nonzero coefÔ¨Åcient) has the same covari-\\nance with the residuals (in absolute value).\\n\\n\\x0c16.4 Least-Angle Regression\\n309\\njhxk; y (cid:0) X OÀáij (cid:20) (cid:21) for all variables not in the active set (i.e. with\\n(cid:15) 1\\nn\\ncoefÔ¨Åcients zero).\\n\\nThese conditions are interesting and have a big impact on computation.\\nSuppose we have the solution OÀá.(cid:21)1/ at (cid:21)1, and we decrease (cid:21) by a small\\namount to (cid:21)2 < (cid:21)1. The coefÔ¨Åcients and hence the residuals change, in\\nsuch a way that the covariances all remain tied at the smaller value (cid:21)2. If\\nin the process the active set has not changed, and nor have the signs of\\ntheir coefÔ¨Åcients, then we get an important consequence: OÀá.(cid:21)/ is linear for\\n(cid:21) 2 ≈í(cid:21)2; (cid:21)1(cid:141). To see this, suppose\\nindexes the active set, which is the\\nsame at (cid:21)1 and (cid:21)2, and let s\\n\\nbe the constant sign vector. Then we have\\n\\nA\\n\\nA\\n\\n.y (cid:0) X OÀá.(cid:21)1// D ns\\n.y (cid:0) X OÀá.(cid:21)2// D ns\\n\\n(cid:21)1;\\n\\n(cid:21)2:\\n\\nA\\n\\nA\\n\\nX 0\\nA\\nX 0\\nA\\n\\nBy subtracting and solving we get\\n\\nOÀá\\n\\n.(cid:21)2/ (cid:0) OÀá\\n\\n.(cid:21)1/ D n.(cid:21)1 (cid:0) (cid:21)2/.X 0\\n\\nX\\n\\n/(cid:0)1s\\nA\\n\\n;\\n\\n(16.5)\\n\\nA\\n\\nA\\nand the remaining coefÔ¨Åcients (with indices not in\\n) are all zero. This\\nshows that the full coefÔ¨Åcient vector OÀá.(cid:21)/ is linear for (cid:21) 2 ≈í(cid:21)2; (cid:21)1(cid:141). In fact,\\nthe coefÔ¨Åcient proÔ¨Åles for the lasso are continuous and piecewise linear\\nover the entire range of (cid:21), with knots occurring whenever the active set\\nchanges, or the signs of the coefÔ¨Åcients change.\\n\\nA\\n\\nA\\n\\nA\\n\\nAnother consequence is that we can easily determine (cid:21)max, the smallest\\nvalue for (cid:21) such that the solution OÀá.(cid:21)max/ D 0. From (16.4) this can be\\nseen to be (cid:21)max D maxj\\n\\njhxj ; yij.\\n\\nThese two facts plus a few more details enable us to compute the exact\\nsolution path for the squared-error-loss lasso; that is the topic of the next\\nsection.\\n\\n1\\nn\\n\\n16.4 Least-Angle Regression\\nWe have just seen that the lasso coefÔ¨Åcient proÔ¨Åle OÀá.(cid:21)/ is piecewise lin-\\near in (cid:21), and that the elements of the active set are tied in their absolute\\ncovariance with the residuals. With r.(cid:21)/ D y (cid:0) X OÀá.(cid:21)/, the covariance be-\\njhxj ; r.(cid:21)/ij. Hence these\\ntween xj and the evolving residual is cj .(cid:21)/ D 1\\nn\\nalso change in a piecewise linear fashion, with cj .(cid:21)/ D (cid:21) for j 2\\n, and\\ncj .(cid:21)/ (cid:20) (cid:21) for j 62\\n. This inspires the Least-Angle Regression algorithm,\\ngiven in Algorithm 16.3, which exploits this linearity to Ô¨Åt the entire lasso\\nregularization path.\\n\\nA\\n\\nA\\n\\n\\x0c310\\n\\nSparse Modeling and the Lasso\\n\\nAlgorithm 16.3 LEAST-ANGLE REGRESSION.\\n\\n1 Standardize the predictors to have mean zero and unit `2 norm. Start\\n\\nwith the residual r0 D y (cid:0) Ny, Àá0 D .Àá1; Àá2; : : : ; Àáp/ D 0.\\n\\n2 Find the predictor xj most correlated with r0; i.e., with largest value for\\n,\\n\\njhxj ; r0ij. Call this value (cid:21)0, deÔ¨Åne the active set\\n\\n1\\nn\\nthe matrix consisting of this single variable.\\n\\nD fj g, and X\\n\\nA\\n\\nA\\n\\n3 For k D 1; 2; : : : ; K D min.n (cid:0) 1; p/ do:\\n(a) DeÔ¨Åne the least-squares direction ƒ± D 1\\n\\ndeÔ¨Åne the p-vector (cid:129) such that (cid:129)\\nare zero.\\n\\nA\\n\\nrk(cid:0)1, and\\nA\\nD ƒ±, and the remaining elements\\n\\n/(cid:0)1X 0\\nA\\n\\n.X 0\\nA\\n\\nn(cid:21)k(cid:0)1\\n\\nX\\n\\nA\\n\\n(b) Move the coefÔ¨Åcients Àá from Àák(cid:0)1 in the direction (cid:129) toward their\\n: Àá.(cid:21)/ D Àák(cid:0)1 C .(cid:21)k(cid:0)1 (cid:0) (cid:21)/(cid:129) for\\nleast-squares solution on X\\n0 < (cid:21) (cid:20) (cid:21)k(cid:0)1, keeping track of the evolving residuals r.(cid:21)/ D\\ny (cid:0) X Àá.(cid:21)/ D rk(cid:0)1 (cid:0) .(cid:21)k(cid:0)1 (cid:0) (cid:21)/X\\nA\\njhx`; r.(cid:21)/ij for ` ‚Ä¶\\n\\n, identify the largest value of\\n(cid:21) at which a variable ‚Äúcatches up‚Äù with the active set; if the variable\\njhx`; r.(cid:21)/ij D (cid:21). This deÔ¨Ånes the next\\nhas index `, that means 1\\nn\\n‚Äúknot‚Äù (cid:21)k.\\n\\n(c) Keeping track of 1\\nn\\n\\nA\\n\\nƒ±.\\n\\n[ `, Àák D Àá.(cid:21)k/ D Àák(cid:0)1 C .(cid:21)k(cid:0)1 (cid:0) (cid:21)k/(cid:129), and rk D\\n\\n(d) Set\\n\\nD\\nA\\ny (cid:0) X Àák.\\n\\nA\\n\\n4 Return the sequence f(cid:21)k; ÀákgK\\n0 .\\n\\nA\\n\\nA\\n\\nX\\n\\n/(cid:0)1s\\n\\nIn step 3(a) ƒ± D .X 0\\nA\\n\\nas in (16.5). We can think of the LAR al-\\ngorithm as a democratic version of forward-stepwise regression. In forward-\\nstepwise regression, we identify the variable that will improve the Ô¨Åt the\\nmost, and then move all the coefÔ¨Åcients toward the new least-squares Ô¨Åt.\\nAs described in endnotes (cid:142)1 and (cid:142)3, this is sometimes done by computing\\nthe inner products of each (unadjusted) variable with the residual, and pick-\\ning the largest in absolute value. In step 3 of Algorithm 16.3, we move the\\ntoward their least-squares\\ncoefÔ¨Åcients for the variables in the active set\\nÔ¨Åt (keeping their inner products tied), but stop when a variable not in\\nA\\ncatches up in inner product. At that point, it is invited into the club, and the\\nprocess continues.\\n\\nA\\n\\nStep 3(c) can be performed efÔ¨Åciently because of the linearity of the\\nevolving inner products; for each variable not in\\n, we can determine ex-\\nactly when (in (cid:21) time) it would catch up, and hence which catches up Ô¨Årst\\nand when. Since the path is piecewise linear, and we know the slopes, this\\n\\nA\\n\\n\\x0c16.4 Least-Angle Regression\\n\\n311\\n\\nFigure 16.7 Covariance evolution on the spam data. As\\nvariables tie for maximal covariance, they become part of the\\nactive set. These occasions are indicated by the vertical gray bars,\\nagain plotted against the training R2 as in Figure 16.5.\\n\\nmeans we know the path exactly without further computation between (cid:21)k(cid:0)1\\nand the newly found (cid:21)k.\\n\\nThe name ‚Äúleast-angle regression‚Äù derives from the fact that in step 3(b)\\nthe Ô¨Åtted vector evolves in the direction X (cid:129) D X\\nƒ±, and its inner product\\nwith each active vector is given by X 0\\nƒ± D s\\n. Since all the columns\\nA\\nof X have unit norm, this means the angles between each active vector and\\nthe evolving Ô¨Åtted vector are equal and hence minimal.\\n\\nX\\n\\nA\\n\\nA\\n\\nA\\n\\nThe main computational burden in Algorithm 16.3 is in step 3(a), com-\\nputing the new direction, each time the active set is updated. However, this\\nis easily performed using standard updating of a QR decomposition, and\\nhence the computations for the entire path are of the same order as that of\\na single least-squares Ô¨Åt using all the variables.\\n\\nThe vertical gray lines in Figure 16.5 show when the active set changes.\\nWe see the slopes change at each of these transitions. Compare with the\\ncorresponding Figure 16.1 for forward-stepwise regression.\\n\\nFigure 16.7 shows the the decreasing covariance during the steps of the\\n\\n0.00.10.20.30.40.50.60.00.10.20.30.4R2 on Training DataCovariance with Residuals\\x0c312\\n\\nSparse Modeling and the Lasso\\n\\nLAR algorithm. As each variable joins the active set, the covariances be-\\ncome tied. At the end of the path, the covariances are all zero, because this\\nis the unregularized ordinary least-squares solution.\\n\\nIt turns out that the LAR algorithm is not quite the lasso path; variables\\ncan drop out of the active set as the path evolves. This happens when a coef-\\nÔ¨Åcient curve passes through zero. The subgradient equations (16.4) imply\\nthat the sign of each active coefÔ¨Åcient matches the sign of the gradient.\\nHowever, a simple addition to step 3(c) in Algorithm 16.3 takes care of the\\nissue:\\n\\n3(c)+ lasso modiÔ¨Åcation: If a nonzero coefÔ¨Åcient crosses zero before the\\nand recompute the joint least-squares\\n\\nnext variable enters, drop it from\\nA\\ndirection (cid:129) using the reduced set.\\n\\nFigure 16.5 was computed using the lars package in R, with the lasso\\noption set to accommodate step 3(c)+; in this instance there was no need for\\ndropping. Dropping tends to occur when some of the variables are highly\\ncorrelated.\\n\\nLasso and Degrees of Freedom\\n\\nWe see in Figure 16.6 (left panel) that forward-stepwise regression is more\\naggressive than the lasso, in that it brings down the training MSE faster.\\nWe can use the covariance formula for df from Chapter 12 to quantify the\\namount of Ô¨Åtting at each step.\\n\\nIn the right panel we show the results of a simulation for estimating the\\ndf of forward-stepwise regression and the lasso for the spam data. Recall\\nthe covariance formula\\n\\ndf D 1\\n(cid:27) 2\\n\\nn\\nX\\n\\niD1\\n\\ncov.yi ; Oyi /:\\n\\n(16.6)\\n\\nThese covariances are of course with respect to the sampling distribution of\\nthe yi , which we do not have access to since these are real data. So instead\\nwe simulate from Ô¨Åtted values from the full least-squares Ô¨Åt, by adding\\nGaussian errors with the appropriate (estimated) standard deviation. (This\\nis the parametric bootstrap calculation (12.64).)\\n\\nIt turns out that each step of the LAR algorithm spends one df, as is\\nevidenced by the brown curve in the right plot of Figure 16.8. Forward\\nstepwise spends more df in the earlier stages, and can be erratic.\\n\\nUnder some technical conditions on the X matrix (that guarantee that\\n\\n\\x0c16.5 Fitting Generalized Lasso Models\\n\\n313\\n\\nFigure 16.8 Left: Training mean-squared error (MSE) on the\\nspam data, for forward-stepwise regression and the lasso, as a\\nfunction of the size of the active set. Forward stepwise is more\\naggressive than the lasso, in that it (over-)Ô¨Åts the training data\\nmore quickly. Right: Simulation showing the degrees of freedom\\nor df of forward-stepwise regression versus lasso. The lasso uses\\none df per step, while forward stepwise is greedier and uses more,\\nespecially in the early steps. Since these df were computed using\\n5000 random simulated data sets, we include standard-error bands\\non the estimates.\\n\\nLAR delivers the lasso path), one can show that the df is exactly one per\\nstep. More generally, for the lasso, if we deÔ¨Åne cdf .(cid:21)/ D j\\n.(cid:21)/j (the size\\nof the active set at (cid:21)), we have that E≈ícdf .(cid:21)/(cid:141) D df .(cid:21)/. In other words, the\\nsize of the active set is an unbiased estimate of df.\\n\\nA\\n\\nOrdinary least squares with a predetermined sequence of variables spends\\none df per variable. Intuitively forward stepwise spends more, because it\\npays a price (in some extra df) for searching. (cid:142) Although the lasso does (cid:142)5\\nsearch for the next variable, it does not Ô¨Åt the new model all the way, but\\njust until the next variable enters. At this point, one new df has been spent.\\n\\n16.5 Fitting Generalized Lasso Models\\n\\nSo far we have focused on the lasso for squared-error loss, and exploited\\nthe piecewise-linearity of its coefÔ¨Åcient proÔ¨Åle to efÔ¨Åciently compute the\\nentire path. Unfortunately this is not the case for most other loss functions,\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllll010203040500.40.50.60.70.80.9Spam Training DataStepTraining MSEllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward‚àíStepwiseLasso010203040500102030405060StepDfllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward StepwiseLasso\\x0c314\\n\\nSparse Modeling and the Lasso\\n\\nso obtaining the coefÔ¨Åcient path is potentially more costly. As a case in\\npoint, we will use logistic regression as an example; in this case in (16.2) L\\nrepresents the negative binomial log-likelihood. Writing the loss explicitly\\nand using the Lagrange form for the penalty, we wish to solve\\n\\nminimize\\nÀá02R; Àá 2Rp\\n\\n\"\\n\\n(cid:0)\\n\\n1\\nn\\n\\nn\\nX\\n\\niD1\\n\\n#\\nyi log (cid:22)i C .1 (cid:0) yi / log.1 (cid:0) (cid:22)i /\\n\\nC (cid:21)kÀák1: (16.7)\\n\\nHere we assume the yi 2 f0; 1g and (cid:22)i are the Ô¨Åtted probabilities\\n(cid:22)i D eÀá0Cx0\\ni Àá\\n1 C eÀá0Cx0\\ni Àá\\n\\n:\\n\\n(16.8)\\n\\nSimilar to (16.4), the solution satisÔ¨Åes the subgradient condition\\n\\n1\\nn\\n\\nhxj ; y (cid:0) (cid:22)i (cid:0) (cid:21)sj D 0;\\n\\nj D 1; : : : ; p;\\n\\n(16.9)\\n\\nwhere sj 2 sign.Àáj /; j D 1; : : : ; p, and (cid:22)0 D .(cid:22)1; : : : ; (cid:22)n/.5 How-\\never, the nonlinearity of (cid:22)i in Àáj results in piecewise nonlinear coefÔ¨Åcient\\nproÔ¨Åles. Instead we settle for a solution path on a sufÔ¨Åciently Ô¨Åne grid of\\nvalues for (cid:21). It is once again easy to see that the largest value of (cid:21) we need\\nconsider is\\n\\n(cid:21)max D max\\n\\nj\\n\\njhxj ; y (cid:0) Ny1ij;\\n\\n(16.10)\\n\\nsince this is the smallest value of (cid:21) for which OÀá D 0, and OÀá0 D logit. Ny/. A\\nreasonable sequence is 100 values (cid:21)1 > (cid:21)2 > : : : > (cid:21)100 equally spaced\\non the log-scale from (cid:21)max down to (cid:15)(cid:21)max, where (cid:15) is some small fraction\\nsuch as 0:001.\\n\\nAn approach that has proven to be surprisingly efÔ¨Åcient is path-wise\\n\\ncoordinate descent.\\n\\n(cid:15) For each value (cid:21)k, solve the lasso problem for one Àáj only, holding all\\n\\nthe others Ô¨Åxed. Cycle around until the estimates stabilize.\\n\\n(cid:15) By starting at (cid:21)1, where all the parameters are zero, we use warm starts\\nin computing the solutions at the decreasing sequence of (cid:21) values. The\\nwarm starts provide excellent initializations for the sequence of solutions\\nOÀá.(cid:21)k/.\\n\\n(cid:15) The active set grows slowly as (cid:21) decreases. Computational hedges that\\nguess the active set prove to be particularly efÔ¨Åcient. If the guess is good\\n(and correct), one iterates coordinate descent using only those variables,\\n\\n5 The equation for the intercept is 1\\nn\\n\\nPn\\n\\niD1 yi D 1\\nn\\n\\nPn\\n\\niD1 (cid:22)i .\\n\\n\\x0c16.5 Fitting Generalized Lasso Models\\n\\n315\\n\\nuntil convergence. One more sweep through all the variables conÔ¨Årms\\nthe hunch.\\n\\nThe R package glmnet employs a proximal-Newton strategy at each\\n\\nvalue (cid:21)k.\\n\\n1 Compute a weighted least squares (quadratic) approximation to the log-\\nlikelihood L at the current estimate for the solution vector OÀá.(cid:21)k/; This\\nproduces a working response and observation weights, as in a regular\\nGLM.\\n\\n2 Solve the weighted least-squares lasso at (cid:21)k by coordinate descent, using\\n\\nwarm starts and active-set iterations.\\n\\nWe now give some details, which illustrate why these particular strate-\\n\\ngies are effective. Consider the weighted least-squares problem\\n\\nminimize\\nÀáj\\n\\n1\\n2n\\n\\nn\\nX\\n\\ni D1\\n\\nwi .zi (cid:0) Àá0 (cid:0) x0\\n\\ni Àá/2 C (cid:21)kÀák1;\\n\\n(16.11)\\n\\nwith all but Àáj Ô¨Åxed at their current values. Writing Qri D zi (cid:0) Àá0 (cid:0)\\nP\\n\\n`¬§j xi`Àá`, we can recast (16.11) as\\n\\nminimize\\nÀáj\\n\\n1\\n2n\\n\\nn\\nX\\n\\niD1\\n\\nwi . Qri (cid:0) xij Àáj /2 C (cid:21)jÀáj j;\\n\\n(16.12)\\n\\na one-dimensional problem. The subgradient equation is\\n\\n1\\nn\\n\\nn\\nX\\n\\ni D1\\n\\nwi xij . Qri (cid:0) xij Àáj / (cid:0) (cid:21) (cid:1) sign.Àáj / D 0:\\n\\n(16.13)\\n\\nThe simplest form of the solution occurs if each variable is standardized to\\nhave weighted mean zero and variance one, and the weights sum to one; in\\nthat case we have a two-step solution.\\n\\n1 Compute the weighted simple least-squares coefÔ¨Åcient\\n\\nQÀáj D hxj ; Qriw D\\n\\nn\\nX\\n\\ni D1\\n\\nwi xij Qri :\\n\\n2 Soft-threshold QÀáj to produce OÀáj :\\n\\n(\\n\\nOÀáj D\\n\\nif j QÀáj < (cid:21)I\\n0\\nsign. QÀáj /.j QÀáj j (cid:0) (cid:21)/ otherwise:\\n\\n(16.14)\\n\\n(16.15)\\n\\n\\x0c316\\n\\nSparse Modeling and the Lasso\\n\\nWithout the standardization, the solution is almost as simple but less\\nintuitive.\\n\\nHence each coordinate-descent update essentially requires an inner prod-\\nuct, followed by the soft thresholding operation. This is especially conve-\\nnient for xij that are stored in sparse-matrix format, since then the inner\\nproducts need only visit the nonzero values. If the coefÔ¨Åcient is zero be-\\nfore the step, and remains zero, one just moves on, otherwise the model is\\nupdated.\\n\\nMoving from the solution at (cid:21)k (for which jhxj ; riw j D (cid:21)k for all the\\nnonzero coefÔ¨Åcients OÀáj ), down to the smaller (cid:21)kC1, one might expect all\\nvariables for which jhxj ; riw j (cid:21) (cid:21)kC1 would be natural candidates for the\\nnew active set. The strong rules lower the bar somewhat, and include any\\nvariables for which jhxj ; riw j (cid:21) (cid:21)kC1 (cid:0) .(cid:21)k (cid:0) (cid:21)kC1/; this tends to rarely\\nmake mistakes, and still leads to considerable computational savings.\\n\\nApart from variations in the loss function, other penalties are of interest\\nas well. In particular, the elastic net penalty bridges the gap between the\\nlasso and ridge regression. That penalty is deÔ¨Åned as\\nPÀõ.Àá/ D 1\\n2\\n\\n.1 (cid:0) Àõ/kÀák2\\n2\\n\\nC ÀõkÀák1;\\n\\n(16.16)\\n\\nwhere the factor 1=2 in the Ô¨Årst term is for mathematical convenience.\\nWhen the predictors are excessively correlated, the lasso performs some-\\nwhat poorly, since it has difÔ¨Åculty in choosing among the correlated cousins.\\nLike ridge regression, the elastic net shrinks the coefÔ¨Åcients of correlated\\nvariables toward each other, and tends to select correlated variables in\\ngroups. In this case the coordinate-descent update is almost as simple as\\nin (16.15)\\n\\nOÀáj D\\n\\n( 0\\n\\nsign. QÀáj /.j QÀáj j(cid:0)Àõ(cid:21)/\\n1C.1(cid:0)Àõ/(cid:21)\\n\\nif j QÀáj < Àõ(cid:21)I\\notherwise;\\n\\n(16.17)\\n\\nagain assuming the observations have weighted variance equal to one. When\\nÀõ D 0, the update corresponds to a coordinate update for ridge regression.\\nFigure 16.9 compares lasso with forward-stepwise logistic regression on\\nthe spam data, here using all binarized variables and their pairwise interac-\\ntions. This amounts to 3061 variables in all, once degenerate variables have\\nbeen excised. Forward stepwise takes a long time to run, since it enters one\\nvariable at a time, and after each one has been selected, a new GLM must\\nbe Ô¨Åt. The lasso path, as Ô¨Åt by glmnet, includes many new variables at\\neach step ((cid:21)k), and is extremely fast (6 s for the entire path). For very large\\n\\n\\x0c16.6 Post-Selection Inference for the Lasso\\n\\n317\\n\\nFigure 16.9 Test misclassiÔ¨Åcation error for lasso versus\\nforward-stepwise logistic regression on the spam data, where we\\nconsider pairwise interactions as well as main effects (3061\\npredictors in all). Here the minimum error for lasso is 0:057\\nversus 0:064 for stepwise logistic regression, and 0:071 for the\\nmain-effects-only lasso logistic regression model. The stepwise\\nmodels went up to 134 variables before encountering convergence\\nissues, while the lasso had a largest active set of size 682.\\n\\nand wide modern data sets (millions of examples and millions of variables),\\nthe lasso path algorithm is feasible and attractive.\\n\\n16.6 Post-Selection Inference for the Lasso\\n\\nThis chapter is mostly about building interpretable models for prediction,\\nwith little attention paid to inference; indeed, inference is generally difÔ¨Åcult\\nfor adaptively selected models.\\n\\nSuppose we have Ô¨Åt a lasso regression model with a particular value for\\nj D k of the p avail-\\n(cid:21), which ends up selecting a subset\\nable variables. The question arises as to whether we can assign p-values\\nto these selected variables, and produce conÔ¨Ådence intervals for their co-\\nefÔ¨Åcients. A recent burst of research activity has made progress on these\\nimportant problems. We give a very brief survey here, with references ap-\\n\\nof size j\\n\\nA\\n\\nA\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.00.10.20.30.4Spam Data with InteractionsPercentage NULL Deviance Explained on Training DataTest Misclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllForward‚àíStepwise Logistic RegressionLasso Logistic Regression\\x0c318\\n\\nSparse Modeling and the Lasso\\n\\n(cid:142)6\\n\\npearing in the notes. (cid:142) We discuss post-selection inference more generally\\nin Chapter 20.\\n\\nOne question that arises is whether we are interested in making infer-\\nences about the population regression parameters using the full set of p\\npredictors, or whether interest is restricted to the population regression pa-\\nrameters using only the subset\\n\\nFor the Ô¨Årst case, it has been proposed that one can view the coefÔ¨Åcients\\nof the selected model as an efÔ¨Åcient but biased estimate of the full popu-\\nlation coefÔ¨Åcient vector. The idea is to then debias this estimate, allowing\\ninference for the full vector of coefÔ¨Åcients. Of course, sharper inference\\nwill be available for the stronger variables that were selected in the Ô¨Årst\\nplace.\\n\\n.\\nA\\n\\nFigure 16.10 HIV data. Linear regression of drug resistance in\\nHIV-positive patients on seven sites, indicators of mutations at\\nparticular genomic locations. These seven sites were selected\\nfrom a total of 30 candidates, using the lasso. The naive 95%\\nconÔ¨Ådence intervals (dark) use standard linear-regression\\ninference, ignoring the selection event. The light intervals are\\n95% conÔ¨Ådence intervals, using linear regression, but conditioned\\non the selection event.\\n\\nFor the second case, the idea is to condition on the selection event(s)\\nitself, and then perform conditional inference on the\\n\\nand hence the set\\n\\nA\\n\\n‚àí2‚àí1012PredictorCoefficients5s8s9s16s25s26s28Naive intervalSelection‚àíadjusted interval\\x0c16.7 Connections and Extensions\\n\\n319\\n\\nA\\n\\nunrestricted (i.e. not lasso-shrunk) regression coefÔ¨Åcients of the response\\non only the variables in\\n. For the case of a lasso with squared-error loss,\\nit turns out that the set of response vectors y 2 RN that would lead to a\\nparticular subset\\nof variables in the active set form a convex polytope\\nin RN (if we condition on the signs of the coefÔ¨Åcients as well; ignoring\\nthe signs leads to a Ô¨Ånite union of such polytopes). This, along with del-\\nicate Gaussian conditioning arguments, leads to truncated Gaussian and\\nt-distrubtions for parameters of interest.\\n\\nA\\n\\nFigure 16.10 shows the results of using the lasso to select variables in\\nan HIV study. The outcome Y is a measure of the resistence to an HIV-1\\ntreatment (nucleoside reverse transcriptase inhibitor), and the 30 predictors\\nare indicators of whether mutations had occurred at particular genomic\\nsites. Lasso regression with 10-fold cross-validation selected a value of\\n(cid:21) D 0:003 and the seven sites indicated in the Ô¨Ågure had nonzero coefÔ¨Å-\\ncients. The dark bars in the Ô¨Ågure indicate standard 95% conÔ¨Ådence inter-\\nvals for the coefÔ¨Åcients of the selected variables, using linear regression,\\nand ignoring the fact that the lasso was used to select the variables. Three\\nvariables are signiÔ¨Åcant, and two more nearly so. The lighter bars are con-\\nÔ¨Ådence intervals in a similar regression, but conditioned on the selection\\nevent. (cid:142)We see that they are generally wider, and only variable s25 remains (cid:142)7\\nsigniÔ¨Åcant.\\n\\n16.7 Connections and Extensions\\n\\nThere are interesting connections between lasso models and other popular\\napproaches to the prediction problem. We will brieÔ¨Çy cover two of these\\nhere, namely support-vector machines and boosting.\\n\\nLasso Logistic Regression and the SVM\\n\\nWe show in Section 19.3 that ridged logistic regression has a lot in com-\\nmon with the linear support-vector machine. For separable data the limit\\nas (cid:21) # 0 in ridged logistic regression coincides with the SVM. In addition\\ntheir loss functions are somewhat similar. The same holds true for `1 regu-\\nlarized logistic regression versus the `1 SVM‚Äîtheir end-path limits are the\\nsame. In fact, due to the similarity of the loss functions, their solutions are\\nnot too different elsewhere along the path. However, the end-path behavior\\nis a little more complex. They both converge to the `1 maximizing margin\\nseparator‚Äîthat is, the margin is measured with respect to the `1 distance\\nof points to the decision boundary, or maximum absolute coordinate.(cid:142)\\n\\n(cid:142)8\\n\\n\\x0c320\\n\\nSparse Modeling and the Lasso\\n\\nLasso and Boosting\\n\\nIn Chapter 17 we discuss boosting, a general method for building a com-\\nplex prediction model using simple building components. In its simplest\\nform (regression) boosting amounts to the following simple iteration:\\n\\n1 Inititialize b D 0 and F 0.x/ WD 0.\\n2 For b D 1; 2; : : : ; B:\\n(a) compute the residuals ri D yi (cid:0) F b(cid:0)1.xi /; i D 1; : : : ; n;\\n(b) Ô¨Åt a small regression tree to the observations .xi ; ri /n\\n\\nthink of as estimating a function gb.x/; and\\n\\n(c) update F b.x/ D F b(cid:0)1.x/ C (cid:15) (cid:1) gb.x/.\\n\\n1, which we can\\n\\nThe ‚Äúsmallness‚Äù of the tree limits the interaction order of the model (e.g.\\na tree with only two splits involves at most two variables). The number\\nof terms B and the shrinkage parameter (cid:15) are both tuning parameters that\\ncontrol the rate of learning (and hence overÔ¨Åtting), and need to be set, for\\nexample by cross-validation.\\n\\nIn words this algorithm performs a search in the space of trees for the one\\nmost correlated with the residual, and then moves the Ô¨Åtted function F b\\na small amount in that direction‚Äîa process known as forward-stagewise\\nÔ¨Åtting. One can paraphrase this simple algorithm in the context of linear\\nregression, where in step 2(b) the space of small trees is replaced by linear\\nfunctions.\\n\\n1 Inititialize Àá0 D 0, and standardize all the variables xj ; j D 1; : : : ; p.\\n2 For b D 1; 2; : : : ; B:\\n(a) compute the residuals r D y (cid:0) X Àáb;\\n(b) Ô¨Ånd the predictor xj most correlated with the residual vector r; and\\n(c) update Àáb to ÀábC1, where ÀábC1\\nC (cid:15) (cid:1) sj (sj being the sign of\\n\\nj\\nthe correlation), leaving all the other components alone.\\n\\nD Àáb\\nj\\n\\nFor small (cid:15) the solution paths for this least-squares boosting and the lasso\\nare very similar. It is natural to consider the limiting case or inÔ¨Ånitesimal\\nforward stagewise Ô¨Åtting, which we will abbreviate iFS. One can imagine\\na scenario where a number of variables are vying to win the competition\\nin step 2(b), and once they are tied their coefÔ¨Åcients move in concert as\\nthey each get incremented. This was in fact the inspiration for the LAR\\nrepresents the set of tied variables, and ƒ± is the\\nalgorithm 16.3, where\\nrelative number of turns they each have in getting their coefÔ¨Åcients up-\\ndated. It turns out that iFS is often but not always exactly the lasso; it can\\ninstead be characterized as a type of monotone lasso.(cid:142)\\n\\nA\\n\\n(cid:142)9\\n\\n\\x0c16.8 Notes and Details\\n\\n321\\n\\nNot only do these connections inspire new insights and algorithms for\\nthe lasso, they also offer insights into boosting. We can think of boosting\\nas Ô¨Åtting a monotone lasso path in the high-dimensional space of variables\\ndeÔ¨Åned by all possible trees of a certain size.\\n\\nExtensions of the Lasso\\n\\nThe idea of using `1 regularization to induce sparsity has taken hold, and\\nvariations of these ideas have spread like wildÔ¨Åre in applied statistical mod-\\neling. Along with advances in convex optimization, hardly any branch of\\napplied statistics has been left untouched. We don‚Äôt go into detail here, but\\nrefer the reader to the references in the endnotes. Instead we will end this\\nsection with a (non-exhaustive) list of such applications, which may entice\\nthe reader to venture into this domain.\\n(cid:15) The group lasso penalty PK\\n\\nk(cid:18)kk2 applies to vectors (cid:18)k of parame-\\nters, and selects whole groups at a time. Armed with these penalties, one\\ncan derive lasso-like schemes for including multilevel factors in linear\\nmodels, as well as hierarchical schemes for including low-order interac-\\ntions.\\n\\nkD1\\n\\n(cid:15) The graphical lasso applies `1 penalties in the problem of edge selection\\n\\nin dependence graphs.\\n\\n(cid:15) Sparse principal components employ `1 penalties to produce compo-\\nnents with many loadings zero. The same ideas are applied to discrimi-\\nnant analysis and canonical correlation analysis.\\n\\n(cid:15) The nuclear norm of a matrix is the sum of its singular values‚Äîa lasso\\npenalty on matrices. Nuclear-norm regularization is popular in matrix\\ncompletion for estimating missing entries in a matrix.\\n\\n16.8 Notes and Details\\n\\nClassical regression theory aimed for an unbiased estimate of each predic-\\ntor variable‚Äôs effect. Modern wide data sets, often with enormous numbers\\nof predictors p, make that an untenable goal. The methods described here,\\nby necessity, use shrinkage methods, biased estimation, and sparsity.\\n\\nThe lasso was introduced by Tibshirani (1996), and has spawned a great\\ndeal of research. The recent monograph by Hastie et al. (2015) gives a\\ncompact summary of some of the areas where the lasso and sparsity have\\nbeen applied. The regression version of boosting was given in Hastie et al.\\n(2009, Chapter 16), and inspired the least-angle regression algorithm (Efron\\n\\n\\x0c322\\n\\nSparse Modeling and the Lasso\\n\\net al., 2004)‚Äîa new and more democratic version of forward-stepwise re-\\ngression, as well as a fast algorithm for Ô¨Åtting the lasso. These authors\\nshowed under some conditions that each step of the LAR algorithm corre-\\nsponds to one df; Zou et al. (2007) show that, with a Ô¨Åxed (cid:21), the size of\\nthe active set is unbiased for the df for the lasso. Hastie et al. (2009) also\\nview boosting as Ô¨Åtting a lasso regularization path in the high-dimensional\\nspace of trees.\\n\\nFriedman et al. (2010) developed the pathwise coordinate-descent algo-\\nrithm for generalized lasso problems, and provide the glmnet package\\nfor R (Friedman et al., 2009). Strong rules for lasso screening are due to\\nTibshirani et al. (2012). Hastie et al. (2015, Chapter 3) show the similarity\\nbetween the `1 SVM and lasso logistic regression.\\n\\nWe now give some particular technical details on topics covered in the\\n\\nchapter.\\n\\n(cid:142)1 [p. 301] Forward-stepwise computations. Building up the forward-stepwise\\nmodel can be seen as a guided Gram‚ÄìSchmidt orthogonalization (QR de-\\ncomposition). After step r, all p (cid:0) r variables not in the model are orthog-\\nonal to the r in the model, and the latter are in QR form. Then the next\\nvariable to enter is the one most correlated with the residuals. This is the\\none that will reduce the residual sum-of-squares the most, and one requires\\np (cid:0) r n-vector inner products to identify it. The regression is then updated\\ntrivially to accommodate the chosen one, which is then regressed out of the\\np (cid:0) r (cid:0) 1 remaining variables.\\n\\n(cid:142)2 [p. 301] Iteratively reweighted least squares (IRLS). Generalized linear\\nmodels (Chapter 8) are Ô¨Åt by maximum-likelihood, and since the log-likeli-\\nhood is differentiable and concave, typically a Newton algorithm is used.\\nThe Newton algorithm can be recast as an iteratively reweighted linear re-\\ngression algorithm (McCullagh and Nelder, 1989). At each iteration one\\ncomputes a working response variable zi , and a weight per observation wi\\n(both of which depend on the current parameter vector OÀá). Then the New-\\nton update for OÀá is obtained by a weighted least-squares Ô¨Åt of the zi on the\\nxi with weights wi (Hastie et al., 2009, Section 4.4.1).\\n\\n(cid:142)3 [p. 301] Forward-stepwise logistic regression computations. Although the\\ncurrent model is in the form of a weighted least-squares Ô¨Åt, the p (cid:0) r vari-\\nables not in the model cannot be kept orthogonal to those in the model (the\\nweights keep changing!). However, since our current model will have per-\\nformed a weighted QR decomposition (say), this orthogonalization can be\\nobtained without too much cost. We will need p (cid:0) r multiplications of an\\nr (cid:2) n matrix with an n vector‚ÄîO..p (cid:0) r/ (cid:1) r (cid:1) n/ computations. An even\\nsimpler alternative for the selection is to use the size of the gradient of the\\n\\n\\x0c16.8 Notes and Details\\n\\n323\\n\\nlog-likelihood, which simply requires an inner product jhy (cid:0) O(cid:22)r ; xj ij for\\neach omitted variable xj (assuming all the variables are standardized to\\nunit variance).\\n\\n(cid:142)4 [p. 306] Best `1 interpolant. If p > n, then another boundary solution\\nbecomes interesting for the lasso. For t sufÔ¨Åciently large, we will be able\\nto achieve a perfect Ô¨Åt to the data, and hence a zero residual. There will\\nbe many such solutions, so it becomes interesting to Ô¨Ånd the perfect-Ô¨Åt so-\\nlution with smallest value of t: the minimum-`1-norm perfect-Ô¨Åt solution.\\nThis requires solving a separate convex-optimization problem.\\n\\n(cid:142)5 [p. 313] More on df. When the search is easy in that a variable stands out as\\nfar superior, LAR takes a big step, and forward stepwise spends close to a\\nunit df. On the other hand, when there is close competition, the LAR steps\\nare small, and a unit df is spent for little progress, while forward stepwise\\ncan spend a fair bit more than a unit df (the price paid for searching). In\\nfact, the dfj curve for forward stepwise can exceed p for j < p (Jansen\\net al., 2015).\\n\\n(cid:142)6 [p. 318] Post-selection inference. There has been a lot of activity around\\npost-selection inference for lasso and related methods, all of it since 2012.\\nTo a large extent this was inspired by the work of Berk et al. (2013), but\\nmore tailored to the particular selection process employed by the lasso. For\\nthe debiasing approach we look to the work of Zhang and Zhang (2014),\\nvan de Geer et al. (2014) and Javanmard and Montanari (2014). The condi-\\ntional inference approach began with Lockhart et al. (2014), and then was\\ndeveloped further in a series of papers (Lee et al., 2016; Taylor et al., 2015;\\nFithian et al., 2014), with many more in the pipeline.\\n\\n(cid:142)7 [p. 319] Selective inference software. The example in Figure 16.10 was pro-\\nduced using the R package selectiveInference (Tibshirani et al.,\\n2016). Thanks to Rob Tibshirani for providing this example.\\n\\n(cid:142)8 [p. 319] End-path behavior of ridge and lasso logistic regression for sep-\\narable data. The details here are somewhat technical, and rely on dual\\nnorms. Details are given in Hastie et al. (2015, Section 3.6.1).\\n\\n(cid:142)9 [p. 320] LAR and boosting. Least-squares boosting moves the ‚Äúwinning‚Äù\\ncoefÔ¨Åcient in the direction of the correlation of its variable with the resid-\\nual. The direction ƒ± computed in step 3(a) of the LAR algorithm may have\\nsome components whose signs do not agree with their correlations, espe-\\ncially if the variables are very correlated. This can be Ô¨Åxed by a particular\\nnonnegative least-squares Ô¨Åt to yield an exact path algorithm for iFS; de-\\ntails can be found in Efron et al. (2004).\\n\\n\\x0c17\\n\\nRandom Forests and Boosting\\n\\nIn the modern world we are often faced with enormous data sets, both\\nin terms of the number of observations n and in terms of the number of\\nvariables p. This is of course good news‚Äîwe have always said the more\\ndata we have, the better predictive models we can build. Well, we are there\\nnow‚Äîwe have tons of data, and must Ô¨Ågure out how to use it.\\n\\nAlthough we can scale up our software to Ô¨Åt the collection of linear and\\ngeneralized linear models to these behemoths, they are often too modest\\nand can fall way short in terms of predictive power. A need arose for some\\ngeneral purpose tools that could scale well to these bigger problems, and\\nexploit the large amount of data by Ô¨Åtting a much richer class of functions,\\nalmost automatically. Random forests and boosting are two relatively re-\\ncent innovations that Ô¨Åt the bill, and have become very popular as ‚Äúout-the-\\nbox‚Äù learning algorithms that enjoy good predictive performance. Random\\nforests are somewhat more automatic than boosting, but can also suffer a\\nsmall performance hit as a consequence.\\n\\nThese two methods have something in common: they both represent the\\nÔ¨Åtted model by a sum of regression trees. We discuss trees in some detail\\nin Chapter 8. A single regression tree is typically a rather weak prediction\\nmodel; it is rather amazing that an ensemble of trees leads to the state of\\nthe art in black-box predictors!\\n\\nWe can broadly describe both these methods very simply.\\n\\nRandom forest Grow many deep regression trees to randomized versions\\nof the training data, and average them. Here ‚Äúrandomized‚Äù is a wide-\\nranging term, and includes bootstrap sampling and/or subsampling of\\nthe observations, as well as subsampling of the variables.\\n\\nBoosting Repeatedly grow shallow trees to the residuals, and hence build\\n\\nup an additive model consisting of a sum of trees.\\n\\nThe basic mechanism in random forests is variance reduction by averag-\\ning. Each deep tree has a high variance, and the averaging brings the vari-\\n\\n324\\n\\n\\x0c17.1 Random Forests\\n\\n325\\n\\nance down. In boosting the basic mechanism is bias reduction, although\\ndifferent Ô¨Çavors include some variance reduction as well. Both methods\\ninherit all the good attributes of trees, most notable of which is variable\\nselection.\\n\\n17.1 Random Forests\\n\\nSuppose we have the usual setup for a regression problem, with a training\\nset consisting of an n (cid:2) p data matrix X and an n-vector of responses y. A\\ntree (Section 8.4) Ô¨Åts a piecewise constant surface Or.x/ over the domain\\nX\\nby recursive partitioning. The model is built in a greedy fashion, each time\\ncreating two daughter nodes from a terminal node by deÔ¨Åning a binary split\\nusing one of the available variables. The model can hence be represented\\nby a binary tree. Part of the art in using regression trees is to know how\\ndeep to grow the tree, or alternatively how much to prune it back. Typi-\\ncally that is achieved using left-out data or cross-validation. Figure 17.1\\nshows a tree Ô¨Åt to the spam training data. The splitting variables and split\\npoints are indicated. Each node is labeled as spam or ham (not spam;\\nsee footnote 7 on page 115). The numbers beneath each node show mis-\\nclassiÔ¨Åed/total. The overall misclassiÔ¨Åcation error on the test data is 9:3%,\\nwhich compares poorly with the performance of the lasso (Figure 16.9:\\n7:1% for linear lasso, 5:7% for lasso with interactions). The surface Or.x/\\nhere is clearly complex, and by its nature represents a rather high-order\\ninteraction (the deepest branch is eight levels, and involves splits on eight\\ndifferent variables). Despite the promise to deliver interpretable models,\\nthis bushy tree is not easy to interpret. Nevertheless, trees have some desir-\\nable properties. The following lists some of the good and bad properties of\\ntrees.\\n\\n(cid:115) Trees automatically select variables; only variables used in deÔ¨Åning splits\\n\\nare in the model.\\n\\n(cid:115) Tree-growing algorithms scale well to large n; growing a tree is a divide-\\n\\nand-conquer operation.\\n\\n(cid:115) Trees handle mixed features (quantitative/qualitative) seamlessly, and\\n\\ncan deal with missing data.\\n(cid:115) Small trees are easy to interpret.\\n(cid:116) Large trees are not easy to interpret.\\n(cid:116) Trees do not generally have good prediction performance.\\n\\nTrees are inherently high-variance function estimators, and the bushier\\nthey are, the higher the variance. The early splits dictate the architecture of\\n\\n\\x0c326\\n\\nRandom Forests and Boosting\\n\\nFigure 17.1 Regression tree Ô¨Åt to the binary spam data, a bigger\\nversion of Figure 8.7. The initial trained tree was far bushier than\\nthe one displayed; it was then optimally pruned using 10-fold\\ncross-validation.\\n\\nthe tree. On the other hand, deep bushy trees localize the training data (us-\\ning the variables that matter) to a relatively small region around the target\\npoint. This suggests low bias. The idea of random forests (and its predeces-\\nsor bagging) is to grow many very bushy trees, and get rid of the variance\\nby averaging. In order to beneÔ¨Åt from averaging, the individual trees should\\nnot be too correlated. This is achieved by injecting some randomness into\\nthe tree-growing process. Random forests achieve this in two ways.\\n\\n600/1536280/1177180/1065 80/861 80/652 77/423 20/238 19/236  1/2 57/185 48/113 37/101  1/12  9/72  3/229  0/209100/204 36/123 16/94 14/89  3/5  9/29 16/81  9/112  6/109  0/3 48/359 26/337 19/110 18/109  0/1  7/227  0/22spamspamspamspamspamspamspamspamspamspamspamspamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamhamch$<0.0555remove<0.06ch!<0.191george<0.005hp<0.03CAPMAX<10.5receive<0.125edu<0.045our<1.2CAPAVE<2.7505free<0.065business<0.145george<0.15hp<0.405CAPAVE<2.9071999<0.58ch$>0.0555remove>0.06ch!>0.191george>0.005hp>0.03CAPMAX>10.5receive>0.125edu>0.045our>1.2CAPAVE>2.7505free>0.065business>0.145george>0.15hp>0.405CAPAVE>2.9071999>0.58\\x0c17.1 Random Forests\\n\\n327\\n\\n1 Bootstrap: each tree is grown to a bootstrap resampled training data set,\\n\\nwhich makes them different and somewhat decorrelates them.\\n\\n2 Split-variable randomization: each time a split is to be performed, the\\nsearch for the split variable is limited to a random subset of m of the p\\np\\nvariables. Typical values of m are\\n\\np or p=3.\\n\\nWhen m D p, the randomization amounts to using only step 1, and was\\nan earlier ancestor of random forests called bagging. In most examples the\\nsecond level of randomization pays dividends.\\n\\nAlgorithm 17.1 RANDOM FOREST.\\n\\n1 Given training data set d D .X ; y/. Fix m (cid:20) p and the number of trees\\n\\nB.\\n\\n2 For b D 1; 2; : : : ; B, do the following.\\n(a) Create a bootstrap version of the training data d (cid:3)\\n\\nb , by randomly sam-\\npling the n rows with replacement n times. The sample can be repre-\\nsented by the bootstrap frequency vector w(cid:3)\\nb.\\n\\n(b) Grow a maximal-depth tree Orb.x/ using the data in d (cid:3)\\nof the p features at random prior to making each split.\\n\\nb , sampling m\\n\\n(c) Save the tree, as well as the bootstrap sampling frequencies for each\\n\\nof the training observations.\\n\\n3 Compute the random-forest Ô¨Åt at any prediction point x0 as the average\\n\\nOrrf.x0/ D 1\\nB\\n\\nB\\nX\\n\\nbD1\\n\\nOrb.x0/:\\n\\n4 Compute the OOBi error for each response observation yi in the training\\ndata, by using the Ô¨Åt Or .i/\\n, obtained by averaging only those Orb.xi / for\\nrf\\nwhich observation i was not in the bootstrap sample. The overall OOB\\nerror is the average of these OOBi .\\n\\nAlgorithm 17.1 gives some of the details; some more are given in the\\n\\ntechnical notes.(cid:142)\\n\\n(cid:142)1\\n\\nRandom forests are easy to use, since there is not much tuning needed.\\nThe package randomForest in R sets as a default m D\\np for clas-\\nsiÔ¨Åcation trees, and m D p=3 for regression trees, but one can use other\\nvalues. With m D 1 the split variable is completely random, so all vari-\\nables get a chance. This will decorrelate the trees the most, but can create\\nbias, somewhat similar to that in ridge regression. Figure 17.2 shows the\\n\\np\\n\\n\\x0c328\\n\\nRandom Forests and Boosting\\n\\nFigure 17.2 Test misclassiÔ¨Åcation error of random forests on the\\nspam data, as a function of the number of trees. The red curve\\nselects m D 7 of the p D 57 features at random as candidates for\\nthe split variable, each time a split is made. The blue curve uses\\nm D 57, and hence amounts to bagging. Both bagging and\\nrandom forests outperform the lasso methods, and a single tree.\\n\\nmisclassiÔ¨Åcation performance of a random forest on the spam test data, as\\na function of the number of trees averaged. We see that in this case, after\\na relatively small number of trees (500), the error levels off. The number\\nB of trees averaged is not a real tuning parameter; as with the bootstrap\\n(Chapters 10 and 11), we need a sufÔ¨Åcient number for the estimate to sta-\\nbilize, but cannot overÔ¨Åt by having too many.\\n\\nRandom forests have been described as adaptive nearest-neighbor esti-\\nmators‚Äîadaptive in that they select predictors. A k-nearest-neighbor esti-\\nmate Ô¨Ånds the k training observations closest in feature space to the target\\npoint x0, and averages their responses. Each tree in the random forest drills\\ndown by recursive partitioning to pure terminal nodes, often consisting of a\\nsingle observation. Hence, when evaluating the prediction from each tree,\\nOrb.x0/ D y` for some `, and for many of the trees this could be the same\\n\\n0.000.020.040.060.08Random Forest on the Spam DataNumber of TreesTest Error15001000150020002500BaggingRandom ForestSingle TreeLassoLasso (interaction)\\x0c17.1 Random Forests\\n\\n329\\n\\n`. From the whole collection of B trees, the number of distinct `s can be\\nfairly small. Since the partitioning that reaches the terminal nodes involves\\nonly a subset of the predictors, the neighborhoods so deÔ¨Åned are adaptive.\\n\\nOut-of-Bag Error Estimates\\n\\nRandom forests deliver cross-validated error estimates at virtually no ex-\\ntra cost. The idea is similar to the bootstrap error estimates discussed in\\nChapter 10. The computation is described in step 4 of Algorithm 17.1.\\nIn making the prediction for observation pair .xi ; yi /, we average all the\\nrandom-forest trees Orb.xi / for which that pair is not in the corresponding\\nbootstrap sample:\\n\\nFigure 17.3 Out-of-bag misclassiÔ¨Åcation error estimate for the\\nspam data (blue) versus the test error (red), as a function of the\\nnumber of trees.\\n\\nrf .xi / D 1\\nOr .i/\\nBi\\n\\nX\\n\\nOrb.xi /;\\n\\nb W w (cid:3)\\nb i\\n\\nD0\\n\\n(17.1)\\n\\nwhere Bi is the number of times observation i was not in the bootstrap\\nsample (with expected value e(cid:0)1B (cid:25) 0:37B). We then compute the OOB\\nerror estimate\\n\\nerrOOB D 1\\nn\\n\\nn\\nX\\n\\niD1\\n\\nL≈íyi ; Or .i/\\n\\nrf .xi /(cid:141);\\n\\n(17.2)\\n\\n0.000.020.040.060.08Number of TreesMisclassification Error15001000150020002500OOB ErrorTest Error\\x0c330\\n\\nRandom Forests and Boosting\\n\\nwhere L is the loss function of interest, such as misclassiÔ¨Åcation or squared-\\nerror loss. If B is sufÔ¨Åciently large (about 1:58 times the number needed\\nfor the random forest to stabilize), we can see that the OOB error estimate\\nis equivalent to leave-one-out cross-validation error.\\n\\nStandard Errors\\n\\nWe can use very similar ideas to estimate the variance of a random-forest\\nprediction, using the jackknife variance estimator (see (10.6) in Chapter 10).\\nIf O(cid:18) is a statistic estimated using all n training observations, then the jack-\\nknife estimate of the variance of O(cid:18) is given by\\n\\nbVjack. O(cid:18)/ D n (cid:0) 1\\n\\nn\\n\\nn\\nX\\n\\niD1\\n\\n(cid:16) O(cid:18).i/ (cid:0) O(cid:18).(cid:1)/\\n\\n(cid:17)2\\n\\n;\\n\\n(17.3)\\n\\nwhere O(cid:18).i/ is the estimate using all but observation i, and O(cid:18).(cid:1)/ D 1\\n\\nO(cid:18).i/.\\nThe natural jackknife variance estimate for a random-forest prediction\\n\\nP\\ni\\n\\nn\\n\\nat x0 is obtained by simply plugging into this formula:\\n\\nbVjack. Orrf.x0// D n (cid:0) 1\\n\\nn\\n\\nn\\nX\\n\\niD1\\n\\n(cid:16)\\nOr .i/\\nrf .x0/ (cid:0) Orrf.x0/\\n\\n(cid:17)2\\n\\n:\\n\\n(17.4)\\n\\nThis formula is derived under the B D 1 setting, in which case Orrf.x0/ is\\nan expectation under bootstrap sampling, and hence is free of Monte Carlo\\nvariability. This also makes the distinction clear: we are estimating the sam-\\npling variability of a random-forest prediction Orrf.x0/, as distinct from any\\nMonte Carlo variation. In practice B is Ô¨Ånite, and expression (17.4) will\\nhave Monte Carlo bias and variance. All of the Or .i/\\nrf .x0/ are based on B\\nbootstrap samples, and they are hence noisy versions of their expectations.\\nSince the n quantities summed in (17.4) are squared, by Jensen‚Äôs inequal-\\nity we will have positive bias (and it turns out that this bias dominates the\\nMonte Carlo variance). Hence one would want to use a much larger value\\nof B when estimating variances, than was used in the original random-\\nforest Ô¨Åt. Alternatively, one can use the same B bootstrap samples as were\\nused to Ô¨Åt the random forest, along with a bias-corrected version of the\\njackknife variance estimate:(cid:142)\\n\\n(cid:142)2\\n\\nbVu\\n\\njack. Orrf.x0// D bVjack. Orrf.x0// (cid:0) .e (cid:0) 1/\\n\\nn\\nB\\n\\nOv.x0/;\\n\\n(17.5)\\n\\n\\x0c17.1 Random Forests\\n\\n331\\n\\nFigure 17.4 Jackknife Standard Error estimates (with bias\\ncorrection) for the probability estimates in the spam test data.\\nThe points labeled red were misclassiÔ¨Åcations, and tend to\\nconcentrate near the decision boundary (0.5).\\n\\nwhere e D 2:718 : : :, and\\n\\nOv.x0/ D 1\\nB\\n\\nB\\nX\\n\\nbD1\\n\\n. Orb.x0/ (cid:0) Orrf.x0//2 ;\\n\\n(17.6)\\n\\nthe bootstrap estimate of the variance of a single random-forest tree. All\\nthese quantities are easily computed from the output of a random forest, so\\nthey are immediately available. Figure 17.4 shows the predicted probabil-\\nities and their jackknife estimated standard errors for the spam test data.\\nThe estimates near the decision boundary tend to have higher standard er-\\nrors.\\n\\nVariable-Importance Plots\\n\\nA random forest is something of a black box, giving good predictions\\nbut usually not much insight into the underlying surface it has Ô¨Åt. Each\\nrandom-forest tree Orb will have used a subset of the predictors as split-\\nting variables, and each tree is likely to use overlapping but not necessarily\\n\\nlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.00.000.050.100.150.200.250.30Probability PredictionStandard Error Estimate\\x0c332\\n\\nRandom Forests and Boosting\\n\\nFigure 17.5 Variable-importance plots for random forests Ô¨Åt to\\nthe spam data. On the left we have the m D 7 random forest; due\\nto the split-variable randomization, it spreads the importance\\namong the variables. On the right is the m D 57 random forest or\\nbagging, which focuses on a smaller subset of the variables.\\n\\nidentical subsets. One might conclude that any variable never used in any\\nof the trees is unlikely to be important, but we would like a method of\\nassessing the relative importance of variables that are included in the en-\\nsemble. Variable-importance plots Ô¨Åt this bill. Whenever a variable is used\\nin a tree, the algorithm logs the decrease in the split-criterion due to this\\nsplit. These are accumulated over all the trees, for each variable, and sum-\\nmarized as relative importance measures. Figure 17.5 demonstrates this on\\nthe spam data. We see that the m D 7 random forest, by virtue of the\\nsplit-variable randomization, spreads the importance out much more than\\nbagging, which always gets to pick the best variable for splitting. In this\\nsense small m has some similarity to ridge regression, which also tends to\\nshare the coefÔ¨Åcients evenly among correlated variables.\\n\\n!$removefreeCAPAVEyourCAPMAXhpCAPTOTmoneyyouourgeorge000edubusinesshpl1999internet(willallrereceiveemailmailover;meeting650addressorderlabspmpeoplecreditmake#technologydatafont85[labtelnetreportoriginalprojectconferenceaddressesdirect4158573dcspartstableRandom Forest m = 7020406080100Variable Importance$!removehpCAPAVEfreeCAPTOTCAPMAXgeorgeyoueduyourourbusinessmoneywill(reemail000internetreceivemailmeeting1999overpm;dataorderfontpeopleallmakeconference650reportaddresshploriginaltechnologylab3dlabs[#creditcs85telnetpartsproject857addressesdirect415tableBagging m = 57020406080100Variable Importance\\x0c17.2 Boosting with Squared-Error Loss\\n\\n333\\n\\n17.2 Boosting with Squared-Error Loss\\n\\nBoosting was originally proposed as a means for improving the perfor-\\nmance of ‚Äúweak learners‚Äù in binary classiÔ¨Åcation problems. This was achiev-\\ned through resampling training points‚Äîgiving more weight to those which\\nhad been misclassiÔ¨Åed‚Äîto produce a new classiÔ¨Åer that would boost the\\nperformance in previously problematic areas of feature space. This pro-\\ncess is repeated, generating a stream of classiÔ¨Åers, which are ultimately\\ncombined through voting1 to produce the Ô¨Ånal classiÔ¨Åer. The prototypical\\nweak learner was a decision tree.\\n\\nBoosting has evolved since this earliest invention, and different Ô¨Çavors\\nare popular in statistics, computer science, and other areas of pattern recog-\\nnition and prediction. We focus on the version popular in statistics‚Äîgradient\\nboosting‚Äîand return to this early version later in the chapter.\\n\\nAlgorithm 17.2 GRADIENT BOOSTING WITH SQUARED-ERROR LOSS.\\n\\n1 Given a training sample d D .X ; y/. Fix the number of steps B, the\\nshrinkage factor (cid:15) and the tree depth d . Set the initial Ô¨Åt bG0 (cid:17) 0, and\\nthe residual vector r D y.\\n2 For b D 1; 2; : : : ; B repeat:\\n(a) Fit a regression tree Qgb to the data .X ; r/, grown best-Ô¨Årst to depth\\nd : this means the total number of splits are d , and each successive\\nsplit is made to that terminal node that yields the biggest reduction in\\nresidual sum of squares.\\n\\n(b) Update the Ô¨Åtted model with a shrunken version of Qgb: bGb D bGb(cid:0)1 C\\n\\nOgb; with Ogb D (cid:15) (cid:1) Qgb:\\n\\n(c) Update the residuals accordingly: ri D ri (cid:0) Ogb.xi /; i D 1; : : : ; n:\\n3 Return the sequence of Ô¨Åtted functions bGb; b D 1; : : : ; B.\\n\\nAlgorithm 17.2 gives the most basic version of gradient boosting, for\\nsquared-error loss. This amounts to building a model by repeatedly Ô¨Åtting\\na regression tree to the residuals. Importantly, the tree is typically quite\\nsmall, involving a small number d of splits‚Äîit is indeed a weak learner.\\nAfter each tree has been grown to the residuals, it is shrunk down by a\\nfactor (cid:15) before it is added to the current model; this is a means of slowing\\nthe learning process. Despite the obvious similarities with a random forest,\\nboosting is different in a fundamental way. The trees in a random forest are\\n\\n1 Each classiÔ¨Åer Ocb.x0/ predicts a class label, and the class with the most ‚Äúvotes‚Äù wins.\\n\\n\\x0c334\\n\\nRandom Forests and Boosting\\n\\nFigure 17.6 Test performance of a boosted regression-tree model\\nÔ¨Åt to the ALS training data, with n D 1197 and p D 369. Shown\\nis the mean-squared error on the 625 designated test observations,\\nas a function of the number of trees. Here the depth d D 4 and\\n(cid:15) D 0:02. Boosting achieves a lower test MSE than a random\\nforest. We see that as the number of trees B gets large, the test\\nerror for boosting starts to increase‚Äîa consequence of\\noverÔ¨Åtting. The random forest does not overÔ¨Åt. The dotted blue\\nhorizontal line shows the best performance of a linear model, Ô¨Åt\\nby the lasso. The differences are less dramatic than they appear,\\nsince the vertical scale does not extend to zero.\\n\\nidentically distributed‚Äîthe same (random) treatment is repeatedly applied\\nto the same data. With boosting, on the other hand, each tree is trying to\\namend errors made by the ensemble of previously grown trees. The number\\nof terms B is important as well, because unlike random forests, a boosted\\nregression model can overÔ¨Åt if B is too large. Hence there are three tuning\\nparameters, B, d and (cid:15), and each can change the performance of a boosted\\nmodel, sometimes considerably.\\n\\n(cid:142)3\\n\\nFigure 17.6 shows the test performance of boosting on the ALS data. (cid:142)\\nThese data represent measurements on patients with amyotrophic lateral\\nsclerosis (Lou Gehrig‚Äôs disease). The goal is to predict the rate of pro-\\ngression of an ALS functional rating score (FRS). There are 1197 training\\n\\n0.250.260.270.280.290.30Number of TreesMean‚àísquared Error1100200300400500BoostRandom ForestLasso\\x0c17.2 Boosting with Squared-Error Loss\\n\\n335\\n\\nmeasurements on 369 predictors and the response, with a corresponding\\ntest set of size 625 observations.\\n\\nAs is often the case, boosting slightly outperforms a random forest here,\\nbut at a price. Careful tuning of boosting requires considerable extra work,\\nwith time-costly rounds of cross-validation, whereas random forests are al-\\nmost automatic. In the following sections we explore in more detail some\\nof the tuning parameters. The R package gbm implements gradient boost-\\ning, with some added bells and whistles. By default it grows each new tree\\non a 50% random sub-sample of the training data. Apart from speeding up\\nthe computations, this has a similar effect to bagging, and results in some\\nvariance reduction in the ensemble.\\n\\nWe can also compute a variable-importance plot, as we did for random\\nforests; this is displayed in Figure 17.7 for the ALS data. Only 267 of the\\n369 variables were ever used, with one variable Onset.Delta standing\\nout ahead of the others. This measures the amount of time that has elapsed\\nsince the patient was Ô¨Årst diagnosed with ALS, and hence a larger value\\nwill indicate a slower progression rate.\\n\\nTree Depth and Interaction Order\\n\\nTree depth d is an important parameter for gradient boosted models, and\\nthe right choice will depend on the data at hand. Here depth d D 4 appears\\nto be a good choice on the test data. Without test data, we could use cross-\\nvalidation to make the selection. Apart from a general complexity measure,\\ntree depth also controls the interaction order of the model.2 The easiest\\ncase is with d D 1, where each tree consists of a single split (a stump).\\nSuppose we have a Ô¨Åtted boosted model bGB .x/, using B trees. Denote by\\nD f1; 2; : : : ; Bg the indices of the trees that made the single split\\n` can be\\n\\nB\\nusing variable j , for j D 1; : : : ; p. These\\n\\nj are disjoint (some\\n\\nj (cid:18)\\n\\nB\\n\\nB\\n\\nB\\n\\n2 A .k (cid:0) 1/th-order interaction is also known as a k-way interaction. Hence an order-one\\n\\ninteraction model has two-way interactions, and an order-zero model is additive.\\n\\n\\x0c336\\n\\nRandom Forests and Boosting\\n\\nFigure 17.7 Variable importance plot for the ALS data. Here 267\\nof the 369 variables were used in the ensemble. There are too\\nmany variables for the labels to be visible, so this plot serves as a\\nvisual guide. Variable Onset.Delta has relative importance\\n100 (the lowest red bar), more than double the next two at around\\n40 (last.slope.weight and alsfrs.score.slope).\\nHowever, the importances drop off slowly, suggesting that the\\nmodel requires a signiÔ¨Åcant fraction of the variables.\\n\\nempty), and Sp\\n\\nj D\\n\\nj D1 B\\n\\n. Then we can write\\n\\nB\\n\\nbGB .x/ D\\n\\nD\\n\\nD\\n\\nB\\nX\\n\\nbD1\\np\\nX\\n\\nj D1\\n\\np\\nX\\n\\nj D1\\n\\nOgb.x/\\n\\nOgb.x/\\n\\nX\\n\\nb2\\n\\nj\\nB\\n\\nOfj .xj /:\\n\\n(17.7)\\n\\nOnset.Deltalast.slope.weightalsfrs.score.slopelast.slope.bp.systolicmean.slope.svc.literssum.slope.alsfrs.scoremean.slope.weightlast.slope.fvc.litersmeansquares.alsfrs.scoresum.slope.fvc.literslast.alsfrs.scorefvc.liters.slopesum.bp.diastolicweight.slopemean.speechmax.slope.fvc.litersmean.slope.handwritingslope.bp.systolic.slopemean.slope.fvc.litersmin.slope.alsfrs.scoresd.fvc.litersmeansquares.climbing.stairssd.salivationlast.speechmin.slope.bp.systolicmean.alsfrs.scoremax.slope.salivationsum.slope.climbing.stairslast.fvc.literssum.fvc.litersmean.slope.alsfrs.scorebp.diastolic.slopelast.slope.handwritingmin.slope.fvc.litersspeech.slopesd.slope.alsfrs.scoremeansquares.slope.weightslope.handwriting.slopeslope.weight.slopesum.alsfrs.scoreslope.turning.slopelast.slope.swallowingsum.slope.speechsum.weightmax.fvc.literslast.svc.liters.datemax.bp.diastolicmin.alsfrs.scoremean.climbing.stairsmax.dressingmeansquares.bp.diastoliclast.weightsum.slope.weightmeansquares.resp.rateslope.fvc.liters.slopemean.slope.climbing.stairsswallowing.slopesum.slope.handwritinglast.slope.alsfrs.score.datemeansquares.dressingsd.bp.diastolicclimbing.stairs.slopeslope.alsfrs.score.slopelast.slope.svc.liters.datesum.handwritingmean.slope.speechsum.slope.dressingsd.bp.systolicmin.slope.cuttingmax.weightlast.slope.alsfrs.scoremean.fvc.litersslope.bp.diastolic.slopeslope.salivation.slopemeansquares.slope.dressingmin.slope.speechsd.slope.fvc.litersmax.alsfrs.scoresum.slope.bp.systolicmeansquares.slope.bp.diastolicsd.slope.bp.systolicslope.dressing.slopemin.slope.dressinglast.slope.climbing.stairsmax.slope.alsfrs.scoremax.slope.bp.systolicsd.dressingsum.climbing.stairssd.slope.weightslope.resp.rate.slopesd.resp.ratemean.slope.bp.systoliclast.slope.weight.datemax.heightlast.slope.svc.litersmin.bp.systolicsd.slope.turningsum.slope.salivationfirst.slope.fvc.liters.datelast.bp.diastolicsd.slope.svc.literssd.slope.dressingmeansquares.slope.fvc.litersmeansquares.speechmeansquares.fvc.literslast.alsfrs.score.datesd.slope.resp.ratemeansquares.slope.cuttingmin.slope.turningmin.fvc.litersslope.swallowing.slopelast.slope.bp.diastolicmax.slope.weightsd.slope.swallowinglast.slope.resp.rate.datemean.turningmean.bp.diastolicsum.dressingmeansquares.cuttingsum.resp.ratemeansquares.slope.turningbp.systolic.slopemean.slope.dressingsd.cuttingmean.slope.bp.diastolicsum.heightsalivation.slopemin.slope.weightfirst.slope.weight.datesum.slope.resp.ratemax.slope.swallowinglast.swallowingmean.slope.cuttingmax.slope.svc.litersmean.slope.turningmax.cuttinglast.turningmean.slope.resp.ratesum.bp.systolicmeansquares.slope.climbing.stairsfirst.slope.resp.rate.datesd.slope.salivationslope.cutting.slopelast.resp.rate.datesd.alsfrs.scoresd.slope.cuttingSite.of.Onset.Onset..Bulbarfirst.slope.svc.liters.datemin.turningsd.slope.bp.diastolicmin.cuttingmax.slope.resp.rateAgesum.slope.swallowingsum.swallowingsd.speechlast.dressingfirst.slope.alsfrs.score.datemean.slope.salivationmeansquares.handwritingmin.weightlast.bp.systolicmean.salivationmeansquares.slope.alsfrs.scoremin.slope.climbing.stairsmean.slope.swallowinglast.slope.fvc.liters.datesum.salivationmin.slope.salivationmean.dressingslope.climbing.stairs.slopemeansquares.slope.walkingmeansquares.slope.bp.systolicresp.rate.slopelast.slope.dressinglast.slope.resp.ratemax.slope.dressingSex.Femalemin.swallowingmax.bp.systolicmin.bp.diastolicmeansquares.slope.handwritingmin.slope.bp.diastolicfirst.slope.bp.diastolic.datemean.handwritinglast.height.datesd.svc.literssd.slope.handwritingmean.slope.walkingmean.cuttingsum.slope.bp.diastoliclast.slope.salivationmin.slope.swallowinglast.fvc.liters.datewalking.slopesum.cuttingmean.swallowingsd.swallowingsum.speechmax.slope.climbing.stairshandwriting.slopemean.svc.litersmax.speechmeansquares.salivationmeansquares.slope.svc.literslast.slope.bp.diastolic.datefirst.resp.rate.datemeansquares.slope.salivationmean.resp.ratemax.slope.bp.diastolicmin.speechslope.speech.slopemean.weightSex.Malemax.svc.literssum.turningmax.slope.turninglast.svc.litersmax.swallowingmeansquares.turningmax.handwritingmeansquares.swallowingsd.slope.climbing.stairsmin.climbing.stairssvc.liters.slopelast.cuttingmeansquares.bp.systoliclast.slope.turningmax.salivationlast.weight.datemeansquares.slope.resp.ratemeansquares.weightmeansquares.slope.speechmeansquares.slope.swallowingmax.walkinglast.handwritingnum.slope.weight.visitssum.slope.cuttinglast.resp.ratesd.weightsd.climbing.stairssum.slope.turningsum.walkingslope.walking.slopeno.height.datafirst.alsfrs.score.datemin.salivationmin.slope.resp.ratefirst.slope.bp.systolic.datelast.slope.walkingStudy.Arm.PLACEBOfirst.slope.height.datemin.slope.handwritinglast.climbing.stairssd.handwritingSymptom.WEAKNESSmax.slope.handwritinglast.slope.cuttingmax.turningmin.slope.svc.litersdressing.slopesd.slope.speechMothernum.slope.resp.rate.visitsSymptom.AtrophySymptom.Swallowingcutting.slopenum.slope.bp.systolic.visitslessthan2.slope.bp.diastolicnum.slope.bp.diastolic.visitsno.slope.resp.rate.datalessthan2.slope.resp.rateno.slope.weight.datalessthan2.slope.weightslope.svc.liters.slopeno.slope.fvc.liters.datalessthan2.slope.fvc.litersnum.slope.fvc.liters.visitssd.slope.walkingsum.slope.walkingmin.slope.walkingmax.slope.walkingmax.slope.cuttinglast.slope.speechmax.slope.speechlessthan2.slope.alsfrs.scorenum.slope.alsfrs.score.visitsnum.bp.systolic.visitsmean.bp.systolicno.bp.diastolic.datalessthan2.bp.diastoliclast.bp.diastolic.datefirst.bp.diastolic.datenum.bp.diastolic.visitsno.resp.rate.datalessthan2.resp.ratenum.resp.rate.visitsmin.resp.ratemax.resp.ratesd.heightmeansquares.heightfirst.height.datenum.height.visitsmean.heightmin.heightlessthan2.weightfirst.weight.datenum.weight.visitsmeansquares.svc.literssum.svc.litersnum.svc.liters.visitsmin.svc.litersno.fvc.liters.datalessthan2.fvc.litersfirst.fvc.liters.datenum.fvc.liters.visitsmax.climbing.stairssd.walkingmeansquares.walkingmean.walkinglast.walkingmin.walkingturning.slopesd.turningmin.dressingmin.handwritinglast.salivationno.alsfrs.score.datalessthan2.alsfrs.scorenum.alsfrs.score.visitsStudy.Arm.ACTIVENeurological.Disease.STROKE.HEMORRHAGICNeurological.Disease.STROKE.ISCHEMICNeurological.Disease.BRAIN.TUMORNeurological.Disease.ALSNeurological.Disease.DATNeurological.Disease.PARKINSON.S.DISEASENeurological.Disease.DEMENTIA.NOSNeurological.Disease.STROKE.NOSNeurological.Disease.OTHERFamilyBrotherSisterDaughterSonUncle..Paternal.Uncle..Maternal.UncleGrandmother..Maternal.GrandmotherGrandfather..Maternal.FatherCousinAunt..Maternal.AuntRace...OtherRace...CaucasianRace...Black.African.AmericanRace...AsianSite.of.Onset.Onset..Limb.and.BulbarSite.of.Onset.Onset..LimbSymptom..Symptom.StiffnessSymptom.SENSORY_CHANGESSymptom.FasciculationsSymptom.CrampsSymptom.GAIT_CHANGESSymptom.OTHERSymptom.Speech020406080100Variable‚àíImportance Plot for Boosting on the  ALS Data\\x0c17.2 Boosting with Squared-Error Loss\\n\\n337\\n\\nFigure 17.8 ALS test error for boosted models with different\\ndepth parameters d , and all using the same shrinkage parameter\\n(cid:15) D 0:02. It appears that d D 1 is inferior to the rest, with d D 4\\nabout the best. With d D 7, overÔ¨Åtting begins around 200 trees,\\nwith d D 4 around 300, while neither of the other two show\\nevidence of overÔ¨Åtting by 500 trees.\\n\\nHence boosted stumps Ô¨Åts an additive model, but in a fully adaptive way.\\nIt selects variables, and also selects how much action to devote to each\\nvariable. We return to additive models in Section 17.5. Figure 17.9 shows\\nthe three functions with highest relative importance. The Ô¨Årst function con-\\nÔ¨Årms that a longer time since diagnosis (more negative Onset.Delta)\\npredicts a slower decline. last.slope.weight is the difference in\\nbody weight at the last two visits‚Äîagain positive is good. Likewise for\\nalsfrs.score.slope, which measures the local slope of the FRS\\nscore after the Ô¨Årst two visits.\\n\\nIn a similar way, boosting with d D 2 Ô¨Åts a two-way interaction model;\\neach tree involves at most two variables. In general, boosting with d D k\\nleads to a .k (cid:0) 1/th-order interaction model. Interaction order is perhaps a\\nmore natural way to think of model complexity.\\n\\n0.250.260.270.280.290.30Number of TreesMean‚àísquared Error1100200300400500Depth1247\\x0c338\\n\\nRandom Forests and Boosting\\n\\nFigure 17.9 Three of the Ô¨Åtted functions (17.7) for the ALS data,\\nin a boosted stumps model (d D 1), each centered to average zero\\nover the training data. In terms of the outcome, bigger is better\\n(slower decline in FRS). The Ô¨Årst function conÔ¨Årms that a longer\\ntime since diagnosis (more negative value of Onset.Delta)\\npredicts a slower decline. The variable last.slope.weight\\nis the difference in body weight at the last two visits‚Äîagain\\npositive is good. Likewise for alsfrs.score.slope, which\\nmeasures the local slope of the FRS score after the Ô¨Årst two visits.\\n\\nShrinkage\\n\\nThe shrinkage parameter (cid:15) controls the rate at which boosting Ô¨Åts‚Äîand\\nhence overÔ¨Åts‚Äîthe data. Figure 17.10 demonstrates the effect of shrink-\\nage on the ALS data. The under-shrunk ensemble (red) quickly overÔ¨Åts the\\ndata, leading to poor validation error. The blue ensemble uses a shrink-\\nage parameter 25 times smaller, and reaches a lower validation error. The\\ndownside of a very small shrinkage parameter is that it can take many trees\\nto adequately Ô¨Åt the data. On the other hand, the shrunken Ô¨Åts are smoother,\\ntake much longer to overÔ¨Åt, and hence are less sensitive to the stopping\\npoint B.\\n\\n17.3 Gradient Boosting\\n\\nWe now turn our attention to boosting models using other than square-error\\nloss. We focus on the family of generalized models generated by the expo-\\nnential family of response distributions (see Chapter 8). The most popular\\nand relevant in this class is logistic regression, where we are interested in\\nmodeling (cid:22).x/ D Pr.Y D 1jX D x/ for a Bernoulli response variable.\\n\\n‚àí2000‚àí10000‚àí0.3‚àí0.2‚àí0.10.00.10.2Onset.DeltaFitted Function‚àí20‚àí1001020‚àí0.3‚àí0.2‚àí0.10.00.10.2last.slope.weightFitted Function‚àí10‚àí6‚àí4‚àí2024‚àí0.3‚àí0.2‚àí0.10.00.10.2alsfrs.score.slopeFitted Function\\x0c17.3 Gradient Boosting\\n\\n339\\n\\nFigure 17.10 Boosted d D 3 models with different shrinkage\\nparameters, Ô¨Åt to a subset of the ALS data. The solid curves are\\nvalidation errors, the dashed curves training errors, with red for\\n(cid:15) D 0:5 and blue for (cid:15) D 0:02. With (cid:15) D 0:5, the training error\\ndrops rapidly with the number of trees, but the validation error\\nstarts to increase rapidly after an initial decrease. With (cid:15) D 0:02\\n(25 times smaller), the training error drops more slowly. The\\nvalidation error also drops more slowly, but reaches a lower\\nminimum (the horizontal dotted line) than the (cid:15) D 0:5 case. In\\nthis case, the slower learning has paid off.\\n\\nThe idea is to Ô¨Åt a model of the form\\n\\n(cid:21).x/ D GB .x/ D\\n\\nB\\nX\\n\\nbD1\\n\\ngb.xI (cid:13)b/;\\n\\n(17.8)\\n\\nwhere (cid:21).x/ is the natural parameter in the conditional distribution of Y jX D\\nx, and the gb.xI (cid:13)b/ are simple functions such as shallow trees. Here we\\nhave indexed each function by a parameter vector (cid:13)b; for trees these would\\ncapture the identity of the split variables, their split values, and the con-\\nstants in the terminal nodes. In the case of the Bernoulli response, we have\\n\\n(cid:21).x/ D log\\n\\n(cid:18) Pr.Y D 1jX D x/\\nPr.Y D 0jX D x/\\n\\n(cid:19)\\n\\n;\\n\\n(17.9)\\n\\n0.00.10.20.30.4Number of TreesMean‚àísquared Error150100150200250«´=0.02«´=0.50TrainValidate\\x0c340\\n\\nRandom Forests and Boosting\\n\\nthe logit link function that relates the mean to the natural parameter. In gen-\\neral, if (cid:22).x/ D E.Y jX D x/ is the conditional mean, we have (cid:17)≈í(cid:22).x/(cid:141) D\\n(cid:21).x/, where (cid:17) is the monotone link function.\\n\\nAlgorithm 17.3 outlines a general strategy for building a model by for-\\nward stagewise Ô¨Åtting. L is the loss function, such as the negative log-\\nlikelihood for Bernoulli responses, or squared-error for Gaussian responses.\\nAlthough we are thinking of trees for the simple functions g.xI (cid:13)/, the\\nideas generalize. This algorithm is easier to state than to implement. For\\n\\nAlgorithm 17.3 GENERALIZED BOOSTING BY FORWARD-STAGEWISE\\nFITTING\\n\\n1 DeÔ¨Åne the class of functions g.xI (cid:13)/. Start with bG0.x/ D 0, and set B\\n\\nand the shrinkage parameter (cid:15) > 0.\\n\\n2 For b D 1; : : : ; B repeat the following steps.\\n(a) Solve\\n\\nO(cid:13)b D arg min\\n\\n(cid:13)\\n\\nn\\nX\\n\\niD1\\n\\n(cid:16)\\nyi ; bGb(cid:0)1.xi / C g.xi I (cid:13)/\\n\\n(cid:17)\\n\\nL\\n\\n(b) Update bGb.x/ D bGb(cid:0)1.x/ C Ogb.x/; with Ogb.x/ D (cid:15) (cid:1) g.xI O(cid:13)b/.\\n3 Return the sequence bGb.x/; b D 1; : : : ; B.\\n\\nsquared-error loss, at each step we need to solve\\n\\nminimize\\n(cid:13)\\n\\nn\\nX\\n\\niD1\\n\\n.ri (cid:0) g.xi I (cid:13)//2 ;\\n\\n(17.10)\\n\\nwith ri D yi (cid:0) bGb(cid:0)1.xi /; i D 1; : : : ; n. If g.(cid:1)I (cid:13)/ represents a depth-d tree,\\n(17.10) is still difÔ¨Åcult to solve. But here we can resort to the usual greedy\\nheuristic, and grow a depth-d tree to the residuals by the usual top-down\\nsplitting, as in step 2(a) of Algorithm 17.2. Hence in this case, we have\\nexactly the squared-error boosting Algorithm 17.2. For more general loss\\nfunctions, we rely on one more heuristic for solving step 2(a), inspired by\\ngradient descent. Algorithm 17.4 gives the details. The idea is to perform\\nfunctional gradient descent on the loss function, in the n-dimensional space\\nof the Ô¨Åtted vector. However, we want to be able to evaluate our new func-\\ntion everywhere, not just at the n original values xi . Hence once the (neg-\\native) gradient vector has been computed, it is approximated by a depth-d\\ntree (which can be evaluated everywhere). Taking a step of length (cid:15) down\\n\\n\\x0c17.4 Adaboost: the Original Boosting Algorithm\\n\\n341\\n\\nthe gradient amounts to adding (cid:15) times the tree to the current function. (cid:142)\\nGradient boosting is quite general, and can be used with any differentiable\\n\\n(cid:142)4\\n\\nAlgorithm 17.4 GRADIENT BOOSTING\\n\\n1 Start with OG0.x/ D 0, and set B and the shrinkage parameter (cid:15) > 0.\\n2 For b D 1; : : : ; B repeat the following steps.\\n(a) Compute the pointwise negative gradient of the loss function at the\\n\\ncurrent Ô¨Åt:\\n\\nri D (cid:0) @L.yi ; (cid:21)i /\\n\\n@(cid:21)i\\n\\nÀá\\nÀá\\nÀá\\nÀá(cid:21)i D bGb(cid:0)1.xi /\\n\\n; i D 1; : : : ; n:\\n\\n(b) Approximate the negative gradient by a depth-d tree by solving\\n\\nn\\nX\\n\\n.ri (cid:0) g.xi I (cid:13)//2 :\\n\\nminimize\\n(cid:13)\\n\\niD1\\n(c) Update OGb.x/ D OGb(cid:0)1.x/ C Ogb.x/; with Ogb.x/ D (cid:15) (cid:1) g.xI O(cid:13)b/.\\n3 Return the sequence OGb.x/; b D 1; : : : ; B.\\n\\nloss function. The R package gbm implements Algorithm 17.4 for a variety\\nof loss functions, including squared-error, binomial (Bernoulli), Laplace\\n(`1 loss), multinomial, and others. Included as well is the partial likelihood\\nfor the Cox proportional hazards model (Chapter 9). Figure 17.11 com-\\npares the misclassiÔ¨Åcation error of boosting on the spam data, with that of\\nrandom forests and bagging. Since boosting has more tuning parameters, a\\ncareful comparison must take these into account. Using the McNemar test\\nwe would conclude that boosting and random forest are not signiÔ¨Åcantly\\ndifferent from each other, but both outperform bagging.\\n\\n17.4 Adaboost: the Original Boosting Algorithm\\n\\nThe original proposal for boosting looked quite different from what we\\nhave presented so far. Adaboost was developed for the two-class classiÔ¨Å-\\ncation problem, where the response is coded as -1/1. The idea was to Ô¨Åt a\\nsequence of classiÔ¨Åers to modiÔ¨Åed versions of the training data, where the\\nmodiÔ¨Åcations give more weight to misclassiÔ¨Åed points. The Ô¨Ånal classiÔ¨Å-\\ncation is by weighted majority vote. The details are rather speciÔ¨Åc, and are\\ngiven in Algorithm 17.5. Here we distinguish a classiÔ¨Åer C.x/ 2 f(cid:0)1; 1g,\\nwhich returns a class label, rather than a probability. Algorithm 17.5 gives\\n\\n\\x0c342\\n\\nRandom Forests and Boosting\\n\\nFigure 17.11 Test misclassiÔ¨Åcation for gradient boosting on the\\nspam data, compared with a random forest and bagging.\\nAlthough boosting appears to be better, it requires crossvaldiation\\nor some other means to estimate its tuning parameters, while the\\nrandom forest is essentially automatic.\\n\\nthe Adaboost.M1 algorithm. Although the classiÔ¨Åer in step 2(a) can be ar-\\nbitrary, it was intended for weak learners such as shallow trees. Steps 2(c)‚Äì\\n(d) look mysterious. Its easy to check that, with the reweighted points, the\\nclassiÔ¨Åer Ocb just learned would have weighted error 0.5, that of a coin Ô¨Çip.\\nWe also notice that, although the individual classiÔ¨Åers Ocb.x/ produce val-\\nues Àô1, the ensemble bGb.x/ takes values in R.\\n\\nIt turns out that the Adaboost Algorithm 17.5 Ô¨Åts a logistic regression\\nmodel via a version of the general boosting Algorithm 17.3, using an ex-\\nponential loss function. The functions bGb.x/ output in step 3 of Algo-\\nrithm 17.5 are estimates of (half) the logit function (cid:21).x/.\\n\\nTo show this, we Ô¨Årst motivate the exponential loss, a somewhat unusual\\nchoice, and show how it is linked to logistic regression. For a -1/1 response\\ny and function f .x/, the exponential loss is deÔ¨Åned as LE .y; f .x// D\\nexp≈í(cid:0)yf .x/(cid:141). A simple calculation shows that the solution to the (condi-\\n\\n0.000.020.040.060.08Gradient Boosting on the Spam DataNumber of TreesTest Error15001000150020002500BaggingRandom ForestBoosting (depth 4)\\x0c17.4 Adaboost: the Original Boosting Algorithm\\n\\n343\\n\\nAlgorithm 17.5 ADABOOST\\n\\n1 Initialize the observation weights wi D 1=n; i D 1; : : : ; n.\\n2 For b D 1; : : : ; B repeat the following steps.\\n(a) Fit a classiÔ¨Åer Ocb.x/ to the training data, using observation weights\\n\\nwi .\\n\\n(b) Compute the weighted misclassiÔ¨Åcation error for Ocb:\\n\\nerrb D\\n\\nPn\\n\\niD1 wi I ≈íyi ¬§ Ocb.xi /(cid:141)\\ni D1 wi\\n\\nPn\\n\\n:\\n\\n(c) Compute Àõb D log≈í.1 (cid:0) errb/=errb(cid:141).\\n(d) Update the weights wi   wi\\n\\n(cid:1) exp .Àõb (cid:1) I ≈íyi ¬§ cb.xi /(cid:141)/ ;\\n\\ni D\\n\\n1; : : : ; n.\\n\\n3 Output the sequence of functions bGb.x/ D Pb\\n\\n`D1 Àõ` Oc`.x/ and corre-\\n\\nsponding classiÔ¨Åers bCb.x/ D sign\\n\\nh\\nbGb.x/\\n\\ni\\n, b D 1; : : : ; B.\\n\\ntional) population minimization problem\\n\\nminimize\\nf .x/\\n\\nE≈íe(cid:0)yf .x/ j x(cid:141)\\n\\nis given by\\n\\nInverting, we get\\n\\nf .x/ D 1\\n2\\n\\nlog\\n\\n(cid:18) Pr.y D C1jx/\\nPr.y D (cid:0)1jx/\\n\\n(cid:19)\\n\\n:\\n\\n(17.11)\\n\\n(17.12)\\n\\nef .x/\\n\\nPr.y D C1jx/ D\\n\\ne(cid:0)f .x/ C ef .x/ and Pr.y D (cid:0)1jx/ D\\n\\ne(cid:0)f .x/\\ne(cid:0)f .x/ C ef .x/ ;\\n(17.13)\\na perfectly reasonable (and symmetric) model for a probability. The quan-\\ntity yf .x/ is known as the margin (see also Chapter 19); if the margin\\nis positive, the classiÔ¨Åcation using Cf .x/ D sign.f .x// is correct for y,\\nelse it is incorrect if the margin is negative. The magnitude of yf .x/ is\\nproportional to the (signed) distance of x from the classiÔ¨Åcation boundary\\n(exactly for linear models, approximately otherwise). For -1/1 data, we can\\nalso write the (negative) binomial log-likelihood in terms of the margin.\\n\\n\\x0c344\\n\\nRandom Forests and Boosting\\n\\nUsing (17.13) we have\\n\\nLB .y; f .x// D (cid:0) fI.y D (cid:0)1/ log Pr.y D (cid:0)1jx/\\n\\nC I.y D C1/ log Pr.y D C1jx/g\\n1 C e(cid:0)2yf .x/(cid:17)\\n(cid:16)\\n\\n:\\n\\nD log\\n\\n(17.14)\\n\\nE (cid:2)log (cid:0)1 C e(cid:0)2yf .x/(cid:1) j x(cid:3) also has population minimizer f .x/ equal to\\nhalf the logit (17.12).3 Figure 17.12 compares the exponential loss function\\nwith this binomial loss. They both asymptote to zero in the right tail‚Äîthe\\narea of correct classiÔ¨Åcation. In the left tail, the binomial loss asymptotes\\nto a linear function, much less severe than the exponential loss.\\n\\nFigure 17.12 Exponential loss used in Adaboost, versus the\\nbinomial loss used in the usual logistic regression. Both estimate\\nthe logit function. The exponential left tail, which punishes\\nmisclassiÔ¨Åcations, is much more severe than the asymptotically\\nlinear tail of the binomial.\\n\\nThe exponential loss simpliÔ¨Åes step 2(a) in the gradient boosting Algo-\\n\\n3 The half comes from the symmetric representation we use.\\n\\n‚àí3‚àí2‚àí101230123456yf(x)LossBinomialExponential\\x0c17.5 Connections and Extensions\\n\\n345\\n\\nrithm 17.3.\\n\\n(cid:17)\\n(cid:16)\\nyi ; bGb(cid:0)1.xi / C g.xi I (cid:13)/\\n\\nD\\n\\nLE\\n\\nn\\nX\\n\\ni D1\\n\\nD\\n\\nD\\n\\nn\\nX\\n\\ni D1\\nn\\nX\\n\\ni D1\\nn\\nX\\n\\ni D1\\n\\nexp≈í(cid:0)yi . bGb(cid:0)1.xi / C g.xi I (cid:13)//(cid:141)\\n\\nwi exp≈í(cid:0)yi g.xi I (cid:13)/(cid:141)\\n\\n(17.15)\\n\\nwi LE .yi ; g.xi I (cid:13)// ;\\n\\nwith wi D exp≈í(cid:0)yi bGb(cid:0)1.xi /(cid:141). This is just a weighted exponential loss with\\nthe past history encapsulated in the observation weight wi (see step 2(a) in\\nAlgorithm 17.5). We give some more details in the chapter endnotes on\\nhow this reduces to the Adaboost algorithm.(cid:142)\\n\\nThe Adaboost algorithm achieves an error rate on the spam data com-\\n\\n(cid:142)5\\n\\nparable to binomial gradient boosting.\\n\\n17.5 Connections and Extensions\\n\\nBoosting is a general nonparametric function-Ô¨Åtting algorithm, and shares\\nattributes with a variety of existing methods. Here we relate boosting to two\\ndifferent approaches: generalized additive models and the lasso of Chap-\\nter 16.\\n\\nGeneralized Additive Models\\n\\nBoosting Ô¨Åts additive, low-order interaction models by a forward stage-\\nwise strategy. Generalized additive models (GAMs) are a predecessor, a\\nsemi-parametric approach toward nonlinear function Ô¨Åtting. A GAM has\\nthe form\\n\\np\\nX\\n\\n(cid:21).x/ D\\n\\nfj .xj /;\\n\\n(17.16)\\n\\nj D1\\nwhere again (cid:21).x/ D (cid:17)≈í(cid:22).x/(cid:141) is the natural parameter in an exponential\\nfamily. The attraction of a GAM is that the components are interpretable\\nand can be visualized, and they can move us a big step up from a linear\\nmodel.\\n\\nThere are many ways to specify and Ô¨Åt additive models. For the fj , we\\ncould use parametric functions (e.g. polynomials), Ô¨Åxed-knot regression\\nsplines, or even linear functions for some terms. Less parametric options\\n\\n\\x0c346\\n\\nRandom Forests and Boosting\\n\\nare smoothing splines and local regression (see Section 19.8). In the case of\\nsquared-error loss (the Gaussian case), there is a natural set of backÔ¨Åtting\\nequations for Ô¨Åtting a GAM:\\n\\nOfj  \\n\\nj .y (cid:0) X\\nS\\n\\n`¬§j\\n\\nOf`/; j D 1; : : : ; p:\\n\\n(17.17)\\n\\nHere Of` D ≈í Of`.x1`/; : : : ; . Of`.xn`/(cid:141)0 is the n-vector of Ô¨Åtted values for the\\ncurrent estimate of function f`. Hence the term in parentheses is a partial\\nresidual, removing all the current function Ô¨Åts from y except the one about\\nj is a smoothing operator derived from variable xj that\\nto be updated.\\ngets applied to this residual and delivers the next estimate for function f`.\\nBackÔ¨Åtting starts with all the functions zero, and then cycles through these\\nequations for j D 1; 2; : : : ; p; 1; 2; : : : in a block-coordinate fashion, until\\nall the functions stabilize.\\n\\nS\\n\\nThe Ô¨Årst pass through all the variables is similar to the regression boost-\\ning Algorithm 17.2, where each new function takes the residuals from the\\npast Ô¨Åts, and models them using a tree (for\\nj ). The difference is that\\nboosting never goes back and Ô¨Åxes up past functions, but Ô¨Åts in a forward-\\nstagewise fashion, leaving all past functions alone. Of course, with its adap-\\ntive Ô¨Åtting mechanism, boosting can select the same variables as used be-\\nfore, and thereby update that component of the Ô¨Åt. Boosting with stumps\\n(single-split trees, see the discussion on tree depth on 335 in Section 17.2)\\ncan hence be seen as an adaptive way for Ô¨Åtting an additive model, that si-\\nmultaneously performs variable selection and allows for different amounts\\nof smoothing for different variables.\\n\\nS\\n\\nBoosting and the Lasso\\n\\nIn Section 16.7 we drew attention to the close connection between the\\nforward-stagewise Ô¨Åtting of boosting (with shrinkage ) and the lasso, via\\ninÔ¨Ånitesimal forward-stagewise regression. Here we take this a step further,\\nby using the lasso as a post-processor for boosting (or random forests).\\n\\nBoosting with shrinkage does a good job in building a prediction model,\\nbut at the end of the day can involve a lot of trees. Because of the shrink-\\nage, many of these trees could be similar to each other. The idea here is\\nto use the lasso to select a subset of these trees, reweight them, and hence\\nproduce a prediction model with far fewer trees and, one hopes, compa-\\nrable accuracy. Suppose boosting has produced a sequence of Ô¨Åtted trees\\nOgb.x/; b D 1; : : : ; B. We then solve the lasso problem\\n\\n\\x0c17.6 Notes and Details\\n\\n347\\n\\nFigure 17.13 Post-processing of the trees produced by boosting\\non the ALS data. Shown is the test prediction error as a function\\nof the number of trees selected by the (nonnegative) lasso. We see\\nthat the lasso can do as good a job with one-third the number of\\ntrees, although selecting the correct number is critical.\\n\\nminimize\\nfÀáb gB\\n1\\n\\n\"\\n\\nL\\n\\nyi ;\\n\\nn\\nX\\n\\ni D1\\n\\nB\\nX\\n\\nbD1\\n\\n#\\n\\nOgb.xi /Àáb\\n\\nC (cid:21)\\n\\nB\\nX\\n\\nbD1\\n\\njÀábj\\n\\n(17.18)\\n\\nfor different values of (cid:21). This model selects some of the trees, and as-\\nsigns differential weights to them. A reasonable variant is to insist that the\\nweights are nonnegative. Figure 17.13 illustrates this approach on the ALS\\ndata. Here we could use one-third of the trees. Often the savings are much\\nmore dramatic.\\n\\n17.6 Notes and Details\\n\\nRandom forests and boosting live at the cutting edge of modern predic-\\ntion methodology. They Ô¨Åt models of breathtaking complexity compared\\nwith classical linear regression, or even with standard GLM modeling as\\npracticed in the late twentieth century (Chapter 8). They are routinely used\\nas prediction engines in a wide variety of industrial and scientiÔ¨Åc appli-\\ncations. For the more cautious, they provide a terriÔ¨Åc benchmark for how\\nwell a traditional parametrized model is performing: if the random forests\\n\\n0.250.260.270.280.290.30Number of TreesMean‚àísquared Error1100200300400500Depth 2 BoostLasso Post Fit\\x0c348\\n\\nRandom Forests and Boosting\\n\\ndoes much better, you probably have some work to do, by including some\\nimportant interactions and the like.\\n\\nThe regression and classiÔ¨Åcation trees discussed in Chapter 8 (Breiman\\net al., 1984) took traditional models to a new level, with their ability to\\nadapt to the data, select variables, and so on. But their prediction per-\\nformance is somewhat lacking, and so they stood the risk of falling by\\nthe wayside. With their new use as building blocks in random forests and\\nboosting, they have reasserted themselves as critical elements in the mod-\\nern toolbox.\\n\\nRandom forests and bagging were introduced by Breiman (2001), and\\nboosting by Schapire (1990) and Freund and Schapire (1996). There has\\nbeen much discussion on why boosting works (Breiman, 1998; Friedman\\net al., 2000; Schapire and Freund, 2012); the statistical interpretation given\\nhere can also be found in Hastie et al. (2009), and led to the gradient boost-\\ning algorithm (Friedman, 2001). Adaboost was Ô¨Årst described in Freund\\nand Schapire (1997). Hastie et al. (2009, Chapter 15) is devoted to random\\nforests. For the examples in this chapter we used the randomForest\\npackage in R (Liaw and Wiener, 2002), and for boosting the gbm (Ridge-\\nway, 2005) package. The lasso post-processing idea is due to Friedman and\\nPopescu (2005), which we implemented using glmnet (Friedman et al.,\\n2009). Generalized additive models are described in Hastie and Tibshirani\\n(1990).\\n\\nWe now give some particular technical details on topics covered in the\\n\\nchapter.\\n\\n(cid:142)1 [p. 327] Averaging trees. A maximal-depth tree splits every node until it is\\npure, meaning all the responses are the same. For very large n this might\\nbe unreasonable; in practice, one can put a lower bound on the minimum\\ncount in a terminal node. We are deliberately vague about the response type\\nin Algorithm 17.1. If it is quantitative, we would Ô¨Åt a regression tree. If it\\nis binary or multilevel qualitative, we would Ô¨Åt a classiÔ¨Åcation tree. In this\\ncase at the averaging stage, there are at least two strategies. The original\\nrandom-forest paper (Breiman, 2001) proposed that each tree should make\\na classiÔ¨Åcation, and then the ensemble uses a plurality vote. An alterna-\\ntive reasonable strategy is to average the class probabilities produced by\\nthe trees; these procedures are identical if the trees are grown to maximal\\ndepth.\\n\\n(cid:142)2 [p. 330] Jackknife variance estimate. The jackknife estimate of variance\\nfor a random forest, and the bias-corrected version, is described in Wager\\net al. (2014). The jackknife formula (17.3) is applied to the B D 1 ver-\\n\\n\\x0c17.6 Notes and Details\\n\\n349\\n\\nsion of the random forest, but of course is estimated by plugging in Ô¨Ånite\\nB versions of the quantities involved. Replacing Or .(cid:1)/\\nrf .x0/ by its expectation\\nOrrf.x0/ is not the problem; its that each of the Or .i/\\nrf .x0/ vary about their boot-\\nstrap expectations, compounded by the square in expression (17.4). Calcu-\\nlating the bias requires some technical derivations, which can be found in\\nthat reference.\\n\\nThey also describe the inÔ¨Ånitesimal jackknife estimate of variance, given\\n\\nby\\n\\nwith\\n\\nbVIJ. Orrf.x0// D\\n\\nn\\nX\\n\\ni D1\\n\\ndcov2\\ni ;\\n\\n(17.19)\\n\\ndcovi D\\n\\ndcov.w(cid:3); Or(cid:3).x0// D 1\\n\\nB\\n\\nB\\nX\\n\\n.w(cid:3)\\nb i\\n\\nbD1\\n\\n(cid:0) 1/. Orb.x0/ (cid:0) Orrf.x0//;\\n\\n(17.20)\\n\\nas discussed in Chapter 20. It too has a bias-corrected version, given by\\nIJ. Orrf.x0// D bVIJ. Orrf.x0// (cid:0) n\\nbVu\\nB\\n\\nOv.x0/;\\n\\n(17.21)\\n\\nsimilar to (17.5).\\n\\n(cid:142)3 [p. 334] The ALS data. These data were kindly provided by Lester Mackey\\nand Lilly Fang, who won the DREAM challenge prediction prize in 2012\\n(Kuffner et al., 2015). It includes some additional variables created by\\nthem. Their winning entry used Bayesian trees, not too different from ran-\\ndom forests.\\n\\n(cid:142)4 [p. 341] Gradient-boosting details. In Friedman‚Äôs gradient-boosting algo-\\nrithm (Hastie et al., 2009, Chapter 10, for example), a further reÔ¨Ånement\\nis implemented. The tree in step 2(b) of Algorithm 17.4 is used to de-\\nÔ¨Åne the structure (split variables and splits), but the values in the terminal\\nnodes are left to be updated. We can think of partitioning the parameters\\n(cid:13) D .(cid:13)s; (cid:13)t /, and then represent the tree as g.xI (cid:13)/ D T .xI (cid:13)s/0(cid:13)t . Here\\nT .xI (cid:13)s/ is a vector of d C 1 binary basis functions that indicate the termi-\\nnal node reached by input x, and (cid:13)t are the d C 1 values of the terminal\\nnodes of the tree. We learn O(cid:13)s by approximating the gradient in step 2(b)\\nby a tree, and then (re-)learn the terminal-node parameters O(cid:13)t by solving\\nthe optimization problem\\n\\nminimize\\n(cid:13)t\\n\\nn\\nX\\n\\ni D1\\n\\n(cid:16)\\nyi ; OGb(cid:0)1.xi / C T .xi I O(cid:13)s/0(cid:13)t\\n\\n(cid:17)\\n\\n:\\n\\nL\\n\\n(17.22)\\n\\nSolving (17.22) amounts to Ô¨Åtting a simple GLM with an offset.\\n\\n\\x0c350\\n\\nRandom Forests and Boosting\\n\\n(cid:142)5 [p. 345] Adaboost and gradient boosting. Hastie et al. (2009, Chapter\\n10) derive Adaboost as an instance of Algorithm 17.3. One detail is that\\nthe trees g.xI (cid:13)/ are replaced by a simpliÔ¨Åed scaled classiÔ¨Åer Àõ (cid:1) c.xI (cid:13) 0/.\\nHence, from (17.15), in step 2(a) of Algorithm 17.3 we need to solve\\n\\nminimize\\nÀõ;(cid:13) 0\\n\\nn\\nX\\n\\niD1\\n\\nwi exp≈í(cid:0)yi Àõc.xi I (cid:13) 0/(cid:141):\\n\\n(17.23)\\n\\nThe derivation goes on to show that\\n(cid:15) minimizing (17.23) for any value of Àõ > 0 can be achieved by Ô¨Åtting\\na classiÔ¨Åcation tree c.xI O(cid:13) 0/ to minimize the weighted misclassiÔ¨Åcation\\nerror\\n\\nn\\nX\\n\\ni D1\\n\\nwi I ≈íyi ¬§ c.xi ; (cid:13) 0/(cid:141)I\\n\\n(cid:15) given c.xI O(cid:13) 0/, Àõ is estimated as in step 2(c) of Algorithm 17.5 (and is\\n\\nnon-negative);\\n\\n(cid:15) the weight-update scheme in step 2(d) of Algorithm 17.5 corresponds\\n\\nexactly to the weights as computed in (17.15).\\n\\n\\x0c18\\n\\nNeural Networks and Deep Learning\\n\\nSomething happened in the mid 1980s that shook up the applied statistics\\ncommunity. Neural networks (NNs) were introduced, and they marked a\\nshift of predictive modeling towards computer science and machine learn-\\ning. A neural network is a highly parametrized model, inspired by the ar-\\nchitecture of the human brain, that was widely promoted as a universal\\napproximator‚Äîa machine that with enough data could learn any smooth\\npredictive relationship.\\n\\nFigure 18.1 Neural network diagram with a single hidden layer.\\nThe hidden layer derives transformations of the inputs‚Äînonlinear\\ntransformations of linear combinations‚Äîwhich are then used to\\nmodel the output.\\n\\nFigure 18.1 shows a simple example of a feed-forward neural network\\ndiagram. There are four predictors or inputs xj , Ô¨Åve hidden units a` D\\n`j xj /, and a single output unit o D h.w.2/\\ng.w.1/\\n`0\\nThe language associated with NNs is colorful: memory units or neurons\\nautomatically learn new features from the data through a process called\\n\\nj D1 w.1/\\n\\n`D1 w.2/\\n\\nCP4\\n\\nCP5\\n\\n0\\n\\n` a`/.\\n\\n351\\n\\nx1x2x3x4f(x)HiddenlayerL2InputlayerL1OutputlayerL3\\x0c352\\n\\nNeural Networks\\n\\nsupervised learning. Each neuron al is connected to the input layer via a\\nvector of parameters or weights fw.1/\\ngp\\n1 (the .1/ refers to the Ô¨Årst layer\\n`j\\nand `j refers to the j th variable and `th unit). The intercept terms w.1/\\n`0\\nare called a bias, and the function g is a nonlinearity, such as the sigmoid\\nfunction g.t/ D 1=.1 C e(cid:0)t /. The idea was that each neuron will learn a\\nsimple binary on/off function; the sigmoid function is a smooth and dif-\\nferentiable compromise. The Ô¨Ånal or output layer also has weights, and\\nan output function h. For quantitative regression h is typically the identity\\nfunction, and for a binary response it is once again the sigmoid. Note that\\nwithout the nonlinearity in the hidden layer, the neural network would re-\\nduce to a generalized linear model (Chapter 8). Typically neural networks\\nare Ô¨Åt by maximum likelihood, usually with a variety of forms of regular-\\nization.\\n\\nThe knee-jerk response from statisticians was ‚ÄúWhat‚Äôs the big deal? A\\nneural network is just a nonlinear model, not too different from many other\\ngeneralizations of linear models.‚Äù\\n\\nWhile this may be true, neural networks brought a new energy to the\\nÔ¨Åeld. They could be scaled up and generalized in a variety of ways: many\\nhidden units in a layer, multiple hidden layers, weight sharing, a variety\\nof colorful forms of regularization, and innovative learning algorithms for\\nmassive data sets. And most importantly, they were able to solve problems\\non a scale far exceeding what the statistics community was used to. This\\nwas part computing scale and expertise, part liberated thinking and cre-\\nativity on the part of this computer science community. New journals were\\ndevoted to the Ô¨Åeld, (cid:142) and several popular annual conferences (initially at\\nski resorts) attracted their denizens, and drew in members of the statistics\\ncommunity.\\n\\nAfter enjoying considerable popularity for a number of years, neural\\nnetworks were somewhat sidelined by new inventions in the mid 1990s,\\nsuch as boosting (Chapter 17) and SVMs (Chapter 19). Neural networks\\nwere pass¬¥e. But then they re-emerged with a vengeance after 2010‚Äîthe\\nreincarnation now being called deep learning. This renewed enthusiasm is\\na result of massive improvements in computer resources, some innovations,\\nand the ideal niche learning tasks such as image and video classiÔ¨Åcation,\\nand speech and text processing.\\n\\n(cid:142)1\\n\\n\\x0c18.1 Neural Networks and the Handwritten Digit Problem\\n\\n353\\n\\n18.1 Neural Networks and the Handwritten Digit Problem\\n\\nNeural networks really cut their baby teeth on an optical character recogni-\\ntion (OCR) task: automatic reading of handwritten digits, as in a zipcode.\\nFigure 18.2 shows some examples, taken from the MNIST corpus. (cid:142) The (cid:142)2\\nidea is to build a classiÔ¨Åer C.x/ 2 f0; 1; : : : ; 9g based on the input image\\nx 2 R28(cid:2)28, a 28 (cid:2) 28 grid of image intensities. In fact, as is often the\\ncase, it is more useful to learn the probability function Pr.y D j jx/; j D\\n0; 1; 2; : : : ; 9; this is indeed the target for our neural network. Figure 18.3\\n\\nFigure 18.2 Examples of handwritten digits from the MNIST\\ncorpus. Each digit is represented by a 28 (cid:2) 28 grayscale image,\\nderived from normalized binary images of different shapes and\\nsizes. The value stored for each pixel in an image is a nonnegative\\neight-bit representation of the amount of gray present at that\\nlocation. The 784 pixels for each image are the predictors, and the\\n0‚Äì9 class labels the response. There are 60,000 training images in\\nthe full data set, and 10,000 in the test set.\\n\\nshows a neural network with three hidden layers, a successful conÔ¨Ågura-\\ntion for this digit classiÔ¨Åcation problem. In this case the output layer has\\n10 nodes, one for each of the possible class labels. We use this example\\nto walk the reader through some of the aspects of the conÔ¨Åguration of a\\nnetwork, and Ô¨Åtting it to training data. Since all of the layers are functions\\nof their previous layers, and Ô¨Ånally functions of the input vector x, the net-\\nwork represents a somewhat complex function f .xI\\nrepre-\\nsents the entire collection of weights. Armed with a suitable loss function,\\nwe could simply barge right in and throw it at our favorite optimizer. In the\\nearly days this was not computationally feasible, especially when special\\n\\n/, where\\n\\nW\\n\\nW\\n\\n\\x0c354\\n\\nNeural Networks\\n\\nFigure 18.3 Neural network diagram with three hidden layers\\nand multiple outputs, suitable for the MNIST handwritten-digit\\nproblem. The input layer has p D 784 units. Such a network with\\nhidden layer sizes .1024; 1024; 2048/, and particular choices of\\ntuning parameters, achieves the state-of-the art error rate of\\n0:93% on the ‚ÄúofÔ¨Åcial‚Äù test data set. This network has close to\\nfour million weights, and hence needs to be heavily regularized.\\n\\nstructure is imposed on the weight vectors. Today there are fairly automatic\\nsystems for setting up and Ô¨Åtting neural networks, and this view is not too\\nfar from reality. They mostly use some form of gradient descent, and rely\\non an organization of parameters that leads to a manageable calculation of\\nthe gradient.\\n\\nThe network in Figure 18.3 is complex, so it is essential to establish a\\nconvenient notation for referencing the different sets of parameters. We\\ncontinue with the notation established for the single-layer network, but\\nwith some additional annotations to distinguish aspects of different layers.\\nFrom the Ô¨Årst to the second layer we have\\n\\nz.2/\\n`\\n\\nD w.1/\\n`0\\n\\nC\\n\\np\\nX\\n\\nj D1\\n\\nw.1/\\n\\n`j xj ;\\n\\na.2/\\n`\\n\\nD g.2/.z.2/\\n\\n` /:\\n\\n(18.1)\\n\\n(18.2)\\n\\nx1x2x3...xpy0y1...y9HiddenlayerL4HiddenlayerL3HiddenlayerL2InputlayerL1OutputlayerL5W(1)a(2)W(2)a(3)W(3)a(4)W(4)a(5)\\x0c18.1 Neural Networks and the Handwritten Digit Problem\\n\\n355\\n\\nWe have separated the linear transformations z.2/\\n` of the xj from the nonlin-\\near transformation of these, and we allow for layer-speciÔ¨Åc nonlinear trans-\\nformations g.k/. More generally we have the transition from layer k (cid:0) 1 to\\nlayer k:\\n\\nz.k/\\n`\\n\\nD w.k(cid:0)1/\\n\\n`0\\n\\nC\\n\\npk(cid:0)1\\nX\\n\\nj D1\\n\\nw.k(cid:0)1/\\n\\n`j\\n\\na.k(cid:0)1/\\n\\nj\\n\\n;\\n\\na.k/\\n`\\n\\nD g.k/.z.k/\\n\\n` /:\\n\\n(18.3)\\n\\n(18.4)\\n\\nIn fact (18.3)‚Äì(18.4) can serve for the input layer (18.1)‚Äì(18.2) if we adopt\\nthe notation that a.1/\\n(cid:17) x` and p1 D p, the number of input variables.\\n`\\nHence each of the arrows in Figure 18.3 is associated with a weight param-\\neter.\\n\\nIt is simpler to adopt a vector notation\\n\\nz.k/ D W .k(cid:0)1/a.k(cid:0)1/\\na.k/ D g.k/.z.k//;\\n\\n(18.5)\\n\\n(18.6)\\n\\nwhere W .k(cid:0)1/ represents the matrix of weights that go from layer Lk(cid:0)1\\nto layer Lk, a.k/ is the entire vector of activations at layer Lk, and our\\nnotation assumes that g.k/ operates elementwise on its vector argument.\\nWe have also absorbed the bias parameters w.k(cid:0)1/\\ninto the matrix W .k(cid:0)1/,\\nwhich assumes that we have augmented each of the activation vectors a.k/\\nwith a constant element 1.\\n\\n`0\\n\\nSometimes the nonlinearities g.k/ at the inner layers are the same func-\\ntion, such as the function (cid:27) deÔ¨Åned earlier. In Section 18.5 we present a\\nnetwork for natural color image classiÔ¨Åcation, where a number of different\\nactivation functions are used.\\n\\nDepending on the response, the Ô¨Ånal transformation g.K/ is usually spe-\\ncial. For M -class classiÔ¨Åcation, such as here with M D 10, one typically\\nuses the softmax function\\n\\ng.K/.z.K/\\n\\nm\\n\\nI z.K// D\\n\\nm\\n\\nez.K/\\n`D1 ez.K/\\n\\n`\\n\\nPM\\n\\n;\\n\\n(18.7)\\n\\nwhich computes a number (probability) between zero and one, and all M\\nof them sum to one.1\\n\\n1 This is a symmetric version of the inverse link function used for multiclass logistic\\n\\nregression.\\n\\n\\x0c356\\n\\nNeural Networks\\n\\n18.2 Fitting a Neural Network\\n\\n/ of the the feature vector x, and the collection of weights\\n\\nAs we have seen, a neural network model is a complex, hierarchical func-\\ntion f .xI\\n.\\nW\\nW\\nFor typical choices for the g.k/, this function will be differentiable. Given\\na training set fxi ; yi gn\\n1 and a loss function L≈íy; f .x/(cid:141), along familiar lines\\nwe might seek to solve\\n\\nminimize\\nW\\n\\n(\\n\\n1\\nn\\n\\nn\\nX\\n\\ni D1\\n\\nL≈íyi ; f .xi I\\n\\n/(cid:141) C (cid:21)J.\\n\\nW\\n\\nW\\n\\n)\\n\\n/\\n\\n;\\n\\n(18.8)\\n\\nW\\n\\n/ is a nonnegative regularization term on the elements of\\n\\nwhere J.\\n,\\nW\\nand (cid:21) (cid:21) 0 is a tuning parameter. (In practice there may be multiple reg-\\nularization terms, each with their own (cid:21).) For example an early popular\\npenalty is the quadratic\\n\\n/ D 1\\n2\\n\\nJ.\\n\\nW\\n\\nK(cid:0)1\\nX\\n\\npk\\nX\\n\\npkC1\\nX\\n\\nkD1\\n\\nj D1\\n\\n`D1\\n\\no2\\n\\nn\\nw.k/\\n`j\\n\\n;\\n\\n(18.9)\\n\\nas in ridge regression (7.41). Also known as the weight-decay penalty, it\\npulls the weights toward zero (typically the biases are not penalized). Lasso\\npenalties (Chapter 16) are also popular, as are mixtures of these (an elastic\\nnet).\\n\\nFor binary classiÔ¨Åcation we could take L to be binomial deviance (8.14),\\nin which case the neural network amounts to a penalized logistic regres-\\nsion, Section 8.1, albeit a highly parametrized and penalized one. Loss\\nfunctions are usually convex in f , but not in the elements of\\n, so solving\\n(18.8) is difÔ¨Åcult, and at best we seek good local optima. Most methods\\nare based on some form of gradient descent, with many associated bells\\nand whistles. We brieÔ¨Çy discuss some elements of the current practice in\\nÔ¨Ånding good solutions to (18.8).\\n\\nW\\n\\nComputing the Gradient: Backpropagation\\n\\nW\\n\\noccur in layers, since f .xI\\n\\n/ is deÔ¨Åned as a series\\nThe elements of\\nof compositions, starting from the input layer. Computing the gradient is\\nalso done most naturally in layers (the chain rule for differentiation; see\\nfor example (18.10) in Algorithm 18.1 below), and our notation makes this\\neasier to describe in a recursive fashion. We will consider computing the\\nderivative of L≈íy; f .xI\\n, for a\\ngeneric input‚Äìoutput pair x; y; since the loss part of the objective is a sum,\\n\\n(cid:141) with respect to any of the elements of\\n\\nW\\n\\nW\\n\\nW\\n\\n\\x0c18.2 Fitting a Neural Network\\n\\n357\\n\\nthe overall gradient will be the sum of these individual gradient elements\\nover the training pairs .xi ; yi /.\\n\\n`\\n\\nThe intuition is as follows. Given a training generic pair .x; y/, we Ô¨Årst\\nmake a forward pass through the network, which creates activations at each\\nof the nodes a.k/\\nin each of the layers, including the Ô¨Ånal output layer. We\\nwould then like to compute an error term ƒ±.k/\\nthat measures the responsibil-\\nity of each node for the error in predicting the true output y. For the output\\nactivations a.K/\\nthese errors are easy: either residuals or generalized resid-\\nuals, depending on the loss function. For activations at inner layers, ƒ±.k/\\nwill be a weighted sum of the errors terms of nodes that use a.k/\\nas inputs.\\nThe backpropagation Algorithm 18.1 gives the details for computing the\\ngradient for a single input‚Äìoutput pair x; y. We leave it to the reader to\\nverify that this indeed implements the chain rule for differentiation.\\n\\n`\\n\\n`\\n\\n`\\n\\n`\\n\\nAlgorithm 18.1 BACKPROPAGATION\\n\\n1 Given a pair x; y, perform a ‚Äúfeedforward pass,‚Äù computing the activa-\\n/ at\\n, saving each of the intermediary quantities along\\n\\nat each of the layers L2; L3; : : : ; LK; i.e. compute f .xI\\n\\nW\\n\\n`\\n\\ntions a.k/\\nx using the current\\nthe way.\\n\\nW\\n\\n2 For each output unit ` in layer LK, compute\\n\\nƒ±.K/\\n`\\n\\nD @L≈íy; f .x;\\n@z.K/\\n`\\nD @L≈íy; f .xI\\n@a.K/\\n`\\n\\n/(cid:141)\\n\\n/(cid:141)\\n\\nW\\n\\nW\\n\\nPg.K/.z.K/\\n\\n`\\n\\n/;\\n\\n(18.10)\\n\\nwhere Pg denotes the derivative of g.z/ wrt z. For example for L.y; f / D\\n1\\n2\\n\\n2, (18.10) becomes (cid:0).y` (cid:0) f`/ (cid:1) Pg.K/.z.K/\\n\\nky (cid:0) f k2\\n\\n3 For layers k D K (cid:0) 1; K (cid:0) 2; : : : ; 2, and for each node ` in layer k, set\\n\\n/.\\n\\n`\\n\\nƒ±.k/\\n`\\n\\nD\\n\\n0\\n\\n@\\n\\npkC1\\nX\\n\\nj D1\\n\\nj ` ƒ±.kC1/\\nw.k/\\n\\nj\\n\\n1\\nA Pg.k/.z.k/\\n` /:\\n\\n(18.11)\\n\\n4 The partial derivatives are given by\\n\\n@L≈íy; f .xI\\n@w.k/\\n`j\\n\\n/(cid:141)\\n\\nW\\n\\nD a.k/\\n\\nj ƒ±.kC1/\\n\\n`\\n\\n:\\n\\n(18.12)\\n\\nOne again matrix‚Äìvector notation simpliÔ¨Åes these expressions a bit:\\n\\n\\x0c358\\n\\nNeural Networks\\n\\n(18.10) becomes (for squared-error loss)\\n\\nƒ±.K/ D (cid:0).y (cid:0) a.K// ƒ± Pg.K/.z.K//;\\n\\n(18.13)\\n\\nwhere ƒ± denotes the Hadamard (elementwise) product;\\n(18.11) becomes\\n\\nƒ±.k/ D\\n\\n(cid:16)\\nW .k/\\n\\n0\\n\\nƒ±.kC1/(cid:17)\\n\\nƒ± Pg.k/.z.k//I\\n\\n(18.12) becomes\\n\\n@L≈íy; f .xI\\n@W .k/\\n\\n/(cid:141)\\n\\nW\\n\\nD ƒ±.kC1/a.k/\\n\\n0\\n\\n:\\n\\n(18.14)\\n\\n(18.15)\\n\\nBackpropagation was considered a breakthrough in the early days of\\nneural networks, since it made Ô¨Åtting a complex model computationally\\nmanageable.\\n\\nGradient Descent\\n\\nAlgorithm 18.1 computes the gradient of the loss function at a single generic\\npair .x; y/; with n training pairs the gradient of the Ô¨Årst part of (18.8) is\\ngiven by\\n\\n(cid:129)W .k/ D 1\\nn\\n\\nn\\nX\\n\\ni D1\\n\\n@L≈íyi ; f .xi I\\n@W .k/\\n\\n(cid:141)\\n\\n:\\n\\nW\\n\\n(18.16)\\n\\nWith the quadratic form (18.9) for the penalty, a gradient-descent update is\\n\\nW .k/   W .k/ (cid:0) Àõ\\n\\n(cid:16)\\n\\n(cid:129)W .k/ C (cid:21)W .k/(cid:17)\\n\\n; k D 1; : : : ; K (cid:0) 1;\\n\\n(18.17)\\n\\nwhere Àõ 2 .0; 1(cid:141) is the learning rate.\\n\\nGradient descent requires starting values for all the weights\\n\\n. Zero is\\nnot an option, because each layer is symmetric in the weights Ô¨Çowing to the\\ndifferent neurons, hence we rely on starting values to break the symmetries.\\nTypically one would use random starting weights, close to zero; random\\nuniform or Gaussian weights are common.\\n\\nW\\n\\nThere are a multitude of ‚Äútricks of the trade‚Äù in Ô¨Åtting or ‚Äúlearning‚Äù a\\nneural network, and many of them are connected with gradient descent.\\nHere we list some of these, without going into great detail.\\n\\nStochastic Gradient Descent\\n\\nRather than process all the observations before making a gradient step, it\\ncan be more efÔ¨Åcient to process smaller batches at a time‚Äîeven batches\\n\\n\\x0c18.2 Fitting a Neural Network\\n\\n359\\n\\nFigure 18.4 Training and test misclassiÔ¨Åcation error as a\\nfunction of the number of epochs of training, for the MNIST digit\\nclassiÔ¨Åcation problem. The architecture for the network is shown\\nin Figure 18.3. The network was Ô¨Åt using accelerated gradient\\ndescent with adaptive rate control, a rectiÔ¨Åed linear activation\\nfunction, and dropout regularization (Section 18.5). The\\nhorizontal broken line shows the error rate of a random forest\\n(Section 17.1). A logistic regression model (Section 8.1) achieves\\nonly 0.072 (off the scale).\\n\\nof size one! These batches can be sampled at random, or systematically\\nprocessed. For large data sets distributed on multiple computer cores, this\\ncan be essential for reasons of efÔ¨Åciency. An epoch of training means that\\nall n training samples have been used in gradient steps, irrespective of how\\nthey have been grouped (and hence how many gradient steps have been\\nmade).\\n\\nAccelerated Gradient Methods\\n\\nThe idea here is to allow previous iterations to build up momentum and\\ninÔ¨Çuence the current iterations. The iterations have the form\\n\\nt C1 D (cid:22)\\nt C1 D\\n\\nt (cid:0) Àõ.(cid:129)\\nV\\nt C\\nt C1;\\nW\\n\\nV\\n\\nV\\nW\\n\\nW\\n\\nt C (cid:21)\\n\\nt /;\\n\\nW\\n\\n(18.18)\\n\\n(18.19)\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll020040060080010000.000.010.020.030.04EpochsMisclassification ErrorllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllTrainTestTest ‚àí RF\\x0c360\\n\\nNeural Networks\\n\\nW\\n\\nt to represent the entire collection of weights at iteration t.\\n\\nt is\\nusing\\na velocity vector that accumulates gradient information from previous iter-\\nations, and is controlled by an additional momentum parameter (cid:22). When\\ncorrectly tuned, accelerated gradient descent can achieve much faster con-\\nvergence rates; however, tuning tends to be a difÔ¨Åcult process, and is typi-\\ncally done adaptively.\\n\\nV\\n\\nRate Annealing\\n\\n(cid:142)3\\n\\nA variety of creative methods have been proposed to adapt the learning rate\\nto avoid jumping across good local minima. These tend to be a mixture of\\nprincipled approaches combined with ad-hoc adaptations that tend to work\\nwell in practice. (cid:142) Figure 18.4 shows the performance of our neural net\\non the MNIST digit data. This achieves state-of-the art misclassiÔ¨Åcation\\nerror rates on these data (just under 0.93% errors), and outperforms random\\nforests (2.8%) and a generalized linear model (7.2%). Figure 18.5 shows\\nthe 93 misclassiÔ¨Åed digits.\\n\\nFigure 18.5 All 93 misclassiÔ¨Åed digits in the MNIST test set.\\nThe true digit class is labeled in blue, the predicted in red.\\n\\n425360825897896572467294499571578379874693063793239453203749619091942461539561653295359712496037916450857246134603972732878961809472704953383839899771079585053985497272720897274763564250\\x0c18.2 Fitting a Neural Network\\n\\n361\\n\\nOther Tuning Parameters\\n\\nApart from the many details associated with gradient descent, there are sev-\\neral other important structural and operational aspects of neural networks\\nthat have to be speciÔ¨Åed.\\n\\nNumber of Hidden Layers, and Their Sizes\\n\\nWith a single hidden layer, the number of hidden units determines the num-\\nber of parameters. In principle, one could treat this number as a tuning\\nparameter, which could be adjusted to avoid overÔ¨Åtting. The current col-\\nlective wisdom suggests it is better to have an abundant number of hidden\\nunits, and control the model complexity instead by weight regularization.\\nHaving deeper networks (more hidden layers) increases the complexity as\\nwell. The correct number tends to be task speciÔ¨Åc; having two hidden lay-\\ners with the digit recognition problem leads to competitive performance.\\n\\nChoice of Nonlinearities\\nThere are a number of activation functions g.k/ in current use. Apart from\\nthe sigmoid function, which transforms its input to a values in .0; 1/, other\\npopular choices are\\n\\nFigure 18.6 Activation functions. ReLU is a rectiÔ¨Åed linear\\n(unit).\\n\\ntanh:\\n\\ng.z/ D ez (cid:0) e(cid:0)z\\nez C e(cid:0)z ;\\n\\n‚àí2‚àí1012‚àí1.0‚àí0.50.00.51.0zg(z)sigmoidtanhReLUleaky ReLU\\x0c362\\n\\nNeural Networks\\n\\nwhich delivers values in .(cid:0)1; 1/.\\n\\nrectiÔ¨Åed linear:\\n\\ng.z/ D zC;\\n\\nor the positive-part function. This has the advantage of making the gra-\\ndient computations cheaper to compute.\\n\\nleaky rectiÔ¨Åed linear:\\n\\ngÀõ.z/ D zC (cid:0) Àõz(cid:0);\\n\\nfor Àõ nonnegative and close to zero. The rectiÔ¨Åed linear tends to have Ô¨Çat\\nspots, because of the many zero activations; this is an attempt to avoid\\nthese and the accompanying zero gradients.\\n\\nChoice of Regularization\\n\\nTypically this is a mixture of `2 and `1 regularization, each of which re-\\nquires a tuning parameter. As in lasso and regression applications, the bias\\nterms (intercepts) are usually not regularized. The weight regularization\\nis typically light, and serves several roles. The `2 reduces problems with\\ncollinearity, the `1 can ignore irrelevant features, and both slow the rate of\\noverÔ¨Åtting, especially with deep (over-parametrized) networks.\\n\\nEarly Stopping\\n\\nNeural nets are typically over-parametrized, and hence are prone to overÔ¨Åt-\\nting. Originally early stopping was set up as the primary tuning parameter,\\nand the stopping time was determined using a held-out set of validation\\ndata. In modern networks the regularization is tuned adaptively to avoid\\noverÔ¨Åtting, and hence it is less of a problem. For example, in Figure 18.4\\nwe see that the test misclassiÔ¨Åcation error has Ô¨Çattened out, and does not\\nrise again with increasing number of epochs.\\n\\n18.3 Autoencoders\\n\\nAn autoencoder is a special neural network for computing a type of non-\\nlinear principal-component decomposition.\\n\\nThe linear principal component decomposition is a popular and effective\\nlinear method for reducing a large set of correlated variables to a typically\\nsmaller number of linear combinations that capture most of the variance in\\nthe original set. Hence, given a collection of n vectors xi 2 Rp (assumed to\\nhave mean zero), we produce a derived set of uncorrelated features zi 2 Rq\\n\\n\\x0c18.3 Autoencoders\\n\\n363\\n\\n(q (cid:20) p, and typically smaller) via zi D V 0xi . The columns of V are or-\\nthonormal, and are derived such that the Ô¨Årst component of zi has maximal\\nvariance, the second has the next largest variance and is uncorrelated with\\nthe Ô¨Årst, and so on. It is easy to show that the columns of V are the leading\\nq eigenvectors of the sample covariance matrix S D 1\\n\\nn X 0X .\\n\\nPrincipal components can also be derived in terms of a best-approx-\\nimating linear subspace, and it is this version that leads to the nonlinear\\ngeneralization presented here. Consider the optimization problem\\n\\nminimize\\n\\nA2Rp(cid:2)q ; f(cid:13)i gn\\n1\\n\\n2Rq(cid:2)n\\n\\nn\\nX\\n\\niD1\\n\\nkxi (cid:0) A(cid:13)i k2\\n2;\\n\\n(18.20)\\n\\nfor q < p. The subspace is deÔ¨Åned by the column space of A, and for\\neach point xi we wish to locate its best approximation in the subspace (in\\nterms of Euclidean distance). Without loss of generality, we can assume A\\nhas orthonormal columns, in which case O(cid:13)i D A0xi for each i (n separate\\nlinear regressions). Plugging in, (18.20) reduces to\\n\\nminimize\\nA2Rp(cid:2)q ; A0ADIq\\n\\nn\\nX\\n\\ni D1\\n\\nkxi (cid:0) AA0xi k2\\n2:\\n\\n(18.21)\\n\\nA solution is given by OA D V , the matrix above of the Ô¨Årst q principal-\\ncomponent direction vectors computed from the xi . By analogy, a single-\\nlayer autoencoder solves a nonlinear version of this problem:\\n\\nminimize\\nW 2Rq(cid:2)p\\n\\nn\\nX\\n\\ni D1\\n\\nkxi (cid:0) W 0g.W xi /k2\\n2;\\n\\n(18.22)\\n\\nfor some nonlinear activation function g; see Figure 18.7 (left panel). If g\\nis the identity function, these solutions coincide (with W D V 0).\\n\\nFigure 18.7 (right panel) represents the learned row of W as images,\\nwhen the autoencoder is Ô¨Åt to the MNIST digit database. Since autoen-\\ncoders do not require a response (the class labels in this case), this decom-\\nposition is unsupervised. It is often expensive to label images, for example,\\nwhile unlabeled images are abundant. Autoencoders provide a means for\\nextracting potentially useful features from such data, which can then be\\nused with labeled data to train a classiÔ¨Åer. In fact, they are often used as\\nwarm starts for the weights when Ô¨Åtting a supervised neural network.\\n\\nOnce again there are a number of bells and whistles that make autoen-\\n\\ncoders more effective.\\n\\n\\x0c364\\n\\nNeural Networks\\n\\nFigure 18.7 Left: Network representation of an autoencoder used\\nfor unsupervised learning of nonlinear principal components. The\\nmiddle layer of hidden units creates a bottleneck, and learns\\nnonlinear representations of the inputs. The output layer is the\\ntranspose of the input layer, so the network tries to reproduce the\\ninput data using this restrictive representation. Right: Images\\nrepresenting the estimated rows of W using the MNIST database;\\nthe images can be seen as Ô¨Ålters that detect local gradients in the\\nimage pixels. In each image, most of the weights are zero, and the\\nnonzero weights are localized in the two-dimensional image\\nspace.\\n\\n(cid:15) `1 regularization applied to the rows of W lead to sparse weight vectors,\\n\\nand hence local features, as was the case in our example.\\n\\n(cid:15) Denoising is a process where noise is added to the input layer (but not the\\noutput), resulting in features that do not focus on isolated values, such\\nas pixels, but instead have some volume. We discuss denoising further in\\nSection 18.5.\\n\\n(cid:15) With regularization, the bottleneck is not necessary, as in the Ô¨Ågure or in\\nprincipal components. In fact we can learn many more than p compo-\\nnents.\\n\\n(cid:15) Autoencoders can also have multiple layers, which are typically learned\\nsequentially. The activations learned in the Ô¨Årst layer are treated as the\\ninput (and output) features, and a model like (18.22) is Ô¨Åt to them.\\n\\n18.4 Deep Learning\\n\\nNeural networks were reincarnated around 2010 with ‚Äúdeep learning‚Äù as a\\nÔ¨Çashier name, largely a result of much faster and larger computing systems,\\nplus a few new ideas. They have been shown to be particularly successful\\n\\nx1x2x3x4x5ÀÜx1ÀÜx2ÀÜx3ÀÜx4ÀÜx5InputlayerHiddenlayerOutputlayerWg(Wx)W0\\x0c18.4 Deep Learning\\n\\n365\\n\\nin the difÔ¨Åcult task of classifying natural images, using what is known as a\\nconvolutional architecture. Initially autoencoders were considered a crucial\\naspect of deep learning, since unlabeled images are abundant. However,\\nas labeled corpora become more available, the word on the street is that\\nsupervised learning is sufÔ¨Åcient.\\n\\nFigure 18.8 shows examples of natural images, each with a class label\\nsuch as beaver, sunflower, trout etc. There are 100 class labels in\\n\\nFigure 18.8 Examples of natural images. The CIFAR-100\\ndatabase consists of 100 color image classes, with 600 examples\\nin each class (500 train, 100 test). Each image is 32 (cid:2) 32 (cid:2) 3 (red,\\ngreen, blue). Here we display a randomly chosen image from each\\nclass. The classes are organized by hierarchical structure, with 20\\ncoarse levels and Ô¨Åve subclasses within each. So, for example, the\\nÔ¨Årst Ô¨Åve images in the Ô¨Årst column are aquatic mammals,\\nnamely beaver, dolphin, otter, seal and whale.\\n\\n\\x0c366\\n\\nNeural Networks\\n\\nall, and 500 training images and 100 test images per class. The goal is to\\nbuild a classiÔ¨Åer to assign a label to an image. We present the essential\\ndetails of a deep-learning network for this task‚Äîone that achieves a re-\\nspectable classiÔ¨Åcation performance of 35% errors on the designated test\\nset.2 Figure 18.9 shows a typical deep-learning architecture, with many\\n\\nFigure 18.9 Architecture of a deep-learning network for the\\nCIFAR-100 image classiÔ¨Åcation task. The input layer and\\nhidden layers are all represented as images, except for the last\\nhidden layer, which is ‚ÄúÔ¨Çattened‚Äù (vectorized). The input layer\\nconsists of the p1 D 3 color (red, green, and blue) versions of an\\ninput image (unlike earlier, here we use the pk to refer to the\\nnumber of images rather than the totality of pixels). Each of these\\ncolor panes is 32 (cid:2) 32 pixels in dimension. The Ô¨Årst hidden layer\\ncomputes a convolution using a bank of p2 distinct q (cid:2) q (cid:2) p1\\nlearned Ô¨Ålters, producing an array of images of dimension\\np2 (cid:2) 32 (cid:2) 32. The next pool layer reduces each non-overlapping\\nblock of ` (cid:2) ` numbers in each pane of the Ô¨Årst hidden layer to a\\nsingle number using a ‚Äúmax‚Äù operation. Both q and ` are\\ntypically small; each was 2 for us. These convolve and pool layers\\nare repeated here three times, with changing dimensions (in our\\nactual implementation, there are 13 layers in total). Finally the\\n500 derived features are Ô¨Çattened, and a fully connected layer\\nmaps them to the 100 classes via a ‚Äúsoftmax‚Äù activation.\\n\\nhidden layers. These consist of two special types of layers: ‚Äúconvolve‚Äù and\\n‚Äúpool.‚Äù We describe each in turn.\\n\\nConvolve Layer\\n\\nFigure 18.10 illustrates a convolution layer, and some details are given in\\n\\n2 ClassiÔ¨Åcation becomes increasingly difÔ¨Åcult as the number of classes grows. With equal\\n\\nrepresentation in each class, the NULL or random error rate for K classes is\\n.K (cid:0) 1/=K; 50% for two classes, 99% for 100.\\n\\n100323221650048convolvepoolconvolvepoolconvolvepoolconnect fully\\x0c18.4 Deep Learning\\n\\n367\\n\\nthe caption. If an image x is represented by a k (cid:2) k matrix, and a Ô¨Ålter f\\n\\nFigure 18.10 Convolution layer for the input images. The input\\nimage is split into its three color components. A single Ô¨Ålter is a\\nq (cid:2) q (cid:2) p1 array (here one q (cid:2) q for each of the p1 D 3 color\\npanes), and is used to compute an inner product with a\\ncorrespondingly sized subimage in each pane, and summed across\\nthe p1 panes. We used q D 2, and small values are typical. This is\\nrepeated over all (overlapping) q (cid:2) q subimages (with boundary\\npadding), and hence produces an image of the same dimension as\\none of the input panes. This is the convolution operation. There\\nare p2 different versions of this Ô¨Ålter, and hence p2 new panes are\\nproduced. Each of the p2 Ô¨Ålters has p1q2 weights, which are\\nlearned via backpropagation.\\n\\n`D1\\n\\nPq\\n\\nis a q (cid:2) q matrix with q (cid:28) k, the convolved image is another k (cid:2) k matrix\\nQx with elements Qxi; j D Pq\\n`0D1 xi C`; j C`0f`; `0 (with edge padding to\\nachieve a full-sized k (cid:2) k output image). In our application we used 2 (cid:2) 2,\\nbut other sizes such as 3 (cid:2) 3 are popular. It is most natural to represent\\nthe structure in terms of these images as in Figure 18.9, but they could all\\nbe vectorized into a massive network diagram as in Figures 18.1 and 18.3.\\nHowever, the weights would have special sparse structure, with most being\\nzero, and the nonzero values repeated (‚Äúweight sharing‚Äù).\\n\\n++.........\\x0c368\\n\\nNeural Networks\\n\\nPool Layer\\n\\nThe pool layer corresponds to a kind of nonlinear activation. It reduces\\neach nonoverlapping block of r (cid:2)r pixels (r D 2 for us) to a single number\\nby computing their maximum. Why maximum? The convolution Ô¨Ålters are\\nthemselves small image patches, and are looking to identify similar patches\\nin the target image (in which case the inner product will be high). The max\\noperation introduces an element of local translation invariance. The pool\\noperation reduces the size of each image by a factor r in each dimension.\\nTo compensate, the number of tiles in the next convolution layer is typically\\nincreased accordingly. Also, as these tiles get smaller, the effective weights\\nresulting from the convolution operator become denser. Eventually the tiles\\nare the same size as the convolution Ô¨Ålter, and the layer becomes fully\\nconnected.\\n\\n18.5 Learning a Deep Network\\n\\nDespite the additional structure imposed by the convolution layers, deep\\nnetworks are learned by gradient descent. The gradients are computed by\\nbackpropagation as before, but with special care taken to accommodate the\\ntied weights in the convolution Ô¨Ålters. However, a number of additional\\ntricks have been introduced that appear to improve the performance of\\nmodern deep learning networks. These are mostly aimed at regularization;\\nindeed, our 100-class image network has around 50 million parameters, so\\nregularization is essential to avoid overÔ¨Åtting. We brieÔ¨Çy discuss some of\\nthese.\\n\\nDropout\\n\\nThis is a form of regularization that is performed when learning a network,\\ntypically at different rates at the different layers. It applies to all networks,\\nnot just convolutional; in fact, it appears to work better when applied at the\\ndeeper, denser layers. Consider computing the activation z.k/\\nin layer k as\\nin (18.3) for a single observation during the feed-forward stage. The idea\\nis to randomly set each of the pk(cid:0)1 nodes a.k(cid:0)1/\\nto zero with probability\\n(cid:30), and inÔ¨Çate the remaining ones by a factor 1=.1 (cid:0) (cid:30)/. Hence, for this ob-\\nservation, those nodes that survive have to stand in for those omitted. This\\ncan be shown to be a form of ridge regularization, and when done correctly\\nimproves performance. (cid:142) The fraction (cid:30) omitted is a tuning parameter, and\\nfor convolutional networks it appears to be better to use different values at\\n\\n`\\n\\nj\\n\\n(cid:142)4\\n\\n\\x0c18.5 Learning a Deep Network\\n\\n369\\n\\ndifferent layers. In particular, as the layers become denser, (cid:30) is increased:\\nfrom 0 in the input layer to 0:5 in the Ô¨Ånal, fully connected layer.\\n\\nInput Distortion\\n\\nThis is another form of regularization that is particularly suitable for tasks\\nlike image classiÔ¨Åcation. The idea is to augment the training set with many\\ndistorted copies of an input image (but of course the same label). These\\ndistortions can be location shifts and other small afÔ¨Åne transformations, but\\nalso color and shading shifts that might appear in natural images. We show\\n\\nFigure 18.11 Each column represents distorted versions of an\\ninput image, including afÔ¨Åne and color distortions. The input\\nimages are padded on the boundary to increase the size, and\\nhence allow space for some of the distortions.\\n\\nsome distorted versions of input images in Figure 18.11. The distortions are\\nsuch that a human would have no trouble identifying any of the distorted\\nimages if they could identify the original. (cid:142) This both enriches the training (cid:142)5\\ndata with hints, and also prevents overÔ¨Åtting to the original image. One\\ncould also apply distortions to a test image, and then ‚Äúpoll‚Äù the results to\\nproduce a Ô¨Ånal classiÔ¨Åcation.\\n\\nConÔ¨Åguration\\n\\nDesigning the correct architecture for a deep-learning network, along with\\nthe various choices at each layer, appears to require experience and trial\\n\\n\\x0c370\\n\\nNeural Networks\\n\\n(cid:142)6\\n\\nand error. We summarize the third and Ô¨Ånal architecture which we built\\nfor classifying the CIFAR-100 data set in Algorithm 18.2.(cid:142) In addition to\\nthese size parameters for each layer, we must select the activation functions\\nand additional regularization. In this case we used the leaky rectiÔ¨Åed linear\\nfunctions gÀõ.z/ (Section 18.2), with Àõ increasing from 0:05 in layer 5 up to\\n0:5 in layer 13. In addition a type of `2 regularization was imposed on the\\nweights, restricting all incoming weight vectors to a node to have `2 norm\\nbounded by one. Figure 18.12 shows both the progress of the optimization\\nobjective (red) and the test misclassiÔ¨Åcation error (blue) as the gradient-\\ndescent algorithm proceeds. The accelerated gradient method maintains a\\nmemory, which we can see was restarted twice to get out of local minima.\\nOur network achieved a test error rate of 35% on the 10,000 test images\\n(100 images per class). The best reported error rate we have seen is 25%,\\nso apparently we have some way to go!\\n\\nFigure 18.12 Progress of the algorithm as a function of the\\nnumber of epochs. The accelerated gradient algorithm is\\n‚Äúrestarted‚Äù every 100 epochs, meaning the long-term memory is\\nforgotten, and a new trail is begun, starting at the current solution.\\nThe red curve shows the objective (negative penalized\\nlog-likelihood on the training data). The blue curve shows test-set\\nmisclassiÔ¨Åcation error. The vertical axis is on the log scale, so\\nzero cannot be included.\\n\\n050100150200250300405060708090EpochTest Misclassification Error3001900390063009200ObjectiveObjective CostMisclassification Error\\x0c18.6 Notes and Details\\n\\n371\\n\\nAlgorithm 18.2 CONFIGURATION PARAMETERS FOR DEEP-LEARNING\\nNETWORK USED ON THE CIFAR-100 DATA.\\nLayer 1: 100 convolution maps each with 2 (cid:2) 2 (cid:2) 3 kernel (the 3 for three\\ncolors). The input image is padded from 32 (cid:2) 32 to 40 (cid:2) 40 to accom-\\nmodate input distortions.\\n\\nLayers 2 and 3: 100 convolution maps each 2 (cid:2) 2 (cid:2) 100. Compositions of\\nconvolutions are roughly equivalent to convolutions with a bigger band-\\nwidth, and the smaller ones have fewer parameters.\\n\\nLayer 4: Max pool 2 (cid:2) 2 layer, pooling nonoverlapping 2 (cid:2) 2 blocks of\\n\\npixels, and hence reducing the images to size 20 (cid:2) 20.\\n\\nLayer 5: 300 convolution maps each 2 (cid:2) 2 (cid:2) 100, with dropout learning\\n\\nwith rate (cid:30)5 D 0:05.\\nLayer 6: Repeat of Layer 5.\\nLayer 7: Max pool 2 (cid:2) 2 layer (down to 10 (cid:2) 10 images).\\nLayer 8: 600 convolution maps each 2 (cid:2) 2 (cid:2) 300, with dropout rate (cid:30)8 D\\n\\n0:10.\\n\\nLayer 9: 800 convolution maps each 2 (cid:2) 2 (cid:2) 600, with dropout rate (cid:30)9 D\\n\\n0:10.\\n\\nLayer 10: Max pool 2 (cid:2) 2 layer (down to 5 (cid:2) 5 images).\\nLayer 11: 1600 convolution maps, each 1 (cid:2) 1 (cid:2) 800. This is a pixelwise\\n\\nweighted sum across the 800 images from the previous layer.\\nLayer 12: 2000 fully connected units, with dropout rate (cid:30)12 D 0:25.\\nLayer 13: Final 100 output units, with softmax activation, and dropout rate\\n\\n(cid:30)13 D 0:5.\\n\\n18.6 Notes and Details\\n\\nThe reader will notice that probability models have disappeared from the\\ndevelopment here. Neural nets are elaborate regression methods aimed\\nsolely at prediction‚Äînot estimation or explanation in the language of Sec-\\ntion 8.4. In place of parametric optimality criteria, the machine learning\\ncommunity has focused on a set of speciÔ¨Åc prediction data sets, like the\\ndigits MNIST corpus and CIFAR-100, as benchmarks for measuring per-\\nformance.\\n\\nThere is a vast literature on neural networks, with hundreds of books and\\nthousands of papers. With the resurgence of deep learning, this literature is\\nagain growing. Two early statistical references on neural networks are Rip-\\nley (1996) and Bishop (1995), and Hastie et al. (2009) devote one chapter\\nto the topic. Part of our description of backpropagation in Section 18.2 was\\n\\n\\x0c372\\n\\nNeural Networks\\n\\nguided by Andrew Ng‚Äôs online Stanford lecture notes (Ng, 2015). Bengio\\net al. (2013) provide a useful review of autoencoders. LeCun et al. (2015)\\ngive a brief overview of deep learning, written by three pioneers of this\\nÔ¨Åeld: Yann LeCun, Yoshua Bengio and Geoffrey Hinton; we also bene-\\nÔ¨Åted from reading Ngiam et al. (2010). Dropout learning (Srivastava et al.,\\n2014) is a relatively new idea, and its connections with ridge regression\\nwere most usefully described in Wager et al. (2013). The most popular\\nversion of accelerated gradient descent is due to Nesterov (2013). Learn-\\ning with hints is due to Abu-Mostafa (1995). The material in Sections 18.4\\nand 18.5 beneÔ¨Åted greatly from discussions with Rakesh Achanta (Achanta\\nand Hastie, 2015), who produced some of the color images and diagrams,\\nand designed and Ô¨Åt the deep-learning network to the CIFAR-100 data.\\n(cid:142)1 [p. 352] The Neural Information Processing Systems (NIPS, now NeurIPS)\\nconferences started in late Fall 1987 in Denver, Colorado, and post-confer-\\nence workshops were held at the nearby ski resort at Vail. These are still\\nvery popular today, although the venue has changed over the years. The\\nNeurIPS proceedings are refereed, and NeurIPS papers count as publica-\\ntions in most Ô¨Åelds, especially Computer Science and Engineering. Al-\\nthough neural networks were initially the main topic of the conferences,\\na modern NeurIPS conference covers all the latest ideas in machine learn-\\ning.\\n\\n(cid:142)2 [p. 353] MNIST is a curated database of images of handwritten digits\\n(LeCun and Cortes, 2010). There are 60,000 training images, and 10,000\\ntest images, each a 28 (cid:2) 28 grayscale image. These data have been used as\\na testbed for many different learning algorithms, so the reported best error\\nrates might be optimistic.\\n\\n(cid:142)3 [p. 360] Tuning parameters. Typical neural network implementations have\\ndozens of tuning parameters, and many of these are associated with the Ô¨Åne\\ntuning of the descent algorithm. We used the h2o.deepLearning func-\\ntion in the R package h2o to Ô¨Åt our model for the MNIST data set. It has\\naround 20 such parameters, although most default to factory-tuned con-\\nstants that have been found to work well on many examples. Arno Candel\\nwas very helpful in assisting us with the software.\\n\\n(cid:142)4 [p. 368] Dropout and ridge regression. Dropout was originally proposed in\\nSrivastava et al. (2014), and reinterpreted in Wager et al. (2013). Dropout\\nwas inspired by the random selection of variables at each tree split in a\\nrandom forest (Section 17.1). Consider a simple version of dropout for\\nthe linear regression problem with squared-error loss. We have an n (cid:2) p\\nregression matrix X, and a response n-vector y. For simplicity we assume\\nall variables have mean zero, so we can ignore intercepts. Consider the\\n\\n\\x0c18.6 Notes and Details\\n\\n373\\n\\nfollowing random least-squares criterion:\\n\\nLI .Àá/ D 1\\n2\\n\\n0\\n@yi (cid:0)\\n\\nn\\nX\\n\\ni D1\\n\\np\\nX\\n\\nj D1\\n\\n1\\n\\n2\\n\\nxij Iij Àáj\\n\\nA\\n\\n:\\n\\nHere the Iij are i.i.d variables 8i; j with\\n(cid:26)\\n\\nIij D\\n\\n0 with probability (cid:30);\\n\\n1=.1 (cid:0) (cid:30)/ with probability 1 (cid:0) (cid:30);\\n\\n(this particular form is used so that E≈íIij (cid:141) D 1). Using simple probability\\nit can be shown that the expected score equations can be written\\n\\n(cid:21)\\n\\n(cid:20) @LI .Àá/\\n@Àá\\n\\nE\\n\\nD (cid:0)X 0y C X 0X Àá C (cid:30)\\n1 (cid:0) (cid:30)\\n\\nDÀá D 0;\\n\\n(18.23)\\n\\nwith D D diagfkx1k2; kx2k2; : : : ; kxpk2g. Hence the solution is given by\\n\\nOÀá D\\n\\n(cid:18)\\nX 0X C (cid:30)\\n1 (cid:0) (cid:30)\\n\\nD\\n\\n(cid:19)(cid:0)1\\n\\nX 0y;\\n\\n(18.24)\\n\\na generalized ridge regression. If the variables are standardized, the term\\nD becomes a scalar, and the solution is identical to ridge regression. With\\na nonlinear activation function, the interpretation changes slightly; see Wa-\\nger et al. (2013) for details.\\n\\n(cid:142)5 [p. 369] Distortion and ridge regression. We again show in a simple ex-\\nample that input distortion is similar to ridge regression. Assume the same\\nsetup as in the previous example, except a different randomized version of\\nthe criterion:\\n\\nLN .Àá/ D 1\\n2\\n\\nn\\nX\\n\\ni D1\\n\\n0\\n@yi (cid:0)\\n\\np\\nX\\n\\n1\\n\\n2\\n\\n.xij C nij /Àáj\\n\\nA\\n\\n:\\n\\nj D1\\n\\nHere we have added random noise to the prediction variables, and we as-\\nsume this noise is i.i.d .0; (cid:21)/. Once again the expected score equations can\\nbe written\\n\\n(cid:21)\\n\\n(cid:20) @LN .Àá/\\n@Àá\\n\\nE\\n\\nD (cid:0)X 0y C X 0X Àá C (cid:21)Àá D 0;\\n\\n(18.25)\\n\\nij / D (cid:21). Once again\\nbecause of the independence of all the nij and E.n2\\nthis leads to a ridge regression. So replacing each observation pair xi ; yi\\nby the collection fx(cid:3)b\\nbD1, where each x(cid:3)b\\nis a noisy version of xi , is\\napproximately equivalent to a ridge regression on the original data.\\n\\n; yi gB\\n\\ni\\n\\ni\\n\\n\\x0c374\\n\\nNeural Networks\\n\\n(cid:142)6 [p. 370] Software for deep learning. Our deep learning convolutional net-\\nwork for the CIFAR-100 data was constructed and run by Rakesh Achanta\\nin Theano, a Python-based system (Bastien et al., 2012; Bergstra et al.,\\n2010). Theano has a user-friendly language for specifying the host of\\nparameters for a deep-learning network, and uses symbolic differentiation\\nfor computing the gradients needed in stochastic gradient descent. In 2015\\nGoogle announced an open-source version of their TensorFlow software\\nfor Ô¨Åtting deep networks.\\n\\n\\x0c19\\n\\nSupport-Vector Machines and Kernel\\nMethods\\n\\nWhile linear logistic regression has been the mainstay in biostatistics and\\nepidemiology, it has had a mixed reception in the machine-learning com-\\nmunity. There the goal is often classiÔ¨Åcation accuracy, rather than statistical\\ninference. Logistic regression builds a classiÔ¨Åer in two steps: Ô¨Åt a condi-\\ntional probability model for Pr.Y D 1jX D x/, and then classify as a\\none if bPr.Y D 1jX D x/ (cid:21) 0:5. SVMs bypass the Ô¨Årst step, and build a\\nclassiÔ¨Åer directly.\\n\\nAnother rather awkward issue with logistic regression is that it fails if\\nthe training data are linearly separable! What this means is that, in the\\nfeature space, one can separate the two classes by a linear boundary. In\\ncases such as this, maximum likelihood fails and some parameters march\\noff to inÔ¨Ånity. While this might have seemed an unlikely scenario to the\\nearly users of logistic regression, it becomes almost a certainty with mod-\\nern wide genomics data. When p (cid:29) n (more features than observations),\\nwe can typically always Ô¨Ånd a separating hyperplane. Finding an optimal\\nseparating hyperplane was in fact the launching point for SVMs. As we\\nwill see, they have more than this to offer, and in fact live comfortably\\nalongside logistic regression.\\n\\nSVMs pursued an age-old approach in statistics, of enriching the feature\\nspace through nonlinear transformations and basis expansions; a classical\\nexample being augmenting a linear regression with interaction terms. A\\nlinear model in the enlarged space leads to a nonlinear model in the ambient\\nspace. This is typically achieved via the ‚Äúkernel trick,‚Äù which allows the\\ncomputations to be performed in the n-dimensional space for an arbitrary\\nnumber of predictors p. As the Ô¨Åeld matured, it became clear that in fact\\nthis kernel trick amounted to estimation in a reproducing-kernel Hilbert\\nspace.\\n\\nFinally, we contrast the kernel approach in SVMs with the nonparame-\\n\\nteric regression techniques known as kernel smoothing.\\n\\n375\\n\\n\\x0c376\\n\\nSVMs and Kernel Methods\\n\\n19.1 Optimal Separating Hyperplane\\nFigure 19.1 shows a small sample of points in R2, each belonging to one of\\ntwo classes (blue or orange). Numerically we would score these classes as\\ny D C1 for say blue, and y D (cid:0)1 for orange.1 We deÔ¨Åne a two-class lin-\\near classiÔ¨Åer via a function f .x/ D Àá0 C x0Àá, with the convention that we\\nclassify a point x0 as +1 if f .x0/ > 0, and as -1 if f .x0/ < 0 (on the fence\\nwe Ô¨Çip a coin). Hence the classiÔ¨Åer itself is C.x/ D sign≈íf .x/(cid:141). The deci-\\n\\nFigure 19.1 Left panel: data in two classes in R2. Three potential\\ndecision boundaries are shown; each separate the data perfectly.\\nRight panel: the optimal separating hyperplane (a line in R2)\\ncreates the biggest margin between the two classes.\\n\\nsion boundary is the set fx j f .x/ D 0g. We see three different classiÔ¨Åers\\nin the left panel of Figure 19.1, and they all classify the points perfectly.\\nThe optimal separating hyperplane is the linear classiÔ¨Åer that creates the\\nlargest margin between the two classes, and is shown in the right panel\\n(it is also known as an optimal-margin classiÔ¨Åer). The underlying hope is\\nthat, by making a big margin on the training data, it will also classify future\\nobservations well.\\n\\n(cid:142)1\\n\\nSome elementary geometry (cid:142) shows that the (signed) Euclidean distance\\n\\nfrom a point x0 to the linear decision boundary deÔ¨Åned by f is given by\\n\\n1\\nkÀák2\\n\\nf .x0/:\\n\\n(19.1)\\n\\nWith this in mind, for a separating hyperplane the quantity 1\\nkÀá k2\\n\\nyi f .xi / is\\n\\n1 In this chapter, the Àô1 scoring leads to convenient notation.\\n\\n‚àí10123‚àí10123X1X2llllllllllllllllllllllll‚àí10123‚àí10123X1X2llllllllllllllllllllllllllllllllllllllllllllllll\\x0c19.1 Optimal Separating Hyperplane\\n\\n377\\n\\nthe distance of xi from the decision boundary.2 This leads to an optimiza-\\ntion problem for creating the optimal margin classiÔ¨Åer:\\n\\nmaximize\\nÀá0; Àá\\n\\nM\\n\\n1\\nkÀák2\\nA rescaling argument reduces this to the simpler form\\n\\nsubject to\\n\\nyi .Àá0 C x0Àá/ (cid:21) M; i D 1; : : : ; n:\\n\\nminimize\\nÀá0; Àá\\n\\nkÀák2\\n\\nsubject to yi .Àá0 C x0Àá/ (cid:21) 1; i D 1; : : : ; n:\\n\\n(19.2)\\n\\n(19.3)\\n\\nThis is a quadratic program, which can be solved by standard techniques\\nin convex optimization.(cid:142) One noteworthy property of the solution is that\\nOÀá D X\\ni 2\\n\\nOÀõi xi ;\\n\\n(19.4)\\n\\nS\\n\\n(cid:142)2\\n\\nS\\n\\nS\\n\\nS\\n\\nS\\n\\n, and their coefÔ¨Åcients Àõi ;\\n\\nis the support set. We can see in Figure 19.1 that the margin\\nwhere\\ntouches three points (vectors); in this case there are j\\nj D 3 support vec-\\ntors, and clearly the orientation of OÀá is determined by them. However, we\\nstill have to solve the optimization problem to identify the three points\\ni 2\\nin\\n. Figure 19.2 shows an optimal-\\nmargin classiÔ¨Åer Ô¨Åt to wide data, that is data where p (cid:29) n. These are\\ngene-expression measurements on p D 3571 genes measured on blood\\nsamples from n D 72 leukemia patients (Ô¨Årst seen in Chapter 1). They\\nwere classiÔ¨Åed into two classes, 47 acute lymphoblastic leukemia (ALL)\\nand 25 myeloid leukemia (AML). In cases like this, we are typically guar-\\nanteed a separating hyperplane3. In this case 42 of the 72 points are support\\npoints. One might be justiÔ¨Åed in thinking that this solution is overÔ¨Åt to this\\nsmall amount of data. Indeed, when broken into a training and test set, we\\nsee that the test data encroaches well into the margin region, but in this\\ncase none are misclassiÔ¨Åed. Such classiÔ¨Åers are very popular in the wide-\\ndata world of genomics, largely because they seem to work very well. They\\noffer a simple alternative to logistic regression, in a situation where the lat-\\nter fails. However, sometimes the solution is overÔ¨Åt, and a modiÔ¨Åcation is\\ncalled for. This same modiÔ¨Åcation takes care of nonseparable situations as\\nwell.\\n\\n2 Since all the points are correctly classiÔ¨Åed, the sign of f .xi / agrees with yi , hence this\\n\\nquantity is always positive.\\n\\n3 If n (cid:20) p C 1 we can always Ô¨Ånd a separating hyperplane, unless there are exact feature\\n\\nties across the class barrier!\\n\\n\\x0c378\\n\\nSVMs and Kernel Methods\\n\\nFigure 19.2 Left panel: optimal margin classiÔ¨Åer Ô¨Åt to\\nleukemia data. There are 72 observations from two\\nclasses‚Äî47 ALL and 25 AML‚Äîand 3571 gene-expression\\nvariables. Of the 72 observations, 42 are support vectors, sitting\\non the margin. The points are plotted against their Ô¨Åtted classiÔ¨Åer\\nfunction Of .x/, labeled SVM projection, and the Ô¨Åfth principal\\ncomponent of the data (chosen for display purposes, since it has\\nlow correlation with the former). Right panel: here the optimal\\nmargin classiÔ¨Åer was Ô¨Åt to a random subset of 50 of the 72\\nobservations, and then used to classify the remaining 22 (shown\\nin color). Although these points fall on the wrong sides of their\\nrespective margins, they are all correctly classiÔ¨Åed.\\n\\n19.2 Soft-Margin ClassiÔ¨Åer\\nFigure 19.3 shows data in R2 that are not separable. The generalization to\\na soft margin allows points to violate their margin. Each of the violators\\nhas a line segment connecting it to its margin, showing the extent of the\\nviolation. The soft-margin classiÔ¨Åer solves\\n\\nminimize\\nÀá0; Àá\\n\\nkÀák2\\n\\nsubject to yi .Àá0 C x0\\n\\ni Àá/ (cid:21) 1 (cid:0) (cid:15)i ;\\n\\n(cid:15)i (cid:21) 0; i D 1; : : : ; n; and\\n\\nn\\nX\\n\\ni D1\\n\\n(cid:15)i (cid:20) B:\\n\\n(19.5)\\n\\nHere B is the budget for the total amount of overlap. Once again, the solu-\\nincludes any vectors\\ntion has the form (19.4), except now the support set\\non the margin as well as those that violate the margin. The bigger B, the\\n\\nS\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll‚àí1.5‚àí1.0‚àí0.50.00.51.01.5‚àí0.2‚àí0.10.00.10.20.3Leukemia: All DataSVM ProjectionPCA 5 Projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll‚àí1.5‚àí1.0‚àí0.50.00.51.0‚àí0.2‚àí0.10.00.10.20.3Leukemia: Train and TestSVM ProjectionPCA 5 Projectionllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\\x0c19.3 SVM Criterion as Loss Plus Penalty\\n\\n379\\n\\nFigure 19.3 For data that are not separable, such as here, the\\nsoft-margin classiÔ¨Åer allows margin violations. The budget B for\\nthe total measure of violation becomes a tuning parameter. The\\nbigger the budget, the wider the soft margin and the more support\\npoints there are involved in the Ô¨Åt.\\n\\nbigger the support set, and hence the more points that have a say in the\\nsolution. Hence bigger B means more stability and lower variance. In fact,\\neven for separable data, allowing margin violations via B lets us regularize\\nthe solution by tuning B.\\n\\n19.3 SVM Criterion as Loss Plus Penalty\\n\\nIt turns out that one can reformulate (19.5) and (19.3) in more traditional\\nterms as the minimization of a loss plus a penalty:\\n\\nminimize\\nÀá0; Àá\\n\\nn\\nX\\n\\n≈í1 (cid:0) yi .Àá0 C x0\\n\\ni Àá/(cid:141)C C (cid:21)kÀák2\\n2:\\n\\n(19.6)\\n\\ni D1\\n\\nHere the hinge loss LH .y; f .x// D ≈í1 (cid:0) yf .x/(cid:141)C operates on the margin\\nquantity yf .x/, and is piecewise linear as in Figure 19.4.(cid:142)The same margin (cid:142)3\\nquantity came up in boosting in Section 17.4. The quantity ≈í1 (cid:0) yi .Àá0 C\\nx0\\ni Àá/(cid:141)C is the cost for xi being on the wrong side of its margin (the cost\\nis zero if it‚Äôs on the correct side). The correspondence between (19.6) and\\n(19.5) is exact; large (cid:21) corresponds to large B, and this formulation makes\\nexplicit the form of regularization. For separable data, the optimal separat-\\ning hyperplane solution (19.3) corresponds to the limiting minimum-norm\\nsolution as (cid:21) # 0. One can show that the population minimizer of the\\n\\n‚àí2‚àí101234‚àí101234X1X2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll‚àí2‚àí101234‚àí101234X1X2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\\x0c380\\n\\nSVMs and Kernel Methods\\n\\nFigure 19.4 The hinge loss penalizes observation margins yf .x/\\nless than C1 linearly, and is indifferent to margins greater than\\nC1. The negative binomial log-likelihood (deviance) has the\\nsame asymptotes, but operates in a smoother fashion near the\\nelbow at yf .x/ D 1.\\n\\nhinge loss is in fact the Bayes classiÔ¨Åer.4 This shows that the SVM is in\\nfact directly estimating the classiÔ¨Åer C.x/ 2 f(cid:0)1; C1g.(cid:142)\\n\\n(cid:142)4\\n\\nThe red curve in Figure 19.4 is (half) the binomial deviance for logistic\\nregression (i.e. f .x/ D Àá0 C x0Àá is now modeling logit Pr.Y D C1jX D\\nx/). With Y D Àô1, the deviance can also be written in terms of the margin,\\nand the ridged logistic regression corresponding to (19.6) has the form\\n\\nminimize\\nÀá0; Àá\\n\\nn\\nX\\n\\niD1\\n\\nlog≈í1 C e(cid:0)yi .Àá0Cx0\\n\\ni Àá /(cid:141) C (cid:21)kÀák2\\n2:\\n\\n(19.7)\\n\\nLogistic regression is discussed in Section 8.1, as well as Sections 16.5 and\\n17.4. This form of the binomial deviance is derived in (17.13) on page 343.\\nThese loss functions have some features in common, as can be seen in the\\nÔ¨Ågure. The binomial loss asymptotes to zero for large positive margins, and\\nto a linear loss for large negative margins, matching the hinge loss in this\\nregard. The main difference is that the hinge has a sharp elbow at +1, while\\nthe binomial bends smoothly. A consequence of this is that the binomial\\nsolution involves all the data, via weights pi .1 (cid:0) pi / that fade smoothly\\nwith distance from the decision boundary, as opposed to the binary nature\\n\\n4 The Bayes classiÔ¨Åer C.x/ for a two-class problem using equal costs for\\n\\nmisclassiÔ¨Åcation errors assigns x to the class for which Pr.yjx/ is largest.\\n\\n‚àí3‚àí2‚àí101230.00.51.01.52.02.53.0yf(x)LossBinomialSVM\\x0c19.4 Computations and the Kernel Trick\\n\\n381\\n\\nof support points. Also, as seen in Section 17.4 as well, the population\\nminimizer of the binomial deviance is the logit of the class probability\\n\\n(cid:21).x/ D log\\n\\n(cid:18) Pr.y D C1jx/\\nPr.y D (cid:0)1jx/\\n\\n(cid:19)\\n\\n;\\n\\n(19.8)\\n\\nwhile that of the hinge loss is its sign C.x/ D sign≈í(cid:21).x/(cid:141). Interestingly,\\nas (cid:21) # 0 the solution direction OÀá to the ridged logistic regression prob-\\nlem (19.7) converges to that of the SVM.(cid:142)\\n\\n(cid:142)5\\n\\nThese forms immediately suggest other generalizations of the linear\\nSVM. In particular, we can replace the ridge penalty kÀák2\\n2 by the sparsity-\\ninducing lasso penalty kÀák1, which will set some coefÔ¨Åcients to zero and\\nhence perform feature selection. Publicly available software (e.g. package\\nliblineaR in R) is available for Ô¨Åtting such lasso-regularized support-\\nvector classiÔ¨Åers.\\n\\n19.4 Computations and the Kernel Trick\\n\\nThe form of the solution OÀá D P\\nOÀõi xi for the optimal- and soft-margin\\nclassiÔ¨Åer has some important consequences. For starters, we can write the\\nÔ¨Åtted function evaluated at a point x as\\n\\ni 2\\n\\nS\\n\\nOf .x/ D OÀá0 C x0 OÀá\\nD OÀá0 C X\\n\\nOÀõi hx; xi i;\\n\\ni 2\\n\\nS\\n\\n(19.9)\\n\\nwhere we have deliberately replaced the transpose notation with the more\\nsuggestive inner product. Furthermore, we show in (19.23) in Section 19.9\\nthat the Lagrange dual involves the data only through the n2 pairwise inner\\nproducts hxi ; xj i (the elements of the n (cid:2) n gram matrix XX 0). This means\\nthat the computations for computing the SVM solution scale linearly with\\np, although potentially cubic5 in n. With very large p (in the tens of thou-\\nsands and even millions as we will see), this can be convenient.\\n\\nIt turns out that all ridge-regularized linear models with wide data can\\n\\nbe reparametrized in this way. Take ridge regression, for example:\\n\\nminimize\\nÀá\\n\\nky (cid:0) X Àák2\\n2\\n\\nC (cid:21)kÀák2\\n2:\\n\\n(19.10)\\n\\nThis has solution OÀá D .X 0X C (cid:21)Ip/(cid:0)1X 0y, and with p large requires\\ninversion of a p (cid:2) p matrix. However, it can be shown that OÀá D X 0 OÀõ D\\n\\n5 In practice O.n2jSj/, and, with modern approximate solutions, much faster than that.\\n\\n\\x0c(cid:142)6\\n\\n(cid:142)7\\n\\n382\\n\\nSVMs and Kernel Methods\\n\\niD1\\n\\nPn\\nOÀõi xi , with OÀõ D .XX 0 C (cid:21)In/(cid:0)1y, which means the solution can\\nbe obtained in O.n2p/ rather than O.np2/ computations. Again the gram\\nmatrix has played a role, and OÀá has the same form as for the SVM.(cid:142)\\n\\nWe now imagine expanding the p-dimensional feature vector x into a\\npotentially much larger set h.x/ D ≈íh1.x/; h2.x/; : : : ; hm.x/(cid:141); for an ex-\\nample to latch onto, think polynomial basis of total degree d . As long as we\\nhave an efÔ¨Åcient way to compute the inner products hh.x/; h.xj /i for any\\nx, we can compute the SVM solution in this enlarged space just as easily\\nas in the original. It turns out that convenient kernel functions exist that do\\njust that. For example Kd .x; z/ D .1 C hx; zi/d creates a basis expansion\\nhd of polynomials of total degree d , and Kd .x; z/ D hhd .x/; hd .z/i.(cid:142)\\n\\nThe polynomial kernels are mainly useful as existence proofs; in practice\\nother more useful kernels are used. Probably the most popular is the radial\\nkernel\\n\\nK.x; z/ D e(cid:0)(cid:13) kx(cid:0)zk2\\n2:\\n\\n(19.11)\\n\\nThis is a positive-deÔ¨Ånite function, and can be thought of as computing an\\ninner product in some feature space. Here the feature space is in principle\\ninÔ¨Ånite-dimensional, but of course effectively Ô¨Ånite.6 Now one can think\\nof the representation (19.9) in a different light;\\n\\nOf .x/ D OÀõ0 C X\\n\\nOÀõi K.x; xi /;\\n\\n(19.12)\\n\\ni2\\n\\nS\\n\\nan expansion of radial basis functions, each centered on one of the train-\\ning examples. Figure 19.5 illustrates such an expansion in R1. Using such\\nnonlinear kernels expands the scope of SVMs considerably, allowing one\\nto Ô¨Åt classiÔ¨Åers with nonlinear decision boundaries.\\n\\nOne may ask what objective is being optimized when we move to this\\nkernel representation. This is covered in the next section, but as a sneak\\npreview we present the criterion\\n\"\\n\\n!#\\n\\n1 (cid:0) yj\\n\\nÀõ0 C\\n\\nÀõi K.xj ; xi /\\n\\nC (cid:21)Àõ0K Àõ;\\n\\n(19.13)\\n\\nn\\nX\\n\\nn\\nX\\n\\nminimize\\nÀõ0; Àõ\\n\\nj D1\\n\\ni D1\\n\\nC\\n\\nwhere the n (cid:2) n matrix K has entries K.xj ; xi /.\\n\\nAs an illustrative example in R2 (so we can visualize the nonlinear\\nboundaries), we generated the data in Figure 19.6. We show two SVM\\n\\n6 A bivariate function K.x; z/ (Rp (cid:2) Rp 7! R1) is positive-deÔ¨Ånite if, for every q,\\n\\nevery q (cid:2) q matrix K D fK.xi ; xj /g formed using distinct entries x1; x2; : : : ; xq is\\npositive deÔ¨Ånite. The feature space is deÔ¨Åned in terms of the eigen-functions of the\\nkernel.\\n\\n \\n\\x0c19.4 Computations and the Kernel Trick\\n\\n383\\n\\nFigure 19.5 Radial basis functions in R1. The left panel shows a\\ncollection of radial basis functions, each centered on one of the\\nseven observations. The right panel shows a function obtained\\nfrom a particular linear expansion of these basis functions.\\n\\nsolutions, both using a radial kernel. In the left panel, some margin errors\\nare committed, but the solution looks reasonable. However, with the Ô¨Çex-\\nibility of the enlarged feature space, by decreasing the budget B we can\\ntypically overÔ¨Åt the training data, as is the case in the right panel. A sepa-\\nrate little blue island was created to accommodate the one blue point in a\\nsea of brown.\\n\\nFigure 19.6 Simulated data in two classes in R2, with SVM\\nclassiÔ¨Åers computed using the radial kernel (19.11). The left\\npanel uses a larger value of B than the right. The solid lines are\\nthe decision boundaries in the original space (linear boundaries in\\nthe expanded feature space). The dashed lines are the projected\\nmargins in both cases.\\n\\n‚àí2‚àí10120.00.51.01.5‚àí2‚àí1012‚àí0.40.00.20.4RadialBasisFunctionsf(x)K(x,xj)f(x)=Œ±0+PjŒ±jK(x,xj)xx‚àí4‚àí2024‚àí4‚àí2024X1X2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll        llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll‚àí4‚àí2024‚àí4‚àí2024X1X2llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll                    lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll\\x0c384\\n\\nSVMs and Kernel Methods\\n\\n19.5 Function Fitting Using Kernels\\n\\nThe analysis in the previous section is heuristic‚Äîreplacing inner products\\nby kernels that compute inner products in some (implicit) feature space.\\nIndeed, this is how kernels were Ô¨Årst introduced in the SVM world. There\\nis however a rich literature behind such approaches, which goes by the\\nname function Ô¨Åtting in reproducing-kernel Hilbert spaces (RKHSs). We\\ngive a very brief overview here. One starts with a bivariate positive-deÔ¨Ånite\\nkernel K W Rp (cid:2) Rp ! R1, and we consider a space\\nK of functions\\nf W Rp ! R1 generated by the kernel: f 2 spanfK.(cid:1); z/; z 2 Rpg7 The\\nkernel also induces a norm on the space kf k\\nK ,(cid:142) which can be thought of\\nas a roughness measure.\\n\\nH\\n\\nH\\n\\nWe can now state a very general optimization problem for Ô¨Åtting a func-\\n\\n(cid:142)8\\n\\ntion to data, when restricted to this class;\\n( n\\nX\\n\\nminimize\\nH\\n\\nf 2\\n\\nK\\n\\ni D1\\n\\nL.yi ; Àõ0 C f .xi // C (cid:21)kf k2\\nH\\n\\nK\\n\\n)\\n\\n;\\n\\n(19.14)\\n\\na search over a possibly inÔ¨Ånite-dimensional function space. Here L is an\\narbitrary loss function. The ‚Äúmagic‚Äù of these spaces in the context of this\\nproblem is that one can show that the solution is Ô¨Ånite-dimensional:\\n\\nOf .x/ D\\n\\nn\\nX\\n\\niD1\\n\\nOÀõi K.x; xi /;\\n\\n(19.15)\\n\\na linear basis expansion with basis functions ki .x/ D K.x; xi / anchored\\nat each of the observed ‚Äúvectors‚Äù xi in the training data. Moreover, using\\nthe ‚Äúreproducing‚Äù property of the kernel in this space, one can show that\\nthe penalty reduces to\\n\\nk Of k2\\n\\nK\\n\\nH\\n\\nn\\nX\\n\\nn\\nX\\n\\nD\\n\\ni D1\\n\\nj D1\\n\\nOÀõi OÀõj K.xi ; xj / D OÀõ0K OÀõ:\\n\\n(19.16)\\n\\nHere K is the n (cid:2) n gram matrix of evaluations of the kernel, equivalent to\\nthe XX 0 matrix for the linear case.\\n\\nHence the abstract problem (19.14) reduces to the generalized ridge\\n\\nproblem\\n\\nminimize\\nÀõ2Rn\\n\\n0\\n@yi ; Àõ0 C\\n\\nL\\n\\n8\\n<\\n\\nn\\nX\\n\\n:\\n\\niD1\\n\\nn\\nX\\n\\nj D1\\n\\nÀõi K.xi ; xj /\\n\\n1\\nA C (cid:21)Àõ0K Àõ\\n\\n9\\n=\\n\\n;\\n\\n:\\n\\n(19.17)\\n\\n7 Here kz D K.(cid:1); z/ is considered a function of the Ô¨Årst argument, and the second\\n\\nargument is a parameter.\\n\\n\\x0c19.6 Example: String Kernels for Protein ClassiÔ¨Åcation\\n\\n385\\n\\nIndeed, if L is the hinge loss as in (19.6), this is the equivalent ‚Äúloss plus\\npenalty‚Äù criterion being Ô¨Åt by the kernel SVM. Alternatively, if L is the bi-\\nnomial deviance loss as in (19.7), this would Ô¨Åt a kernel version of logistic\\nregression. Hence most Ô¨Åtting methods can be generalized to accommodate\\nkernels.\\n\\nThis formalization opens the door to a wide variety of applications, de-\\npending on the kernel function used. Alternatively, as long as we can com-\\npute suitable similarities between objects, we can build sophisticated clas-\\nsiÔ¨Åers and other models for making predictions about other attributes of\\nthe objects.8 In the next section we consider a particular example.\\n\\n19.6 Example: String Kernels for Protein ClassiÔ¨Åcation\\n\\nOne of the important problems in computational biology is to classify pro-\\nteins into functional and structural classes based on their sequence simi-\\nlarities. Protein molecules can be thought of as strings of amino acids, and\\ndiffer in terms of length and composition. In the example we consider, the\\nlengths vary between 75 and 160 amino-acid molecules, each of which can\\nbe one of 20 different types, labeled using the letters of the alphabet.\\n\\nHere follow two protein examples x1 and x2, of length 110 and 153\\n\\nrespectively:\\n\\nIPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGGTV\\n\\nERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDYLQEFLGVMNTEWI\\n\\nPHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAERLQENLQAYRTFHVLLA\\n\\nRLLEDQQVHFTPTEGDFHQAIHTLLLQVAAFAYQIEELMILLEYKIPRNEADGMLFEKK\\n\\nLWGLKVLQELSQWTVRSIHDLRFISSHQTGIP\\n\\nWe treat the proteins x as documents consisting of letters, with a dictio-\\nnary of size 20. Our feature vector hm.x/ will consist of the counts for all\\nm-grams in the protein‚Äîthat is, distinct sequences of consecutive letters of\\nlength m. As an illustration, we use m D 3, which results in 203 D8,000\\npossible sub-sequences; hence h3.x/ will be a vector of length 8,000, with\\neach element the number of times that particular sub-sequence occurs in\\nthe protein x. In our example, the sub-sequence LQE occurs once in the\\nÔ¨Årst, and twice in the second protein, so h3\\n\\nLQE.x1/ D 1 and h3\\n\\nLQE.x2/ D 2.\\n\\nThe number of possible sequences of length m is 20m, which can be very\\n\\n8 As long as the similarities behave like inner products; i.e. they form positive\\n\\nsemi-deÔ¨Ånite matrices.\\n\\n\\x0c386\\n\\nSVMs and Kernel Methods\\n\\nlarge for moderate m. Also the vast majority of the sub-sequences do not\\nmatch the strings in our training set, which means hm.x/ will be sparse. It\\nturns out that we can compute the n(cid:2)n inner product matrix or string kernel\\nKm.x1; x2/ D hhm.x1/; hm.x2/i efÔ¨Åciently using tree structures, without\\nactually computing the individual vectors. (cid:142) Armed with the kernel, we\\n\\n(cid:142)9\\n\\nFigure 19.7 ROC curves for two classiÔ¨Åers Ô¨Åt to the protein data.\\nThe ROC curves were computed using 10-fold cross-validation,\\nand trace the trade-off between false-positive and true-positive\\nerror rates as the classiÔ¨Åer threshold is varied. The area under the\\ncurve (AUC) summarizes the overall performance of each\\nclassiÔ¨Åer. Here the SVM is slightly superior to kernel logistic\\nregression.\\n\\ncan now use it to Ô¨Åt a regularized SVM or logistic regression model, as\\noutlined in the previous section. The data consist of 1708 proteins in two\\nclasses‚Äînegative (1663) and positive (45). We Ô¨Åt both the kernel SVM\\nand kernel logistic regression models. For both methods, cross-validation\\nsuggested a very small value for (cid:21). Figure 19.7 shows the ROC trade-off\\ncurve for each, using 10-fold cross-validation. Here the SVM outperforms\\nlogistic regression.\\n\\nProtein ClassificationFalse-positive rateTrue-positive rate0.00.20.40.60.81.00.00.20.40.60.81.0AUCSVM  0.84KLR  0.78\\x0c19.7 SVMs: Concluding Remarks\\n\\n387\\n\\n19.7 SVMs: Concluding Remarks\\n\\nSVMs have been wildly successful, and are one of the ‚Äúmust have‚Äù tools in\\nany machine-learning toolbox. They have been extended to cover many dif-\\nferent scenarios, other than two-class classiÔ¨Åcation, with some awkward-\\nness in cases. The extension to nonlinear function-Ô¨Åtting via kernels (in-\\nspiring the ‚Äúmachine‚Äù in the name) generated a mini industry. Kernels are\\nparametrized, learned from data, with special problem-speciÔ¨Åc structure,\\nand so on.\\n\\nOn the other hand, we know that Ô¨Åtting high-dimensional nonlinear\\nfunctions is intrinsically difÔ¨Åcult (the ‚Äúcurse of dimensionality‚Äù), and SVMs\\nare not immune. The quadratic penalty implicit in kernel methodology\\nmeans all features are included in the model, and hence sparsity is gen-\\nerally not an option. Why then this unbridled enthusiasm? ClassiÔ¨Åers are\\nfar less sensitive to bias‚Äìvariance tradeoffs, and SVMs are mostly popular\\nfor their classiÔ¨Åcation performance. The ability to deÔ¨Åne a kernel for mea-\\nsuring similarities between abstract objects, and then train a classiÔ¨Åer, is a\\nnovelty added by these approaches that was missed in the past.\\n\\n19.8 Kernel Smoothing and Local Regression\\n\\nThe phrase ‚Äúkernel methodology‚Äù might mean something a little different\\nto statisticians trained in the 1970‚Äì90 period. Kernel smoothing represents\\na broad range of tools for performing non- and semi-parametric regres-\\nsion. Figure 19.8 shows a Gaussian kernel smooth Ô¨Åt to some artiÔ¨Åcial data\\nfxi ; yi gn\\n1. It computes at each point x0 a weighted average of the y-values\\nof neighboring points, with weights given by the height of the kernel. In its\\nsimplest form, this estimate can be written as\\n\\nOf .x0/ D\\n\\nn\\nX\\n\\niD1\\n\\nyi K(cid:13) .x0; xi /;\\n\\n(19.18)\\n\\nwhere K(cid:13) .x0; xi / represents the radial kernel with width parameter (cid:13).9\\nNotice the similarity to (19.15); here the OÀõi D yi , and the complexity of\\nthe model is controlled by (cid:13). Despite this similarity, and the use of the\\nsame kernel, these methodologies are rather different.\\n\\nThe focus here is on local estimation, and the kernel does the local-\\nizing. Expression (19.18) is almost a weighted average‚Äîalmost because\\n\\n9 Here K(cid:13) .x; (cid:22)/ is the normalized Gaussian density with mean (cid:22) and variance 1=(cid:13) .\\n\\n\\x0c388\\n\\nSVMs and Kernel Methods\\n\\nFigure 19.8 A Gaussian kernel smooth of simulated data. The\\npoints come from the blue curve with added random errors. The\\nkernel smoother Ô¨Åts a weighted mean of the observations, with\\nthe weighting kernel centered at the target point, x0 in this case.\\nThe points shaded orange contribute to the Ô¨Åt at x0. As x0 moves\\nacross the domain, the smoother traces out the green curve. The\\nwidth of the kernel is a tuning parameter. We have depicted the\\nGaussian weighting kernel in this Ô¨Ågure for illustration; in fact its\\nvertical coordinates are all positive and integrate to one.\\n\\nPn\\n\\niD1 K(cid:13) .x0; xi / (cid:25) 1. In fact, the Nadaraya‚ÄìWatson estimator is more ex-\\n\\nplicit:\\n\\nOfN W .x0/ D\\n\\nPn\\nPn\\n\\ni D1 yi K(cid:13) .x0; xi /\\ni D1 K(cid:13) .x0; xi /\\n\\n:\\n\\n(19.19)\\n\\nAlthough Figure 19.8 is one-dimensional, the same formulation applies to\\nx in higher dimensions.\\n\\nWeighting kernels other than the Gaussian are typically favored; in par-\\nticular, near-neighbor kernels with compact support. For example, the tricube\\nkernel used by the lowess smoother in R is deÔ¨Åned as follows:\\n1 DeÔ¨Åne di D kx0 (cid:0) xi k2; i D 1; : : : ; n, and let d.m/ be the mth smallest\\n(the distance of the mth nearest neighbor to x0). Let ui D di =d.m/; i D\\n1; : : : ; n.\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0‚àí1.0‚àí0.50.00.51.01.5Gaussian KernelXYlllllllllllllllllllllllllllllllllllllllllllllllllx0\\x0c19.8 Kernel Smoothing and Local Regression\\n\\n389\\n\\n2 The tricube kernel is given by\\n\\nKs.x0; xi / D\\n\\n( (cid:0)1 (cid:0) u3\\n\\ni\\n\\n(cid:1)3\\n\\n0\\n\\nif ui (cid:20) 1;\\notherwise,\\n\\n(19.20)\\n\\nwhere s D m=n, the span of the kernel. Near-neighbor kernels such as\\nthis adapt naturally to the local density of the xi ; wider in low-density\\nregions, narrower in high-density regions. A tricube kernel is illustrated\\nin Figure 19.9.\\n\\nFigure 19.9 Local regression Ô¨Åt to the simulated data. At each\\npoint x0, we Ô¨Åt a locally weighted linear least-squares model, and\\nuse the Ô¨Åtted value to estimate OfLR.x0/. Here we use the tricube\\nkernel (19.20), with a span of 25%. The orange points are in the\\nweighting neighborhood, and we see the orange linear Ô¨Åt\\ncomputed by kernel weighted least squares. The green dot is the\\nÔ¨Åtted value at x0 from this local linear Ô¨Åt.\\n\\nWeighted means suffer from boundary bias‚Äîwe can see in Figure 19.8 that\\nthe estimate appears biased upwards at both boundaries. The reason is that,\\nfor example on the left, the estimate for the function on the boundary aver-\\nages points always to the right, and since the function is locally increasing,\\nthere is an upward bias. Local linear regression is a natural generalization\\nthat Ô¨Åxes such problems. At each point x0 we solve the following weighted\\n\\nllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0.00.20.40.60.81.0‚àí1.0‚àí0.50.00.51.01.5Local Regression (tricube)XYlllllllllllllllllllllllllllx0\\x0c390\\n\\nSVMs and Kernel Methods\\n\\nleast-squares problem\\n\\n(cid:142)10\\n\\n. OÀá0.x0/; OÀá.x0// D arg min\\n\\nn\\nX\\n\\nKs.x0; xi /.yi (cid:0) Àá0 (cid:0) xi Àá/2:\\n\\n(19.21)\\n\\nÀá0;Àá\\n\\ni D1\\nThen OfLR.x0/ D OÀá0.x0/ C x0\\nOÀá.x0/. One can show that, to Ô¨Årst order,\\nOfLR.x0/ removes the boundary bias exactly.(cid:142)\\n\\nFigure 19.9 illustrates the procedure on our simulated data, using the\\ntricube kernel with a span of 25% of the data. In practice, the width of the\\nkernel (the span here) has to be selected by some means; typically we use\\ncross-validation.\\n\\nLocal regression works in any dimension; that is, we can Ô¨Åt two- or\\nhigher-dimensional surfaces using exactly the same technique. Here the\\nability to remove boundary bias really pays off, since the boundaries can\\nbe complex. These are referred to as memory-based methods, since there\\nis no Ô¨Åtted model. We have to save all the training data, and recompute the\\nlocal Ô¨Åt every time we make a prediction.\\n\\nLike kernel SVMs and their relatives, kernel smoothing and local regres-\\nsion break down in high dimensions. Here the near neighborhoods become\\nso wide that they are no longer local.\\n\\n19.9 Notes and Details\\n\\nIn the late 1980s and early 1990s, machine-learning research was largely\\ndriven by prediction problems, and the neural-network community at AT&T\\nBell laboratories was amongst the leaders. The problem of the day was\\nthe US Post-OfÔ¨Åce handwritten zip-code OCR challenge‚Äîa 10-class im-\\nage classiÔ¨Åcation problem. Vladimir Vapnik was part of this team, and\\nalong with colleagues invented a more direct approach to classiÔ¨Åcation, the\\nsupport-vector machine. This started with the seminal paper by Boser et al.\\n(1992), which introduced the optimal margin classiÔ¨Åer (optimal separating\\nhyperplane); see also Vapnik (1996). The ideas took off quite rapidly, at-\\ntracting a large cohort of researchers, and evolved into the more general\\nclass of ‚Äúkernel‚Äù methods‚Äîthat is, models framed in reproducing-kernel\\nHilbert spaces. A good general reference is Sch¬®olkopf and Smola (2001).\\n(cid:142)1 [p. 376] Geometry of separating hyperplanes. Let f .x/ D Àá0x C Àá0 de-\\nÔ¨Åne a linear decision boundary fx j f .x/ D 0g in Rp (an afÔ¨Åne set of co-\\ndimension one). The unit vector normal to the boundary is Àá=kÀák2, where\\nk (cid:1) k2 denotes the `2 or Euclidean norm. How should one compute the dis-\\ntance from a point x to this boundary? If x0 is any point on the boundary\\n\\n\\x0c19.9 Notes and Details\\n\\n391\\n\\n(i.e. f .x0/ D 0), we can project x (cid:0) x0 onto the normal, giving us\\n\\nÀá0.x (cid:0) x0/\\nkÀák2\\n\\nD 1\\nkÀák2\\n\\nf .x/;\\n\\nas claimed in (19.1). Note that this is the signed distance, since f .x/ will\\nbe positive or negative depending on what side of the boundary it lies on.\\n(cid:142)2 [p. 377] The ‚Äúsupport‚Äù in SVM. The Lagrange primal problem correspond-\\n\\ning to (19.3) can be written as\\n\\nminimize\\nÀá0; Àá\\n\\n(\\n\\n1\\n2\\n\\nÀá0Àá C\\n\\nn\\nX\\n\\ni D1\\n\\n(cid:13)i ≈í1 (cid:0) yi .Àá0 C x0\\n\\n)\\ni Àá/(cid:141)\\n\\n;\\n\\n(19.22)\\n\\ni D1 (cid:13)i yi xi and Pn\\n\\nwhere (cid:13)i (cid:21) 0 are the Lagrange multipliers. On differentiating we Ô¨Ånd that\\nÀá D Pn\\niD1 yi (cid:13)i D 0. With Àõi D yi (cid:13)i , we get (19.4), and\\nnote that the positivity constraint on (cid:13)i will lead to some of the Àõi being\\nzero. Plugging into (19.22) we obtain the Lagrange dual problem\\n\\nmaximize\\nf(cid:13)i gn\\n1\\n\\n( n\\nX\\n\\ni D1\\n\\n(cid:13)i (cid:0) 1\\n2\\n\\nn\\nX\\n\\nn\\nX\\n\\niD1\\n\\nj D1\\n\\n(cid:13)i (cid:13)j yi yj x0\\n\\ni xj\\n\\n9\\n=\\n\\n;\\n\\nsubject to (cid:13)i (cid:21) 0;\\n\\nn\\nX\\n\\niD1\\n\\nyi (cid:13)i D 0:\\n\\n(19.23)\\n\\n(cid:142)3 [p. 379] The SVM loss function. The constraint in (19.5) can be succinctly\\n\\ncaptured via the expression\\n\\nn\\nX\\n\\n≈í1 (cid:0) yi .Àá0 C x0\\n\\ni Àá/(cid:141)C (cid:20) B:\\n\\n(19.24)\\n\\ni D1\\n\\nWe only require a (positive) (cid:15)i if our margin is less than 1, and we get\\ncharged for the sum of these (cid:15)i . We now use a Lagrange multiplier to en-\\nforce the constraint, leading to\\n\\nminimize\\nÀá0; Àá\\n\\nkÀák2\\n2\\n\\nC (cid:13)\\n\\nn\\nX\\n\\n≈í1 (cid:0) yi .Àá0 C x0\\n\\ni Àá/(cid:141)C:\\n\\ni D1\\n\\n(19.25)\\n\\nMultiplying by (cid:21) D 1=(cid:13) gives us (19.6).\\n\\n(cid:142)4 [p. 380] The SVM estimates a classiÔ¨Åer. The following derivation is due to\\n\\nWahba et al. (2000). Consider\\n\\nminimize\\nf .x/\\n\\nEY jXDx f≈í1 (cid:0) Yf .x/(cid:141)Cg :\\n\\n(19.26)\\n\\n\\x0c392\\n\\nSVMs and Kernel Methods\\n\\nDropping the dependence on x, the objective can be written as PC≈í1 (cid:0)\\nf (cid:141)C C P(cid:0)≈í1 C f (cid:141)C, where PC D Pr.Y D C1jX D x/, and P(cid:0) D Pr.Y D\\n(cid:0)1jX D x/ D 1 (cid:0) PC. From this we see that\\n\\nf D\\n\\n(cid:26) C1 if PC > 1\\n2\\n(cid:0)1 if P(cid:0) < 1\\n2 :\\n\\n(19.27)\\n\\n(cid:142)5 [p. 381] SVM and ridged logistic regression. Rosset et al. (2004) show\\nthat the limiting solution as (cid:21) # 0 to (19.7) for separable data coincides\\nwith that of the SVM, in the sense that OÀá=k OÀák2 converges to the same\\nquantity for the SVM. However, because of the required normalization\\nfor logistic regression, the SVM solution is preferable. On the other hand,\\nfor overlapped situations, the logistic-regression solution has some advan-\\ntages, since its target is the logit of the class probabilities.\\n\\n(cid:142)6 [p. 382] The kernel trick. The trick here is to observe that from the score\\nequations we have (cid:0)X 0.y (cid:0) X Àá/ C (cid:21)Àá D 0, which means we can write\\nOÀá D X 0Àõ for some Àõ. We now plug this into the score equations, and\\nsome simple manipulation gives the result. A similar result holds for ridged\\nlogistic regression, and in fact any linear model with a ridge penalty on the\\ncoefÔ¨Åcients (Hastie and Tibshirani, 2004).\\n\\n(cid:142)7 [p. 382] Polynomial kernels. Consider K2.x; z/ D .1 C hx; zi/2, for x\\n\\n(and z) in R2. Expanding we get\\n\\nK2.x; z/ D 1 C 2x1z1 C 2x2z2 C 2x1x2z1z2 C x2\\n\\n1z2\\n\\n1\\n\\nC x2\\n\\n2z2\\n2:\\n\\nThis corresponds to hh2.x/; h2.z/i with\\n\\np\\n\\np\\n\\nh2.x/ D .1;\\n\\n2x1;\\n\\n2x2;\\n\\np\\n\\n2x1x2; x2\\n\\n1; x2\\n\\n2/:\\n\\nThe same is true for p > 2 and for degree d > 2.\\n\\n(cid:142)8 [p. 384] Reproducing kernel Hilbert spaces. Suppose K has eigen expan-\\niD1 (cid:13)i < 1. Then\\n\\nsion K.x; z/ D P1\\nwe say f 2\\n\\nK if f .x/ D P1\\n\\niD1 (cid:13)i (cid:30)i .x/(cid:30)i .z/, with (cid:13)i (cid:21) 0 and P1\\ni D1 ci (cid:30)i .x/, with\\n1\\nX\\n\\nH\\n\\n(19.28)\\n\\nkf k2\\nH\\n\\nK\\n\\n(cid:17)\\n\\nc2\\ni\\n(cid:13)i\\n\\ni D1\\n\\n< 1:\\n\\nH\\n\\nOften kf k\\nK behaves like a roughness penalty, in that it penalizes unlikely\\nmembers in the span of K.(cid:1); z/ (assuming that these correspond to ‚Äúrough‚Äù\\nfunctions). If f has some high loadings cj on functions (cid:30)j with small\\neigenvalues (cid:13)j (i.e. not prominent members of the span), the norm becomes\\nlarge. Smoothing splines and their generalizations correspond to function\\nÔ¨Åtting in a RKHS (Wahba, 1990).\\n\\n\\x0c19.9 Notes and Details\\n\\n393\\n\\n(cid:142)9 [p. 386] This methodology and the data we use in our example come from\\n\\nLeslie et al. (2003).\\n\\n(cid:142)10 [p. 390] Local regression and bias reduction. By expanding the unknown\\ntrue f .x/ in a Ô¨Årst-order Taylor expansion about the target point x0, one\\ncan show that E OfLR.x0/ (cid:25) f .x0/ (Hastie and Loader, 1993).\\n\\n\\x0c20\\n\\nInference After Model Selection\\n\\nThe classical theory of model selection focused on ‚ÄúF tests‚Äù performed\\nwithin Gaussian regression models. Inference after model selection (for in-\\nstance, assessing the accuracy of a Ô¨Åtted regression curve) was typically\\ndone ignoring the model selection process. This was a matter of neces-\\nsity: the combination of discrete model selection and continuous regression\\nanalysis was too awkward for simple mathematical description. Electronic\\ncomputation has opened the door to a more honest analysis of estimation\\naccuracy, one that takes account of the variability induced by data-based\\nmodel selection.\\n\\nFigure 20.1 displays the cholesterol data, an example we will use\\nfor illustration in what follows: cholestyramine, a proposed cholesterol-\\nlowering drug, was administered to n D 164 men for an average of seven\\nyears each. The response variable di was the ith man‚Äôs decrease in choles-\\nterol level over the course of the experiment. Also measured was ci , his\\ncompliance or the proportion of the intended dose actually taken, ranging\\nfrom 1 for perfect compliers to zero for the four men who took none at all.\\nHere the 164 ci values have been transformed to approximately follow a\\nstandard normal distribution,\\n\\nci P(cid:24)\\n\\nN\\n\\n.0; 1/:\\n\\n(20.1)\\n\\nWe wish to predict cholesterol decrease from compliance. Polynomial\\nregression models, with di a J th-order polynomial in ci , were considered,\\nfor degrees J D 1; 2; 3; 4; 5, or 6. The Cp criterion (12.51) was applied\\nand selected a cubic model, J D 3, as best. The curve in Figure 20.1 is the\\nOLS (ordinary least squares) cubic regression curve Ô¨Åt to the cholesterol\\ndata set\\n\\nf.ci ; di /; i D 1; 2; : : : ; 164g :\\n\\n(20.2)\\n\\nWe are interested in answering the following question: how accurate is the\\n\\n394\\n\\n\\x0c20.1 Simultaneous ConÔ¨Ådence Intervals\\n\\n395\\n\\nFigure 20.1 Cholesterol data: cholesterol decrease plotted\\nversus adjusted compliance for 164 men taking\\ncholestyramine. The green curve is OLS cubic regression,\\nwith ‚Äúcubic‚Äù selected by the Cp criterion. How accurate is the\\nÔ¨Åtted curve?\\n\\nÔ¨Åtted curve, taking account of Cp selection as well as OLS estimation?\\n(See Section 20.2 for an answer.)\\n\\nCurrently, there is no overarching theory for inference after model selec-\\ntion. This chapter, more modestly, presents a short series of vignettes that\\nillustrate promising analyses of individual situations. See also Section 16.6\\nfor a brief report on progress in post-selection inference for the lasso.\\n\\n20.1 Simultaneous ConÔ¨Ådence Intervals\\n\\nIn the early 1950s, just before the beginnings of the computer revolution,\\nsubstantial progress was made on the problem of setting simultaneous con-\\nÔ¨Ådence intervals. ‚ÄúSimultaneous‚Äù here means that there exists a catalog of\\nparameters of possible interest,\\n\\nC D f(cid:18)1; (cid:18)2; : : : ; (cid:18)J g;\\n\\n(20.3)\\n\\nand we wish to set a conÔ¨Ådence interval for each of them with some Ô¨Åxed\\nprobability, typically 0.95, that all of the intervals will contain their respec-\\ntive parameters.\\n\\n********************************************************************************************************************************************************************‚àí2‚àí1012‚àí20020406080100Adjusted complianceCholesterol decrease\\x0c396\\n\\nInference After Model Selection\\n\\nAs a Ô¨Årst example, we return to the diabetes data of Section 7.3: n D\\n442 diabetes patients each have had p D 10 medical variables measured at\\nbaseline, with the goal of predicting prog, disease progression one year\\nlater. Let X be the 442 (cid:2) 10 matrix with ith row x0\\ni the 10 measurements\\nfor patient i; X has been standardized so that each of its columns has mean\\n0 and sum of squares 1. Also let y be the 442-vector of centered prog\\nmeasurements (that is, subtracting off the mean of the prog values).\\n\\nOrdinary least squares applied to the normal linear model,\\n\\ny (cid:24)\\n\\nN\\n\\nn.X Àá; (cid:27) 2I/;\\n\\nOÀá D .X 0X /(cid:0)1X 0y;\\n\\nOÀá (cid:24)\\n\\nN\\n\\np.Àá; (cid:27) 2V /;\\n\\nV D .X 0X /(cid:0)1;\\n\\n(20.4)\\n\\n(20.5)\\n\\n(20.6)\\n\\nyields MLE\\n\\nsatisfying\\n\\nas at (7.34).\\n\\nThe 95% Student-t conÔ¨Ådence interval (11.49) for Àáj , the j th compo-\\n\\nnent of Àá, is\\n\\nOÀáj Àô O(cid:27)V 1=2\\n\\njj\\n\\nt :975\\nq\\n\\n;\\n\\n(20.7)\\n\\nwhere O(cid:27) D 54:2 is the usual unbiased estimate of (cid:27),\\nO(cid:27) 2 D ky (cid:0) X OÀák2=q;\\n\\n(20.8)\\nD 1:97 is the 0.975 quantile of a Student-t distribution with q\\n\\nq D n (cid:0) p D 432;\\n\\nand t :975\\nq\\ndegrees of freedom.\\n\\nThe catalog C in (20.3) is now fÀá1; Àá2; : : : ; Àá10g. The individual inter-\\nvals (20.7), shown in Table 20.1, each have 95% coverage, but they are not\\nsimultaneous: there is a greater than 5% chance that at least one of the Àáj\\nvalues lies outside its claimed interval.\\n\\nValid 95% simultaneous intervals for the 10 parameters appear on the\\n\\nright side of Table 20.1. These are the Scheff¬¥e intervals\\n\\njj k.Àõ/\\np;q;\\n\\nOÀáj Àô O(cid:27)V 1=2\\ndiscussed next. The crucial constant k.Àõ/\\np;q equals 4.30 for p D 10, q D\\n432, and Àõ D 0:95. That makes the Scheff¬¥e intervals wider than the t\\nintervals (20.7) by a factor of 2.19. One expects to pay an extra price for\\nsimultaneous coverage, but a factor greater than two induces sticker shock.\\n\\n(20.9)\\n\\nScheff¬¥e‚Äôs method depends on the pivotal quantity\\n(cid:16) OÀá (cid:0) Àá\\n(cid:17) .\\n\\nV (cid:0)1 (cid:16) OÀá (cid:0) Àá\\n\\nQ D\\n\\n(cid:17)0\\n\\nO(cid:27) 2;\\n\\n(20.10)\\n\\n\\x0c20.1 Simultaneous ConÔ¨Ådence Intervals\\n\\n397\\n\\nTable 20.1 Maximum likelihood estimates OÀá for 10 diabetes predictor\\nvariables (20.6); separate 95% Student-t conÔ¨Ådence limits, also\\nsimultaneous 95% Scheff¬¥e intervals. The Scheff¬¥e intervals are wider by a\\nfactor of 2.19.\\n\\nStudent-t\\n\\nScheff¬¥e\\n\\nLower Upper\\n\\nLower Upper\\n\\nOÀá\\n(cid:0)6.1\\n(cid:0)0.5\\nage\\nsex (cid:0)11.4 (cid:0)17.1\\nbmi\\n18.5\\n24.8\\nmap\\n9.3\\n15.4\\n(cid:0)37.7 (cid:0)76.7\\ntc\\n(cid:0)9.0\\nldl\\n22.7\\n4.8 (cid:0)15.1\\nhdl\\n(cid:0)6.7\\ntch\\n8.4\\nltg\\n19.7\\n35.8\\n(cid:0)3.0\\nglu\\n3.2\\n\\n(cid:0)12.7\\n5.1\\n(cid:0)24.0\\n(cid:0)5.7\\n11.1\\n31.0\\n21.6\\n2.1\\n1.2 (cid:0)123.0\\n(cid:0)46.7\\n54.4\\n(cid:0)38.7\\n24.7\\n(cid:0)24.6\\n23.5\\n0.6\\n51.9\\n(cid:0)10.3\\n9.4\\n\\n11.8\\n1.1\\n38.4\\n28.8\\n47.6\\n92.1\\n48.3\\n41.5\\n71.0\\n16.7\\n\\nwhich under model (20.4) has a scaled ‚ÄúF distribution,‚Äù1\\n\\nQ (cid:24) pFp;q:\\n\\n(20.11)\\n\\np;q is the Àõth quantile of a pFp;q distribution then PrfQ (cid:20) k.Àõ/2\\n\\np;q g D Àõ\\n\\nIf k.Àõ/2\\nyields\\n\\n(cid:16)\\n\\nÀá (cid:0) OÀá\\n\\n(cid:17)0\\n\\n8\\nÀÜ<\\n\\nÀÜ:\\n\\nPr\\n\\n(cid:17)\\n\\nÀá (cid:0) OÀá\\n\\nV (cid:0)1 (cid:16)\\nO(cid:27) 2\\n\\n(cid:20) k.Àõ/2\\np;q\\n\\n9\\n>=\\n\\n>;\\n\\nD Àõ\\n\\n(20.12)\\n\\nfor any choice of Àá and (cid:27) in model (20.4). Having observed OÀá and O(cid:27),\\nfor the parameter vector\\n(20.12) deÔ¨Ånes an elliptical conÔ¨Ådence region\\nÀá.\\n\\nE\\n\\nSuppose we are interested in a particular linear combination of the coor-\\n\\ndinates of Àá, say\\n\\nÀác D c0Àá;\\n\\n(20.13)\\n\\n1 Fp;q is distributed as .(cid:31)2\\n\\np=p/=.(cid:31)2\\n\\nq=q/, the two chi-squared variates being\\n\\nindependent. Calculating the percentiles of Fp;q was a major project of the pre-war\\nperiod.\\n\\n\\x0c398\\n\\nInference After Model Selection\\n\\nFigure 20.2 Ellipsoid of possible vectors Àá deÔ¨Åned by (20.12)\\ndetermines conÔ¨Ådence intervals for Àác D c0Àá according to the\\n‚Äúbounding hyperplane‚Äù construction illustrated. The red line\\nshows the conÔ¨Ådence interval for Àác if c is a unit vector,\\nc0V c D 1.\\n\\nwhere c is a Ô¨Åxed p-dimensional vector. If Àá exists in\\n\\n(cid:20)\\n\\nÀác 2\\n\\nmin\\nÀá 2\\nE\\n\\n.c0Àá/; max\\nÀá 2\\nE\\n\\nthen we must have\\n\\nE\\n\\n(20.14)\\n\\n(cid:21)\\n\\n;\\n\\n.c0Àá/\\n\\n(cid:142)1\\n\\nwhich turns out(cid:142) to be the interval centered at OÀác D c0 OÀá,\\n\\nÀác 2 OÀác Àô O(cid:27).c0V c/1=2k.Àõ/\\np;q:\\n\\n(20.15)\\n\\n(This agrees with (20.9) where c is the j th coordinate vector .0; : : : ; 0; 1; 0,\\n: : : ; 0/0.) The construction is illustrated in Figure 20.2.\\n\\nq=q,\\nTheorem (Scheff¬¥e)\\nthen with probability Àõ the conÔ¨Ådence statement (20.15) for Àác D c0Àá will\\nbe simultaneously true for all choices of the vector c.\\n\\np.Àá; (cid:27) 2V / independently of O(cid:27) 2 (cid:24) (cid:27) 2(cid:31)2\\n\\nN\\n\\nIf OÀá (cid:24)\\n\\nHere we can think of ‚Äúmodel selection‚Äù as the choice of the linear com-\\nbination of interest (cid:18)c D c0Àá. Scheff¬¥e‚Äôs theorem allows ‚Äúdata snooping‚Äù:\\nthe statistician can examine the data and then choose which (cid:18)c (or many\\n(cid:18)c‚Äôs) to estimate, without invalidating the resulting conÔ¨Ådence intervals.\\n\\nAn important application has the OÀáj ‚Äôs as independent estimates of efÔ¨Å-\\ncacy for competing treatments‚Äîperhaps different experimental drugs for\\nthe same target disease:\\nOÀáj\\n\\nfor j D 1; 2; : : : ; J;\\n\\n.Àáj ; (cid:27) 2=nj /;\\n\\n(20.16)\\n\\nind(cid:24)\\n\\nN\\n\\n lcb^\\x0c20.1 Simultaneous ConÔ¨Ådence Intervals\\n\\n399\\n\\nthe nj being known sample sizes. In this case the catalog C might comprise\\nall pairwise differences Àái (cid:0) Àáj , as the statistician tries to determine which\\ntreatments are better or worse than the others.\\n\\nThe fact that Scheff¬¥e‚Äôs limits apply to all possible linear combinations\\nc0Àá is a blessing and a curse, the curse being their very large width, as seen\\nin Table 20.1. Narrower simultaneous limits(cid:142) are possible if we restrict the (cid:142)2\\ncatalog C , for instance to just the pairwise differences Àái (cid:0) Àáj .\\n\\nA serious objection, along Fisherian lines, is that the Scheff¬¥e conÔ¨Ådence\\nlimits are accurate without being correct. That is, the intervals have the\\nclaimed overall frequentist coverage probability, but may be misleading\\nwhen applied to individual cases. Suppose for instance that (cid:27) 2=nj D 1 for\\nj D 1; 2; : : : ; J in (20.16) and that we observe OÀá1 D 10, with j OÀáj j < 2 for\\nall the others. Even if we looked at the data before singling out OÀá1 for at-\\ntention, the usual Student-t interval (20.7) seems more appropriate than its\\nmuch longer Scheff¬¥e version (20.9). This point is made more convincingly\\nin our next vignette.\\n\\n(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)\\n\\nA familiar but pernicious abuse of model selection concerns multiple\\nhypothesis testing. Suppose we observe N independent normal variates zi ,\\neach with its own effect size (cid:22)i ,\\n\\nind(cid:24)\\n\\nzi\\n\\n.(cid:22)i ; 1/\\n\\nN\\n\\nfor i D 1; 2; : : : ; N;\\n\\n(20.17)\\n\\nand, as in Section 15.1, we wish to test the null hypotheses\\n\\nH0i W (cid:22)i D 0:\\n\\n(20.18)\\n\\nBeing alert to the pitfalls of simultaneous testing, we employ a false-discov-\\nery rate control algorithm (15.14), which rejects R of the N null hypothe-\\nses, say for cases i1; i2; : : : ; iR. (R equaled 28 in the example of Fig-\\nure 15.3.)\\n\\nSo far so good. The ‚Äúfamiliar abuse‚Äù comes in then setting the usual\\n\\nconÔ¨Ådence intervals\\n\\n(cid:22)i 2 O(cid:22)i Àô 1:96\\n\\n(20.19)\\n\\n(95% coverage) for the R selected cases. This ignores the model selection\\nprocess: the data-based selection of the R cases must be taken into account\\nin making legitimate inferences, even if R is only 1 so multiplicity is not a\\nconcern.\\n\\nThis problem is addressed by the theory of false-coverage control. Sup-\\nsets conÔ¨Ådence intervals for R of the N cases, of which\\n\\npose algorithm\\n\\nA\\n\\n\\x0c400\\n\\nInference After Model Selection\\n\\nr are actually false coverages, i.e., ones not containing the true effect size\\n(cid:22)i . The false-coverage rate (FCR) of\\nis the expected proportion of non-\\ncoverages\\n\\nA\\n\\nFCR.\\n\\n/ D Efr=Rg;\\n\\nA\\n\\n(20.20)\\n\\nthe expectation being with respect to model (20.17). The goal, as with the\\nFDR theory of Section 15.2, is to construct algorithm\\nto control FCR\\nbelow some Ô¨Åxed value q.\\n\\nA\\n\\nThe BYq algorithm2 controls FCR below level q in three easy steps,\\n\\nbeginning with model (20.17).\\n\\n1 Let pi be the p-value corresponding to zi ,\\n\\npi D ÀÜ.zi /\\n\\n(20.21)\\n\\nfor left-sided signiÔ¨Åcance testing, and order the p.i/ values in ascending\\norder,\\n\\np.1/ (cid:20) p.2/ (cid:20) p.3/ (cid:20) : : : (cid:20) p.N /:\\n\\n(20.22)\\n\\n2 Calculate R D maxfi W p.i/ (cid:20) i (cid:1) q=N g, and (as in the BHq algorithm\\n(15.14)‚Äì(15.15)) declare the R corresponding null hypotheses false.\\n\\n3 For each of the R cases, construct the conÔ¨Ådence interval\\n\\n(cid:22)i 2 zi Àô z.ÀõR/;\\n\\nwhere ÀõR D 1 (cid:0) Rq=N\\n\\n(20.23)\\n\\n(z.Àõ/ D ÀÜ(cid:0)1.Àõ/).\\n\\nTheorem 20.1 Under model (20.17), BYq has FCR (cid:20) q; moreover, none\\nof the intervals (20.23) contain (cid:22)i D 0.\\n\\nA simulated example of BYq was run according to these speciÔ¨Åcations:\\n\\nN D 10,000;\\n(cid:22)i D 0\\n.(cid:0)3; 1/\\n\\nq D 0:05;\\n\\nzi (cid:24)\\n\\nN\\nfor i D 1; 2; : : : ; 9000;\\nfor i D 9001; : : : ; 10,000:\\n\\n.(cid:22)i ; 1/\\n\\n(cid:22)i (cid:24)\\n\\nN\\n\\n(20.24)\\n\\nIn this situation we have 9000 null cases and 1000 non-null cases (all but 2\\nof which had (cid:22)i < 0).\\n\\nBecause this is a simulation, we can plot the pairs .zi ; (cid:22)i / to assess the\\nBYq algorithm‚Äôs performance. This is done in Figure 20.3 for the 1000\\nnon-null cases (the green points). BYq declared R D 565 cases non-null,\\nthose having zi (cid:20) (cid:0)2:77 (the circled points); 14 of the 565 declarations\\n\\n2 Short for ‚ÄúBenjamini‚ÄìYekutieli;‚Äù see the chapter endnotes.\\n\\n\\x0c20.1 Simultaneous ConÔ¨Ådence Intervals\\n\\n401\\n\\nFigure 20.3 Simulation experiment (20.24) with N D10,000\\ncases, of which 1000 are non-null; the green points .zi ; (cid:22)i / are\\nthese non-null cases. The FDR control algorithm BHq (q D 0:05)\\ndeclared the 565 circled cases having zi (cid:20) (cid:0)2:77 to be non-null,\\nof which the 14 red points were actually null. The heavy black\\nlines show BYq 95% conÔ¨Ådence intervals for the 565 cases, only\\n17 of which failed to contain (cid:22)i . Actual Bayes posterior 95%\\nintervals for non-null cases (20.26), dotted lines, have half the\\nwidth and slope of BYq limits.\\n\\nwere actually null cases (the red circled points), giving false-discovery pro-\\nportion 14=565 D 0:025. The heavy black lines trace the BYq conÔ¨Ådence\\nlimits (20.23) as a function of z (cid:20) (cid:0)2:77.\\n\\nThe Ô¨Årst thing to notice is that FCR control has indeed been achieved:\\nonly 17 of the declared cases lie outside their limits (the 14 nulls and 3\\nnon-nulls), for a false-coverage rate of 17=565 D 0:030, safely less than\\nq D 0:05. The second thing, however, is that the BYq limits provide a\\nmisleading idea of the location of (cid:22)i given zi : they are much too wide and\\nslope too low, especially for more negative zi values.\\n\\nIn this situation we can describe precisely the posterior distribution of\\n\\n(cid:22)i given zi for the non-null cases,\\n\\n(cid:22)i jzi (cid:24)\\n\\n(cid:18) zi (cid:0) 3\\n2\\n\\n;\\n\\n1\\n2\\n\\n(cid:19)\\n\\n;\\n\\nN\\n\\n(20.25)\\n\\n‚àí8‚àí6‚àí4‚àí20‚àí10‚àí8‚àí6‚àí4‚àí20Observed zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllBY.loBY.up‚àí2.77llllllllllllllBayes.loBayes.up\\x0c402\\n\\nInference After Model Selection\\n\\n.(cid:0)3; 1/, zi j(cid:22)i (cid:24)\\nthis following from (cid:22)i (cid:24)\\n(5.20)‚Äì(5.21). The Bayes credible 95% limits\\n\\n.(cid:22)i ; 1/, and Bayes‚Äô rule\\n\\nN\\n\\nN\\n(cid:22)i 2 zi (cid:0) 3\\n2\\n\\nÀô 1p\\n2\\n\\n1:96\\n\\n(20.26)\\n\\nare indicated by the dotted lines in Figure 20.3. They are half as wide as\\nthe BYq limits, and have slope 1=2 rather than 1.\\n\\nIn practice, of course, we would only see the zi , not the (cid:22)i , making\\n(20.26) unavailable to us. We return to this example in Chapter 21, where\\nempirical Bayes methods will be seen to provide a good approximation to\\nthe Bayes limits. (See Figure 21.5.)\\n\\nAs with Scheff¬¥e‚Äôs method, the BYq intervals can be accused of being\\naccurate but not correct. ‚ÄúCorrect‚Äù here has a Bayesian/Fisherian Ô¨Çavor\\nthat is hard to pin down, except perhaps in large-scale applications, where\\nempirical Bayes analyses can suggest appropriate inferences.\\n\\n20.2 Accuracy After Model Selection\\n\\nThe cubic regression curve for the cholesterol data seen in Figure 20.1\\nwas selected according to the Cp criterion of Section 12.3. Polynomial\\nregression models, predicting cholesterol decrease di in terms of powers\\n(‚Äúdegrees‚Äù) of adjusted compliance ci , were Ô¨Åt by ordinary least squares\\nfor degrees 0; 1; 2; : : : ; 6. Table 20.2 shows Cp estimates (12.51) being\\nminimized at degree 3.\\n\\nTable 20.2 Cp table for cholesterol data of Figure 20.1, comparing OLS\\npolynomial models of degrees 0 through 6. The cubic model, degree D 3,\\nis the minimizer (80,000 subtracted from the Cp values for easier\\ncomparison; assumes (cid:27) D 22:0).\\n\\nDegree\\n\\nCp\\n\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n\\n71887\\n1132\\n1412\\n667\\n1591\\n1811\\n2758\\n\\nWe wish to assess the accuracy of the Ô¨Åtted curve, taking account of both\\nthe Cp model selection method and the OLS Ô¨Åtting process. The bootstrap\\n\\n\\x0c20.2 Accuracy After Model Selection\\n\\n403\\n\\nis a natural candidate for the job. Here we will employ the nonparamet-\\nric bootstrap of Section 10.2 (rather than the parametric bootstrap of Sec-\\ntion 10.4, though this would be no more difÔ¨Åcult to carry out).\\n\\nThe cholesterol data set (20.2) comprises n D 164 pairs xi D\\n.ci ; di /; a nonparametric bootstrap sample x(cid:3) (10.13) consists of 164 pairs\\nchosen at random and with replacement from the original 164. Let t.x/ be\\nthe curve obtained by applying the Cp/OLS algorithm to the original data\\nset x and likewise t.x(cid:3)/ for the algorithm applied to x(cid:3); and for a given\\npoint c on the compliance scale let\\n\\nO(cid:18) (cid:3)\\n\\nc\\n\\nD t.c; x(cid:3)/\\n\\n(20.27)\\n\\nbe the value of t.x(cid:3)/ evaluated at compliance D c.\\n\\nFigure 20.4 A histogram of 4000 nonparametric bootstrap\\nreplications for polynomial regression estimates of cholesterol\\ndecreases d at adjusted compliance c D (cid:0)2. Blue histogram,\\nadaptive estimator O(cid:18) (cid:3)\\neach bootstrap data set; line histogram, using OLS only with\\ndegree 3 for each bootstrap data set. Bootstrap standard errors are\\n5.98 and 3.97.\\n\\nc (20.27), using full Cp/OLS algorithm for\\n\\n Cholesterol decrease q^‚àí2*Frequency‚àí20‚àí10010200100200300400fixed degree (3)adaptive degree1.27\\x0c404\\n\\nInference After Model Selection\\n\\nB D 4000 nonparametric bootstrap replications t.x(cid:3)/ were generated.3\\nFigure 20.4 shows the histogram of the 4000 O(cid:18) (cid:3)\\nc replications for c D (cid:0)2:0.\\nIt is labeled ‚Äúadaptive‚Äù to indicate that Cp model selection, as well as OLS\\nÔ¨Åtting, was carred out anew for each x(cid:3). This is as opposed to the ‚ÄúÔ¨Åxed‚Äù\\nhistogram, where there was no Cp selection, cubic OLS regression always\\nbeing used.\\n\\nFigure 20.5 Bootstrap standard-error estimates of O(cid:18)c, for\\n(cid:0)2:2 (cid:20) c (cid:20) 2. Solid black curve, adaptive estimator (20.27)\\nusing full Cp/OLS model selection estimate; red dashed curve,\\nusing OLS only with polynomial degree Ô¨Åxed at 3;\\nblue dotted curve, ‚Äúbagged estimator‚Äù using bootstrap smoothing\\n(20.28). Average standard-error ratios: adaptive/Ô¨Åxed D 1:43,\\nadaptive/smoothed D 1:14.\\n\\nThe bootstrap estimate of standard error (10.16) obtained from the adap-\\ntive values O(cid:18) (cid:3)\\nc was 5.98, compared with 3.97 for the Ô¨Åxed values.4 In this\\ncase, accounting for model selection (‚Äúadaptation‚Äù) adds more than 50% to\\nthe standard error estimates. The same comparison was made at all values\\n\\n3 Ten times more than needed for assessing standard errors, but helpful for the\\n\\ncomparisons that follow.\\n\\n4 The latter is not the usual OLS assessment, following (8.30), that would be appropriate\\n\\nfor a parametric bootstrap comparison. Rather, it‚Äôs the nonparametric one-sample\\nbootstrap assessment, resampling pairs .xi ; yi / as individual sample points.\\n\\n‚àí2‚àí101201234567Adjusted compliance cStandard errors of q^cfixedadaptivesmoothed\\x0c20.2 Accuracy After Model Selection\\n\\n405\\n\\nof the adjusted compliance c. Figure 20.5 graphs the results: the adaptive\\nstandard errors averaged 43% greater than the Ô¨Åxed values. The standard\\n95% conÔ¨Ådence intervals O(cid:18)c Àô\\nbse (cid:1) 1:96 would be roughly 43% too short if\\nwe ignored model selection in assessing bse.\\n\\nFigure 20.6 ‚ÄúAdaptive‚Äù histogram of Figure 20.4 now split into\\n19% of 4000 bootstrap replications where Cp selected linear\\nregression (m(cid:3) D 1) as best, versus 81% having m(cid:3) > 1. m(cid:3) D 1\\ncases are shifted about 10 units downward. (The m(cid:3) > 1 cases\\nresemble the ‚ÄúÔ¨Åxed‚Äù histogram in Figure 20.4.) Histograms are\\nscaled to have equal areas.\\n\\nHaving an honest assessment of standard error doesn‚Äôt mean that t.c; x/\\n(20.27) is a good estimator. Model selection can induce an unpleasant\\n‚Äújumpiness‚Äù in an estimator, as the original data vector x crosses deÔ¨Åni-\\ntional boundaries. This happened in our example: for 19% of the 4000\\nbootstrap samples x(cid:3), the Cp algorithm selected linear regression, m(cid:3) D 1,\\nas best, and in these cases O(cid:18) (cid:3)\\n(cid:0)2:0 tended toward smaller values. Figure 20.6\\nshows the m(cid:3) D 1 histogram shifted about 10 units down from the m(cid:3) > 1\\nhistogram (which now resembles the ‚ÄúÔ¨Åxed‚Äù histogram in Figure 20.4).\\n\\nDiscontinuous estimators such as t.c; x/ can‚Äôt be Bayesian, Bayes pos-\\nterior expectations being continuous. They can also suffer frequentist difÔ¨Å-\\nculties, (cid:142) including excess variability and overly long conÔ¨Ådence intervals. (cid:142)3\\n\\n Cholesterol decrease q^‚àí2*Frequency‚àí20‚àí10010200204060801.27adaptivem* = 1adaptivem* > 1fixed m = 3\\x0c406\\n\\nInference After Model Selection\\n\\nBagging, or bootstrap smoothing, is a tactic for improving a discontinuous\\nestimation rule by averaging (as in (12.80) and Chapter 17).\\n\\nSuppose t.x/ is any estimator for which we have obtained bootstrap\\nreplications ft.x(cid:3)b/, b D 1; 2; : : : ; Bg. The bagged version of t.x/ is the\\naverage\\n\\ns.x/ D 1\\nB\\n\\nB\\nX\\n\\nbD1\\n\\nt.x(cid:3)b/:\\n\\n(20.28)\\n\\nThe letter s here stands for ‚Äúsmooth.‚Äù Small changes in x, even ones that\\nmove across a model selection deÔ¨Ånitional boundary, produce only small\\nchanges in the bootstrap average s.x/.\\n\\nAveraging over the 4000 bootstrap replications of t.c; x(cid:3)/ (20.27) gave\\na bagged estimate sc.x/ for each value of c. Bagging reduced the standard\\nerrors of the Cp/OLS estimates t.c; x/ by about 12%, as indicated by the\\nblue dotted curve in Figure 20.5.\\n\\nWhere did the blue dotted curve come from? All 4000 bootstrap values\\nt.c; x(cid:3)b/ were needed to produce the single value sc.x/. It seems as if\\nwe would need to bootstrap the bootstrap in order to compute bse≈ísc.x/(cid:141).\\nFortunately, a more economical calculation is possible, one that requires\\nonly the original B bootstrap computations for t.c; x/.\\n\\nDeÔ¨Åne\\n\\nNbj D #ftimes xj occurs in x(cid:3)bg;\\n\\n(20.29)\\n\\nfor b D 1; 2; : : : ; B and j D 1; 2; : : : ; n. For instance N4000;7 D 3 says\\nthat data point x7 occurred three times in nonparametric bootstrap sample\\nx(cid:3)4000. The B by n matrix fNbj g completely describes the B bootstrap\\nsamples. Also denote\\n\\nt (cid:3)b D t.x(cid:3)b/\\n\\n(20.30)\\n\\nand let covj indicate the covariance in the bootstrap sample between Nbj\\nand t (cid:3)b,\\n\\ncovj D 1\\nB\\n\\nB\\nX\\n\\n.Nbj (cid:0) N(cid:1)j /.t (cid:3)b (cid:0) t (cid:3)(cid:1)/;\\n\\nbD1\\n\\nwhere dots denote averaging over B: N(cid:1)j D 1\\nB\\n\\nP\\n\\nb Nbj and t (cid:3)(cid:1) D 1\\n\\nB\\n\\n(20.31)\\n\\nP\\n\\nb t (cid:3)b.\\n\\n(cid:142)4\\n\\nTheorem 20.2 (cid:142) The inÔ¨Ånitesimal jackknife estimate of standard error\\n\\n\\x0c20.2 Accuracy After Model Selection\\n\\n407\\n\\n(10.41) for the bagged estimate (20.28) is\\n\\nbseIJ ≈ísc.x/(cid:141) D\\n\\n1\\n\\n1=2\\n\\ncov2\\nj\\n\\nA\\n\\n:\\n\\n0\\n\\n@\\n\\nn\\nX\\n\\nj D1\\n\\n(20.32)\\n\\nKeeping track of Nbj as we generate the bootstrap replications t (cid:3)b allows\\nus to compute covj and bse≈ísc.x/(cid:141) without any additional computational\\neffort.\\n\\nWe expect averaging to reduce variability, and this is seen to hold true in\\nFigure 20.5, the ratio of bseIJ≈ísc.x/(cid:141)=bseboot≈ít.c; x/(cid:141) averaging 0.88. In fact,\\nwe have the following general result.\\nCorollary The ratio bseIJ≈ísc.x/(cid:141)=bseboot≈ít.c; x/(cid:141) is always (cid:20) 1.\\n\\nThe savings due to bagging increase with the nonlinearity of t.x(cid:3)/ as\\na function of the counts Nbj (or, in the language of Section 10.3, in the\\nnonlinearity of S.P/ as a function of P). Model-selection estimators such\\nas the Cp/OLS rule tend toward greater nonlinearity and bigger savings.\\n\\nTable 20.3 Proportion of 4000 nonparametric bootstrap replications of\\nCp/OLS algorithm that selected degrees m D 1; 2; : : : ; 6; also\\ninÔ¨Ånitesimal jackknife standard deviations for proportions (20.32), which\\nmostly exceed the estimates themselves.\\n\\nm D 1\\n\\n.19\\n.24\\n\\n2\\n\\n.12\\n.20\\n\\n3\\n\\n.35\\n.24\\n\\n4\\n\\n.07\\n.13\\n\\n5\\n\\n.20\\n.26\\n\\n6\\n\\n.06\\n.06\\n\\nproportion\\nbsdIJ\\n\\nThe Ô¨Årst line of Table 20.3 shows the proportions in which the various\\ndegrees were selected in the 4000 cholesterol bootstrap replications, 19%\\nfor linear, 12% for quadratic, 35% for cubic, etc. With B D 4000, the\\nproportions seem very accurate, the binomial standard error for 0.19 being\\njust .0:19 (cid:1) 0:81=4000/1=2 D 0:006, for instance.\\n\\nTheorem 20.2 suggests otherwise. Now let t (cid:3)b (20.30) indicate whether\\n\\nthe bth bootstrap sample x(cid:3) made the Cp choice m(cid:3) D 1,\\n\\nt (cid:3)b D\\n\\n(\\n1 if m(cid:3)b D 1\\n0 if m(cid:3)b > 1:\\n\\n(20.33)\\n\\nThe bagged value of ft (cid:3)b; b D 1; 2; : : : ; Bg is the observed proportion\\n\\n\\x0c408\\n\\nInference After Model Selection\\n\\n0.19. Applying the bagging theorem yielded bseIJ D 0:24, as seen in the\\nsecond line of the table, with similarly huge standard errors for the other\\nproportions.\\n\\nThe binomial standard errors are internal, saying how quickly the boot-\\nstrap resampling process is converging to its ultimate value as B ! 1.\\nThe inÔ¨Ånitesimal jackknife estimates are external: if we collected a new\\nset of 164 data pairs .ci ; di / (20.2) the new proportion table might look\\ncompletely different than the top line of Table 20.3.\\n\\nFrequentist statistics has the advantage of being applicable to any algo-\\nrithmic procedure, for instance to our Cp/OLS estimator. This has great\\nappeal in an era of enormous data sets and fast computation. The draw-\\nback, compared with Bayesian statistics, is that we have no guarantee that\\nour chosen algorithm is best in any way. Classical statistics developed a\\ntheory of best for a catalog of comparatively simple estimation and testing\\nproblems. In this sense, modern inferential theory has not yet caught up\\nwith modern problems such as data-based model selection, though tech-\\nniques such as model averaging (e.g., bagging) suggest promising steps\\nforward.\\n\\n20.3 Selection Bias\\n\\nMany a sports fan has been victimized by selection bias. Your team does\\nwonderfully well and tops the league standings. But the next year, with\\nthe same players and the same opponents, you‚Äôre back in the pack. This\\nis the winner‚Äôs curse, a more picturesque name for selection bias, the ten-\\ndency of unusually good (or bad) comparative performances not to repeat\\nthemselves.\\n\\nModern scientiÔ¨Åc technology allows the simultaneous investigation of\\nhundreds or thousands of candidate situations, with the goal of choosing\\nthe top performers for subsequent study. This is a setup for the heartbreak\\nof selection bias. An apt example is offered by the prostate study data of\\nSection 15.1, where we observe statistics zi measuring patient‚Äìcontrol dif-\\nferences for N D 6033 genes,\\n\\nzi (cid:24)\\n\\nN\\n\\n.(cid:22)i ; 1/;\\n\\ni D 1; 2; : : : ; N:\\n\\n(20.34)\\n\\nHere (cid:22)i is the effect size for gene i, the true difference between the patient\\nand control populations.\\n\\nGenes with large positive or negative values of (cid:22)i would be promising\\ntargets for further investigation. Gene number 610, with z610 D 5:29, at-\\n\\n\\x0c20.3 Selection Bias\\n\\n409\\n\\ntained the biggest z-value; (20.34) says that z610 is unbiased for (cid:22)610. Can\\nwe believe the obvious estimate O(cid:22)610 D 5:29?\\n\\n‚ÄúNo‚Äù is the correct selection bias answer. Gene 610 has won a contest for\\nbigness among 6033 contenders. In addition to being good (having a large\\nvalue of (cid:22)) it has almost certainly been lucky, with the noise in (20.34)\\npushing z610 in the positive direction‚Äîor else it would not have won the\\ncontest. This is the essence of selection bias.\\n\\nFalse-discovery rate theory, Chapter 15, provided a way to correct for\\nselection bias in simultaneous hypothesis testing. This was extended to\\nfalse-coverage rates in Section 20.1. Our next vignette concerns the re-\\nalistic estimation of effect sizes (cid:22)i in the face of selection bias.\\n\\nWe begin by assuming that an effect size (cid:22) has been obtained from a\\nprior density g.(cid:22)/ (which might include discrete atoms) and then z (cid:24)\\n\\nN\\n\\n.(cid:22); (cid:27) 2/ observed,\\n\\n(cid:22) (cid:24) g.(cid:1)/\\n\\nand zj(cid:22) (cid:24)\\n\\n.(cid:22); (cid:27) 2/\\n\\nN\\n\\n(20.35)\\n\\n((cid:27) 2 is assumed known for this discussion). The marginal density of z is\\n\\nf .z/ D\\n\\nZ 1\\n\\n(cid:0)1\\n\\ng.(cid:22)/(cid:30)(cid:27) .z (cid:0) (cid:22)/ d(cid:22);\\n\\nwhere (cid:30)(cid:27) .z/ D .2(cid:25)(cid:27) 2/(cid:0)1=2 exp\\n\\n(cid:18)\\n(cid:0) 1\\n2\\n\\nz2\\n(cid:27) 2\\n\\n(cid:19)\\n\\n:\\n\\n(20.36)\\n\\nTweedie‚Äôs formula(cid:142)is an intriguing expression for the Bayes expectation (cid:142)5\\n\\nof (cid:22) given z.\\n\\nTheorem 20.3\\nobserved z is\\n\\nIn model (20.35), the posterior expectation of (cid:22) having\\n\\nEf(cid:22)jzg D z C (cid:27) 2l 0.z/\\n\\nwith l 0.z/ D d\\ndz\\n\\nlog f .z/:\\n\\n(20.37)\\n\\nThe especially convenient feature of Tweedie‚Äôs formula is that Ef(cid:22)jzg\\nis expressed directly in terms of the marginal density f .z/. This is a setup\\nfor empirical Bayes estimation. We don‚Äôt know g.(cid:22)/, but in large-scale\\nsituations we can estimate the marginal density f .z/ from the observa-\\ntions z D .z1; z2; : : : ; zN /, perhaps by Poisson regression as in Table 15.1,\\nyielding\\n\\nOEf(cid:22)i jzi g D zi C (cid:27) 2 Ol 0.zi /\\n\\nwith Ol 0.z/ D d\\ndz\\n\\nlog Of .z/:\\n\\n(20.38)\\n\\nThe solid curve in Figure 20.7 shows OEf(cid:22)jzg for the prostate study data,\\n\\n\\x0c410\\n\\nInference After Model Selection\\n\\nFigure 20.7 The solid curve is Tweedie‚Äôs estimate OEf(cid:22)jzg\\n(20.38) for the prostate study data. The dashed line shows the\\nlocal false-discovery rate cfdr.z/ from Figure 15.5 (red scale on\\nright). At z D 3:5, OEf(cid:22)jzg D 1:96 and cfdr.z/ D 0:15. For gene\\n610, with z610 D 5:29, Tweedie‚Äôs estimate is 4.03.\\n\\nwith (cid:27) 2 D 1 and Of .z/ obtained using fourth-degree log polynomial re-\\ngression as in Section 15.4. The curve has Ef(cid:22)jzg hovering near zero for\\njzi j (cid:20) 2, agreeing with the local false-discovery rate curve cfdr.z/ of Fig-\\nure 15.5 that says these are mostly null genes.\\n\\nOEf(cid:22)jzg increases for z > 2, equaling 1.96 for z D 3:5. At that point\\ncfdr.z/ D 0:15. So even though zi D 3:5 has a one-sided p-value of 0.0002,\\nwith 6033 genes to consider at once, it still is not a sure thing that gene i\\nis non-null. About 85% of the genes with zi near 3.5 will be non-null,\\nand these will have effect sizes averaging about 2.31 (D 1:96=0:85). All of\\nthis nicely illustrates the combination of frequentist and Bayesian inference\\npossible in large-scale studies, and also the combination of estimation and\\nhypothesis-testing ideas in play.\\n\\nIf the prior density g.(cid:22)/ in (20.35) is assumed to be normal, Tweedie‚Äôs\\nformula (20.38) gives (almost) the James‚ÄìStein estimator (7.13). The cor-\\nresponding curve in Figure 20.7 in that case would be a straight line pass-\\ning through the origin at slope 0.22. Like the James‚ÄìStein estimator, ridge\\nregression, and the lasso of Chapter 16, Tweedie‚Äôs formula is a shrink-\\nage estimator. For z610 D 5:29, the most extreme observation, it gave\\n\\n‚àí4‚àí2024‚àí2024z‚àívalueE^( m | z)lllz = 3.51.96.1500.20.40.60.81 fdrTweediefdr\\x0c20.3 Selection Bias\\n\\n411\\n\\nO(cid:22)610 D 4:03, shrinking the maximum likelihood estimate more than one (cid:27)\\nunit toward the origin.\\n\\nBayes estimators are immune to selection bias, as discussed in Sections\\n3.3 and 3.4. This offers some hope that Tweedie‚Äôs empirical Bayes esti-\\nmates might be a realistic cure for the winners‚Äô curse. A small simulation\\nexperiment was run as a test.\\n\\n(cid:15) A hundred data sets z, each of length N D 1000, were generated accord-\\n\\ning to a combination of exponential and normal sampling,\\n\\n(cid:22)i\\n\\nind(cid:24) e(cid:0)(cid:22)\\n\\n.(cid:22) > 0/\\n\\nand\\n\\nzi j(cid:22)i\\n\\nind(cid:24)\\n\\n.(cid:22)i ; 1/;\\n\\nN\\n\\n(20.39)\\n\\nfor i D 1; 2; : : : ; 1000.\\n\\n(cid:15) For each z, Ol.z/ was computed as in Section 15.4, now using a natural\\n\\nspline model with Ô¨Åve degrees of freedom.\\n\\n(cid:15) This gave Tweedie‚Äôs estimates\\n\\nO(cid:22)i D zi C Ol 0.zi /;\\n\\ni D 1; 2; : : : ; 1000;\\n\\n(20.40)\\n\\nfor that data set z.\\n\\n(cid:15) For each data set z, the 20 largest zi values and the corresponding O(cid:22)i and\\n\\n(cid:22)i values were recorded, yielding the\\n\\nuncorrected differences\\n\\nand corrected differences\\n\\nzi (cid:0) (cid:22)i\\nO(cid:22)i (cid:0) (cid:22)i ;\\n\\n(20.41)\\n\\nthe hope being that empirical Bayes shrinkage would correct the selection\\nbias in the zi values.\\n\\n(cid:15) Figure 20.8 shows the 2000 (100 data sets, 20 top cases each) uncorrected\\nand corrected differences. Selection bias is quite obvious, with the uncor-\\nrected differences shifted one unit to the right of zero. In this case at least,\\nthe empirical Bayes corrections have worked well, the corrected differ-\\nences being nicely centered at zero. Bias correction often adds variance,\\nbut in this case it hasn‚Äôt.\\n\\nFinally, it is worth saying that the ‚Äúempirical‚Äù part of empirical Bayes is\\nless the estimation of Bayesian rules from the aggregate data than the appli-\\ncation of such rules to individual cases. For the prostate data we began with\\nno deÔ¨Ånite prior opinions but arrived at strong (i.e., not ‚Äúuninformative‚Äù)\\nBayesian conclusions for, say, (cid:22)610 in the prostate study.\\n\\n\\x0c412\\n\\nInference After Model Selection\\n\\nFigure 20.8 Corrected and uncorrected differences for 20 top\\ncases in each of 100 simulations (20.39)‚Äì(20.41). Tweedie\\ncorrections effectively counteracted selection bias.\\n\\n20.4 Combined Bayes‚ÄìFrequentist Estimation\\n\\nAs mentioned previously, Bayes estimates are, at least theoretically, im-\\nmune from selection bias. Let z D .z1; z2; : : : , zN / represent the prostate\\nstudy data of the previous section, with parameter vector (cid:22) D .(cid:22)1; (cid:22)2; : : : ;\\n(cid:22)N /. Bayes‚Äô rule (3.5)\\n\\ng.(cid:22)jz/ D g.(cid:22)/f(cid:22).z/=f .z/\\n\\n(20.42)\\n\\nyields the posterior density of (cid:22) given z. A data-based model selection\\nrule such as ‚Äúestimate the (cid:22)i corresponding to the largest observation zi ‚Äù\\nhas no effect on the likelihood function f(cid:22).z/ (with z Ô¨Åxed) or on g.(cid:22)jz/.\\nHaving chosen a prior g.(cid:22)/, our posterior estimate of (cid:22)610 is unaffected\\nby the fact that z610 D 5:29 happens to be largest.\\n\\nThis same argument applies just as well to any data-based model selec-\\ntion procedure, for instance a preliminary screening of possible variables\\nto include in a regression analysis‚Äîthe Cp choice of a cubic regression in\\nFigure 20.1 having no effect on its Bayes posterior accuracy.\\n\\nThere is a catch: the chosen prior g.(cid:22)/ must apply to the entire param-\\neter vector (cid:22) and not just the part we are interested in (e.g., (cid:22)610). This is\\n\\n DifferencesFrequency‚àí4‚àí2024020406080100120140uncorrecteddifferencescorrecteddifferences\\x0c20.4 Combined Bayes‚ÄìFrequentist Estimation\\n\\n413\\n\\nfeasible in one-parameter situations like the stopping rule example of Fig-\\nure 3.3. It becomes difÔ¨Åcult and possibly dangerous in higher dimensions.\\nEmpirical Bayes methods such as Tweedie‚Äôs rule can be thought of as al-\\nlowing the data vector z to assist in the choice of a high-dimensional prior,\\nan effective collaboration between Bayesian and frequentist methodology.\\nOur chapter‚Äôs Ô¨Ånal vignette concerns another Bayes‚Äìfrequentist estima-\\nD ffÀõ.x/g\\ntion technique. Dropping the boldface notation, suppose that\\nis a multi-dimensional family of densities (5.1) (now with Àõ playing the\\nrole of (cid:22)), and that we are interested in estimating a particular parameter\\n(cid:18) D t.Àõ/. A prior g.Àõ/ has been chosen, yielding posterior expectation\\nO(cid:18) D E ft.Àõ/jxg :\\n(20.43)\\nHow accurate is O(cid:18)? The usual answer would be calculated from the pos-\\nterior distribution of (cid:18) given x. This is obviously the correct answer if g.Àõ/\\nis based on genuine prior experience. Most often though, and especially in\\nhigh-dimensional problems, the prior reÔ¨Çects mathematical convenience\\nand a desire to be uninformative, as in Chapter 13. There is a danger of\\ncircular reasoning in using a self-selected prior distribution to calculate the\\naccuracy of its own estimator.\\n\\nF\\n\\nAn alternate approach, discussed next, is to calculate the frequentist ac-\\ncuracy of O(cid:18); that is, even though (20.43) is a Bayes estimate, we consider\\nO(cid:18) simply as a function of x, and compute its frequentist variability. The\\nnext theorem leads to a computationally efÔ¨Åcient way of doing so. (The\\nBayes and frequentist standard errors for O(cid:18) operate in conceptually orthog-\\nonal directions as pictured in Figure 3.5. Here we are supposing that the\\nprior g.(cid:1)/ is unavailable or uncertain, forcing more attention on frequentist\\ncalculations.)\\n\\nFor convenience, we will take the family\\n\\nnential family (5.50),\\n\\nto be a p-parameter expo-\\n\\nF\\n\\nfÀõ.x/ D eÀõ0x(cid:0) .Àõ/f0.x/;\\n\\n(20.44)\\n\\nnow with Àõ being the parameter vector called (cid:22) above. The p (cid:2) p covari-\\nance matrix of x (5.59) is denoted\\n\\nVÀõ D covÀõ.x/:\\n\\n(20.45)\\n\\nLet Covx indicate the posterior covariance given x between (cid:18) D t.Àõ/, the\\nparameter of interest, and Àõ,\\n\\nCovx D cov fÀõ; t.Àõ/jxg ;\\n\\n(20.46)\\n\\n\\x0c414\\n\\nInference After Model Selection\\n\\na p (cid:2) 1 vector. Covx leads directly to a frequentist estimate of accuracy for\\nO(cid:18).\\nTheorem 20.4 (cid:142) The delta method estimate of standard error for O(cid:18) D\\nEft.Àõ/jxg (20.43) is\\n\\n(cid:142)6\\n\\no\\n\\nn O(cid:18)\\n\\nbsedelta\\n\\nD (cid:0)Cov0\\n\\nx V OÀõ Covx\\n\\n(cid:1)1=2\\n\\n;\\n\\n(20.47)\\n\\nwhere V OÀõ is VÀõ evaluated at the MLE OÀõ.\\n\\nThe theorem allows us to calculate the frequentist accuracy estimate\\nbsedeltaf O(cid:18)g with hardly any additional computational effort beyond that re-\\nquired for O(cid:18) itself. Suppose we have used an MCMC or Gibbs sampling\\nalgorithm, Section 13.4, to generate a sample from the Bayes posterior dis-\\ntribution of Àõ given x,\\n\\nÀõ.1/; Àõ.2/; : : : ; Àõ.B/:\\nThese yield the usual estimate for Eft.Àõ/jxg,\\n\\nO(cid:18) D 1\\nB\\n\\nB\\nX\\n\\nbD1\\n\\nÀõ.b/(cid:17)\\n(cid:16)\\n\\n:\\n\\nt\\n\\nThey also give a similar expression for covfÀõ; t.Àõ/jxg,\\n\\n(20.48)\\n\\n(20.49)\\n\\nCovx D 1\\nB\\n\\nB\\nX\\n\\nbD1\\n\\nÀõ.b/ (cid:0) Àõ.(cid:1)/(cid:17) (cid:16)\\n(cid:16)\\n\\nt .b/ (cid:0) t .(cid:1)/(cid:17)\\n\\n;\\n\\n(20.50)\\n\\nt .b/ D t .Àõ.b//, t .(cid:1)/ D P\\ncan calculate5\\n\\nb t .b/=B, and Àõ.(cid:1)/ D P\\n\\nb Àõ.b/=B, from which we\\n\\nbsedelta. O(cid:18)/ (20.47).\\nFor an example of Theorem 20.4 in action we consider the diabetes\\ndata of Section 20.1, with x0\\ni the ith row of X , the 442 (cid:2) 10 matrix of\\nprediction, so xi is the vector of 10 predictors for patient i. The response\\nvector y of progression scores has now been rescaled to have (cid:27) 2 D 1 in\\nthe normal regression model,6\\n\\ny (cid:24)\\n\\nn.X Àá; I/:\\n\\nN\\n\\nThe prior distribution g.Àá/ was taken to be\\ng.Àá/ D ce(cid:0)(cid:21)kÀá k1;\\n\\n(20.51)\\n\\n(20.52)\\n\\n5 V OÀõ may be known theoretically, calculated by numerical differentiation in (5.57), or\\n\\nobtained from parametric bootstrap resampling‚Äîtaking the empirical covariance matrix\\nof bootstrap replications OÀá (cid:3)\\ni .\\n\\n6 By dividing the original data vector y by its estimated standard error from the linear\\n\\nmodel E fyg D X Àá .\\n\\n\\x0c20.4 Combined Bayes‚ÄìFrequentist Estimation\\n\\n415\\n\\nwith (cid:21) D 0:37 and c the constant that makes g.Àá/ integrate to 1. This is\\nthe ‚ÄúBayesian lasso prior,‚Äù(cid:142)so called because of its connection to the lasso, (cid:142)7\\n(7.42) and (16.1). (The lasso plays no part in what follows).\\n\\nAn MCMC algorithm generated B D10,000 samples (20.48) from the\\n\\nposterior distribution g.Àájy/. Let\\n\\n(cid:18)i D x0\\n\\ni Àá;\\n\\n(20.53)\\n\\nthe (unknown) expectation of the ith patient‚Äôs response yi . The Bayes pos-\\nterior expectation of (cid:18)i is\\n\\nO(cid:18)i D 1\\nB\\n\\nB\\nX\\n\\nbD1\\n\\nx0\\ni Àá.b/:\\n\\nIt has Bayes posterior standard error\\n\\n(cid:17)\\n\\n(cid:16) O(cid:18)i\\n\\nD\\n\\nbseBayes\\n\\n\"\\n\\n1\\nB\\n\\nB\\nX\\n\\n(cid:16)\\n\\nbD1\\n\\ni Àá.b/ (cid:0) O(cid:18)i\\nx0\\n\\n#1=2\\n\\n(cid:17)2\\n\\n;\\n\\n(20.54)\\n\\n(20.55)\\n\\nFigure 20.9 shows the 10,000 MCMC replications O(cid:18) .b/\\n\\nwhich we can compare with bsedelta. O(cid:18)i /, the frequentist standard error (20.47).\\ni Àá.b/ for\\npatient i D 322. The point estimate O(cid:18)i equaled 2.41, with Bayes and fre-\\nquentist standard error estimates\\nbseBayes D 0:203\\n\\nand bsedelta D 0:186:\\n\\n(20.56)\\n\\nD x0\\n\\ni\\n\\nThe frequentist standard error is 9% smaller in this case; bsedelta was less\\nthan bseBayes for all 442 patients, the difference averaging a modest 5%.\\n\\nThings can work out differently. Suppose we are interested in the poste-\\n\\nrior cdf of (cid:18)332 given y. For any given value of c let\\n\\nc; Àá.b/(cid:17)\\n(cid:16)\\n\\nt\\n\\nD\\n\\n(\\n1\\n0\\n\\nif x0\\nif x0\\n\\n322Àá.b/ (cid:20) c\\n332Àá.b/ > c;\\n\\nso\\n\\ncdf.c/ D 1\\nB\\n\\nB\\nX\\n\\nc; Àá.b/(cid:17)\\n(cid:16)\\n\\nt\\n\\n(20.57)\\n\\n(20.58)\\n\\nbD1\\nis our MCMC assessment of Prf(cid:18)322 (cid:20) cjyg. The solid curve in Fig-\\nure 20.10 graphs cdf.c/.\\n\\nIf we believe prior (20.52) then the curve exactly represents the posterior\\ndistribution of (cid:18)322 given y (except for the simulation error due to stopping\\nat B D10,000 replications). Whether or not we believe the prior we can use\\n\\n\\x0c416\\n\\nInference After Model Selection\\n\\nFigure 20.9 A histogram of 10,000 MCMC replications for\\nposterior distribution of (cid:18)322, expected progression for patient\\n322 in the diabetes study; model (20.51) and prior (20.52).\\nThe Bayes posterior expectation is 2.41. Frequentist standard\\nerror (20.47) for O(cid:18)322 D 2:41 was 9% smaller than Bayes\\nposterior standard error (20.55).\\n\\nTheorem 20.4, with t .b/ D t .c; Àá.b// in (20.50), to evaluate the frequentist\\naccuracy of the curve.\\n\\nThe dashed vertical red lines show cdf.c/ plus or minus one bsedelta unit.\\nThe standard errors are disturbingly large, for instance 0:687 Àô 0:325 at\\nc D 2:5. The central 90% credible interval for (cid:18)322 (the c-values between\\ncdf.c/ 0.05 and 0.95),\\n\\n.2:08; 2:73/\\n\\n(20.59)\\n\\nhas frequentist standard errors about 0.185 for each endpoint‚Äî28% of the\\ninterval‚Äôs length.\\n\\nIf we believe prior (20.52) then .2:08; 2:73/ is an (almost) exact 90%\\ncredible interval for (cid:18)322, and moreover is immune to any selection bias\\ninvolved in our focus on (cid:18)322. If not, the large frequentist standard errors\\nare a reminder that calculation (20.59) might turn out much differently in\\na new version of the diabetes study, even ignoring selection bias.\\n\\nTo return to our main theme, Bayesian calculations encourage a disre-\\ngard for model selection effects. This can be dangerous in objective Bayes\\n\\n MCMC q322 valuesFrequency2.02.53.001002003004005006002.41Standard ErrorsBayes Posterior .205Frequentist .186\\x0c20.5 Notes and Details\\n\\n417\\n\\nFigure 20.10 The solid curve is the posterior cdf of (cid:18)322. Vertical\\nred bars indicate Àô one frequentist standard error, as obtained\\nfrom Theorem 20.4. Black triangles are endpoints of the 0.90\\ncentral credible interval.\\n\\nsettings where one can‚Äôt rely on genuine prior experience. Theorem 20.4\\nserves as a frequentist checkpoint, offering some reassurance as in Fig-\\nure 20.9, or sounding a warning as in Figure 20.10.\\n\\n20.5 Notes and Details\\n\\nOptimality theories‚Äîstatements of best possible results‚Äîare marks of ma-\\nturity in applied mathematics. Classical statistics achieved two such theo-\\nries: for unbiased or asymptotically unbiased estimation, and for hypothe-\\nsis testing. Most of this book and all of this chapter venture beyond these\\nsafe havens. How far from best are the Cp/OLS bootstrap smoothed esti-\\nmates of Section 20.2? At this time we can‚Äôt answer such questions, though\\nwe can offer appealing methodologies in their pursuit, a few of which have\\nbeen highlighted here.\\n\\nThe cholestyramine example comes from Efron and Feldman (1991)\\nwhere it is discussed at length. Data for a control group is also analyzed\\nthere.\\n\\n(cid:142)1 [p. 398] Scheff¬¥e intervals. Scheff¬¥e‚Äôs 1953 paper came at the beginning\\n\\n2.02.22.42.62.80.00.20.40.60.81.0c‚àívaluePr(q322 < c)\\x0c418\\n\\nInference After Model Selection\\n\\nof a period of healthy development in simultaneous inference techniques,\\nmostly in classical normal theory frameworks. Miller (1981) gives a clear\\nand thorough summary. The 1980s followed with a more computer-intensive\\napproach, nicely developed in Westfall and Young‚Äôs 1993 book, leading up\\nto Benjamini and Hochberg‚Äôs 1995 false-discovery rate paper (Chapter 15\\nhere), and Benjamini and Yekutieli‚Äôs (2005) false-coverage rate algorithm.\\nScheff¬¥e‚Äôs construction (20.15) is derived by transforming (20.6) to the\\n\\ncase V D I using the inverse square root of matrix V ,\\nO(cid:13) D V (cid:0)1=2 OÀá and (cid:13) D V (cid:0)1=2Àá\\n(.V (cid:0)1=2/2 D V (cid:0)1), which makes the ellipsoid of Figure 20.2 into a circle.\\nThen Q D k O(cid:13) (cid:0) (cid:13)k2= O(cid:27) 2 in (20.10), and for a linear combination (cid:13)d D d 0(cid:13)\\nit is straightforward to see that PrfQ (cid:20) k.Àõ/2\\n\\np;q g D Àõ amounts to\\n\\n(20.60)\\n\\n(cid:13)d 2 O(cid:13)d Àô O(cid:27) kd k k.Àõ/\\np;q\\n\\n(20.61)\\n\\nfor all choices of d , the geometry of Figure 20.2 now being transparent.\\nChanging coordinates back to OÀá D V 1=2 O(cid:13), Àá D V 1=2(cid:13), and c D V (cid:0)1=2d\\nyields (20.15).\\n\\n(cid:142)2 [p. 399] Restricting the catalog C . Suppose that all the sample sizes nj in\\n(20.16) take the same value n, and that we wish to set simultaneous con-\\nÔ¨Ådence intervals for all pairwise differences Àái (cid:0) Àáj . Tukey‚Äôs studentized\\nrange pivotal quantity (1952, unpublished)\\n\\n(cid:16) OÀái (cid:0) OÀáj\\n\\n(cid:17)\\n\\nÀá\\nÀá\\nÀá\\n\\nÀá\\nÀá\\nÀá\\n\\n(cid:0) .Àái (cid:0) Àáj /\\nO(cid:27)\\n\\nT D max\\ni¬§j\\n\\nhas a distribution not depending on (cid:27) or Àá. This implies that\\n\\nÀái (cid:0) Àáj 2 OÀái (cid:0) OÀáj Àô\\n\\nO(cid:27)p\\nn\\n\\nT .Àõ/\\n\\n(20.62)\\n\\n(20.63)\\n\\nis a set of simultaneous level-Àõ conÔ¨Ådence intervals for all pairwise dif-\\nferences Àái (cid:0) Àáj , where T .Àõ/ is the Àõth quantile of T . (The factor 1=\\nn\\ncomes from OÀáj (cid:24)\\n\\n.Àáj ; (cid:27) 2=n/ in (20.16).)\\n\\np\\n\\nN\\n\\nTable 20.4 Half-width of Tukey studentized range simultaneous 95%\\np\\nconÔ¨Ådence intervals for pairwise differences Àái (cid:0) Àáj (in units of O(cid:27)=\\nn)\\nfor p D 2; 3; : : : ; 6 and n D 20; compared with Scheff¬¥e intervals (20.15).\\n\\np\\n\\n2\\n\\n3\\n\\n4\\n\\n5\\n\\n6\\n\\nTukey\\nScheff¬¥e\\n\\n2.95\\n3.74\\n\\n3.58\\n4.31\\n\\n3.96\\n4.79\\n\\n4.23\\n5.21\\n\\n4.44\\n5.58\\n\\n\\x0c20.5 Notes and Details\\n\\n419\\n\\nReducing the catalog C from all linear combinations c0Àá to only pair-\\nwise differences shortens the simultaneous intervals. Table 20.4 shows the\\ncomparison between the Tukey and Scheff¬¥e 95% intervals for p D 2; 3;\\n: : : ; 6 and n D 20.\\n\\nCalculating T .Àõ/ was a substantial project in the early 1980s. Berk et al.\\n(2013) now carry out the analogous computations for general catalogs of\\nlinear constraints. They discuss at length the inferential basis of such pro-\\ncedures.\\n\\n(cid:142)3 [p. 405] Discontinuous estimators. Looking at Figure 20.6 suggests that a\\nconÔ¨Ådence interval for (cid:18)(cid:0)2:0 will move far left for data sets x where Cp\\nselects linear regression (m D 1) as best. This kind of ‚Äújumpy‚Äù behav-\\nior lengthens the intervals needed to attain a desired coverage level. More\\nseriously, intervals for m D 1 may give misleading inferences, another ex-\\nample of ‚Äúaccurate but incorrect‚Äù behavior. Bagging (20.28), in addition to\\nreducing interval length, improves inferential correctness, as discussed in\\nEfron (2014a).\\n\\n(cid:142)4 [p. 406] Theorem 20.2 and its corollary. Theorem 20.2 is proved in Sec-\\ntion 3 of Efron (2014a), with a parametric bootstrap version appearing in\\nSection 4. The corollary is a projection result illustrated in Figure 4 of\\n.N / be the n-dimensional subspace of B-dimensional Eu-\\nthat paper: let\\nclidean space spanned by the columns of the B (cid:2) n matrix .Nbj / (20.29)\\nand t (cid:3) the B-vector with components t (cid:3)b (cid:0) t (cid:3)(cid:1); then\\nbseIJ.s/ƒ±\\nƒ±kt (cid:3)k;\\n\\nbseboot.t/ D (cid:13)\\n\\n(cid:13)Ot (cid:3)(cid:13)\\n(cid:13)\\n\\n(20.64)\\n\\nL\\n\\nwhere Ot (cid:3) is the projection of t (cid:3) into\\n.N /. In the language of Section 10.3,\\nL\\nif O(cid:18) (cid:3) D S.P/ is very nonlinear as a function of P, then the ratio in (20.64)\\nwill be substantially less than 1.\\n\\n(cid:142)5 [p. 409] Tweedie‚Äôs formula. For convenience, take (cid:27) 2 D 1 in (20.35).\\n\\nBayes‚Äô rule (3.5) can then be arranged to give\\n\\nwith\\n\\ng.(cid:22)jz/ D e(cid:22)z(cid:0) .z/g.(cid:22)/e(cid:0) 1\\n\\n2 (cid:22)2ƒ±\\n\\np\\n\\n2(cid:25)\\n\\n .z/ D 1\\n2\\n\\nz2 C log f .z/:\\n\\n(20.65)\\n\\n(20.66)\\n\\nThis is a one-parameter exponential family (5.46) having natural parameter\\nÀõ equal to z. Differentiating   as in (5.55) gives\\n\\nEf(cid:22)jzg D d \\ndz\\n\\nD z C d\\ndz\\n\\nlog f .z/;\\n\\n(20.67)\\n\\n\\x0c420\\n\\nInference After Model Selection\\n\\nwhich is Tweedie‚Äôs formula (20.37) when (cid:27) 2 D 1. The formula Ô¨Årst ap-\\npears in Robbins (1956), who credits it to a personal communication from\\nM. K. Tweedie. Efron (2011) discusses general exponential family versions\\nof Tweedie‚Äôs formula, and its application to selection bias situations.\\n(cid:142)6 [p. 414] Theorem 20.4. The delta method standard error approximation for\\n\\na statistic T .x/ is\\n\\nbsedelta D\\n\\nh\\n.rT .x//\\n\\n0 OV .rT .x//\\n\\ni1=2\\n\\n;\\n\\n(20.68)\\n\\nwhere rT .x/ is the gradient vector .@T =@xj / and OV is an estimate of the\\ncovariance matrix of x. Other names include the ‚ÄúTaylor series method,‚Äù\\nas in (2.10), and ‚Äúpropagation of errors‚Äù in the physical sciences literature.\\nThe proof of Theorem 20.4 in Section 2 of Efron (2015) consists of show-\\ning that Covx D rT .x/ when T .x/ D Eft.Àõ/jxg. Standard deviations are\\nonly a Ô¨Årst step in assessing the frequentist accuracy of T .x/. The paper\\ngoes on to show how Theorem 20.4 can be improved to give conÔ¨Ådence\\nintervals, correcting the impression in Figure 20.10 that cdf.c/ can range\\noutside ≈í0; 1(cid:141).\\n\\n(cid:142)7 [p. 415] Bayesian lasso. Applying Bayes‚Äô rule (3.5) with density (20.51)\\n\\nand prior (20.52) gives\\n\\nlog g.Àájy/ D (cid:0)\\n\\n(cid:26) ky (cid:0) X Àák2\\n2\\n\\nC (cid:21)kÀák1\\n\\n(cid:27)\\n\\n;\\n\\n(20.69)\\n\\nas discussed in Tibshirani (2006). Comparison with (7.42) shows that the\\nmaximizing value of Àá (the ‚ÄúMAP‚Äù estimate) agrees with the lasso esti-\\nmate. Park and Casella (2008) named the ‚ÄúBayesian lasso‚Äù and suggested\\nan appropriate MCMC algorithm. Their choice (cid:21) D 0:37 was based on\\nmarginal maximum likelihood calculations, giving their analysis an empir-\\nical Bayes aspect ignored in their and our analyses.\\n\\n\\x0c21\\n\\nEmpirical Bayes Estimation Strategies\\n\\nClassic statistical inference was focused on the analysis of individual cases:\\na single estimate, a single hypothesis test. The interpretation of direct evi-\\ndence bearing on the case of interest‚Äîthe number of successes and failures\\nof a new drug in a clinical trial as a familiar example‚Äîdominated statistical\\npractice.\\n\\nThe story of modern statistics very much involves indirect evidence,\\n‚Äúlearning from the experience of others‚Äù in the language of Sections 7.4\\nand 15.3, carried out in both frequentist and Bayesian settings. The computer-\\nintensive prediction algorithms described in Chapters 16‚Äì19 use regression\\ntheory, the frequentist‚Äôs favored technique, to mine indirect evidence on a\\nmassive scale. False-discovery rate theory, Chapter 15, collects indirect ev-\\nidence for hypothesis testing by means of Bayes‚Äô theorem as implemented\\nthrough empirical Bayes estimation.\\n\\nEmpirical Bayes methodology has been less studied than Bayesian or\\nfrequentist theory. As with the James‚ÄìStein estimator (7.13), it can seem to\\nbe little more than plugging obvious frequentist estimates into Bayes esti-\\nmation rules. This conceals a subtle and difÔ¨Åcult task: learning the equiva-\\nlent of a Bayesian prior distribution from ongoing statistical observations.\\nOur Ô¨Ånal chapter concerns the empirical Bayes learning process, both as an\\nexercise in applied deconvolution and as a relatively new form of statistical\\ninference. This puts us back where we began in Chapter 1, examining the\\ntwo faces of statistical analysis, the algorithmic and the inferential.\\n\\n21.1 Bayes Deconvolution\\n\\nA familiar formulation of empirical Bayes inference begins by assuming\\nthat an unknown prior density g.(cid:18)/, our object of interest, has produced a\\nrandom sample of real-valued variates ‚Äö1; ‚Äö2; : : : ; ‚ÄöN ,\\n\\n‚Äöi\\n\\niid(cid:24) g.(cid:18) /;\\n\\ni D 1; 2; : : : ; N:\\n\\n(21.1)\\n\\n421\\n\\n\\x0c422\\n\\nEmpirical Bayes Estimation Strategies\\n\\n(The ‚Äúdensity‚Äù g.(cid:1)/ may include discrete atoms of probability.) The ‚Äöi are\\nunobservable, but each yields an observable random variable Xi according\\nto a known family of density functions\\n\\nXi\\n\\nind(cid:24) pi .Xi j‚Äöi /:\\n\\n(21.2)\\n\\nFrom the observed sample X1; X2; : : : ; XN we wish to estimate the prior\\ndensity g.(cid:18)/.\\n\\nA famous example has pi .Xi j‚Äöi / the Poisson family,\\n\\nXi (cid:24) Poi.‚Äöi /;\\n\\n(21.3)\\n\\nas in Robbins‚Äô formula, Section 6.1. Still more familiar is the normal model\\n(3.28),\\n\\n(21.4)\\noften with (cid:27) 2 D 1. A binomial model was used in the medical example of\\nSection 6.3,\\n\\nN\\n\\n.‚Äöi ; (cid:27) 2/;\\n\\nXi (cid:24)\\n\\nXi (cid:24) Bi.ni ; ‚Äöi /:\\n\\n(21.5)\\n\\nThere the ni differ from case to case, accounting for the need for the Ô¨Årst\\nsubscript i in pi .Xi j‚Äöi / (21.2).\\n\\nLet fi .Xi / denote the marginal density of Xi obtained from (21.1)‚Äì\\n\\n(21.2),\\n\\nZ\\n\\nfi .Xi / D\\n\\npi .Xi j(cid:18)i /g.(cid:18)i / d(cid:18)i ;\\n\\n(21.6)\\n\\nT\\nthe integral being over the space\\nhas only the marginal observations available,\\n\\nT\\n\\nof possible ‚Äö values. The statistician\\n\\nXi\\n\\nind(cid:24) fi .(cid:1)/;\\n\\ni D 1; 2; : : : ; N;\\n\\n(21.7)\\n\\nfrom which he or she wishes to estimate the density g.(cid:1)/ in (21.6).\\n\\nIn the normal model (21.4), fi is the convolution of the unknown g.(cid:18)/\\n\\nwith a known normal density, denoted\\nf D g (cid:3)\\n\\n.0; (cid:27) 2/\\n\\n(21.8)\\n\\nN\\n(now fi not depending on i). Estimating g using a sample X1; X2; : : : ; XN\\nfrom f is a problem in deconvolution. In general we might call the estima-\\ntion of g in model (21.1)‚Äì(21.2) the ‚ÄúBayes deconvolution problem.‚Äù\\n\\nAn artiÔ¨Åcial example appears in Figure 21.1, where g.(cid:18)/ is a mixture\\n.0; 0:52/ and one-eighth uniform over the in-\\ndistribution: seven-eighths\\nterval ≈í(cid:0)3; 3(cid:141). A normal sampling model Xi\\n.‚Äöi ; 1/ is assumed, yield-\\ning f by convolution as in (21.8). The convolution process makes f wider\\n\\nind(cid:24)\\n\\nN\\n\\nN\\n\\n\\x0c21.1 Bayes Deconvolution\\n\\n423\\n\\nFigure 21.1 An artiÔ¨Åcial example of the Bayes deconvolution\\nproblem. The solid curve is g.(cid:18)/, the prior density of ‚Äö (21.1);\\nthe dashed curve is the density of an observation X from marginal\\ndistribution f D g (cid:3)\\n.0; 1/ (21.8). We wish to estimate g.(cid:18)/ on\\nthe basis of a random sample X1; X2; : : : ; XN from f .x/.\\n\\nN\\n\\nand smoother than g, as illustrated in the Ô¨Ågure. Having observed a ran-\\ndom sample from f , we wish to estimate the deconvolute g, which begins\\nto look difÔ¨Åcult in the Ô¨Ågure‚Äôs example.\\n\\nDeconvolution has a well-deserved reputation for difÔ¨Åculty. It is the\\nclassic ill-posed problem: because of the convolution process (21.6), large\\nchanges in g.(cid:18)/ are smoothed out, often yielding only small changes in\\nf .x/. Deconvolution operates in the other direction, with small changes in\\nthe estimation of f disturbingly magniÔ¨Åed on the g scale. Nevertheless,\\nmodern computation, modern theory, and most of all modern sample sizes,\\ntogether can make empirical deconvolution a practical reality.\\n\\nWhy would we want to estimate g.(cid:18)/? In the prostate data example\\n(3.28) (where ‚Äö is called (cid:22)) we might wish to know Prf‚Äö D 0g, the proba-\\nbility of a null gene, ones whose effect size is zero; or perhaps Prfj‚Äöj (cid:21) 2g,\\nthe proportion of genes that are substantially non-null. Or we might want to\\nestimate Bayesian posterior expectations like Ef‚ÄöjX D xg in Figure 20.7,\\nor posterior densities as in Figure 6.5.\\n\\nTwo main strategies have developed for carrying out empirical Bayes\\nestimation: modeling on the (cid:18) scale, called g-modeling here, and modeling\\n\\n‚àí4‚àí20240.000.050.100.15q and xg(q) and f(x)f(x)g(q)\\x0c424\\n\\nEmpirical Bayes Estimation Strategies\\n\\non the x scale, called f -modeling. We begin in the next section with g-\\nmodeling.\\n\\n21.2 g-Modeling and Estimation\\n\\nThere has been a substantial amount of work on the asymptotic accuracy\\nof estimates Og.(cid:18)/ in the empirical Bayes model (21.1)‚Äì(21.2), most often\\nin the normal sampling framework (21.4). The results are discouraging,\\nwith the rate of convergence of Og.(cid:18)/ to g.(cid:18)/ as slow as .log N /(cid:0)1. In our\\nterminology, much of this work has been carried out in a nonparametric g-\\nmodeling framework, allowing the unknown prior density g.(cid:18)/ to be virtu-\\nally anything at all. More optimistic results are possible if the g-modeling\\nis pursued parametrically, that is, by restricting g.(cid:18)/ to lie within some\\nparametric family of possibilities.\\n\\nWe assume, for the sake of simpler exposition, that the space\\n\\nsible ‚Äö values is Ô¨Ånite and discrete, say\\n\\nof pos-\\n\\nT\\n\\nD Àö(cid:18).1/; (cid:18).2/; : : : ; (cid:18).m/\\nThe prior density g.(cid:18) / is now represented by a vector g D .g1; g2; : : : ; gm/0,\\nwith components\\n\\n(21.9)\\n\\n(cid:9) :\\n\\nT\\n\\ngj D Pr Àö‚Äö D (cid:18).j /\\n\\n(cid:9)\\n\\nfor j D 1; 2; : : : ; m:\\n\\n(21.10)\\n\\nA p-parameter exponential family (5.50) for g can be written as\\n\\ng D g.Àõ/ D eQÀõ(cid:0) .Àõ/;\\n\\n(21.11)\\n\\nwhere the p-vector Àõ is the natural parameter and Q is a known m (cid:2) p\\nstructure matrix. Notation (21.11) means that the j th component of g.Àõ/\\nis\\n\\ngj .Àõ/ D eQ0\\n\\nj Àõ(cid:0) .Àõ/;\\n\\n(21.12)\\n\\nwith Q0\\ng.Àõ/ sum to 1,\\n\\nj the j th row of Q; the function  .Àõ/ is the normalizer that makes\\n\\n .Àõ/ D log\\n\\n1\\n\\neQ0\\nj Àõ\\n\\nA :\\n\\n0\\n\\n@\\n\\nm\\nX\\n\\nj D1\\n\\n(21.13)\\n\\nIn the nodes example of Figure 6.4, the set of possible ‚Äö values was\\nD f0:01; 0:02; : : : ; 0:99g, and Q was a Ô¨Åfth-degree polynomial matrix,\\n\\nT\\n\\nQ D poly(\\nT\\n\\n,5)\\n\\n(21.14)\\n\\n\\x0c21.2 g-Modeling and Estimation\\n\\n425\\n\\nin R notation, indicating a Ô¨Åve-parameter exponential family for g, (6.38)‚Äì\\n(6.39).\\n\\nIn the development that follows we will assume that the kernel pi .(cid:1)j(cid:1)/ in\\n(21.2) does not depend on i, i.e., that Xi has the same family of conditional\\ndistributions p.Xi j‚Äöi / for all i, as in the Poisson and normal situations\\n(21.3) and (21.4), but not the binomial case (21.5). And moreover we as-\\nfor the Xi observations is Ô¨Ånite and discrete,\\nsume that the sample space\\nsay\\n\\nX\\nD Àöx.1/; x.2/; : : : ; x.n/\\n\\nX\\n\\n(cid:9) :\\n\\n(21.15)\\n\\nNone of this is necessary, but it simpliÔ¨Åes the exposition.\\n\\nDeÔ¨Åne\\n\\npkj D Pr ÀöXi D x.k/j‚Äöi D (cid:18).j /\\nfor k D 1; 2; : : : ; n and j D 1; 2; : : : ; m, and the corresponding n (cid:2) m\\nmatrix\\n\\n(21.16)\\n\\n(cid:9) ;\\n\\nP D .pkj /;\\n\\n(21.17)\\n\\nhaving kth row Pk D .pk1; pk2; : : : ; pkm/0. The convolution-type for-\\nmula (21.6) for the marginal density f .x/ now reduces to an inner product,\\n\\nÀöXi D x.k/\\n\\n(cid:9) D Pm\\n\\nj D1 pkj gj .Àõ/\\n\\nfk.Àõ/ D PrÀõ\\nD P 0\\n\\nkg.Àõ/:\\n\\n(21.18)\\n\\nIn fact we can write the entire marginal density f .Àõ/ D .f1.Àõ/; f2.Àõ/; : : : ,\\nfn.Àõ//0 in terms of matrix multiplication,\\n\\nf .Àõ/ D Pg.Àõ/:\\n\\nThe vector of counts y D .y1; y2; : : : ; yn/, with\\n\\nyk D # ÀöXi D x.k/\\n\\n(cid:9) ;\\n\\n(21.19)\\n\\n(21.20)\\n\\nis a sufÔ¨Åcient statistic in the iid situation. It has a multinomial distribution\\n(5.38),\\n\\ny (cid:24) Multn.N; f .Àõ//;\\n\\n(21.21)\\n\\nindicating N independent draws for a density f .Àõ/ on n categories.\\n\\nAll of this provides a concise description of the g-modeling probability\\n\\nmodel:\\n\\nÀõ ! g.Àõ/ D eQÀõ(cid:0) .Àõ/ ! f .Àõ/ D Pg.Àõ/ ! y (cid:24) Multn.N; f .Àõ//:\\n\\n(21.22)\\n\\n\\x0c426\\n\\nEmpirical Bayes Estimation Strategies\\n\\nThe inferential task goes in the reverse direction,\\n\\ny ! OÀõ ! f . OÀõ/ ! g. OÀõ/ D eQ OÀõ(cid:0) . OÀõ/:\\n\\n(21.23)\\n\\nFigure 21.2 A schematic diagram of empirical Bayes estimation,\\nas explained in the text.\\ncontaining the p-parameter family\\nof allowable probability\\ndistributions f .Àõ/. The vector of observed proportions y=N\\nyields MLE f . OÀõ/, which is then deconvolved to obtain estimate\\ng. OÀõ/.\\n\\nn is the n-dimensional simplex,\\n\\nF\\n\\nS\\n\\nA schematic diagram of the estimation process appears in Figure 21.2.\\n\\n(cid:15) The vector of observed proportions y=N is a point in\\n\\nn, the simplex\\n(5.39) of all possible probability vectors f on n categories; y=N is the\\nusual nonparametric estimate of f .\\n\\nS\\n\\n(cid:15) The parametric family of allowable f vectors (21.19)\\n\\nD ff .Àõ/; Àõ 2 Ag;\\n\\nF\\n\\n(21.24)\\n\\nindicated by the red curve, is a curved p-dimensional surface in\\nA is the space of allowable vectors Àõ in family (21.11).\\n\\nn. Here\\n\\nS\\n\\n(cid:15) The nonparametric estimate y=N is ‚Äúprojected‚Äù down to the parametric\\nestimate f . OÀõ/; if we are using MLE estimation, f . OÀõ/ will be the closest\\nto y=N measured according to a deviance metric, as in (8.35).\\npoint in\\n(cid:15) Finally, f . OÀõ/ is mapped back to the estimate g. OÀõ/, by inverting map-\\nping (21.19). (Inversion is not actually necessary with g-modeling since,\\nhaving found OÀõ, g. OÀõ/ is obtained directly from (21.11); the inversion\\nstep is more difÔ¨Åcult for f -modeling, Section 21.6.)\\n\\nF\\n\\n\\x0c21.3 Likelihood, Regularization, and Accuracy\\n\\n427\\n\\nThe maximum likelihood estimation process for g-modeling is discussed\\nin more detail in the next section, where formulas for its accuracy will be\\ndeveloped.\\n\\n21.3 Likelihood, Regularization, and Accuracy1\\n\\nParametric g-modeling, as in (21.11), allows us to work in low-dimensional\\nparametric families‚Äîjust Ô¨Åve parameters for the nodes example (21.14)‚Äî\\nwhere classic maximum likelihood methods can be more conÔ¨Ådently ap-\\nplied. Even here though, some regularization will be necessary for stable\\nestimation, as discussed in what follows.\\n\\nThe g-model probability mechanism (21.22) yields a log likelihood for\\n\\nthe multinomial vector y of counts as a function of Àõ, say ly .Àõ/;\\n\\nly .Àõ/ D log\\n\\n!\\n\\nfk.Àõ/yk\\n\\nD\\n\\n  n\\nY\\n\\nkD1\\n\\nn\\nX\\n\\nkD1\\n\\nyk log fk.Àõ/:\\n\\n(21.25)\\n\\nIts score function Ply .Àõ/, the vector of partial derivatives @ly .Àõ/=@Àõh for\\nh D 1; 2; : : : ; p, determines the MLE OÀõ according to Ply . OÀõ/ D 0. The\\np (cid:2) p matrix of second derivatives Rly .Àõ/ D .@2ly .Àõ/=@Àõh@Àõl / gives the\\nFisher information matrix (5.26)\\n\\nI.Àõ/ D Ef(cid:0) Rly .Àõ/g:\\n(21.26)\\nThe exponential family model (21.11) yields simple expressions for Ply .Àõ/\\n\\nand I.Àõ/. DeÔ¨Åne\\n\\nwkj D gj .Àõ/\\n\\n(cid:18) pkj\\nfk.Àõ/\\n\\n(cid:19)\\n\\n(cid:0) 1\\n\\n(21.27)\\n\\nand the corresponding m-vector\\n\\nWk.Àõ/ D .wk1.Àõ/; wk2.Àõ/; : : : ; wkm.Àõ//0:\\n\\n(21.28)\\n\\nLemma 21.1 The score function Ply .Àõ/ under model (21.22) is\\n\\nPly .Àõ/ D QWC.Àõ/;\\n\\nwhere WC.Àõ/ D\\n\\nn\\nX\\n\\nkD1\\n\\nWk.Àõ/yk\\n\\n(21.29)\\n\\nand Q is the m (cid:2) p structure matrix in (21.11).\\n\\n1 The technical lemmas in this section are not essential to following the subsequent\\n\\ndiscussion.\\n\\n\\x0c428\\n\\nEmpirical Bayes Estimation Strategies\\n\\nLemma 21.2 The Fisher information matrix I.Àõ/, evaluated at Àõ D OÀõ,\\nis\\n\\n( n\\nX\\n\\n)\\n\\nI. OÀõ/ D Q0\\n\\nWk. OÀõ/Nfk. OÀõ/Wk. OÀõ/0\\n\\nQ;\\n\\n(21.30)\\n\\nkD1\\n\\nwhere N D Pn\\n(21.2).\\n\\n1 yk is the sample size in the empirical Bayes model (21.1)‚Äì\\n\\n(cid:142)1\\n\\nSee the chapter endnotes (cid:142) for a brief discussion of Lemmas 21.1 and\\n21.2. I. OÀõ/(cid:0)1 is the usual maximum likelihood estimate of the covariance\\nmatrix of OÀõ, but we will use a regularized version of the MLE that is less\\nvariable.\\n\\nIn the examples that follow, OÀõ was found by numerical maximization.2\\nEven though g.Àõ/ is an exponential family, the marginal density f .Àõ/ in\\n(21.22) is not. As a result, some care is needed in avoiding local maxima of\\nly .Àõ/. These tend to occur at ‚Äúcorner‚Äù values of Àõ, where one of its compo-\\nnents goes to inÔ¨Ånity. A small amount of regularization pulls OÀõ away from\\nthe corners, decreasing its variance at the possible expense of increased\\nbias.\\n\\nInstead of maximizing ly .Àõ/ we maximize a penalized likelihood\\n\\nm.Àõ/ D ly .Àõ/ (cid:0) s.Àõ/;\\n\\n(21.31)\\n\\nwhere s.Àõ/ is a positive penalty function. Our examples use\\n\\ns.Àõ/ D c0kÀõk D c0\\n\\n!1=2\\n\\n  p\\nX\\n\\nhD1\\n\\nÀõ2\\nh\\n\\n(21.32)\\n\\n(with c0 equal 1), which prevents the maximizer OÀõ of m.Àõ/ from venturing\\ntoo far into corners.\\n\\nThe following lemma is discussed in the chapter endnotes.\\n\\n(cid:142)2\\n\\nLemma 21.3 (cid:142)The maximizer OÀõ of m.Àõ/ has approximate bias vector and\\ncovariance matrix\\n\\nBias. OÀõ/ D (cid:0) .I. OÀõ/ C Rs. OÀõ//\\n\\n(cid:0)1 Ps. OÀõ/\\n\\nand Var. OÀõ/ D .I. OÀõ/ C Rs. OÀõ//\\n\\n(cid:0)1 I. OÀõ/ .I. OÀõ/ C Rs. OÀõ//\\n\\n(cid:0)1 ;\\n\\n(21.33)\\n\\nwhere I. OÀõ/ is given in (21.30).\\n\\nWith s.Àõ/ (cid:17) 0 (no regularization) the bias is zero and Var. OÀõ/ D I. OÀõ/(cid:0)1,\\n\\n2 Using the nonlinear maximizer nlm in R.\\n\\n\\x0c21.3 Likelihood, Regularization, and Accuracy\\n\\n429\\n\\nthe usual MLE approximations: including s.Àõ/ reduces variance while in-\\ntroducing bias.\\n\\nFor s.Àõ/ D c0kÀõk we calculate\\n\\nPs.Àõ/ D c0Àõ=kÀõk\\n\\nand\\n\\nRs.Àõ/ D c0\\nkÀõk\\n\\n(cid:18)\\nI (cid:0) ÀõÀõ0\\nkÀõk2\\n\\n(cid:19)\\n\\n;\\n\\n(21.34)\\n\\nwith I the p (cid:2) p identity matrix. Adding the penalty s.Àõ/ in (21.31) pulls\\nthe MLE of Àõ toward zero and the MLE of g.Àõ/ toward a Ô¨Çat distribution\\n. Looking at Var. OÀõ/ in (21.33), a measure of the regularization effect\\nover\\nis\\n\\nT\\n\\ntr.Rs. OÀõ//= tr.I. OÀõ//;\\n\\n(21.35)\\n\\nwhich was never more than a few percent in our examples.\\n\\nMost often we will be more interested in the accuracy of Og D g. OÀõ/ than\\n\\nin that of OÀõ itself. Letting\\n\\nD. OÀõ/ D diag.g. OÀõ// (cid:0) g. OÀõ/g. OÀõ/0;\\n\\n(21.36)\\n\\nthe m (cid:2) p derivative matrix .@gj =@Àõh/ is\\n\\n@g=@Àõ D D.Àõ/Q;\\n\\n(21.37)\\n\\nwith Q the structure matrix in (21.11). The usual Ô¨Årst-order delta-method\\ncalculations then give the following theorem.\\n\\nTheorem 21.4 The penalized maximum likelihood estimate Og D g. OÀõ/\\nhas estimated bias vector and covariance matrix\\nBias. Og/ D D. OÀõ/Q Bias. OÀõ/\\n\\n(21.38)\\n\\nand Var. Og/ D D. OÀõ/Q Var. OÀõ/Q0D. OÀõ/\\n\\nwith Bias. OÀõ/ and Var. OÀõ/ as in (21.33).3\\n\\nThe many approximations going into Theorem 21.4 can be short-circuited\\nby means of the parametric bootstrap, Section 10.4. Starting from OÀõ and\\nf . OÀõ/ D Pg. OÀõ/, we resample the count vector\\n\\ny (cid:3) (cid:24) Multn.N; f . OÀõ//;\\n\\n(21.39)\\n\\nand calculate4 the penalized MLE OÀõ(cid:3) based on y (cid:3), yielding Og(cid:3) D g. OÀõ(cid:3)/.\\n\\n3 Note that the bias treats model (21.11) as the true prior, and arises as a result of the\\n\\npenalization.\\n\\n4 Convergence of the nlm search process is speeded up by starting from OÀõ.\\n\\n\\x0c430\\n\\nEmpirical Bayes Estimation Strategies\\n\\nB replications Og(cid:3)1; Og(cid:3)2; : : : ; Og(cid:3)B gives bias and covariance estimates\\n\\ndBias D Og(cid:3)(cid:1) (cid:0) Og\\n\\nand cVar D\\n\\nB\\nX\\n\\n. Og(cid:3)b (cid:0) Og(cid:3)(cid:1)/. Og(cid:3)b (cid:0) Og(cid:3)(cid:1)/ƒ±.B (cid:0) 1/;\\n\\n(21.40)\\n\\nbD1\\n\\nand Og(cid:3)(cid:1) D PB\\n\\n1\\n\\nOg(cid:3)b=B.\\n\\nTable 21.1 Comparison of delta method (21.38) and bootstrap (21.40)\\nstandard errors and biases for the nodes study estimate of g in\\nFigure 6.4. All columns except the Ô¨Årst multiplied by 100.\\n\\nStandard Error\\n\\nBias\\n\\ng.(cid:18)/ Delta\\n\\nBoot\\n\\nDelta\\n\\nBoot\\n\\n12.048\\n1.045\\n.381\\n.779\\n1.119\\n.534\\n.264\\n.224\\n.321\\n.576\\n\\n.887\\n.131\\n.058\\n.096\\n.121\\n.102\\n.047\\n.056\\n.054\\n.164\\n\\n.967 (cid:0).518 (cid:0).592\\n.071\\n.056\\n.139\\n.065\\n.033\\n.025\\n.095 (cid:0).011 (cid:0).013\\n.117 (cid:0).040 (cid:0).049\\n.027\\n.019\\n.100\\n.027\\n.023\\n.051\\n.020\\n.018\\n.053\\n.009\\n.048\\n.013\\n.169 (cid:0).008\\n.008\\n\\n(cid:18)\\n\\n.01\\n.12\\n.23\\n.34\\n.45\\n.56\\n.67\\n.78\\n.89\\n.99\\n\\nTable 21.1 compares the delta method of Theorem 20.4 with the para-\\nmetric bootstrap (B D 1000 replications) for the surgical nodes example\\nof Section 6.3. Both the standard errors‚Äîsquare roots of the diagonal el-\\nements of Var. Og/‚Äîand biases are well approximated by the delta method\\nformulas (21.38). The delta method also performed reasonably well on the\\ntwo examples of the next section.\\n\\nIt did less well on the artiÔ¨Åcial example of Figure 21.1, where\\n1p\\n\\n(cid:0) 1\\n2\\n\\n(cid:18)2\\n(cid:27)2\\n\\n.(cid:27) D 0:5/\\n\\ne\\n\\ng.(cid:18) / D 1\\n8\\n\\nI≈í(cid:0)3;3(cid:141).(cid:18)/\\n6\\n\\nC 7\\n8\\n\\n2(cid:25)(cid:27) 2\\n\\n(21.41)\\n\\n(1/8 uniform on ≈í(cid:0)3; 3(cid:141) and 7/8\\n.0; 0:52/). The vertical bars in Fig-\\nN\\nure 21.3 indicate Àô one standard error obtained from the parametric boot-\\nD f(cid:0)3; (cid:0)2:8; : : : ; 3g for the sample space of ‚Äö, and as-\\nstrap, taking\\nsuming a natural spline model in (21.11) with Ô¨Åve degrees of freedom,\\n\\nT\\n\\ng.Àõ/ D eQÀõ(cid:0) .Àõ/;\\n\\nQ D ns(\\nT\\n\\n,df=5):\\n\\n(21.42)\\n\\n\\x0c21.3 Likelihood, Regularization, and Accuracy\\n\\n431\\n\\nFigure 21.3 The red curve is g.(cid:18)/ for the artiÔ¨Åcial example of\\nFigure 21.1. Vertical bars are Àô one standard error for g-model\\nestimate g. OÀõ/; speciÔ¨Åcations (21.41)‚Äì(21.42), sample size\\nN D 1000 observations Xi (cid:24)\\n.‚Äöi ; 1/, using parametric\\nbootstrap (21.40), B D 500. The light dashed line follows\\nbootstrap means Og(cid:3)\\n\\nj . Some deÔ¨Ånitional bias is apparent.\\n\\nN\\n\\nThe sampling model was Xi (cid:24)\\nN\\nthis case the delta method standard errors were about 25% too small.\\n\\n.‚Äöi ; 1/ for i D 1; 2; : : : ; N D 1000. In\\n\\nThe light dashed curve in Figure 21.3 traces Ng.(cid:18)/, the average of the\\nB D 500 bootstrap replications g(cid:3)b. There is noticeable bias, compared\\nwith g.(cid:18) /. The reason is simple: the exponential family (21.42) for g.Àõ/\\ndoes not include g.(cid:18) / (21.41). In fact, Ng.(cid:18)/ is (nearly) the closest mem-\\nber of the exponential family to g.(cid:18)/. This kind of deÔ¨Ånitional bias is a\\ndisadvantage of parametric g-modeling.\\n\\n(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)‚Äî‚Äî(cid:1)(cid:1)\\n\\nOur g-modeling examples, and those of the next section, bring together\\na variety of themes from modern statistical practice: classical maximum\\nlikelihood theory, exponential family modeling, regularization, bootstrap\\nmethods, large data sets of parallel structure, indirect evidence, and a com-\\nbination of Bayesian and frequentist thinking, all of this enabled by mas-\\nsive computer power. Taken together they paint an attractive picture of the\\nrange of inferential methodology in the twenty-Ô¨Årst century.\\n\\n‚àí3‚àí2‚àí101230.000.050.100.15qg(q)\\x0c432\\n\\nEmpirical Bayes Estimation Strategies\\n\\n21.4 Two Examples\\n\\nWe now reconsider two previous data sets from a g-modeling point of\\nview. the Ô¨Årst is the artiÔ¨Åcial microarray-type example (20.24) comprising\\nN D10,000 independent observations\\n\\nind(cid:24)\\n\\nzi\\n\\nN\\n\\n.(cid:22)i ; 1/;\\n\\ni D 1; 2; : : : ; N D 10,000;\\n\\n(21.43)\\n\\nwith\\n\\n(\\n0\\n\\n(cid:22)i (cid:24)\\n\\nN\\n\\nfor i D 1; 2; : : : ; 9000\\nfor i D 9001; : : : ; 10,000:\\n\\n.(cid:0)3; 1/\\n\\n(21.44)\\n\\nFigure 20.3 displays the points .zi ; (cid:22)i / for i D 9001; : : : ; 10; 000, illus-\\n\\ntrating the Bayes posterior 95% conditional intervals (20.26),\\n\\n(cid:22)i 2 .zi (cid:0) 3/=2 Àô 1:96ƒ±\\n\\n2:\\n\\n(21.45)\\n\\np\\n\\nThese required knowing the Bayes prior distribution (cid:22)i (cid:24)\\n.(cid:0)3; 1/. We\\nwould like to recover intervals (21.45) using just the observed data zi , i D\\n1; 2; : : : ; 10; 000, without knowledge of the prior.\\n\\nN\\n\\nFigure 21.4 Histogram of observed sample of N D 10,000\\nvalues zi from simulations (21.43)‚Äì(21.44).\\n\\nA histogram of the 10,000 z-values is shown in Figure 21.4; g-modeling\\n(21.9)‚Äì(21.11) was applied to them (now with (cid:22) playing the role of ‚Äú‚Äö‚Äù\\n\\n z-valuesFrequency‚àí8‚àí6‚àí4‚àí20240200400600800||^^\\x0c21.4 Two Examples\\n\\n433\\n\\nD .(cid:0)6; (cid:0)5:75; : : : ; 3/. Q was composed of a\\nand z being ‚Äúx‚Äù), taking\\ndelta function at (cid:22) D 0 and a Ô¨Åfth-degree polynomial basis for the nonzero\\n(cid:22), again a family of spike-and-slab priors. The penalized MLE Og (21.31),\\n(21.32), c0 D 1, estimated the probability of (cid:22) D 0 as\\n\\nT\\n\\nOg.0/ D 0:891 Àô 0:006\\n\\n(21.46)\\n\\n(using (21.38), which also provided bias estimate 0.001).\\n\\nFigure 21.5 Purple curves show g-modeling estimates of\\nconditional 95% credible intervals for (cid:22) given z in artiÔ¨Åcial\\nmicroarray example (21.43)‚Äì(21.44). They are a close match to\\nthe actual Bayes intervals, dotted lines; cf. Figure 20.3.\\n\\nThe estimated posterior density of (cid:22) given z is\\n\\nOg.(cid:22)jz/ D cz Og.(cid:22)/(cid:30).z (cid:0) (cid:22)/;\\n(21.47)\\n(cid:30).(cid:1)/ the standard normal density and cz the constant required for Og.(cid:22)jz/\\nto integrate to 1. Let q.Àõ/.z/ denote the Àõth quantile of Og.(cid:22)jz/. The purple\\ncurves in Figure 21.5 trace the estimated 95% credible intervals\\n\\n(cid:17)\\n(cid:16)\\nq.:025/.z/; q.:975/.z/\\n\\n:\\n\\n(21.48)\\n\\nThey are a close match to the actual credible intervals (21.45).\\n\\nThe solid black curve in Figure 21.6 shows Og.(cid:22)/ for (cid:22) ¬§ 0 (the ‚Äúslab‚Äù\\nportion of the estimated prior). As an estimate of the actual slab density\\n\\n‚àí8‚àí6‚àí4‚àí20‚àí10‚àí8‚àí6‚àí4‚àí20Observed zmllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllBY.loBY.up‚àí2.77llllllllllllllBayes.loBayes.upEB.loEB.up\\x0c434\\n\\nEmpirical Bayes Estimation Strategies\\n\\nFigure 21.6 The heavy black curve is the g-modeling estimate of\\ng.(cid:22)/ for (cid:22) ¬§ 0 in the artiÔ¨Åcial microarray example, suppressing\\nthe atom at zero, Og.0/ D 0:891. It is only a rough estimate of the\\nactual nonzero density\\n\\n.(cid:0)3; 1/.\\n\\nN\\n\\nN\\n\\n(cid:22) (cid:24)\\n.(cid:0)3; 1/ it is only roughly accurate, but apparently still accurate\\nenough to yield the reasonably good posterior intervals seen in Figure 21.5.\\nThe fundamental impediment to deconvolution‚Äîthat large changes in g.(cid:18)/\\nproduce only small changes in f .x/‚Äîcan sometimes operate in the statis-\\ntician‚Äôs favor, when only a rough knowledge of g sufÔ¨Åces for applied pur-\\nposes.\\n\\nOur second example concerns the prostate study data, last seen in\\nFigure 15.1: n D 102 men, 52 cancer patients and 50 normal controls, each\\nhave had their genetic activities measured on a microarray of N D 6033\\ngenes; genei yields a test statistic zi comparing patients with controls,\\n\\nzi (cid:24)\\n\\n.(cid:22)i ; (cid:27) 2\\n\\n0 /;\\n\\n(21.49)\\n\\nN\\nwith (cid:22)i the gene‚Äôs effect size. (Here we will take the variance (cid:27) 2\\nparameter to be estimated, rather than assuming (cid:27) 2\\n0\\ndensity g.(cid:22)/ for the effects?\\n\\n0 as a\\nD 1.) What is the prior\\n\\nThe local false-discovery rate program locfdr, Section 15.5, was ap-\\nplied to the 6033 zi values, as shown in Figure 21.7. Locfdr is an ‚Äúf -\\nmodeling‚Äù method, where probability models are proposed directly for\\n\\n‚àí6‚àí4‚àí20240.000.020.040.060.080.10qDensitylatom.891N(‚àí3,1)\\x0c21.4 Two Examples\\n\\n435\\n\\nFigure 21.7 The green curve is a six-parameter Poisson\\nregression estimate Ô¨Åt to counts of the observed zi values for the\\nprostate data. The dashed curve is the empirical null (15.48),\\nzi (cid:24)\\n.0:00; 1:062/. The f -modeling program locfdr\\nestimated null probability Prf(cid:22) D 0g D 0:984. Genes with\\nz-values lying beyond the red triangles have estimated fdr values\\nless than 0.20.\\n\\nN\\n\\nthe marginal density f .(cid:1)/ rather than for the prior density g.(cid:1)/; see Sec-\\ntion (21.6). Here we can compare locfdr‚Äôs results with those from g-\\nmodeling. The former gave5\\n\\n(cid:16) Oƒ±0; O(cid:27)0; O(cid:25)0\\n\\n(cid:17)\\n\\nD .0:00; 1:06; 0:984/\\n\\n(21.50)\\n\\nin the notation of (15.50); that is, it estimated the null distribution as (cid:22) (cid:24)\\n.0; 1:062/, with probability O(cid:25)0 D 0:984 of a gene being null ((cid:22) D 0).\\nN\\nOnly 22 genes were estimated to have local fdr values less than 0.20, the\\n9 with zi (cid:20) (cid:0)3:71 and the 12 with zi (cid:21) 3:81. (These are more pessimistic\\n.0; 1/\\nresults than in Figure 15.5, where we used the theoretical null\\nrather than the empirical null\\n\\n.0; 1:062/.)\\nN\\nThe g-modeling approach (21.11) was applied to the prostate study\\n0 /, (cid:27)0 D 1:06 as suggested by (21.50). The\\n.(cid:22)i ; (cid:27) 2\\n\\ndata, assuming zi (cid:24)\\n\\nN\\n\\nN\\n\\n5 Using a six-parameter Poisson regression Ô¨Åt to the zi values, of the type employed in\\n\\nSection 10.4.\\n\\n‚àí4‚àí20240100200300400 z-valuesCounts\\x0c436\\n\\nEmpirical Bayes Estimation Strategies\\n\\nstructure matrix Q in (21.11) had a delta function at (cid:22) D 0 and a Ô¨Åve-\\nparameter natural spline basis for (cid:22) ¬§ 0;\\nD .(cid:0)3:6; (cid:0)3:4; : : : ; 3:6/ for\\nthe discretized ‚Äö space (21.9). This gave a penalized MLE Og having null\\nprobability\\n\\nT\\n\\nOg.0/ D 0:946 Àô 0:011:\\n\\n(21.51)\\n\\nFigure 21.8 The g-modeling estimate for the non-null density\\nOg.(cid:22)/, (cid:22) ¬§ 0, for the prostate study data, also indicating the\\nnull atom Og.0/ D 0:946. About 2% of the genes are estimated to\\nhave effect sizes j(cid:22)i j (cid:21) 2. The red bars show Àô one standard\\nerror as computed from Theorem 21.4 (page 429).\\n\\nThe non-null distribution, Og.(cid:22)/ for (cid:22) ¬§ 0, appears in Figure 21.8, where\\nit is seen to be modestly unimodal around (cid:22) D 0. Dashed red bars indicate\\nÀô one standard error for the Og.(cid:18).j // estimates obtained from Theorem 21.4\\n(page 429). The accuracy is not very good. It is better for larger regions of\\nthe ‚Äö space, for example\\n\\nbPrfj(cid:18)j (cid:21) 2g D 0:020 Àô 0:0014:\\n\\n(21.52)\\n\\nHere g-modeling estimated less prior null probability, 0.946 compared\\nwith 0.984 from f -modeling, but then attributed much of the non-null\\nprobability to small values of j(cid:22)i j.\\n\\nTaking (21.52) literally suggests 121 (D 0:020 (cid:1) 6033) genes with true\\n\\n‚àí4‚àí20240.00000.00050.00100.00150.00200.00250.0030qg(q)||lnull atom0.946\\x0c21.5 Generalized Linear Mixed Models\\n\\n437\\n\\nFigure 21.9 The black curve is the empirical Bayes estimated\\nfalse-discovery rate bPrf(cid:22) D 0jzg from g-modeling. For large\\nvalues of jzj it nearly matches the locfdr f -modeling estimate\\nfdr.z/, red curve.\\n\\neffect sizes j(cid:22)i j (cid:21) 2. That doesn‚Äôt mean we can say with certainty which\\n121. Figure 21.9 compares the g-modeling empirical Bayes false-discovery\\nrate\\n\\nbPrf(cid:22) D 0jzg D cz Og.0/(cid:30)\\n\\n;\\n\\n(21.53)\\n\\n(cid:19)\\n\\n(cid:18) z (cid:0) (cid:22)\\nO(cid:27)0\\n\\nas in (21.47), with the f -modeling estimate cfdr.z/ produced by locfdr.\\nWhere it counts, in the tails, they are nearly the same.\\n\\n21.5 Generalized Linear Mixed Models\\n\\nThe g-modeling theory can be extended to the situation where each ob-\\nservation Xi is accompanied by an observed vector of covariates ci , say\\nof dimension d . We return to the generalized linear model setup of Sec-\\ntion 8.2, where each Xi has a one-parameter exponential family density\\nindexed by its own natural parameter (cid:21)i ,\\n\\nf(cid:21)i .Xi / D expf(cid:21)i Xi (cid:0) (cid:13).(cid:21)i /gf0.Xi /\\n\\n(21.54)\\n\\nin notation (8.20).\\n\\n‚àí4‚àí20240.00.20.40.60.81.0z-valuefdr(z)\\x0c438\\n\\nEmpirical Bayes Estimation Strategies\\n\\nOur key assumption is that each (cid:21)i is the sum of a deterministic compo-\\n\\nnent, depending on the covariates ci , and a random term ‚Äöi ,\\n\\ni Àá:\\n\\n(cid:21)i D ‚Äöi C c0\\n(21.55)\\nHere ‚Äöi is an unobserved realization from g.Àõ/ D expfQÀõ (cid:0)  .Àõ/g\\n(21.11) and Àá is an unknown d -dimensional parameter. If Àá D 0 then\\n(21.55) is a g-model as before,6 while if all the ‚Äöi D 0 then it is a stan-\\ndard GLM (8.20)‚Äì(8.22). Taken together, (21.55) represents a generalized\\nlinear mixed model (GLMM). The likelihood and accuracy calculations of\\nSection 21.3 extend to GLMMs, as referenced in the endnotes, but here we\\nwill only discuss a GLMM analysis of the nodes study of Section 6.3.\\n\\nIn addition to ni the number of nodes removed and Xi the number\\n\\nfound positive (6.33), a vector of four covariates\\n\\nci D .agei , sexi , smokei , progi /\\n\\n(21.56)\\n\\nwas observed for each patient: a standardized version of age in years; sex\\nbeing 0 for female or 1 for male; smoke being 0 for no or 1 for yes to long-\\nterm smoking; and prog being a post-operative prognosis score with large\\nvalues more favorable.\\n\\nGLMM model (21.55) was applied to the nodes data. Now (cid:21)i was the\\n\\nlogit log≈í(cid:25)i =.1 (cid:0) (cid:25)i /(cid:141), where\\n\\nXi (cid:24) Bi.ni ; (cid:25)i /\\n\\n(21.57)\\n\\nas in Table 8.4, i.e., (cid:25)i is the probability that any one node from patient\\ni is positive. To make the correspondence with the analysis in Section 6.3\\nexact, we used a variant of (21.55)\\n\\nT\\n\\ni Àá:\\n\\n(cid:21)i D logit.‚Äöi / C c0\\n(21.58)\\nNow with Àá D 0, ‚Äöi is exactly the binomial probability (cid:25)i for the ith\\ncase. Maximum likelihood estimates were calculated for Àõ in (21.11)‚Äî\\n,5) (21.14)‚Äîand\\nwith\\nÀá in (21.58). The MLE prior g. OÀõ/ was almost the same as that estimated\\nwithout covariates in Figure 6.4.\\n\\nD .0:01; 0:02; : : : ; 0:99/ and Q D poly(\\nT\\n\\nTable 21.2 shows the MLE values . OÀá1; OÀá2; OÀá3; OÀá4/, their standard errors\\n(from a parametric bootstrap simulation), and the z-values OÀák=bsek. Sex\\nlooks like it has a signiÔ¨Åcant effect, with males tending toward larger values\\nof (cid:25)i , that is, a greater number of positive nodes. The big effect though is\\nprog, larger values of prog indicating smaller values of (cid:25)i .\\n\\n6 Here the setup is more speciÔ¨Åc; f is exponential family, and ‚Äöi is on the\\n\\nnatural-parameter scale.\\n\\n\\x0c21.5 Generalized Linear Mixed Models\\n\\n439\\n\\nTable 21.2 Maximum likelihood estimates . OÀá1; OÀá2; OÀá3; OÀá4/ for GLMM\\nanalysis of the nodes data, and standard errors from a parametric\\nbootstrap simulation; large values of progi predict low values of (cid:25)i .\\n\\nage\\n\\nsex\\n\\nsmoke\\n\\nprog\\n\\nMLE\\nBoot st err\\nz-value\\n\\n(cid:0).078\\n.066\\n(cid:0)1.18\\n\\n.192\\n.070\\n2.74\\n\\n.089 (cid:0).698\\n.063\\n.077\\n1.41 (cid:0)9.07\\n\\nFigure 21.10 Distribution of (cid:25)i , individual probabilities of a\\npositive node, for best and worst levels of factor prog; from\\nGLMM analysis of nodes data.\\n\\nFigure 21.10 displays the distribution of (cid:25)i D 1=≈í1 C exp.(cid:0)(cid:21)i /(cid:141) implied\\nby the GLMM model for the best and worst values of prog (setting age,\\nsex, and smoke to their average values and letting ‚Äö have distribution\\ng. OÀõ/). The implied distribution is concentrated near (cid:25) D 0 for the best-\\nlevel prog, while it is roughly uniform over ≈í0; 1(cid:141) for the worst level.\\n\\nThe random effects we have called ‚Äöi are sometimes called frailties: a\\ncomposite of unmeasured individual factors lumped together as an index\\nof disease susceptibility. Taken together, Figures 6.4 and 21.10 show sub-\\nstantial frailty and covariate effects both at work in the nodes data. In\\n\\n0.00.20.40.60.81.00.000.050.100.15Probability positive nodeDensityworst prognosisbest prognosis\\x0c440\\n\\nEmpirical Bayes Estimation Strategies\\n\\nthe language of Section 6.1, we have amassed ‚Äúindirect evidence‚Äù for each\\npatient, using both Bayesian and frequentist methods.\\n\\n21.6 Deconvolution and f -Modeling\\n\\nEmpirical Bayes applications have traditionally been dominated by f -\\nmodeling‚Äînot the g-modeling approach of the previous sections‚Äîwhere\\nprobability models for the marginal density f .x/, usually exponential fam-\\nilies, are Ô¨Åt directly to the observed sample X1; X2; : : : ; XN . We have seen\\nseveral examples: Robbins‚Äô estimator in Table 6.1 (particularly the bottom\\nline), locfdr‚Äôs Poisson regression estimates in Figures 15.6 and 21.7, and\\nTweedie‚Äôs estimate in Figure 20.7.\\n\\nBoth the advantages and the disadvantages of f -modeling can be seen in\\nthe inferential diagram of Figure 21.2. For f -modeling the red curve now\\ncan represent an exponential family ff .Àõ/g, whose concave log likelihood\\nfunction greatly simpliÔ¨Åes the calculation of f . OÀõ/ from y=N . This comes\\nat a price: the deconvolution step, from f . OÀõ/ to a prior distribution g. OÀõ/,\\nis problematical, as discussed below.\\n\\nThis is only a problem if we want to know g. The traditional applications\\nof f -modeling apply to problems where the desired answer can be phrased\\ndirectly in terms of f . This was the case for Robbins‚Äô formula (6.5), the\\nlocal false-discovery rate (15.38), and Tweedie‚Äôs formula (20.37).\\n\\nNevertheless, f -modeling methodology for the estimation of the prior\\ng.(cid:18)/ does exist, an elegant example being the Fourier method described\\nnext. A function f .x/ and its Fourier transform (cid:30).t/ are related by\\nf .x/eitx dx and f .x/ D 1\\n2(cid:25)\\n\\n(cid:30).t/e(cid:0)itx dt:\\n\\n(cid:30).t / D\\n\\nZ 1\\n\\nZ 1\\n\\n(cid:0)1\\n\\n(cid:0)1\\n\\nFor the normal case where Xi D ‚Äöi C Zi with Zi (cid:24)\\ntransform of f .x/ is a multiple of that for g.(cid:18)/,\\n\\nN\\n\\n(21.59)\\n.0; 1/, the Fourier\\n\\n(cid:30)f .t / D (cid:30)g .t/e(cid:0)t 2=2;\\n\\n(21.60)\\n\\nso, on the transform scale, estimating g from f amounts to removing the\\nfactor exp.t 2=2/.\\n\\nThe Fourier method begins with the empirical density Nf .x/ that puts\\nprobability 1=N on each observed value Xi , and then proceeds in three\\nsteps.\\n1 Nf .x/ is smoothed using the ‚Äúsinc‚Äù kernel,(cid:142)\\n\\n(cid:142)3\\n\\n\\x0c21.6 Deconvolution and f -Modeling\\n\\n441\\n\\nQf .x/ D 1\\nN (cid:21)\\n\\nN\\nX\\n\\niD1\\n\\nsinc\\n\\n(cid:18) Xi (cid:0) x\\n(cid:21)\\n\\n(cid:19)\\n\\n;\\n\\nsinc.x/ D sin.x/\\n\\nx\\n\\n:\\n\\n(21.61)\\n\\n2 The Fourier transform of\\n3 Finally, Og.(cid:18)/ is taken to be the inverse Fourier transform of Q(cid:30).t/et 2=2,\\n\\nQf , say Q(cid:30).t/, is calculated.\\n\\nthis last step eliminating the unwanted factor e(cid:0)t 2=2 in (21.60).\\n\\nA pleasantly surprising aspect of the Fourier method is that Og.(cid:18)/ can be\\n\\nexpressed directly as a kernel estimate,\\n\\nOg.(cid:18)/ D 1\\nN\\n\\nN\\nX\\n\\ni D1\\n\\nk(cid:21).Xi (cid:0) (cid:18)/ D\\n\\nZ 1\\n\\n(cid:0)1\\n\\nk(cid:21).x (cid:0) (cid:18)/ Nf .x/ dx;\\n\\n(21.62)\\n\\nwhere the kernel k(cid:21).(cid:1)/ is\\n\\nk(cid:21).x/ D 1\\n(cid:25)\\n\\nZ 1=(cid:21)\\n\\n0\\n\\net 2=2 cos.tx/ dt:\\n\\n(21.63)\\n\\nLarge values of (cid:21) smooth Nf .x/ more in (21.61), reducing the variance of\\nOg.(cid:18)/ at the expense of increased bias.\\n\\nDespite its compelling rationale, there are two drawbacks to the Fourier\\nmethod. First of all, it applies only to situations Xi D ‚Äöi C Zi where Xi is\\n‚Äöi plus iid noise. More seriously, the bias/variance trade-off in the choice\\nof (cid:21) can be quite unfavorable.\\n\\nThis is illustrated in Figure 21.11 for the artiÔ¨Åcial example of Figure 21.1.\\nThe black curve is the standard deviation of the g-modeling estimate of\\ng.(cid:18) / for (cid:18) in ≈í(cid:0)3; 3(cid:141), under speciÔ¨Åcations (21.41)‚Äì(21.42). The red curve\\ngraphs the standard deviation of the f -modeling estimate (21.62), with\\n(cid:21) D 1=3, a value that produced roughly the same amount of bias as the g-\\nmodeling estimate (seen in Figure 21.3). The ratio of red to black standard\\ndeviations averages more than 20 over the range of (cid:18).\\n\\nThis comparison is at least partly unfair: g-modeling is parametric while\\nthe Fourier method is almost nonparametric in its assumptions about f .x/\\nor g.(cid:18)/. It can be greatly improved by beginning the three-step algorithm\\nwith a parametric estimate Of .x/ rather than Nf .x/. The blue dotted curve in\\nFigure 21.11 does this with Of .x/ a Poisson regression on the data X1; X2;\\n: : : ; XN ‚Äîas in Figure 10.5 but here using a natural spline basis ns(df=5)\\n‚Äîgiving the estimate\\n\\nOg.(cid:18)/ D\\n\\nZ 1\\n\\n(cid:0)1\\n\\nk(cid:21).x (cid:0) (cid:18)/ Of .x/ dx:\\n\\n(21.64)\\n\\n\\x0c442\\n\\nEmpirical Bayes Estimation Strategies\\n\\nFigure 21.11 Standard deviations of estimated prior density Og.(cid:18)/\\nfor the artiÔ¨Åcial example of Figure 21.1, based on N D 1000\\nobservations Xi (cid:24)\\n.‚Äöi ; 1/; black curve using g-modeling\\nunder speciÔ¨Åcations (21.41)‚Äì(21.42); red curve nonparametric\\nf -modeling (21.62), (cid:21) D 1=3; blue curve parametric f -modeling\\n(21.64), with Of .x/ estimated from Poisson regression with a\\nstructure matrix having Ô¨Åve degrees of freedom.\\n\\nN\\n\\nWe see a substantial decrease in standard deviation, though still not attain-\\ning g-modeling rates.\\n\\nAs commented before, the great majority of empirical Bayes applica-\\ntions have been of the Robbins/fdr/Tweedie variety, where f -modeling\\nis the natural choice. g-modeling comes into its own for situations like\\nthe nodes data analysis of Figures 6.4 and 6.5, where we really want\\nan estimate of the prior g.(cid:18)/. Twenty-Ô¨Årst-century science is producing\\nmore such data sets, an impetus for the further development of g-modeling\\nstrategies.\\n\\nTable 21.3 concerns the g-modeling estimation of Ex D Ef‚ÄöjX D xg,\\n(cid:30) Z\\n\\nZ\\n\\nEx D\\n\\n(cid:18)g.(cid:18)/f(cid:18) .x/ d(cid:18)\\n\\ng.(cid:18)/f(cid:18) .x/ d(cid:18)\\n\\n(21.65)\\n\\nT\\n\\nT\\n\\nfor the artiÔ¨Åcial example, under the same speciÔ¨Åcations as in Figure 21.11.\\nSamples of size N D 1000 of Xi (cid:24)\\n.‚Äöi ; 1/ were drawn from model\\n(21.41)‚Äì(21.42), yielding MLE Og.(cid:18)/ and estimates OEx for x between (cid:0)4\\n\\nN\\n\\n‚àí3‚àí2‚àí101230.000.010.020.030.040.05qsd g^(q)g‚àímodelparametricf‚àímodelnon‚àíparametricf‚àímodel\\x0c21.6 Deconvolution and f -Modeling\\n\\n443\\n\\nTable 21.3 Standard deviation of OEf‚Äöjxg computed from parametric\\nbootstrap simulations of Og.(cid:18)/. The g-modeling is as in Figure 21.11, with\\nN D 1000 observations Xi (cid:24)\\nN\\neach simulation. The column ‚Äúinfo‚Äù is the implied empirical Bayes\\ninformation for estimating Ef‚Äöjxg obtained from one ‚Äúother‚Äù\\nobservation Xi .\\n\\n.‚Äöi ; 1/ from the artiÔ¨Åcial example for\\n\\nx Ef‚Äöjxg\\n\\nsd. OE/\\n\\ninfo\\n\\n(cid:0)3:5\\n(cid:0)2:5\\n(cid:0)1:5\\n(cid:0):5\\n.5\\n1.5\\n2.5\\n3.5\\n\\n(cid:0)2:00\\n(cid:0)1:06\\n(cid:0):44\\n(cid:0):13\\n.13\\n.44\\n1.06\\n2.00\\n\\n.10\\n.10\\n.05\\n.03\\n.04\\n.05\\n.10\\n.16\\n\\n.11\\n.11\\n.47\\n.89\\n.80\\n.44\\n.10\\n.04\\n\\nand 4. One thousand such estimates OEx were generated, averaging almost\\nexactly Ex, with standard deviations as shown. Accuracy is reasonably\\ngood, the coefÔ¨Åcient of variation sd. OEx/=Ex being about 0.05 for large\\nvalues of jxj. (Estimate (21.65) is a favorable case: results are worse for\\nother conditional estimates(cid:142) such as Ef‚Äö2jX D xg.)\\n\\n(cid:142)4\\n\\nTheorem 21.4 (page 429) implies that, for large values of the sample\\n\\nsize N , the variance of OEx decreases as 1=N , say\\n\\nn OEx\\n\\no :D cx=N:\\n\\nvar\\n\\n(21.66)\\n\\nBy analogy with the Fisher information bound (5.27), we can deÔ¨Åne the\\nempirical Bayes information for estimating Ex in one observation to be\\n\\nix D 1\\n\\n. (cid:16)\\n\\nN (cid:1) var\\n\\no(cid:17)\\n\\nn OEx\\n\\n;\\n\\n(21.67)\\n\\nso that varf OExg :D i (cid:0)1\\n\\nx =N .\\n\\nEmpirical Bayes inference leads us directly into the world of indirect\\nevidence, learning from the experience of others as in Sections 6.4 and\\n7.4. So, if Xi D 2:5, each ‚Äú other‚Äù observation Xj provides 0.10 units of\\ninformation for learning Ef‚ÄöjXi D 2:5g (compared with the usual Fisher\\nD 1 for the direct estimation of ‚Äöi from Xi ). This\\ninformation value\\nis a favorable case, as mentioned, and ix is often much smaller. The main\\npoint, perhaps, is that assuming a Bayes prior is not a casual matter, and\\n\\nI\\n\\n\\x0c444\\n\\nEmpirical Bayes Estimation Strategies\\n\\ncan amount to the assumption of an enormous amount of relevant other\\ninformation.\\n\\n21.7 Notes and Details\\n\\nEmpirical Bayes and James‚ÄìStein estimation, Chapters 6 and 7, exploded\\nonto the statistics scene almost simultaneously in the 1950s. They repre-\\nsented a genuinely new branch of statistical inference, unlike the computer-\\nbased extensions of classical methodology reviewed in previous chapters.\\nTheir development as practical tools has been comparatively slow. The\\npace has quickened in the twenty-Ô¨Årst century, with false-discovery rates,\\nChapter 15, as a major step forward. A practical empirical Bayes method-\\nology for use beyond traditional f -modeling venues such as fdr is the goal\\nof the g-modeling approach.\\n\\n(cid:142)1 [p. 428] Lemmas 21.1 and 21.2. The derivations of Lemmas 21.1 and 21.2\\nare straightforward but somewhat involved exercises in differential calcu-\\nlus, carried out in Remark B of Efron (2016). Here we will present just\\nPfk.Àõ/ D\\na sample of the calculations. From (21.18), the gradient vector\\n.@fk.Àõ/=@Àõl / with respect to Àõ is\\n\\nPfk.Àõ/ D Pg.Àõ/0Pk;\\n\\n(21.68)\\n\\nwhere Pg.Àõ/ is the m (cid:2) p derivative matrix\\n\\nPg.Àõ/ D .@gj .Àõ/=@Àõl / D DQ;\\n\\n(21.69)\\n\\nwith D as in (21.36), the last equality following, after some work, by dif-\\nferentiation of log g.Àõ/ D QÀõ (cid:0) (cid:30).Àõ/.\\n\\nLet lk D log fk (now suppressing Àõ from the notation). The gradient\\n\\nwith respect to Àõ of lk is then\\n\\nPlk D Pfk=fk D Q0DPk=fk:\\n\\n(21.70)\\n\\nThe vector DPk=fk has components\\n\\n.gj pkj (cid:0) gj fk/=fk D wkj\\n(21.71)\\n(21.27), using g0Pk D fk. This gives Plk D Q0Wk.Àõ/ (21.28). Adding up\\nthe independent score functions Plk over the full sample yields the overall\\nscore Ply .Àõ/ D Q0 Pn\\n1 ykWk.Àõ/, which is Lemma 21.1.\\n\\n(cid:142)2 [p. 428] Lemma 21.3. The penalized MLE OÀõ satisÔ¨Åes\\n\\nO D Pm. OÀõ/\\n\\n:D Pm.Àõ0/ C Rm.Àõ0/. OÀõ (cid:0) Àõ0/;\\n\\n(21.72)\\n\\n\\x0c21.7 Notes and Details\\n\\nwhere Àõ0 is the true value of Àõ, or\\n(cid:16)\\n(cid:0) Rly .Àõ0/ C Rs.Àõ0/\\n\\n:D .(cid:0) Rm.Àõ0//(cid:0)1 Pm.Àõ0/\\n\\nOÀõ (cid:0) Àõ0\\n\\n(cid:17)(cid:0)1 (cid:16) Ply .Àõ0/ (cid:0) Ps.Àõ0/\\n\\n445\\n\\n(cid:17)\\n\\n:\\n\\n(21.73)\\nStandard MLE theory shows that the random variable Ply .Àõ0/ has mean 0\\nand covariance Fisher information matrix I.Àõ0/, while (cid:0) Rly .Àõ0/ asymptot-\\nically approximates I.Àõ0/. Substituting in (21.73),\\nOÀõ (cid:0) Àõ0\\n\\n:D .I.Àõ0/ C Rs.Àõ0//(cid:0)1Z;\\n\\n(21.74)\\n\\nwhere Z has mean (cid:0)Ps.Àõ0/ and covariance I.Àõ0/. This gives Bias. OÀõ/ and\\nVar. OÀõ/ as in Lemma 21.3. Note that the bias is with respect to a true para-\\nmetric model (21.11), and is a consequence of the penalization.\\n\\n(cid:142)3 [p. 440] The sinc kernel. The Fourier transform (cid:30)s.t/ of the scaled sinc\\nfunction s.x/ D sin.x=(cid:21)/=.(cid:25)x/ is the indicator of the interval ≈í(cid:0)1=(cid:21); 1=(cid:21)(cid:141),\\nNf .x/ is .1=N / PN\\n1 exp.itXj /. Formula (21.61) is the convo-\\nwhile that of\\nlution Nf (cid:3) s, so Qf has the product transform\\n3\\n\\n2\\n\\n(cid:30) Qf .t / D\\n\\n4\\n\\neitXj\\n\\n5 I≈í(cid:0)1=(cid:21);1=(cid:21)(cid:141).t/:\\n\\n(21.75)\\n\\n1\\nN\\n\\nN\\nX\\n\\nj D1\\n\\nThe effect of the sinc convolution is to censor the high-frequency (large t)\\nNf or (cid:30) Nf . Larger (cid:21) yields more censoring. Formula (21.63)\\ncomponents of\\nhas upper limits 1=(cid:21) because of (cid:30)s.t/. All of this is due to Stefanski and\\nCarroll (1990). Smoothers other than the sinc kernel have been suggested\\nin the literature, but without substantial improvements on deconvolution\\nperformance.\\n\\n(cid:142)4 [p. 443] Conditional expectation (21.65). Efron (2014b) considers estimat-\\ning Ef‚Äö2jX D xg and other such conditional expectations, both for f -\\nmodeling and for g-modeling. Ef‚ÄöjX D xg is by far the easiest case, as\\nmight be expected from the simple form of Tweedie‚Äôs estimate (20.37).\\n\\n\\x0cEpilogue\\n\\nSomething important changed in the world of statistics in the new millen-\\nnium. Twentieth-century statistics, even after the heated expansion of its\\nlate period, could still be contained within the classic Bayesian‚Äìfrequentist‚Äì\\nFisherian inferential triangle (Figure 14.1). This is not so in the twenty-Ô¨Årst\\ncentury. Some of the topics discussed in Part III‚Äîfalse-discovery rates,\\npost-selection inference, empirical Bayes modeling, the lasso‚ÄîÔ¨Åt within\\nthe triangle but others seem to have escaped, heading south from the fre-\\nquentist corner, perhaps in the direction of computer science.\\n\\nThe escapees were the large-scale prediction algorithms of Chapters 17‚Äì\\n19: neural nets, deep learning, boosting, random forests, and support-vector\\nmachines. Notably missing from their development were parametric prob-\\nability models, the building blocks of classical inference. Prediction algo-\\nrithms are the media stars of the big-data era. It is worth asking why they\\nhave taken center stage and what it means for the future of the statistics\\ndiscipline.\\n\\nThe why is easy enough: prediction is commercially valuable. Modern\\nequipment has enabled the collection of mountainous data troves, which\\nthe ‚Äúdata miners‚Äù can then burrow into, extracting valuable information.\\nMoreover, prediction is the simplest use of regression theory (Section 8.4).\\nIt can be carried out successfully without probability models, perhaps with\\nthe assistance of nonparametric analysis tools such as cross-validation, per-\\nmutations, and the bootstrap.\\n\\nA great amount of ingenuity and experimentation has gone into the\\ndevelopment of modern prediction algorithms, with statisticians playing\\nan important but not dominant role.1 There is no shortage of impressive\\nsuccess stories. In the absence of optimality criteria, either frequentist or\\nBayesian, the prediction community grades algorithmic excellence on per-\\n\\n1 All papers mentioned in this section have their complete references in the bibliography.\\n\\nFootnotes will identify papers not fully speciÔ¨Åed in the text.\\n\\n446\\n\\n\\x0cEpilogue\\n\\n447\\n\\nformance within a catalog of often-visited examples such as the spam and\\ndigits data sets of Chapters 17 and 18.2 Meanwhile, ‚Äútraditional statistics‚Äù\\n‚Äîprobability models, optimality criteria, Bayes priors, asymptotics‚Äîhas\\ncontinued successfully along on a parallel track. Pessimistically or opti-\\nmistically, one can consider this as a bipolar disorder of the Ô¨Åeld or as a\\nhealthy duality that is bound to improve both branches. There are histori-\\ncal and intellectual arguments favoring the optimists‚Äô side of the story.\\n\\nThe Ô¨Årst thing to say is that the current situation is not entirely unprece-\\ndented. By the end of the nineteenth century there was available an im-\\npressive inventory of statistical methods‚ÄîBayes‚Äô theorem, least squares,\\ncorrelation, regression, the multivariate normal distribution‚Äîbut these ex-\\nisted more as individual algorithms than as a uniÔ¨Åed discipline. Statistics\\nas a distinct intellectual enterprise was not yet well-formed.\\n\\nA small but crucial step forward was taken in 1914 when the astrophysi-\\ncist Arthur Eddington3 claimed that mean absolute deviation was superior\\nto the familiar root mean square estimate for the standard deviation from a\\nnormal sample. Fisher in 1919 showed that this was wrong, and moreover,\\nin a clear mathematical sense, the root mean square was the best possible\\nestimate. Eddington conceded the point while Fisher went on to develop\\nthe theory of sufÔ¨Åciency and optimal estimation.4\\n\\n‚ÄúOptimal‚Äù is the key word here. Before Fisher, statisticians didn‚Äôt really\\nunderstand estimation. The same can be said now about prediction. Despite\\ntheir impressive performance on a raft of test problems, it might still be\\npossible to do much better than neural nets, deep learning, random forests,\\nand boosting‚Äîor perhaps they are coming close to some as-yet unknown\\ntheoretical minimum.\\n\\nIt is the job of statistical inference to connect ‚Äúdangling algorithms‚Äù to\\nthe central core of well-understood methodology. The connection process\\nis already underway. Section 17.4 showed how Adaboost, the original\\nmachine learning algorithm, could be restated as a close cousin of logis-\\ntic regression. Purely empirical approaches like the Common Task Frame-\\nwork are ultimately unsatisfying without some form of principled justi-\\nÔ¨Åcation. Our optimistic scenario has the big-data/data-science prediction\\nworld rejoining the mainstream of statistical inference, to the beneÔ¨Åt of\\nboth branches.\\n\\n2 This empirical approach to optimality is sometimes codiÔ¨Åed as the Common Task\\n\\nFramework (Liberman, 2015 and Donoho, 2015).\\n\\n3 Eddington became world-famous for his 1919 empirical veriÔ¨Åcation of Einstein‚Äôs\\n\\nrelativity theory.\\n\\n4 See Stigler (2006) for the full story.\\n\\n\\x0c448\\n\\nEpilogue\\n\\nDevelopment of the statistics discipline since the end of the nine-\\nteenth century, as discussed in the text.\\n\\nWhether or not we can predict the future of statistics, we can at least\\nexamine the past to see how we‚Äôve gotten where we are. The next Ô¨Ågure\\ndoes so in terms of a new triangle diagram, this time with the poles la-\\nbeled Applications, Mathematics, and Computation. ‚ÄúMathematics‚Äù here\\nis shorthand for the mathematical/logical justiÔ¨Åcation of statistical meth-\\nods. ‚ÄúComputation‚Äù stands for the empirical/numerical approach.\\n\\nStatistics is a branch of applied mathematics, and is ultimately judged\\nby how well it serves the world of applications. Mathematical logic, `a\\nla Fisher, has been the traditional vehicle for the development and under-\\nstanding of statistical methods. Computation, slow and difÔ¨Åcult before the\\n1950s, was only a bottleneck, but now has emerged as a competitor to (or\\nperhaps a seven-league boots enabler of) mathematical analysis. At any\\none time the discipline‚Äôs energy and excitement is directed unequally to-\\nward the three poles. The Ô¨Ågure attempts, in admittedly crude fashion, to\\ntrack the changes in direction over the past 100C years.\\n\\n lllMathematicsComputationApplicationsl19th Century1900190819251933193719501962197219791995200020012016b2016a\\x0cEpilogue\\n\\n449\\n\\nThe tour begins at the end of the nineteenth century. Mathematicians of\\nthe caliber of Gauss and Laplace had contributed to the available method-\\nology, but the subsequent development was almost entirely applications-\\ndriven. Quetelet5 was especially inÔ¨Çuential, applying the Gauss‚ÄìLaplace\\nformulation to census data and his ‚ÄúAverage Man.‚Äù A modern reader will\\nsearch almost in vain for any mathematical symbology in nineteenth-century\\nstatistics journals.\\n\\n1900\\nKarl Pearson‚Äôs chi-square paper was a bold step into the new century, ap-\\nplying a new mathematical tool, matrix theory, in the service of statisti-\\ncal methodology. He and Weldon went on to found Biometrika in 1901,\\nthe Ô¨Årst recognizably modern statistics journal. Pearson‚Äôs paper, and Bio-\\nmetrika, launched the statistics discipline on a Ô¨Åfty-year march toward the\\nmathematics pole of the triangle.\\n\\n1908\\nStudent‚Äôs t statistic was a crucial Ô¨Årst result in small-sample ‚Äúexact‚Äù infer-\\nence, and a major inÔ¨Çuence on Fisher‚Äôs thinking.\\n\\n1925\\nFisher‚Äôs great estimation paper‚Äîa more coherent version of its 1922 pre-\\ndecessor. It introduced a host of fundamental ideas, including sufÔ¨Åciency,\\nefÔ¨Åciency, Fisher information, maximum likelihood theory, and the notion\\nof optimal estimation. Optimality is a mark of maturity in mathematics,\\nmaking 1925 the year statistical inference went from a collection of inge-\\nnious techniques to a coherent discipline.\\n\\n1933\\nThis represents Neyman and Pearson‚Äôs paper on optimal hypothesis test-\\ning. A logical completion of Fisher‚Äôs program, it nevertheless aroused his\\nstrong antipathy. This was partly personal, but also reÔ¨Çected Fisher‚Äôs con-\\ncern that mathematization was squeezing intuitive correctness out of statis-\\ntical thinking (Section 4.2).\\n\\n1937\\nNeyman‚Äôs seminal paper on conÔ¨Ådence intervals. His sophisticated mathe-\\nmatical treatment of statistical inference was a harbinger of decision theory.\\n\\n5 Adolphe Quetelet was a tireless organizer, helping found the Royal Statistical Society in\\n\\n1834, with the American Statistical Association following in 1839.\\n\\n\\x0c450\\n\\nEpilogue\\n\\n1950\\nThe publication of Wald‚Äôs Statistical Decision Functions. Decision theory\\ncompleted the full mathematization of statistical inference. This date can\\nalso stand for Savage‚Äôs and de Finetti‚Äôs decision-theoretic formulation of\\nBayesian inference. We are as far as possible from the Applications corner\\nof the triangle now, and it is fair to describe the 1950s as a nadir of the\\ninÔ¨Çuence of the statistics discipline on scientiÔ¨Åc applications.\\n\\n1962\\n\\nThe arrival of electronic computation in the mid 1950s began the process\\nof stirring statistics out of its inward-gazing preoccupation with mathe-\\nmatical structure. Tukey‚Äôs paper ‚ÄúThe future of data analysis‚Äù argued for\\na more application- and computation-oriented discipline. Mosteller and\\nTukey later suggested changing the Ô¨Åeld‚Äôs name to data analysis, a pre-\\nscient hint of today‚Äôs data science.\\n\\n1972\\n\\nCox‚Äôs proportional hazards paper. Immensely useful in its own right, it sig-\\nnaled a growing interest in biostatistical applications and particularly sur-\\nvival analysis, which was to assert its scientiÔ¨Åc importance in the analysis\\nof AIDS epidemic data.\\n\\nThe bootstrap, and later the widespread use of MCMC: electronic compu-\\ntation used for the extension of classic statistical inference.\\n\\n1979\\n\\n1995\\nThis stands for false-discovery rates and, a year later, the lasso.6 Both are\\ncomputer-intensive algorithms, Ô¨Årmly rooted in the ethos of statistical in-\\nference. They lead, however, in different directions, as indicated by the\\nsplit in the diagram.\\n\\nMicroarray technology inspires enormous interest in large-scale inference,\\nboth in theory and as applied to the analysis of microbiological data.\\n\\n2000\\n\\n6 Benjamini and Hochberg (1995) and Tibshirani (1996).\\n\\n\\x0cEpilogue\\n\\n451\\n\\n2001\\nRandom forests; it joins boosting7 and the resurgence of neural nets in the\\nranks of machine learning prediction algorithms.\\n\\n2016a\\n\\nData science: a more popular successor to Tukey and Mosteller‚Äôs ‚Äúdata\\nanalysis,‚Äù at one extreme it seems to represent a statistics discipline with-\\nout parametric probability models or formal inference. The Data Science\\nAssociation deÔ¨Ånes a practitioner as one who ‚Äú. . . uses scientiÔ¨Åc methods\\nto liberate and create meaning from raw data.‚Äù In practice the emphasis is\\non the algorithmic processing of large data sets for the extraction of useful\\ninformation, with the prediction algorithms as exemplars.\\n\\n2016b\\n\\nThis represents the traditional line of statistical thinking, of the kind that\\ncould be located within Figure 14.1, but now energized with a renewed\\nfocus on applications. Of particular applied interest are biology and ge-\\nnetics. Genome-wide association studies (GWAS) show a different face of\\nbig data. Prediction is important here,8 but not sufÔ¨Åcient for the scientiÔ¨Åc\\nunderstanding of disease.\\n\\nA cohesive inferential theory was forged in the Ô¨Årst half of the twenti-\\neth century, but unity came at the price of an inwardly focused discipline,\\nof reduced practical utility. In the century‚Äôs second half, electronic com-\\nputation unleashed a vast expansion of useful‚Äîand much used‚Äîstatistical\\nmethodology. Expansion accelerated at the turn of the millennium, further\\nincreasing the reach of statistical thinking, but now at the price of intellec-\\ntual cohesion.\\n\\nIt is tempting but risky to speculate on the future of statistics. What\\nwill the Mathematics‚ÄìApplications‚ÄìComputation diagram look like, say\\n25 years from now? The appetite for statistical analysis seems to be always\\nincreasing, both from science and from society in general. Data science\\nhas blossomed in response, but so has the traditional wing of the Ô¨Åeld. The\\ndata-analytic initiatives represented in the diagram by 2016a and 2016b are\\nin actuality not isolated points but the centers of overlapping distributions.\\n\\n7 Breiman (1996) for random forests, Freund and Schapire (1997) for boosting.\\n8 ‚ÄúPersonalized medicine‚Äù in which an individual‚Äôs genome predicts his or her optimal\\n\\ntreatment has attracted grail-like attention.\\n\\n\\x0c452\\n\\nEpilogue\\n\\nA hopeful scenario for the future is one of an increasing overlap that puts\\ndata science on a solid footing while leading to a broader general formula-\\ntion of statistical inference.\\n\\n\\x0cReferences\\n\\nAbu-Mostafa, Y. 1995. Hints. Neural Computation, 7, 639‚Äì671.\\nAchanta, R., and Hastie, T. 2015. Telugu OCR Framework using Deep Learning. Tech.\\n\\nrept. Statistics Department, Stanford University.\\n\\nAkaike, H. 1973. Information theory and an extension of the maximum likelihood prin-\\nciple. Pages 267‚Äì281 of: Second International Symposium on Information Theory\\n(Tsahkadsor, 1971). Akad¬¥emiai Kiad¬¥o, Budapest.\\n\\nAnderson, T. W. 2003. An Introduction to Multivariate Statistical Analysis. Third edn.\\n\\nWiley Series in Probability and Statistics. Wiley-Interscience.\\n\\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A.,\\nBouchard, N., and Bengio, Y. 2012. Theano: new features and speed improvements.\\nDeep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.\\n\\nBecker, R., Chambers, J., and Wilks, A. 1988. The New S Language: A Programming\\nEnvironment for Data Analysis and Graphics. PaciÔ¨Åc Grove, CA: Wadsworth and\\nBrooks/Cole.\\n\\nBellhouse, D. R. 2004. The Reverend Thomas Bayes, FRS: A biography to celebrate the\\ntercentenary of his birth. Statist. Sci., 19(1), 3‚Äì43. With comments and a rejoinder\\nby the author.\\n\\nBengio, Y., Courville, A., and Vincent, P. 2013. Representation learning: a review and\\nnew perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence,\\n35(8), 1798‚Äì1828.\\n\\nBenjamini, Y., and Hochberg, Y. 1995. Controlling the false discovery rate: A practical\\nand powerful approach to multiple testing. J. Roy. Statist. Soc. Ser. B, 57(1), 289‚Äì\\n300.\\n\\nBenjamini, Y., and Yekutieli, D. 2005. False discovery rate-adjusted multiple conÔ¨Å-\\ndence intervals for selected parameters. J. Amer. Statist. Assoc., 100(469), 71‚Äì93.\\nBerger, J. O. 2006. The case for objective Bayesian analysis. Bayesian Anal., 1(3),\\n\\n385‚Äì402 (electronic).\\n\\nBerger, J. O., and Pericchi, L. R. 1996. The intrinsic Bayes factor for model selection\\n\\nand prediction. J. Amer. Statist. Assoc., 91(433), 109‚Äì122.\\n\\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian,\\nJ., Warde-Farley, D., and Bengio, Y. 2010 (June). Theano: a CPU and GPU math\\nexpression compiler. In: Proceedings of the Python for ScientiÔ¨Åc Computing Con-\\nference (SciPy).\\n\\nBerk, R., Brown, L., Buja, A., Zhang, K., and Zhao, L. 2013. Valid post-selection\\n\\ninference. Ann. Statist., 41(2), 802‚Äì837.\\n\\n453\\n\\n\\x0c454\\n\\nReferences\\n\\nBerkson, J. 1944. Application of the logistic function to bio-assay. J. Amer. Statist.\\n\\nAssoc., 39(227), 357‚Äì365.\\n\\nBernardo, J. M. 1979. Reference posterior distributions for Bayesian inference. J. Roy.\\n\\nStatist. Soc. Ser. B, 41(2), 113‚Äì147. With discussion.\\n\\nBirch, M. W. 1964. The detection of partial association. I. The 2(cid:2)2 case. J. Roy. Statist.\\n\\nSoc. Ser. B, 26(2), 313‚Äì324.\\n\\nBishop, C. 1995. Neural Networks for Pattern Recognition. Clarendon Press, Oxford.\\nBoos, D. D., and SerÔ¨Çing, R. J. 1980. A note on differentials and the CLT and LIL for\\nstatistical functions, with application to M -estimates. Ann. Statist., 8(3), 618‚Äì624.\\nBoser, B., Guyon, I., and Vapnik, V. 1992. A training algorithm for optimal margin\\n\\nclassiÔ¨Åers. In: Proceedings of COLT II.\\n\\nBreiman, L. 1996. Bagging predictors. Mach. Learn., 24(2), 123‚Äì140.\\nBreiman, L. 1998. Arcing classiÔ¨Åers (with discussion). Annals of Statistics, 26, 801‚Äì\\n\\n849.\\n\\nBreiman, L. 2001. Random forests. Machine Learning, 45, 5‚Äì32.\\nBreiman, L., Friedman, J., Olshen, R. A., and Stone, C. J. 1984. ClassiÔ¨Åcation and\\nRegression Trees. Wadsworth Statistics/Probability Series. Wadsworth Advanced\\nBooks and Software.\\n\\nCarlin, B. P., and Louis, T. A. 1996. Bayes and Empirical Bayes Methods for Data\\nAnalysis. Monographs on Statistics and Applied Probability, vol. 69. Chapman &\\nHall.\\n\\nCarlin, B. P., and Louis, T. A. 2000. Bayes and Empirical Bayes Methods for Data\\n\\nAnalysis. 2 edn. Texts in Statistical Science. Chapman & Hall/CRC.\\n\\nChambers, J. M., and Hastie, T. J. (eds). 1993. Statistical Models in S. Chapman &\\n\\nHall Computer Science Series. Chapman & Hall.\\n\\nCleveland, W. S. 1981. LOWESS: A program for smoothing scatterplots by robust\\n\\nlocally weighted regression. Amer. Statist., 35(1), 54.\\n\\nCox, D. R. 1958. The regression analysis of binary sequences. J. Roy. Statist. Soc. Ser.\\n\\nB, 20, 215‚Äì242.\\n\\nCox, D. R. 1970. The Analysis of Binary Data. Methuen‚Äôs Monographs on Applied\\n\\nProbability and Statistics. Methuen & Co.\\n\\nCox, D. R. 1972. Regression models and life-tables. J. Roy. Statist. Soc. Ser. B, 34(2),\\n\\n187‚Äì220.\\n\\nCox, D. R. 1975. Partial likelihood. Biometrika, 62(2), 269‚Äì276.\\nCox, D. R., and Hinkley, D. V. 1974. Theoretical Statistics. Chapman & Hall.\\nCox, D. R., and Reid, N. 1987. Parameter orthogonality and approximate conditional\\n\\ninference. J. Roy. Statist. Soc. Ser. B, 49(1), 1‚Äì39. With a discussion.\\n\\nCrowley, J. 1974. Asymptotic normality of a new nonparametric statistic for use in\\n\\norgan transplant studies. J. Amer. Statist. Assoc., 69(348), 1006‚Äì1011.\\n\\nde Finetti, B. 1972. Probability, Induction and Statistics. The Art of Guessing. John\\n\\nWiley & Sons, London-New York-Sydney.\\n\\nDembo, A., Cover, T. M., and Thomas, J. A. 1991. Information-theoretic inequalities.\\n\\nIEEE Trans. Inform. Theory, 37(6), 1501‚Äì1518.\\n\\nDempster, A. P., Laird, N. M., and Rubin, D. B. 1977. Maximum likelihood from\\nincomplete data via the EM algorithm. J. Roy. Statist. Soc. Ser. B, 39(1), 1‚Äì38.\\nDiaconis, P., and Ylvisaker, D. 1979. Conjugate priors for exponential families. Ann.\\n\\nStatist., 7(2), 269‚Äì281.\\n\\n\\x0cReferences\\n\\n455\\n\\nDiCiccio, T., and Efron, B. 1992. More accurate conÔ¨Ådence intervals in exponential\\n\\nfamilies. Biometrika, 79(2), 231‚Äì245.\\n\\nDonoho, D. L. 2015. 50 years of data science. R-bloggers. www.r-bloggers.\\n\\ncom/50-years-of-data-science-by-david-donoho/.\\n\\nEdwards, A. W. F. 1992. Likelihood. Expanded edn. Johns Hopkins University Press.\\n\\nRevised reprint of the 1972 original.\\n\\nEfron, B. 1967. The two sample problem with censored data. Pages 831‚Äì853 of: Proc.\\n5th Berkeley Symp. Math. Statist. and Prob., Vol. 4. University of California Press.\\nEfron, B. 1975. DeÔ¨Åning the curvature of a statistical problem (with applications to\\nsecond order efÔ¨Åciency). Ann. Statist., 3(6), 1189‚Äì1242. With discussion and a\\nreply by the author.\\n\\nEfron, B. 1977. The efÔ¨Åciency of Cox‚Äôs likelihood function for censored data. J. Amer.\\n\\nStatist. Assoc., 72(359), 557‚Äì565.\\n\\nEfron, B. 1979. Bootstrap methods: Another look at the jackknife. Ann. Statist., 7(1),\\n\\n1‚Äì26.\\n\\nEfron, B. 1982. The Jackknife, the Bootstrap and Other Resampling Plans. CBMS-NSF\\nRegional Conference Series in Applied Mathematics, vol. 38. Society for Industrial\\nand Applied Mathematics (SIAM).\\n\\nEfron, B. 1983. Estimating the error rate of a prediction rule: Improvement on cross-\\n\\nvalidation. J. Amer. Statist. Assoc., 78(382), 316‚Äì331.\\n\\nEfron, B. 1985. Bootstrap conÔ¨Ådence intervals for a class of parametric problems.\\n\\nBiometrika, 72(1), 45‚Äì58.\\n\\nEfron, B. 1986. How biased is the apparent error rate of a prediction rule? J. Amer.\\n\\nStatist. Assoc., 81(394), 461‚Äì470.\\n\\nEfron, B. 1987. Better bootstrap conÔ¨Ådence intervals. J. Amer. Statist. Assoc., 82(397),\\n\\n171‚Äì200. With comments and a rejoinder by the author.\\n\\nEfron, B. 1988. Logistic regression, survival analysis, and the Kaplan‚ÄìMeier curve. J.\\n\\nAmer. Statist. Assoc., 83(402), 414‚Äì425.\\n\\nEfron, B. 1993.\\n\\nBayes and likelihood calculations from conÔ¨Ådence intervals.\\n\\nBiometrika, 80(1), 3‚Äì26.\\n\\nEfron, B. 1998. R. A. Fisher in the 21st Century (invited paper presented at the 1996\\nR. A. Fisher Lecture). Statist. Sci., 13(2), 95‚Äì122. With comments and a rejoinder\\nby the author.\\n\\nEfron, B. 2004. The estimation of prediction error: Covariance penalties and cross-\\nvalidation. J. Amer. Statist. Assoc., 99(467), 619‚Äì642. With comments and a rejoin-\\nder by the author.\\n\\nEfron, B. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Test-\\ning, and Prediction. Institute of Mathematical Statistics Monographs, vol. 1. Cam-\\nbridge University Press.\\n\\nEfron, B. 2011. Tweedie‚Äôs formula and selection bias. J. Amer. Statist. Assoc., 106(496),\\n\\n1602‚Äì1614.\\n\\nEfron, B. 2014a. Estimation and accuracy after model selection. J. Amer. Statist. Assoc.,\\n\\n109(507), 991‚Äì1007.\\n\\nEfron, B. 2014b. Two modeling strategies for empirical Bayes estimation. Statist. Sci.,\\n\\n29(2), 285‚Äì301.\\n\\nEfron, B. 2015. Frequentist accuracy of Bayesian estimates. J. Roy. Statist. Soc. Ser. B,\\n\\n77(3), 617‚Äì646.\\n\\n\\x0c456\\n\\nReferences\\n\\nEfron, B. 2016. Empirical Bayes deconvolution estimates. Biometrika, 103(1), 1‚Äì20.\\nEfron, B., and Feldman, D. 1991. Compliance as an explanatory variable in clinical\\n\\ntrials. J. Amer. Statist. Assoc., 86(413), 9‚Äì17.\\n\\nEfron, B., and Gous, A. 2001. Scales of evidence for model selection: Fisher versus\\nJeffreys. Pages 208‚Äì256 of: Model Selection. IMS Lecture Notes Monograph Series,\\nvol. 38. Beachwood, OH: Institute of Mathematics and Statististics. With discussion\\nand a rejoinder by the authors.\\n\\nEfron, B., and Hinkley, D. V. 1978. Assessing the accuracy of the maximum likelihood\\nestimator: Observed versus expected Fisher information. Biometrika, 65(3), 457‚Äì\\n487. With comments and a reply by the authors.\\n\\nEfron, B., and Morris, C. 1972. Limiting the risk of Bayes and empirical Bayes estima-\\n\\ntors. II. The empirical Bayes case. J. Amer. Statist. Assoc., 67, 130‚Äì139.\\n\\nEfron, B., and Morris, C. 1977. Stein‚Äôs paradox in statistics. ScientiÔ¨Åc American,\\n\\n236(5), 119‚Äì127.\\n\\nEfron, B., and Petrosian, V. 1992. A simple test of independence for truncated data\\n\\nwith applications to redshift surveys. Astrophys. J., 399(Nov), 345‚Äì352.\\n\\nEfron, B., and Stein, C. 1981. The jackknife estimate of variance. Ann. Statist., 9(3),\\n\\n586‚Äì596.\\n\\nEfron, B., and Thisted, R. 1976. Estimating the number of unseen species: How many\\n\\nwords did Shakespeare know? Biometrika, 63(3), 435‚Äì447.\\n\\nEfron, B., and Tibshirani, R. 1993. An Introduction to the Bootstrap. Monographs on\\n\\nStatistics and Applied Probability, vol. 57. Chapman & Hall.\\n\\nEfron, B., and Tibshirani, R. 1997. Improvements on cross-validation: The .632+ boot-\\n\\nstrap method. J. Amer. Statist. Assoc., 92(438), 548‚Äì560.\\n\\nEfron, B., Hastie, T., Johnstone, I., and Tibshirani, R. 2004. Least angle regression. An-\\nnals of Statistics, 32(2), 407‚Äì499. (with discussion, and a rejoinder by the authors).\\nFinney, D. J. 1947. The estimation from individual records of the relationship between\\n\\ndose and quantal response. Biometrika, 34(3/4), 320‚Äì334.\\n\\nFisher, R. A. 1915. Frequency distribution of the values of the correlation coefÔ¨Åcient in\\n\\nsamples from an indeÔ¨Ånitely large population. Biometrika, 10(4), 507‚Äì521.\\n\\nFisher, R. A. 1925. Theory of statistical estimation. Math. Proc. Cambridge Phil. Soc.,\\n\\n22(7), 700‚Äì725.\\nFisher, R. A. 1930.\\n\\n528‚Äì535.\\n\\nInverse probability. Math. Proc. Cambridge Phil. Soc., 26(10),\\n\\nFisher, R. A., Corbet, A., and Williams, C. 1943. The relation between the number of\\nspecies and the number of individuals in a random sample of an animal population.\\nJ. Anim. Ecol., 12, 42‚Äì58.\\n\\nFithian, W., Sun, D., and Taylor, J. 2014. Optimal inference after model selection.\\n\\nArXiv e-prints, Oct.\\n\\nFreund, Y., and Schapire, R. 1996. Experiments with a new boosting algorithm. Pages\\n148‚Äì156 of: Machine Learning: Proceedings of the Thirteenth International Con-\\nference. Morgan Kauffman, San Francisco.\\n\\nFreund, Y., and Schapire, R. 1997. A decision-theoretic generalization of online learn-\\ning and an application to boosting. Journal of Computer and System Sciences, 55,\\n119‚Äì139.\\n\\nFriedman, J. 2001. Greedy function approximation: a gradient boosting machine. An-\\n\\nnals of Statistics, 29(5), 1189‚Äì1232.\\n\\n\\x0cReferences\\n\\n457\\n\\nFriedman, J., and Popescu, B. 2005. Predictive Learning via Rule Ensembles. Tech.\\n\\nrept. Stanford University.\\n\\nFriedman, J., Hastie, T., and Tibshirani, R. 2000. Additive logistic regression: a statis-\\n\\ntical view of boosting (with discussion). Annals of Statistics, 28, 337‚Äì307.\\n\\nFriedman, J., Hastie, T., and Tibshirani, R. 2009. glmnet: Lasso and elastic-net regu-\\n\\nlarized generalized linear models. R package version 1.1-4.\\n\\nFriedman, J., Hastie, T., and Tibshirani, R. 2010. Regularization paths for generalized\\nlinear models via coordinate descent. Journal of Statistical Software, 33(1), 1‚Äì22.\\nGeisser, S. 1974. A predictive approach to the random effect model. Biometrika, 61,\\n\\n101‚Äì107.\\n\\nGerber, M., and Chopin, N. 2015. Sequential quasi Monte Carlo. J. Roy. Statist. Soc.\\n\\nB, 77(3), 509‚Äì580. with discussion, doi: 10.1111/rssb.12104.\\n\\nGholami, S., Janson, L., Worhunsky, D. J., Tran, T. B., Squires, Malcolm, I., Jin, L. X.,\\nSpolverato, G., Votanopoulos, K. I., Schmidt, C., Weber, S. M., Bloomston, M., Cho,\\nC. S., Levine, E. A., Fields, R. C., Pawlik, T. M., Maithel, S. K., Efron, B., Norton,\\nJ. A., and Poultsides, G. A. 2015. Number of lymph nodes removed and survival after\\ngastric cancer resection: An analysis from the US Gastric Cancer Collaborative. J.\\nAmer. Coll. Surg., 221(2), 291‚Äì299.\\n\\nGood, I., and Toulmin, G. 1956. The number of new species, and the increase in popu-\\n\\nlation coverage, when a sample is increased. Biometrika, 43, 45‚Äì63.\\n\\nHall, P. 1988. Theoretical comparison of bootstrap conÔ¨Ådence intervals. Ann. Statist.,\\n\\n16(3), 927‚Äì985. with discussion and a reply by the author.\\n\\nHampel, F. R. 1974. The inÔ¨Çuence curve and its role in robust estimation. J. Amer.\\n\\nStatist. Assoc., 69, 383‚Äì393.\\n\\nHampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. 1986. Robust\\nStatistics: The approach based on inÔ¨Çuence functions. Wiley Series in Probability\\nand Mathematical Statistics. John Wiley & Sons.\\n\\nHarford, T. 2014. Big data: A big mistake? SigniÔ¨Åcance, 11(5), 14‚Äì19.\\nHastie, T., and Loader, C. 1993. Local regression: automatic kernel carpentry (with\\n\\ndiscussion). Statistical Science, 8, 120‚Äì143.\\n\\nHastie, T., and Tibshirani, R. 1990. Generalized Additive Models. Chapman and Hall.\\nHastie, T., and Tibshirani, R. 2004. EfÔ¨Åcient quadratic regularization for expression\\n\\narrays. Biostatistics, 5(3), 329‚Äì340.\\n\\nHastie, T., Tibshirani, R., and Friedman, J. 2009. The Elements of Statistical Learning.\\nData mining, Inference, and Prediction. Second edn. Springer Series in Statistics.\\nSpringer.\\n\\nHastie, T., Tibshirani, R., and Wainwright, M. 2015. Statistical Learning with Sparsity:\\n\\nthe Lasso and Generalizations. Chapman and Hall, CRC Press.\\n\\nHoeffding, W. 1952. The large-sample power of tests based on permutations of obser-\\n\\nvations. Ann. Math. Statist., 23, 169‚Äì192.\\n\\nHoeffding, W. 1965. Asymptotically optimal tests for multinomial distributions. Ann.\\n\\nMath. Statist., 36(2), 369‚Äì408.\\n\\nHoerl, A. E., and Kennard, R. W. 1970. Ridge regression: Biased estimation for nonor-\\n\\nthogonal problems. Technometrics, 12(1), 55‚Äì67.\\n\\nHuber, P. J. 1964. Robust estimation of a location parameter. Ann. Math. Statist., 35,\\n\\n73‚Äì101.\\n\\n\\x0c458\\n\\nReferences\\n\\nJaeckel, L. A. 1972. Estimating regression coefÔ¨Åcients by minimizing the dispersion of\\n\\nthe residuals. Ann. Math. Statist., 43, 1449‚Äì1458.\\n\\nJames, W., and Stein, C. 1961. Estimation with quadratic loss. Pages 361‚Äì379 of:\\nProc. 4th Berkeley Symposium on Mathematical Statistics and Probability, vol. I.\\nUniversity of California Press.\\n\\nJansen, L., Fithian, W., and Hastie, T. 2015. Effective degrees of freedom: a Ô¨Çawed\\n\\nmetaphor. Biometrika, 102(2), 479‚Äì485.\\n\\nJavanmard, A., and Montanari, A. 2014. ConÔ¨Ådence intervals and hypothesis testing\\nfor high-dimensional regression. J. of Machine Learning Res., 15, 2869‚Äì2909.\\nJaynes, E. 1968. Prior probabilities. IEEE Trans. Syst. Sci. Cybernet., 4(3), 227‚Äì241.\\nJeffreys, H. 1961. Theory of Probability. Third ed. Clarendon Press.\\nJohnson, N. L., and Kotz, S. 1969. Distributions in Statistics: Discrete Distributions.\\n\\nHoughton MifÔ¨Çin Co.\\n\\nJohnson, N. L., and Kotz, S. 1970a. Distributions in Statistics. Continuous Univariate\\n\\nDistributions. 1. Houghton MifÔ¨Çin Co.\\n\\nJohnson, N. L., and Kotz, S. 1970b. Distributions in Statistics. Continuous Univariate\\n\\nDistributions. 2. Houghton MifÔ¨Çin Co.\\n\\nJohnson, N. L., and Kotz, S. 1972. Distributions in Statistics: Continuous Multivariate\\n\\nDistributions. John Wiley & Sons.\\n\\nKaplan, E. L., and Meier, P. 1958. Nonparametric estimation from incomplete obser-\\n\\nvations. J. Amer. Statist. Assoc., 53(282), 457‚Äì481.\\n\\nKass, R. E., and Raftery, A. E. 1995. Bayes factors. J. Amer. Statist. Assoc., 90(430),\\n\\n773‚Äì795.\\n\\nKass, R. E., and Wasserman, L. 1996. The selection of prior distributions by formal\\n\\nrules. J. Amer. Statist. Assoc., 91(435), 1343‚Äì1370.\\n\\nKuffner, R., Zach, N., Norel, R., Hawe, J., Schoenfeld, D., Wang, L., Li, G., Fang,\\nL., Mackey, L., Hardiman, O., Cudkowicz, M., Sherman, A., Ertaylan, G., Grosse-\\nWentrup, M., Hothorn, T., van Ligtenberg, J., Macke, J. H., Meyer, T., Scholkopf,\\nB., Tran, L., Vaughan, R., Stolovitzky, G., and Leitner, M. L. 2015. Crowdsourced\\nanalysis of clinical trial data to predict amyotrophic lateral sclerosis progression. Nat\\nBiotech, 33(1), 51‚Äì57.\\n\\nLeCun, Y., and Cortes, C. 2010.\\nhttp://yann.lecun.com/exdb/mnist/.\\n\\nMNIST Handwritten Digit Database.\\n\\nLeCun, Y., Bengio, Y., and Hinton, G. 2015. Deep learning. Nature, 521(7553), 436‚Äì\\n\\n444.\\n\\nLee, J., Sun, D., Sun, Y., and Taylor, J. 2016. Exact post-selection inference, with\\n\\napplication to the Lasso. Annals of Statistics, 44(3), 907‚Äì927.\\n\\nLehmann, E. L. 1983. Theory of Point Estimation. Wiley Series in Probability and\\n\\nMathematical Statistics. John Wiley & Sons.\\n\\nLeslie, C., Eskin, E., Cohen, A., Weston, J., and Noble, W. S. 2003. Mismatch string\\n\\nkernels for discriminative pretein classiÔ¨Åcation. Bioinformatics, 1, 1‚Äì10.\\n\\nLiaw, A., and Wiener, M. 2002. ClassiÔ¨Åcation and regression by randomForest. R\\n\\nNews, 2(3), 18‚Äì22.\\n\\nLiberman, M. 2015 (April). ‚ÄúReproducible Research and the Common Task Method‚Äù.\\nSimons Foundation Frontiers of Data Science Lecture, April 1, 2015; video avail-\\nable.\\n\\n\\x0cReferences\\n\\n459\\n\\nLockhart, R., Taylor, J., Tibshirani, R., and Tibshirani, R. 2014. A signiÔ¨Åcance test for\\nthe lasso. Annals of Statistics, 42(2), 413‚Äì468. With discussion and a rejoinder by\\nthe authors.\\n\\nLynden-Bell, D. 1971. A method for allowing for known observational selection in\\nsmall samples applied to 3CR quasars. Mon. Not. Roy. Astron. Soc., 155(1), 95‚Äì18.\\n\\nMallows, C. L. 1973. Some comments on Cp. Technometrics, 15(4), 661‚Äì675.\\nMantel, N., and Haenszel, W. 1959. Statistical aspects of the analysis of data from\\n\\nretrospective studies of disease. J. Natl. Cancer Inst., 22(4), 719‚Äì748.\\n\\nMardia, K. V., Kent, J. T., and Bibby, J. M. 1979. Multivariate Analysis. Academic\\n\\nPress.\\n\\nMcCullagh, P., and Nelder, J. 1983. Generalized Linear Models. Monographs on Statis-\\n\\ntics and Applied Probability. Chapman & Hall.\\n\\nMcCullagh, P., and Nelder, J. 1989. Generalized Linear Models. Second edn. Mono-\\n\\ngraphs on Statistics and Applied Probability. Chapman & Hall.\\n\\nMetropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E.\\n1953. Equation of state calculations by fast computing machines. J. Chem. Phys.,\\n21(6), 1087‚Äì1092.\\n\\nMiller, Jr, R. G. 1964. A trustworthy jackknife. Ann. Math. Statist, 35, 1594‚Äì1605.\\nMiller, Jr, R. G. 1981. Simultaneous Statistical Inference. Second edn. Springer Series\\n\\nin Statistics. New York: Springer-Verlag.\\n\\nNesterov, Y. 2013. Gradient methods for minimizing composite functions. Mathemati-\\n\\ncal Programming, 140(1), 125‚Äì161.\\n\\nNeyman, J. 1937. Outline of a theory of statistical estimation based on the classical\\n\\ntheory of probability. Phil. Trans. Roy. Soc., 236(767), 333‚Äì380.\\n\\nNeyman, J. 1977. Frequentist probability and frequentist statistics. Synthese, 36(1),\\n\\n97‚Äì131.\\n\\nNeyman, J., and Pearson, E. S. 1933. On the problem of the most efÔ¨Åcient tests of\\n\\nstatistical hypotheses. Phil. Trans. Roy. Soc. A, 231(694-706), 289‚Äì337.\\n\\nNg, A. 2015. Neural Networks. http://deeplearning.stanford.edu/\\n\\nwiki/index.php/Neural_Networks. Lecture notes.\\n\\nNgiam, J., Chen, Z., Chia, D., Koh, P. W., Le, Q. V., and Ng, A. 2010. Tiled convo-\\nlutional neural networks. Pages 1279‚Äì1287 of: Lafferty, J., Williams, C., Shawe-\\nTaylor, J., Zemel, R., and Culotta, A. (eds), Advances in Neural Information Pro-\\ncessing Systems 23. Curran Associates, Inc.\\n\\nO‚ÄôHagan, A. 1995. Fractional Bayes factors for model comparison. J. Roy. Statist. Soc.\\n\\nSer. B, 57(1), 99‚Äì138. With discussion and a reply by the author.\\n\\nPark, T., and Casella, G. 2008. The Bayesian lasso. J. Amer. Statist. Assoc., 103(482),\\n\\n681‚Äì686.\\n\\nPearson, K. 1900. On the criterion that a given system of deviations from the probable in\\nthe case of a correlated system of variables is such that it can be reasonably supposed\\nto have arisen from random sampling. Phil. Mag., 50(302), 157‚Äì175.\\n\\nPritchard, J., Stephens, M., and Donnelly, P. 2000. Inference of Population Structure\\n\\nusing Multilocus Genotype Data. Genetics, 155(June), 945‚Äì959.\\n\\nQuenouille, M. H. 1956. Notes on bias in estimation. Biometrika, 43, 353‚Äì360.\\nR Core Team. 2015. R: A Language and Environment for Statistical Computing. R\\n\\nFoundation for Statistical Computing, Vienna, Austria.\\n\\n\\x0c460\\n\\nReferences\\n\\nRidgeway, G. 2005. Generalized boosted models: A guide to the gbm package. Avail-\\n\\nable online.\\n\\nRidgeway, G., and MacDonald, J. M. 2009. Doubly robust internal benchmarking and\\nfalse discovery rates for detecting racial bias in police stops. J. Amer. Statist. Assoc.,\\n104(486), 661‚Äì668.\\n\\nRipley, B. D. 1996. Pattern Recognition and Neural Networks. Cambridge University\\n\\nPress.\\n\\nRobbins, H. 1956. An empirical Bayes approach to statistics. Pages 157‚Äì163 of: Proc.\\n3rd Berkeley Symposium on Mathematical Statistics and Probability, vol. I. Univer-\\nsity of California Press.\\n\\nRosset, S., Zhu, J., and Hastie, T. 2004. Margin maximizing loss functions. In: Thrun,\\nS., Saul, L., and Sch¬®olkopf, B. (eds), Advances in Neural Information Processing\\nSystems 16. MIT Press.\\n\\nRubin, D. B. 1981. The Bayesian bootstrap. Ann. Statist., 9(1), 130‚Äì134.\\nSavage, L. J. 1954. The Foundations of Statistics. John Wiley & Sons; Chapman &\\n\\nHill.\\n\\nSchapire, R. 1990. The strength of weak learnability. Machine Learning, 5(2), 197‚Äì\\n\\n227.\\n\\nSchapire, R., and Freund, Y. 2012. Boosting: Foundations and Algorithms. MIT Press.\\nScheff¬¥e, H. 1953. A method for judging all contrasts in the analysis of variance.\\n\\nBiometrika, 40(1-2), 87‚Äì110.\\n\\nSch¬®olkopf, B., and Smola, A. 2001. Learning with Kernels: Support Vector Ma-\\nchines, Regularization, Optimization, and Beyond (Adaptive Computation and Ma-\\nchine Learning). MIT Press.\\n\\nSchwarz, G. 1978. Estimating the dimension of a model. Ann. Statist., 6(2), 461‚Äì464.\\nSenn, S. 2008. A note concerning a selection ‚Äúparadox‚Äù of Dawid‚Äôs. Amer. Statist.,\\n\\n62(3), 206‚Äì210.\\n\\nSoric, B. 1989. Statistical ‚Äúdiscoveries‚Äù and effect-size estimation. J. Amer. Statist.\\n\\nAssoc., 84(406), 608‚Äì610.\\n\\nSpevack, M. 1968. A Complete and Systematic Concordance to the Works of Shake-\\n\\nspeare. Vol. 1‚Äì6. Georg Olms Verlag.\\n\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. 2014.\\nDropout: a simple way to prevent neural networks from overÔ¨Åtting. J. of Machine\\nLearning Res., 15, 1929‚Äì1958.\\n\\nStefanski, L., and Carroll, R. J. 1990. Deconvoluting kernel density estimators. Statis-\\n\\ntics, 21(2), 169‚Äì184.\\n\\nStein, C. 1956. Inadmissibility of the usual estimator for the mean of a multivariate nor-\\nmal distribution. Pages 197‚Äì206 of: Proc. 3rd Berkeley Symposium on Mathematical\\nStatististics and Probability, vol. I. University of California Press.\\n\\nStein, C. 1981. Estimation of the mean of a multivariate normal distribution. Ann.\\n\\nStatist., 9(6), 1135‚Äì1151.\\n\\nStein, C. 1985. On the coverage probability of conÔ¨Ådence sets based on a prior distribu-\\ntion. Pages 485‚Äì514 of: Sequential Methods in Statistics. Banach Center Publication,\\nvol. 16. PWN, Warsaw.\\n\\nStigler, S. M. 2006. How Ronald Fisher became a mathematical statistician. Math. Sci.\\n\\nHum. Math. Soc. Sci., 176(176), 23‚Äì30.\\n\\n\\x0cReferences\\n\\n461\\n\\nStone, M. 1974. Cross-validatory choice and assessment of statistical predictions. J.\\n\\nRoy. Statist. Soc. B, 36, 111‚Äì147. With discussion and a reply by the author.\\n\\nStorey, J. D., Taylor, J., and Siegmund, D. 2004. Strong control, conservative point\\nestimation and simultaneous conservative consistency of false discovery rates: A\\nuniÔ¨Åed approach. J. Roy. Statist. Soc. B, 66(1), 187‚Äì205.\\n\\nTanner, M. A., and Wong, W. H. 1987. The calculation of posterior distributions by\\ndata augmentation. J. Amer. Statist. Assoc., 82(398), 528‚Äì550. With discussion and\\na reply by the authors.\\n\\nTaylor, J., Loftus, J., and Tibshirani, R. 2015. Tests in adaptive regression via the Kac-\\n\\nRice formula. Annals of Statistics, 44(2), 743‚Äì770.\\n\\nThisted, R., and Efron, B. 1987. Did Shakespeare write a newly-discovered poem?\\n\\nBiometrika, 74(3), 445‚Äì455.\\n\\nTibshirani, R. 1989. Noninformative priors for one parameter of many. Biometrika,\\n\\n76(3), 604‚Äì608.\\n\\nTibshirani, R. 1996. Regression shrinkage and selection via the lasso. J. Roy. Statist.\\n\\nSoc. B, 58(1), 267‚Äì288.\\n\\nTibshirani, R. 2006. A simple method for assessing sample sizes in microarray experi-\\n\\nments. BMC Bioinformatics, 7(Mar), 106.\\n\\nTibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J., and Tibshirani, R.\\n2012. Strong rules for discarding predictors in lasso-type problems. J. Roy. Statist.\\nSoc. B, 74.\\n\\nTibshirani, R., Tibshirani, R., Taylor, J., Loftus, J., and Reid, S. 2016. selectiveInfer-\\n\\nence: Tools for Post-Selection Inference. R package version 1.1.3.\\n\\nTukey, J. W. 1958. ‚ÄúBias and conÔ¨Ådence in not-quite large samples‚Äù in Abstracts of\\n\\nPapers. Ann. Math. Statist., 29(2), 614.\\n\\nTukey, J. W. 1960. A survey of sampling from contaminated distributions. Pages\\n448‚Äì485 of: Contributions to Probability and Statistics: Essays in Honor of Harold\\nHotelling (I. Olkin, et. al, ed.). Stanford University Press.\\n\\nTukey, J. W. 1962. The future of data analysis. Ann. Math. Statist., 33, 1‚Äì67.\\nTukey, J. W. 1977. Exploratory Data Analysis. Behavioral Science Series. Addison-\\n\\nWesley.\\n\\nvan de Geer, S., B¬®uhlmann, P., Ritov, Y., and Dezeure, R. 2014. On asymptotically op-\\ntimal conÔ¨Ådence regions and tests for high-dimensional models. Annals of Statistics,\\n42(3), 1166‚Äì1202.\\n\\nVapnik, V. 1996. The Nature of Statistical Learning Theory. Springer.\\nWager, S., Wang, S., and Liang, P. S. 2013. Dropout training as adaptive regularization.\\nPages 351‚Äì359 of: Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and Wein-\\nberger, K. (eds), Advances in Neural Information Processing Systems 26. Curran\\nAssociates, Inc.\\n\\nWager, S., Hastie, T., and Efron, B. 2014. ConÔ¨Ådence intervals for random forests: the\\njacknife and the inÔ¨Åntesimal jacknife. J. of Machine Learning Res., 15, 1625‚Äì1651.\\n\\nWahba, G. 1990. Spline Models for Observational Data. SIAM.\\nWahba, G., Lin, Y., and Zhang, H. 2000. GACV for support vector machines. Pages\\n297‚Äì311 of: Smola, A., Bartlett, P., Sch¬®olkopf, B., and Schuurmans, D. (eds), Ad-\\nvances in Large Margin ClassiÔ¨Åers. MIT Press.\\n\\nWald, A. 1950. Statistical Decision Functions. John Wiley & Sons; Chapman & Hall.\\n\\n\\x0c462\\n\\nReferences\\n\\nWedderburn, R. W. M. 1974. Quasi-likelihood functions, generalized linear models,\\n\\nand the Gauss‚ÄìNewton method. Biometrika, 61(3), 439‚Äì447.\\n\\nWelch, B. L., and Peers, H. W. 1963. On formulae for conÔ¨Ådence points based on\\n\\nintegrals of weighted likelihoods. J. Roy. Statist. Soc. B, 25, 318‚Äì329.\\n\\nWestfall, P., and Young, S. 1993. Resampling-based Multiple Testing: Examples and\\nMethods for p-Value Adjustment. Wiley Series in Probability and Statistics. Wiley-\\nInterscience.\\n\\nXie, M., and Singh, K. 2013. ConÔ¨Ådence distribution, the frequentist distribution esti-\\nmator of a parameter: A review. Int. Statist. Rev., 81(1), 3‚Äì39. with discussion.\\nYe, J. 1998. On measuring and correcting the effects of data mining and model selec-\\n\\ntion. J. Amer. Statist. Assoc., 93(441), 120‚Äì131.\\n\\nZhang, C.-H., and Zhang, S. 2014. ConÔ¨Ådence intervals for low-dimensional parame-\\n\\nters with high-dimensional data. J. Roy. Statist. Soc. B, 76(1), 217‚Äì242.\\n\\nZou, H., Hastie, T., and Tibshirani, R. 2007. On the ‚Äúdegrees of freedom‚Äù of the lasso.\\n\\nAnn. Statist., 35(5), 2173‚Äì2192.\\n\\n\\x0cAuthor Index\\n\\nAbu-Mostafa, Y. 372, 453\\nAchanta, R. 372, 453\\nAkaike, H. 231, 453\\nAnderson, T. W. 69, 453\\nBastien, F. 374, 453\\nBecker, R. 128, 453\\nBellhouse, D. R. 36, 453\\nBengio, Y. 372, 374, 453, 458\\nBenjamini, Y. 294, 418, 450, 453\\nBerger, J. O. 36, 261, 453\\nBergeron, A. 374, 453\\nBergstra, J. 374, 453\\nBerk, R. 323, 419, 453\\nBerkson, J. 128, 454\\nBernardo, J. M. 261, 454\\nBibby, J. M. 37, 69, 459\\nBien, J. 322, 461\\nBirch, M. W. 128, 454\\nBishop, C. 371, 454\\nBloomston, M. 89, 457\\nBoos, D. D. 179, 454\\nBoser, B. 390, 454\\nBouchard, N. 374, 453\\nBreiman, L. 128, 348, 451, 454\\nBreuleux, O. 374, 453\\nBrown, L. 323, 419, 453\\nB¬®uhlmann, P. 323, 461\\nBuja, A. 323, 419, 453\\nCarlin, B. P. 89, 261, 454\\nCarroll, R. J. 445, 460\\nCasella, G. 420, 459\\nChambers, J. 128, 453\\nChen, Z. 372, 459\\nChia, D. 372, 459\\nCho, C. S. 89, 457\\nChopin, N. 261, 457\\nCleveland, W. S. 11, 454\\nCohen, A. 393, 458\\n\\nCorbet, A. 456\\nCortes, C. 372, 458\\nCourville, A. 372, 453\\nCover, T. M. 52, 454\\nCox, D. R. 52, 128, 152, 153, 262, 454\\nCrowley, J. 153, 454\\nCudkowicz, M. 349, 458\\n\\nde Finetti, B. 261, 454\\nDembo, A. 52, 454\\nDempster, A. P. 152, 454\\nDesjardins, G. 374, 453\\nDezeure, R. 323, 461\\nDiaconis, P. 262, 454\\nDiCiccio, T. 204, 455\\nDonnelly, P. 261, 459\\nDonoho, D. L. 447, 455\\n\\nEdwards, A. W. F. 37, 455\\nEfron, B. 11, 20, 37, 51, 52, 69, 89, 90,\\n105, 106, 130, 152, 154, 177‚Äì179,\\n204, 206, 207, 231, 232, 262, 267,\\n294‚Äì297, 321, 323, 348, 417, 419,\\n420, 444, 445, 455‚Äì457, 461\\n\\nErtaylan, G. 349, 458\\nEskin, E. 393, 458\\n\\nFang, L. 349, 458\\nFeldman, D. 417, 456\\nFields, R. C. 89, 457\\nFinney, D. J. 262, 456\\nFisher, R. A. 184, 204, 449, 456\\nFithian, W. 323, 456, 458\\nFreund, Y. 348, 451, 456, 460\\nFriedman, J. 128, 231, 321, 322, 348,\\n\\n349, 371, 454, 456, 457, 461\\n\\nGeisser, S. 231, 457\\nGerber, M. 261, 457\\nGholami, S. 89, 457\\nGood, I. 88, 457\\n\\n463\\n\\n\\x0c464\\n\\nAuthor Index\\n\\nGoodfellow, I. J. 374, 453\\nGous, A. 262, 456\\nGrosse-Wentrup, M. 349, 458\\nGuyon, I. 390, 454\\nHaenszel, W. 152, 459\\nHall, P. 204, 457\\nHampel, F. R. 179, 457\\nHardiman, O. 349, 458\\nHarford, T. 232, 457\\nHastie, T. 128, 231, 321‚Äì323, 348, 349,\\n371, 372, 392, 393, 453, 456‚Äì458,\\n460‚Äì462\\n\\nHawe, J. 349, 458\\nHinkley, D. V. 52, 69, 454, 456\\nHinton, G. 372, 458, 460\\nHochberg, Y. 294, 418, 450, 453\\nHoeffding, W. 129, 296, 457\\nHoerl, A. E. 105, 457\\nHothorn, T. 349, 458\\nHuber, P. J. 179, 457\\nJaeckel, L. A. 178, 458\\nJames, W. 104, 458\\nJansen, L. 323, 458\\nJanson, L. 89, 457\\nJavanmard, A. 323, 458\\nJaynes, E. 261, 458\\nJeffreys, H. 261, 458\\nJin, L. X. 89, 457\\nJohnson, N. L. 36, 458\\nJohnstone, I. 231, 321, 323, 456\\nKaplan, E. L. 152, 458\\nKass, R. E. 261, 262, 458\\nKennard, R. W. 105, 457\\nKent, J. T. 37, 69, 459\\nKoh, P. W. 372, 459\\nKotz, S. 36, 458\\nKrizhevsky, A. 372, 460\\nKuffner, R. 349, 458\\nLaird, N. M. 152, 454\\nLamblin, P. 374, 453\\nLe, Q. V. 372, 459\\nLeCun, Y. 372, 458\\nLee, J. 323, 458\\nLehmann, E. L. 52, 458\\nLeitner, M. L. 349, 458\\nLeslie, C. 393, 458\\nLevine, E. A. 89, 457\\nLi, G. 349, 458\\nLiang, P. S. 372, 373, 461\\nLiaw, A. 348, 458\\n\\nLiberman, M. 447, 458\\nLin, Y. 391, 461\\nLoader, C. 393, 457\\nLockhart, R. 323, 459\\nLoftus, J. 323, 461\\nLouis, T. A. 89, 261, 454\\nLynden-Bell, D. 150, 459\\nMacDonald, J. M. 294, 460\\nMacke, J. H. 349, 458\\nMackey, L. 349, 458\\nMaithel, S. K. 89, 457\\nMallows, C. L. 231, 459\\nMantel, N. 152, 459\\nMardia, K. V. 37, 69, 459\\nMcCullagh, P. 128, 322, 459\\nMeier, P. 152, 458\\nMetropolis, N. 261, 459\\nMeyer, T. 349, 458\\nMiller, R. G., Jr 177, 294, 418, 459\\nMontanari, A. 323, 458\\nMorris, C. 105, 456\\nNelder, J. 128, 322, 459\\nNesterov, Y. 372, 459\\nNeyman, J. 20, 204, 449, 459\\nNg, A. 372, 459\\nNgiam, J. 372, 459\\nNoble, W. S. 393, 458\\nNorel, R. 349, 458\\nNorton, J. A. 89, 457\\nO‚ÄôHagan, A. 261, 459\\nOlshen, R. A. 128, 348, 454\\nPark, T. 420, 459\\nPascanu, R. 374, 453\\nPawlik, T. M. 89, 457\\nPearson, E. S. 449, 459\\nPearson, K. 449, 459\\nPeers, H. W. 37, 207, 261, 462\\nPericchi, L. R. 261, 453\\nPetrosian, V. 130, 456\\nPopescu, B. 348, 457\\nPoultsides, G. A. 89, 457\\nPritchard, J. 261, 459\\nQuenouille, M. H. 177, 459\\nR Core Team 128, 459\\nRaftery, A. E. 262, 458\\nReid, N. 262, 454\\nReid, S. 323, 461\\nRidgeway, G. 294, 348, 460\\nRipley, B. D. 371, 460\\nRitov, Y. 323, 461\\n\\n\\x0cAuthor Index\\n\\n465\\n\\nRobbins, H. 88, 104, 419, 460\\nRonchetti, E. M. 179, 457\\nRosenbluth, A. W. 261, 459\\nRosenbluth, M. N. 261, 459\\nRosset, S. 392, 460\\nRousseeuw, P. J. 179, 457\\nRubin, D. B. 152, 179, 454, 460\\nSalakhutdinov, R. 372, 460\\nSavage, L. J. 261, 460\\nSchapire, R. 348, 451, 456, 460\\nScheff¬¥e, H. 417, 460\\nSchmidt, C. 89, 457\\nSchoenfeld, D. 349, 458\\nSch¬®olkopf, B. 390, 460\\nSchwarz, G. 262, 460\\nSenn, S. 37, 460\\nSerÔ¨Çing, R. J. 179, 454\\nSherman, A. 349, 458\\nSiegmund, D. 294, 461\\nSimon, N. 322, 461\\nSingh, K. 51, 207, 462\\nSmola, A. 390, 460\\nSoric, B. 294, 460\\nSpevack, M. 89, 460\\nSpolverato, G. 89, 457\\nSquires, I., Malcolm 89, 457\\nSrivastava, N. 372, 460\\nStahel, W. A. 179, 457\\nStefanski, L. 445, 460\\nStein, C. 104, 106, 178, 232, 261, 456,\\n\\n458, 460\\n\\nStephens, M. 261, 459\\nStigler, S. M. 447, 460\\nStolovitzky, G. 349, 458\\nStone, C. J. 128, 348, 454\\nStone, M. 231, 461\\nStorey, J. D. 294, 461\\nSun, D. 323, 456, 458\\nSun, Y. 323, 458\\nSutskever, I. 372, 460\\nTanner, M. A. 263, 461\\nTaylor, J. 294, 322, 323, 456, 458, 459,\\n\\n461\\n\\nTeller, A. H. 261, 459\\nTeller, E. 261, 459\\nThisted, R. 89, 456, 461\\nThomas, J. A. 52, 454\\n\\nTibshirani, R. 128, 179, 207, 231, 232,\\n261, 321‚Äì323, 348, 349, 371, 392,\\n420, 450, 456, 457, 459, 461, 462\\n\\nToulmin, G. 88, 457\\nTran, L. 349, 458\\nTran, T. B. 89, 457\\nTukey, J. W. 11, 177, 179, 450, 461\\nTurian, J. 374, 453\\n\\nvan de Geer, S. 323, 461\\nvan Ligtenberg, J. 349, 458\\nVapnik, V. 390, 454, 461\\nVaughan, R. 349, 458\\nVincent, P. 372, 453\\nVotanopoulos, K. I. 89, 457\\n\\nWager, S. 348, 372, 373, 461\\nWahba, G. 391, 392, 461\\nWainwright, M. 321‚Äì323, 457\\nWald, A. 450, 461\\nWang, L. 349, 458\\nWang, S. 372, 373, 461\\nWarde-Farley, D. 374, 453\\nWasserman, L. 261, 262, 458\\nWeber, S. M. 89, 457\\nWedderburn, R. W. M. 128, 462\\nWelch, B. L. 37, 207, 261, 462\\nWestfall, P. 294, 418, 462\\nWeston, J. 393, 458\\nWiener, M. 348, 458\\nWilks, A. 128, 453\\nWilliams, C. 456\\nWong, W. H. 263, 461\\nWorhunsky, D. J. 89, 457\\n\\nXie, M. 51, 207, 462\\n\\nYe, J. 231, 462\\nYekutieli, D. 418, 453\\nYlvisaker, D. 262, 454\\nYoung, S. 294, 418, 462\\n\\nZach, N. 349, 458\\nZhang, C.-H. 323, 462\\nZhang, H. 391, 461\\nZhang, K. 323, 419, 453\\nZhang, S. 323, 462\\nZhao, L. 323, 419, 453\\nZhu, J. 392, 460\\nZou, H. 231, 322, 462\\n\\n\\x0c\\x0cSubject Index\\n\\nabc method, 194, 204\\nAccelerated gradient descent, 359\\nAcceleration, 192, 206\\nAccuracy, 14\\n\\nafter model selection, 402‚Äì408\\n\\nAccurate but not correct, 402\\nActivation function, 355, 361\\nleaky rectiÔ¨Åed linear, 362\\nrectiÔ¨Åed linear, 362\\nReLU, 362\\ntanh, 362\\n\\nActive set, 301, 308\\nadaboost algorithm, 341‚Äì345, 447\\nAdaboost.M1, 342\\nAdaptation, 404\\nAdaptive estimator, 404\\nAdaptive rate control, 359\\nAdditive model, 324\\nadaptive, 346\\n\\nAdjusted compliance, 404\\nAdmixture modeling, 256‚Äì260\\nAIC, see Akaike information criterion\\nAkaike information criterion, 208, 218,\\n\\n226, 231, 246, 267\\n\\nAllele frequency, 257\\nAmerican Statistical Association, 449\\nAncillary, 44, 46, 139\\nApparent error, 211, 213, 219\\narcsin transformation, 95\\nArthur Eddington, 447\\nAsymptotics, xvi, 119, 120\\nAutoencoder, 362‚Äì364\\n\\nBackÔ¨Åtting, 346\\nBackpropagation, 356‚Äì358\\nBagged estimate, 404, 406\\nBagging, 226, 327, 406, 408, 419\\nBalance equations, 256\\nBarycentric plot, 259\\n\\nBasis expansion, 375\\nBayes\\n\\ndeconvolution, 421‚Äì424\\nfactor, 244, 285\\nfalse-discovery rate, 279\\nposterior distribution, 254\\nposterior probability, 280\\nshrinkage, 212\\nt -statistic, 255\\ntheorem, 22\\n\\nBayes‚Äìfrequentist estimation, 412‚Äì417\\nBayesian\\n\\ninference, 22‚Äì37\\ninformation criterion, 246\\nlasso, 420\\nlasso prior, 415\\nmodel selection, 244\\ntrees, 349\\n\\nBayesian information criterion, 267\\nBayesianism, 3\\nBCa\\n\\naccuracy and correctness, 205\\nconÔ¨Ådence density, 202, 207, 237, 242,\\n\\n243\\ninterval, 202\\nmethod, 192\\n\\nBenjamini and Hochberg, 276\\nBenjamini‚ÄìYekutieli, 400\\nBernoulli, 338\\nBest-approximating linear subspace, 363\\nBest-subset selection, 299\\nBeta\\n\\ndistribution, 54, 239\\n\\nBHq, 276\\nBias, 14, 352\\nBias-corrected, 330\\n\\nand accelerated, see BCa method\\nconÔ¨Ådence intervals, 190‚Äì191\\npercentile method, 190\\n\\n467\\n\\n\\x0c468\\n\\nSubject Index\\n\\nBias-correction value, 191\\nBiased estimation, 321\\nBIC, see Bayesian information criterion\\nBig-data era, xv, 446\\nBinomial, 109, 117\\n\\ndistribution, 54, 117, 239\\nlog-likelihood, 380\\nstandard deviation, 111\\n\\nBioassay, 109\\nBiometrika, 449\\nBivariate normal, 182\\nBonferroni bound, 273\\nBoole‚Äôs inequality, 274\\nBoosting, 320, 324, 333‚Äì350\\nBootstrap, 7, 155‚Äì180, 266, 327\\n\\nBaron Munchausen, 177\\nBayesian, 168, 179\\ncdf, 187\\nconÔ¨Ådence intervals, 181‚Äì207\\nideal estimate, 161, 179\\njackknife after, 179\\nmoving blocks, 168\\nmultisample, 167\\nnonparametric, 159‚Äì163, 217\\nout of bootstrap, 232\\npackages, 178\\nparametric, 169‚Äì173, 223, 312, 429\\nprobabilities, 164\\nreplication, 159\\nsample, 159\\nsample size, 179, 205\\nsmoothing, 226, 404, 406\\nt, 196\\nt intervals, 195‚Äì198\\n\\nBound form, 305\\nBounding hyperplane, 398\\nBurn-in, 260\\nBYq algorithm, 400\\nCausal inference, xvi\\nCensored\\n\\ndata, 134‚Äì139\\nnot truncated, 150\\n\\nCentering, 107\\nCentral limit theorem, 119\\nChain rule for differentiation, 356\\nClassic statistical inference, 3‚Äì73\\nClassiÔ¨Åcation, 124, 209\\nClassiÔ¨Åcation accuracy, 375\\nClassiÔ¨Åcation error, 209\\nClassiÔ¨Åcation tree, 348\\nCochran‚ÄìMantel‚ÄìHaenszel test, 131\\n\\nCoherent behavior, 261\\nCommon task framework, 447\\nCompliance, 394\\nComputational bottleneck, 128\\nComputer age, xv\\nComputer-intensive, 127\\ninference, 189, 267\\nstatistics, 159\\nConditional, 58\\nConditional distribution\\n\\nfull, 253\\n\\nConditional inference, 45‚Äì48, 139, 142\\n\\nlasso, 318\\n\\nConditionality, 44\\nConÔ¨Ådence\\n\\ndensity, 200, 201, 235\\ndistribution, 198‚Äì203\\ninterval, 17\\nregion, 397\\n\\nConjugate, 253, 259\\n\\nprior, 238\\npriors, 237\\n\\nConvex optimization, 304, 308, 321, 323,\\n\\n377\\n\\nConvolution, 422, 445\\n\\nÔ¨Ålters, 368\\nlayer, 367\\n\\nCorrected differences, 411\\nCorrelation effects, 295\\nCovariance\\n\\nformula, 312\\npenalty, 218‚Äì226\\n\\nCoverage, 181\\nCoverage level, 274\\nCoverage matching prior, 236‚Äì237\\nCox model, see proportional hazards\\n\\nmodel\\n\\nCp, 217, 218, 221, 231, 267, 300, 394,\\n\\n395, 403\\n\\nCram¬¥er‚ÄìRao lower bound, 44\\nCredible interval, 198, 417\\nCross-validation, 208‚Äì232, 267, 335\\n\\n10-fold, 326\\nestimate, 214\\nK-fold, 300\\nleave one out, 214, 231\\n\\nCumulant generating function, 67\\nCurse of dimensionality, 387\\nDark energy, 210, 231\\nData analysis, 450\\nData science, xvii, 450, 451\\n\\n\\x0cSubject Index\\n\\n469\\n\\nData sets\\n\\nALS, 334\\nAML, see leukemia\\nbaseball, 94\\nbutterfly, 78\\ncell infusion, 112\\ncholesterol, 395, 402, 403\\nCIFAR-100, 365\\ndiabetes, 98, 209, 396, 414, 416\\ndose-response, 109\\ngalaxy, 120\\nhandwritten digits\\n(MNIST), 353\\n\\nhead/neck cancer, 135\\nhuman ancestry, 257\\ninsurance, 131\\nkidney function, 157, 222\\nleukemia, 176, 196, 377\\nNCOG, 134\\nnodes, 424, 427, 430, 438, 439, 442\\npediatric cancer, 143\\npolice, 287\\nprostate, 249, 272, 289, 408, 410,\\n\\n423, 434‚Äì436\\n\\nprotein classification, 385\\nshakespear, 81\\nspam, 113, 127, 209, 215, 300‚Äì302,\\n\\n325\\n\\nstudent score, 173, 181, 186,\\n\\n202, 203\\n\\nsupernova, 210, 212, 217, 221, 224\\nvasoconstriction, 240, 241,\\n\\n246, 252\\n\\nData snooping, 398\\nDe Finetti, B., 35, 36, 251, 450\\nDe Finetti‚ÄìSavage school, 251\\nDebias, 318\\nDecision rule, 275\\nDecision theory, xvi\\nDeconvolution, 422\\nDeep learning, 351‚Äì374\\nDeÔ¨Ånitional bias, 431\\nDegrees of freedom, 221, 231, 312‚Äì313\\nDelta method, 15, 414, 420\\nDeviance, 112, 118, 119, 301\\nDeviance residual, 123\\nDiffusion tensor imaging, 291\\nDirect evidence, 105, 109, 421\\nDirectional derivatives, 158\\nDistribution\\n\\nbeta, 54, 239\\n\\nbinomial, 54, 117, 239\\ngamma, 54, 117, 239\\nGaussian, 54\\nnormal, 54, 117, 239\\nPoisson, 54, 117, 239\\n\\nDivide-and-conquer algorithm, 325\\nDocument retrieval, 298\\nDose‚Äìresponse, 109\\nDropout learning, 368, 372\\nDTI, see diffusion tensor imaging\\nEarly computer-age, xvi, 75‚Äì268\\nEarly stopping, 362\\nEffect size, 272, 288, 399, 408\\nEfÔ¨Åciency, 44, 120\\nEigenratio, 162, 173, 194\\nElastic net, 316, 356\\nEllipsoid, 398\\nEM algorithm, 146‚Äì150\\nmissing data, 266\\n\\nEmpirical Bayes, 75‚Äì90, 93, 264\\nestimation strategies, 421‚Äì445\\ninformation, 443\\nlarge-scale testing, 278‚Äì282\\n\\nEmpirical null, 286\\n\\nestimation, 289‚Äì290\\nmaximum-likelihood estimation, 296\\nEmpirical probability distribution, 160\\nEnsemble, 324, 334\\nEphemeral predictors, 227\\nEpoch, 359\\nEquilibrium distribution, 256\\nEquivariant, 106\\nExact inferences, 119\\nExpectation parameter, 118\\nExperimental design, xvi\\nExponential family, 53‚Äì72, 225\\np-parameter, 117, 413, 424\\ncurved, 69\\none-parameter, 116\\n\\nF distribution, 397\\nF tests, 394\\nf -modeling, 424, 434, 440‚Äì444\\nFake-data principle, 148, 154, 266\\nFalse coverage\\ncontrol, 399\\n\\nFalse discovery, 275\\n\\ncontrol, 399\\ncontrol theorem, 294\\nproportion, 275\\nrate, 271‚Äì297\\nFalse-discovery\\n\\n\\x0cSubject Index\\n\\n470\\n\\nrate, 9\\n\\nFamily of probability densities, 64\\nFamily-wise error rate, 274\\nFDR, see false-discovery rate\\nFeed-forward, 351\\nFiducial, 267\\n\\nconstructions, 199\\ndensity, 200\\ninference, 51\\n\\nFisher, 79\\nFisher information, 29, 41, 59\\n\\nbound, 41\\nmatrix, 236, 427\\n\\nFisherian correctness, 205\\nFisherian inference, 38‚Äì52, 235\\nFixed-knot regression splines, 345\\nFlat prior, 235\\nForward pass, 357\\nForward-stagewise, 346\\n\\nÔ¨Åtting, 320\\n\\nForward-stepwise, 298‚Äì303\\n\\ncomputations, 322\\nlogistic regression, 322\\nregression, 300\\n\\nFourier\\n\\nmethod, 440\\ntransform, 440\\n\\nFrailties, 439\\nFrequentism, 3, 12‚Äì22, 30, 35, 51, 146,\\n\\n267\\nFrequentist, 413\\n\\ninference, 12‚Äì21\\nstrongly, 218\\n\\nFully connected layer, 368\\nFunctional gradient descent, 340\\nFWER, see family-wise error rate\\ng-modeling, 423\\nGamma, 117\\n\\ndistribution, 54, 117, 239\\n\\nGeneral estimating equations, xvi\\nGeneral information criterion, 248\\nGeneralized\\n\\nlinear mixed model, 437‚Äì440\\nlinear model, 108‚Äì123, 266\\nridge problem, 384\\n\\nGenome, 257\\nGenome-wide association studies, 451\\nGibbs sampling, 251‚Äì260, 267, 414\\nGLM, see generalized linear model\\nGLMM, see generalized linear mixed\\n\\nmodel\\n\\nGoogle Ô¨Çu trends, 230, 232\\nGradient boosting, 338‚Äì341\\nGradient descent, 354, 356\\nGram matrix, 381\\nGram-Schmidt orthogonalization, 322\\nGraphical lasso, 321\\nGraphical models, xvi\\nGreenwood‚Äôs formula, 137, 151\\nGroup lasso, 321\\n\\nHadamard product, 358\\nHandwritten digits, 353\\nHaplotype estimation, 261\\nHazard rate, 131‚Äì134\\n\\nparametric estimate, 138\\nHidden layer, 351, 352, 354\\nHigh-order interaction, 325\\nHinge loss, 380\\nHints\\n\\nlearning with, 369\\nHoeffding‚Äôs lemma, 118\\nHolm‚Äôs procedure, 274, 294\\nHomotopy path, 306\\nHypergeometric distribution, 141, 152\\n\\nImputation, 149\\nInadmissible, 93\\nIndirect evidence, 102, 109, 266, 290,\\n\\n421, 440, 443\\nInductive inference, 120\\nInference, 3\\nInference after model selection, 394‚Äì420\\nInferential triangle, 446\\nInÔ¨Ånitesimal forward stagewise, 320\\nInÔ¨Ånitesimal jackknife, 167\\n\\nestimate, 406\\nstandard deviations, 407\\nInÔ¨Çuence function, 174‚Äì177\\n\\nempirical, 175\\n\\nInÔ¨Çuenza outbreaks, 230\\nInput distortion, 369, 373\\nInput layer, 355\\nInsample error, 219\\nInverse chi-squared, 262\\nInverse gamma, 239, 262\\nIRLS, see iteratively reweighted least\\n\\nsquares\\n\\nIteratively reweighted least squares, 301,\\n\\n322\\n\\nJackknife, 155‚Äì180, 266, 330\\n\\nestimate of standard error, 156\\nstandard error, 178\\n\\n\\x0cSubject Index\\n\\n471\\n\\nJames‚ÄìStein\\n\\nestimation, 91‚Äì107, 282, 305, 410\\nridge regression, 265\\n\\nJeffreys\\n\\nprior, 237\\n\\nJeffreys‚Äô\\n\\nprior, 28‚Äì30, 36, 198, 203, 236\\nprior, multiparameter, 242\\nscale, 285\\n\\nJumpiness of estimator, 405\\nKaplan‚ÄìMeier, 131, 134, 136, 137\\n\\nestimate, 134‚Äì139, 266\\n\\nKarush‚ÄìKuhn‚ÄìTucker optimality\\n\\nconditions, 308\\n\\nKernel\\n\\nfunction, 382\\nlogistic regression, 386\\nmethod, 375‚Äì393\\nsmoothing, 375, 387‚Äì390\\nSVM, 386\\ntrick, 375, 381‚Äì383, 392\\n\\nKnots, 309\\nKullback‚ÄìLeibler distance, 112\\n`1 regularization, 321\\nLagrange\\n\\ndual, 381\\nform, 305, 308\\nmultiplier, 391\\nprimal, 391\\n\\nLarge-scale\\n\\nhypothesis testing, 271‚Äì297\\ntesting, 272‚Äì275\\n\\nLarge-scale prediction algorithms, 446\\nLasso, 101, 210, 217, 222, 231, 298‚Äì323\\n\\nmodiÔ¨Åcation, 312\\npath, 312\\npenalty, 356\\n\\nLearning from the experience of others,\\n\\n104, 280, 290, 421, 443\\n\\nLearning rate, 358\\nLeast squares, 98, 112, 299\\nLeast-angle regression, 309‚Äì313, 321\\nLeast-favorable family, 262\\nLeft-truncated, 150\\nLehmann alternative, 294\\nLife table, 131‚Äì134\\nLikelihood function, 38\\n\\nconcavity, 118\\n\\nLimited-translation rule, 293\\nLindsey‚Äôs method, 68, 171\\nLinearly separable, 375\\n\\nLink function, 237, 340\\nLocal false-discovery rate, 280, 282‚Äì286\\nLocal regression, 387‚Äì390, 393\\nLocal translation invariance, 368\\nLog polynomial regression, 410\\nLog-rank statistic, 152\\nLog-rank test, 131, 139‚Äì142, 152, 266\\nLogic of inductive inference, 185, 205\\nLogistic regression, 109‚Äì115, 139, 214,\\n\\n299, 375\\nmulticlass, 355\\n\\nLogit, 109\\nLoss plus penalty, 385\\nMachine learning, 208, 267, 375\\nMallows‚Äô Cp, see Cp\\nMantel‚ÄìHaenzel test, 131\\nMAP, 101\\nMAP estimate, 420\\nMargin, 376\\nMarginal density, 409, 422\\nMarkov chain Monte Carlo, see MCMC\\nMarkov chain theory, 256\\nMartingale theory, 294\\nMatching prior, 198, 200\\nMatlab, 271\\nMatrix completion, 321\\nMax pool layer, 366\\nMaximized a-posteriori probability, see\\n\\nMAP\\n\\nMaximum likelihood, 299\\nMaximum likelihood estimation, 38‚Äì52\\nMCMC, 234, 251‚Äì260, 267, 414\\nMcNemar test, 341\\nMean absolute deviation, 447\\nMedian unbiased, 190\\nMemory-based methods, 390\\nMeter reader, 30\\nMeter-reader, 37\\nMicroarrays, 227, 271\\nMinitab, 271\\nMisclassiÔ¨Åcation error, 302\\nMissing data, 146‚Äì150, 325\\n\\nEM algorithm, 266\\n\\nMissing-species problem, 78‚Äì84\\nMixed features, 325\\nMixture density, 279\\nModel averaging, 408\\nModel selection, 243‚Äì250, 398\\n\\ncriteria, 250\\n\\nMonotone lasso, 320\\nMonotonic increasing function, 184\\n\\n\\x0c472\\n\\nSubject Index\\n\\nMultinomial\\n\\ndistribution, 61‚Äì64, 425\\nfrom Poisson, 63\\nMultiple testing, 272\\nMultivariate\\n\\nanalysis, 119\\nnormal, 55‚Äì59\\n\\nn-gram, 385\\nN-P complete, 299\\nNadaraya‚ÄìWatson estimator, 388\\nNatural parameter, 116\\nNatural spline model, 430\\nNCOG, see Northern California\\n\\nOncology Group\\n\\nNested models, 299\\nNeural Information Processing Systems,\\n\\n372\\n\\nNeural network, 351‚Äì374\\nadaptive tuning, 360\\nnumber of hidden layers, 361\\n\\nNeurons, 351\\nNeyman‚Äôs construction, 181, 183, 193,\\n\\n204\\n\\nNeyman‚ÄìPearson, 18, 19, 293\\nNon-null, 272\\nNoncentral chi-square variable, 207\\nNonlinear transformations, 375\\nNonlinearity, 361\\nNonparameteric\\n\\nregression, 375\\n\\nNonparametric, 53, 127\\n\\nMLE, 150, 160\\npercentile interval, 187\\n\\nNormal\\n\\ncorrelation coefÔ¨Åcient, 182\\ndistribution, 54, 117, 239\\nmultivariate, 55‚Äì59\\nregression model, 414\\ntheory, 119\\n\\nNorthern California Oncology Group,\\n\\n134\\n\\nNuclear norm, 321\\nNuisance parameters, 142, 199\\nObjective Bayes, 36, 267\\ninference, 233‚Äì263\\nintervals, 198‚Äì203\\nprior distribution, 234‚Äì237\\n\\nOCR, see optical character recognition\\nOffset, 349\\nOLS\\n\\nalgorithm, 403\\n\\nestimation, 395\\npredictor, 221\\n\\nOne-sample nonparametric bootstrap,\\n\\n161\\n\\nOne-sample problems, 156\\nOOB, see out-of-bag error\\nOptical character recognition, 353\\nOptimal separating hyperplane, 375‚Äì377\\nOptimal-margin classiÔ¨Åer, 376\\nOptimality, 18\\nOracle, 275\\nOrthogonal parameters, 262\\nOut-of-bag error, 232, 327, 329‚Äì330\\nOut-the-box learning algorithm, 324\\nOutput layer, 352\\nOutsample error, 219\\nOver parametrized, 298\\nOverÔ¨Åtting, 304\\nOvershrinks, 97\\np-value, 9, 282\\nPackage/program\\ngbm, 335, 348\\nglmnet, 214, 315, 322, 348\\nh2o, 372\\nlars, 312, 320\\nliblineaR, 381\\nlocfdr, 289‚Äì291, 296, 437\\nlowess, 6, 222, 388\\nnlm, 428\\nrandomForest, 327, 348\\nselectiveInference, 323\\n\\nPairwise inner products, 381\\nParameter space, 22, 29, 54, 62, 66\\nParametric bootstrap, 242\\nParametric family, 169\\nParametric models, 53‚Äì72\\nPartial likelihood, 142, 145, 151, 153,\\n\\n266, 341\\n\\nPartial logistic regression, 152\\nPartial residual, 346\\nPath-wise coordinate descent, 314\\nPenalized\\n\\nleast squares, 101\\nlikelihood, 101, 428\\nlogistic regression, 356\\nmaximum likelihood, 226, 307\\n\\nPercentile method, 185‚Äì190\\n\\ncentral interval, 187\\nPermutation null, 289, 296\\nPermutation test, 49‚Äì51\\nPhylogenetic tree, 261\\n\\n\\x0cSubject Index\\n\\n473\\n\\nPiecewise\\n\\nlinear, 313\\nnonlinear, 314\\n\\nPivotal\\n\\nargument, 183\\nquantity, 196, 198\\nstatistic, 16\\n.632 rule, 232\\nPoisson, 117, 193\\n\\ndistribution, 54, 117, 239\\nregression, 120‚Äì123, 249, 284, 295,\\n\\n435\\n\\nPoisson regression, 171\\nPolynomial kernel, 382, 392\\nPositive-deÔ¨Ånite function, 382\\nPost-selection inference, 317, 394‚Äì420\\nPosterior density, 235, 238\\nPosterior distribution, 416\\nPostwar era, 264\\nPrediction\\n\\nerrors, 216\\nrule, 208‚Äì213\\nPredictors, 124, 208\\nPrincipal components, 362\\nPrior distribution, 234‚Äì243\\n\\nbeta, 239\\nconjugate, 237‚Äì243\\ncoverage matching, 236‚Äì237\\ngamma, 239\\nnormal, 239\\nobjective Bayes, 234\\nproper, 239\\n\\nProbit analysis, 112, 120, 128\\nPropagation of errors, 420\\nProper prior, 239\\nProportional hazards model, 131,\\n\\n142‚Äì146, 266\\nProximal-Newton, 315\\nq-value, 280\\nQQ plot, 287\\nQR decomposition, 311, 322\\nQuadratic program, 377\\nQuasilikelihood, 266\\nQuetelet, Adolphe, 449\\nR, 178, 271\\nRandom forest, 209, 229, 324‚Äì332,\\n\\n347‚Äì350\\n\\nMonte Carlo variance, 330\\nsampling variance, 330\\nstandard error, 330‚Äì331\\n\\nRandomization, 49‚Äì51\\nRao‚ÄìBlackwell, 227, 231\\nRate annealing, 360\\nRectiÔ¨Åed linear, 359\\nRegression, 109\\nRegression rule, 219\\nRegression to the mean, 33\\nRegression tree, 124‚Äì128, 266, 348\\nRegularization, 101, 173, 298, 379, 428\\n\\npath, 306\\n\\nRelevance, 290‚Äì293\\nRelevance function, 293\\nRelevance theory, 297\\nReproducing kernel Hilbert space, 375,\\n\\n384, 392\\nResampling, 163\\nplans, 163‚Äì169\\nsimplex, 164, 169\\nvector, 163\\n\\nResidual deviance, 283\\nResponse, 124, 208\\nRidge regression, 97‚Äì102, 209, 304, 327,\\n\\n332, 372, 381\\nJames‚ÄìStein, 265\\n\\nRidge regularization, 368\\nlogistic regression, 392\\n\\nRight-censored, 150\\nRisk set, 144\\nRKHS, see reproducing-kernel Hilbert\\n\\nspace\\n\\nRobbins‚Äô formula, 75, 77, 422, 440\\nRobust estimation, 174‚Äì177\\nRoyal Statistical Society, 449\\nS language, 271\\nSample correlation coefÔ¨Åcient, 182\\nSample size coherency, 248\\nSampling distribution, 312\\nSAS, 271\\nSavage, L. J., 35, 36, 51, 199, 233, 251,\\n\\n450\\n\\nScale of evidence\\nFisher, 245\\nJeffreys, 245\\n\\nScheff¬¥e\\n\\nadaptive nearest-neighbor estimator,\\n\\n328\\n\\nleave-one-out cross-validated error,\\n\\n329\\n\\ninterval, 396, 397, 417\\ntheorem, 398\\nScore function, 42\\nScore tests, 301\\n\\n\\x0c474\\n\\nSubject Index\\n\\nSecond-order accuracy, 192‚Äì195\\nSelection bias, 33, 408‚Äì411\\nSelf-consistent, 149\\nSeparating hyperplane, 375\\n\\ngeometry, 390\\n\\nSeven-league boots, 448\\nShrinkage, 115, 316, 338\\n\\nestimator, 59, 91, 94, 96, 410\\n\\nSigmoid function, 352\\nSigniÔ¨Åcance level, 274\\nSimulation, 155‚Äì207\\nSimultaneous conÔ¨Ådence intervals,\\n\\n395‚Äì399\\n\\nSimultaneous inference, 294, 418\\nSinc kernel, 440, 445\\nSingle-nucleotide polymorphism, see\\n\\nSNP\\n\\nSmoothing operator, 346\\nSNP, 257\\nSoft-margin classiÔ¨Åer, 378‚Äì379\\nSoft-threshold, 315\\nSoftmax, 355\\nSpam Ô¨Ålter, 115\\nSparse\\n\\nmodels, 298‚Äì323\\nprincipal components, 321\\n\\nSparse matrix, 316\\nSparsity, 321\\nSplit-variable randomization, 327, 332\\nSPSS, 271\\nSquared error, 209\\nStandard candles, 210, 231\\nStandard error, 155\\nexternal, 408\\ninternal, 408\\n\\nStandard interval, 181\\nStein‚Äôs\\n\\nparadox, 105\\nunbiased risk estimate, 218, 231\\n\\nStepwise selection, 299\\nStochastic gradient descent, 358\\nStopping rule, 32, 413\\nStopping rules, 243\\nString kernel, 385, 386\\nStrong rules, 316, 322\\nStructure, 261\\nStructure matrix, 97, 424\\nStudent t\\n\\nconÔ¨Ådence interval, 396\\ndistribution, 196, 272\\nstatistic, 449\\n\\ntwo-sample, 8, 272\\nStudentized range, 418\\nSubgradient\\n\\ncondition, 308\\nequation, 312, 315\\n\\nSubjective prior distribution, 233\\nSubjective probability, 233\\nSubjectivism, 35, 233, 243, 261\\nSufÔ¨Åciency, 44\\nSufÔ¨Åcient\\n\\nstatistic, 66, 112, 116\\nvector, 66\\n\\nSupervised learning, 352\\nSupport\\n\\nset, 377, 378\\nvector, 377\\nvector classiÔ¨Åers, 381\\nvector machine, 319, 375‚Äì393\\n\\nSURE, see Stein‚Äôs unbiased risk estimate\\nSurvival analysis, 131‚Äì154, 266\\nSurvival curve, 137, 279\\nSVM\\n\\nLagrange dual, 391\\nLagrange primal, 391\\nloss function, 391\\nTaylor series, 157, 420\\nTheoretical null, 286\\nTied weights, 368\\nTime series, xvi\\nTraining set, 208\\nTransformation invariance, 183‚Äì185, 236\\nTransient episodes, 228\\nTrees\\n\\naveraging, 348\\nbest-Ô¨Årst, 333\\ndepth, 335\\nterminal node, 126\\nTricube kernel, 388, 389\\nTrimmed mean, 175\\nTriple-point, xv\\nTrue error rate, 210\\nTrue-discovery rates, 286\\nTukey, J. W., 418, 450\\nTukey, J. W., 418\\nTweedie‚Äôs formula, 409, 419, 440\\nTwenty-Ô¨Årst-century methods, xvi,\\n\\n271‚Äì446\\n\\nTwo-groups model, 278\\nUncorrected differences, 411\\nUninformative prior, 28, 169, 233, 261\\nUniversal approximator, 351\\n\\n\\x0cSubject Index\\n\\n475\\n\\nUnlabeled images, 365\\nUnobserved covariates, 288\\nValidation set, 213\\nVariable-importance plot, 331‚Äì332, 336\\nVariance, 14\\nVariance reduction, 324\\nVelocity vector, 360\\nVoting, 333\\nWarm starts, 314, 363\\nWeak learner, 333, 342\\nWeight\\n\\ndecay, 356\\nregularization, 361, 362\\nsharing, 352, 367\\n\\nWeighted exponential loss, 345\\nWeighted least squares, 315\\nWeighted majority vote, 341\\nWeights, 352\\nWide data, 298, 321\\nWilks‚Äô likelihood ratio statistic, 246\\nWinner‚Äôs curse, 33, 408\\nWinsorized mean, 175\\nWorking response, 315, 322\\nz.Àõ/, 188\\nZero set, 296\\n\\n\\x0c', metadata={'source': '../files/Computer Age Statistical Inference Book.txt'})]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"../files/Computer Age Statistical Inference Book.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='The Work, Computer Age Statistical Inference, was Ô¨Årst published by Cambridge University Press.\\nc(cid:13) in the Work, Bradley Efron and Trevor Hastie, 2016.\\nCambridge University Press‚Äôs catalogue entry for the Work can be found at http: // www. cambridge. org/\\n9781107149892\\nNB: The copy of the Work, as displayed on this website, can be purchased through Cambridge University\\nPress and other standard distribution channels. This copy is made available for personal use only and must\\nnot be adapted, sold or re-distributed.\\nCorrected November 10, 2017.', metadata={'source': '../files/Computer Age Statistical Inference Book.txt'}), Document(page_content='The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. ‚ÄúBig data,‚Äù ‚Äúdata science,‚Äù and ‚Äúmachine learning‚Äù have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going?This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories ‚Äì Bayesian, frequentist, Fisherian ‚Äì individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction', metadata={'source': '../files/Computer Age Statistical Inference Book.txt'})]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "documents = text_splitter.split_documents(text_documents)\n",
    "print(documents[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = DocArrayInMemorySearch.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='What might be called the strong deÔ¨Ånition of frequentism insists on exact\\nfrequentist correctness under experimental repetitions. Pivotality, unfortu-\\nnately, is unavailable in most statistical situations. Our looser deÔ¨Ånition\\nof frequentism, supplemented by devices such as those above,7 presents a\\nmore realistic picture of actual frequentist practice.\\n\\n2.2 Frequentist Optimality\\n\\nThe popularity of frequentist methods reÔ¨Çects their relatively modest math-\\nematical modeling assumptions: only a probability model F (more exactly\\na family of probabilities, Chapter 3) and an algorithm of choice t.x/. This\\nÔ¨Çexibility is also a defect in that the principle of frequentist correctness\\ndoesn‚Äôt help with the choice of algorithm. Should we use the sample mean\\nto estimate the location of the gfr distribution? Maybe the 25% Win-\\nsorized mean would be better, as Table 2.1 suggests.', metadata={'source': '../files/Computer Age Statistical Inference Book.txt'}),\n",
       " Document(page_content='Frequentism cannot claim to be a seamless philosophy of statistical in-\\nference. Paradoxes and contradictions abound within its borders, as will\\nbe shown in the next chapter. That being said, frequentist methods have\\na natural appeal to working scientists, an impressive history of success-\\nful application, and, as our list of Ô¨Åve ‚Äúdevices‚Äù suggests, the capacity to\\nencourage clever methodology. The story that follows is not one of aban-\\ndonment of frequentist thinking, but rather a broadening of connections\\nwith other methods.\\n\\n2.3 Notes and Details', metadata={'source': '../files/Computer Age Statistical Inference Book.txt'}),\n",
       " Document(page_content='Despite its simplicity, or perhaps because of it, objective Bayes procedures\\nare vulnerable to criticism from both ends of the statistical spectrum. From\\nthe subjectivist point of view, objective Bayes is only partially Bayesian: it\\nemploys Bayes‚Äô theorem but without doing the hard work of determining a\\nconvincing prior distribution. This introduces frequentist elements into its\\npractice‚Äîclearly so in the case of Jeffreys‚Äô prior‚Äîalong with frequentist\\nincoherencies.\\n\\nFor the frequentist, objective Bayes analysis can seem dangerously un-\\ntethered from the usual standards of accuracy, having only tenuous large-\\nsample claims to legitimacy. This is more than a theoretical objection. The\\npractical advantages claimed for Bayesian methods depend crucially on the\\nÔ¨Åne structure of the prior. Can we safely ignore stopping rules or selective\\ninference (e.g., choosing the largest of many estimated parameters for spe-\\ncial attention) for a prior not based on some form of genuine experience?', metadata={'source': '../files/Computer Age Statistical Inference Book.txt'}),\n",
       " Document(page_content='Frequentist statistics has the advantage of being applicable to any algo-\\nrithmic procedure, for instance to our Cp/OLS estimator. This has great\\nappeal in an era of enormous data sets and fast computation. The draw-\\nback, compared with Bayesian statistics, is that we have no guarantee that\\nour chosen algorithm is best in any way. Classical statistics developed a\\ntheory of best for a catalog of comparatively simple estimation and testing\\nproblems. In this sense, modern inferential theory has not yet caught up\\nwith modern problems such as data-based model selection, though tech-\\nniques such as model averaging (e.g., bagging) suggest promising steps\\nforward.\\n\\n20.3 Selection Bias', metadata={'source': '../files/Computer Age Statistical Inference Book.txt'})]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever()\n",
    "retriever.invoke(\"Who or what do frequentists criticize?\") # the pdf to text is garbage ... but great use case! ü§£"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup & Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "Answer the question based on the context below. Prior to finalizing your response, \n",
    "remember to clean the data and make sense of it. You are a pretrained LLM that understands\n",
    "common language, so use your best judgement if the text is too messy to give a definitive answer. \n",
    "If you can't answer the question because the text is too messy,\n",
    "reply \"The text is too messy to answer this question\". If you can't answer the question in general, reply \"I don't know\". \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = RunnableParallel(context=retriever, question=RunnablePassthrough())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frequentists criticize objective Bayes procedures and methods.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | model | parser\n",
    "chain.invoke(\"Who or what do frequentists criticize?\") # looks like neither of them were right...transformers for the win!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
