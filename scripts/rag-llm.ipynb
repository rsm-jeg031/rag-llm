{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from operator import itemgetter\n",
    "import tempfile\n",
    "import whisper\n",
    "from pytube import YouTube # if you want to do videos in your plug and play \n",
    "import pypdf\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, I'm here. How can I assist you today?\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "chain.invoke(\"hi, are you there gpt?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plug and Play Context üö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The twenty-first century has seen a \n",
      "breathtaking expansion of statistical methodology, both in scope and in influence. ‚ÄúBig data,‚Äù ‚Äúdata science,‚Äù and ‚Äúmachine learning‚Äù have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going?\n",
      "This book takes us on an exhilarating \n",
      "journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories ‚Äì Bayesian, frequentist, Fisherian ‚Äì individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.\n",
      "Efron & hasti EComput E r agE  \n",
      "st\n",
      "atisti Ca\n",
      "l \n",
      "in\n",
      "fE\n",
      "r\n",
      "E\n",
      "nC\n",
      "E‚ÄúHow and why is computational statistics taking over the world? In this serious \n",
      "work of synthesis that is also fun to read, Efron and Hastie give their take on the unreasonable effectiveness of statistics and machine learning in the context of a series of clear, historically informed examples.‚Äù\n",
      "‚Äî Andrew Gelman, Columbia University\n",
      " ‚ÄúComputer Age Statistical Inference is written especially for those who want to hear the big ideas, and see them instantiated through the essential mathematics that defines statistical analysis. It makes a great supplement to the traditional curricula for beginning graduate students.‚Äù\n",
      "‚Äî Rob Kass, Carnegie Mellon University\n",
      " ‚ÄúThis is a terrific book. It gives a clear, accessible, and entertaining account of the interplay between theory and methodological development that has driven statistics in the computer age. The authors succeed brilliantly in locating contemporary algorithmic methodologies for analysis of ‚Äòbig data‚Äô within the framework of established statistical theory.‚Äù\n",
      "‚Äî Alastair Young, Imperial College London\n",
      " ‚ÄúThis is a guided tour of modern statistics that emphasizes the conceptual and computational advances of the last century. Authored by two masters of the field, it offers just the right mix of mathematical analysis and insightful commentary.‚Äù\n",
      "‚Äî Hal Varian, Google\n",
      " ‚ÄúEfron and Hastie guide us through the maze of breakthrough statistical methodologies following the computing evolution: why they were developed, their properties, and how they are used. Highlighting their origins, the book helps us understand each method‚Äôs roles in inference and/or prediction.‚Äù\n",
      "‚Äî Galit Shmueli, National Tsing Hua University\n",
      " ‚ÄúA masterful guide to how the inferential bases of classical statistics can provide a principled disciplinary frame for the data science of the twenty-first century.‚Äù\n",
      " ‚Äî Stephen Stigler, University of Chicago, author of Seven Pillars of Statistical Wisdom\n",
      " ‚ÄúA refreshing view of modern statistics. Algorithmics are put on equal footing with intuition, properties, and the abstract arguments behind them. The methods covered are indispensable to practicing statistical analysts in today‚Äôs big data and big computing landscape.‚Äù\n",
      "‚Äî Robert Gramacy, The University of Chicago Booth School of BusinessBradley Efron is Max H. Stein Professor, \n",
      "Professor of Statistics, and Professor of Biomedical Data Science at Stanford University. He has held visiting faculty appointments at Harvard, UC Berkeley, and Imperial College London. Efron has worked extensively on theories of statistical inference, and is the inventor of the bootstrap sampling technique. He received the National Medal of Science in 2005 and the Guy Medal in Gold of the Royal Statistical Society in 2014. Trevor Hastie is John A. Overdeck Professor, Professor of Statistics, and Professor of Biomedical Data Science at Stanford University. He is coauthor of Elements of Statistical Learning, a key text in the field of modern data analysis. He is also known for his work on generalized additive models and principal curves, and for his contributions to the R computing environment. Hastie was awarded the Emmanuel and Carol Parzen prize for Statistical Innovation in 2014. Institute of Mathematical Statistics Monographs\n",
      "Editorial Board:\n",
      "D. R. Cox (University of Oxford)B. Hambly (University of Oxford)S. Holmes (Stanford University)J. Wellner (University of Washington)\n",
      "Cover illustration: Pacific Ocean wave, North Shore, Oahu, \n",
      "Hawaii. ¬© Brian Sytnyk / Getty Images.\n",
      "Cover designed by Zoe Naylor.\n",
      "P\n",
      "RINTED\n",
      " \n",
      "IN\n",
      " \n",
      "THE\n",
      " U\n",
      "NITED\n",
      " K\n",
      "INGDOM\n",
      "Comput E r agE \n",
      "stat\n",
      "istiCa\n",
      "l  \n",
      "in\n",
      "fE\n",
      "r\n",
      "E\n",
      "nC\n",
      "E\n",
      "al gorithms, Evid En CE,  and da ta sCiEn CEBradl Ey  Efron \n",
      "t\n",
      "rEv\n",
      "or \n",
      "ha\n",
      "stiE\n",
      "9781107149892 Efron & Hastie JKT C M Y KThe Work, Computer Age Statistical Inference, was Ô¨Årst published by Cambridge University Press.\n",
      "c‚Éùin the Work, Bradley Efron and Trevor Hastie, 2016.\n",
      "Cambridge University Press‚Äôs catalogue entry for the Work can be found at http: // www. cambridge. org/\n",
      "9781107149892\n",
      "NB: The copy of the Work, as displayed on this website, can be purchased through Cambridge University\n",
      "Press and other standard distribution channels. This copy is made available for personal use only and must\n",
      "not be adapted, sold or re-distributed.\n",
      "Corrected November 10, 2017.Computer Age Statistical Inference\n",
      "Algorithms, Evidence, and Data Science\n",
      "Bradley Efron Trevor Hastie\n",
      "Stanford UniversityTo Donna and LyndaviiiContents\n",
      "Preface xv\n",
      "Acknowledgments xviii\n",
      "Notation xix\n",
      "Part I Classic Statistical Inference 1\n",
      "1 Algorithms and Inference 3\n",
      "1.1 A Regression Example 4\n",
      "1.2 Hypothesis Testing 8\n",
      "1.3 Notes 11\n",
      "2 Frequentist Inference 12\n",
      "2.1 Frequentism in Practice 14\n",
      "2.2 Frequentist Optimality 18\n",
      "2.3 Notes and Details 20\n",
      "3 Bayesian Inference 22\n",
      "3.1 Two Examples 24\n",
      "3.2 Uninformative Prior Distributions 28\n",
      "3.3 Flaws in Frequentist Inference 30\n",
      "3.4 A Bayesian/Frequentist Comparison List 33\n",
      "3.5 Notes and Details 36\n",
      "4 Fisherian Inference and Maximum Likelihood Estimation 38\n",
      "4.1 Likelihood and Maximum Likelihood 38\n",
      "4.2 Fisher Information and the MLE 41\n",
      "4.3 Conditional Inference 45\n",
      "4.4 Permutation and Randomization 49\n",
      "4.5 Notes and Details 51\n",
      "5 Parametric Models and Exponential Families 53\n",
      "ixx Contents\n",
      "5.1 Univariate Families 54\n",
      "5.2 The Multivariate Normal Distribution 55\n",
      "5.3 Fisher‚Äôs Information Bound for Multiparameter Families 59\n",
      "5.4 The Multinomial Distribution 61\n",
      "5.5 Exponential Families 64\n",
      "5.6 Notes and Details 69\n",
      "Part II Early Computer-Age Methods 73\n",
      "6 Empirical Bayes 75\n",
      "6.1 Robbins‚Äô Formula 75\n",
      "6.2 The Missing-Species Problem 78\n",
      "6.3 A Medical Example 84\n",
      "6.4 Indirect Evidence 1 88\n",
      "6.5 Notes and Details 88\n",
      "7 James‚ÄìStein Estimation and Ridge Regression 91\n",
      "7.1 The James‚ÄìStein Estimator 91\n",
      "7.2 The Baseball Players 94\n",
      "7.3 Ridge Regression 97\n",
      "7.4 Indirect Evidence 2 102\n",
      "7.5 Notes and Details 104\n",
      "8 Generalized Linear Models and Regression Trees 108\n",
      "8.1 Logistic Regression 109\n",
      "8.2 Generalized Linear Models 116\n",
      "8.3 Poisson Regression 120\n",
      "8.4 Regression Trees 124\n",
      "8.5 Notes and Details 128\n",
      "9 Survival Analysis and the EM Algorithm 131\n",
      "9.1 Life Tables and Hazard Rates 131\n",
      "9.2 Censored Data and the Kaplan‚ÄìMeier Estimate 134\n",
      "9.3 The Log-Rank Test 139\n",
      "9.4 The Proportional Hazards Model 143\n",
      "9.5 Missing Data and the EM Algorithm 146\n",
      "9.6 Notes and Details 150\n",
      "10 The Jackknife and the Bootstrap 155\n",
      "10.1 The Jackknife Estimate of Standard Error 156\n",
      "10.2 The Nonparametric Bootstrap 159\n",
      "10.3 Resampling Plans 163Contents xi\n",
      "10.4 The Parametric Bootstrap 169\n",
      "10.5 InÔ¨Çuence Functions and Robust Estimation 174\n",
      "10.6 Notes and Details 177\n",
      "11 Bootstrap ConÔ¨Ådence Intervals 181\n",
      "11.1 Neyman‚Äôs Construction for One-Parameter Problems 181\n",
      "11.2 The Percentile Method 185\n",
      "11.3 Bias-Corrected ConÔ¨Ådence Intervals 190\n",
      "11.4 Second-Order Accuracy 192\n",
      "11.5 Bootstrap- tIntervals 195\n",
      "11.6 Objective Bayes Intervals and the ConÔ¨Ådence Distribution 198\n",
      "11.7 Notes and Details 204\n",
      "12 Cross-Validation and CpEstimates of Prediction Error 208\n",
      "12.1 Prediction Rules 208\n",
      "12.2 Cross-Validation 213\n",
      "12.3 Covariance Penalties 218\n",
      "12.4 Training, Validation, and Ephemeral Predictors 227\n",
      "12.5 Notes and Details 230\n",
      "13 Objective Bayes Inference and MCMC 233\n",
      "13.1 Objective Prior Distributions 234\n",
      "13.2 Conjugate Prior Distributions 237\n",
      "13.3 Model Selection and the Bayesian Information Criterion 243\n",
      "13.4 Gibbs Sampling and MCMC 251\n",
      "13.5 Example: Modeling Population Admixture 256\n",
      "13.6 Notes and Details 261\n",
      "14 Postwar Statistical Inference and Methodology 264\n",
      "Part III Twenty-First-Century Topics 269\n",
      "15 Large-Scale Hypothesis Testing and FDRs 271\n",
      "15.1 Large-Scale Testing 272\n",
      "15.2 False-Discovery Rates 275\n",
      "15.3 Empirical Bayes Large-Scale Testing 278\n",
      "15.4 Local False-Discovery Rates 282\n",
      "15.5 Choice of the Null Distribution 286\n",
      "15.6 Relevance 290\n",
      "15.7 Notes and Details 294\n",
      "16 Sparse Modeling and the Lasso 298xii Contents\n",
      "16.1 Forward Stepwise Regression 299\n",
      "16.2 The Lasso 303\n",
      "16.3 Fitting Lasso Models 308\n",
      "16.4 Least-Angle Regression 309\n",
      "16.5 Fitting Generalized Lasso Models 313\n",
      "16.6 Post-Selection Inference for the Lasso 317\n",
      "16.7 Connections and Extensions 319\n",
      "16.8 Notes and Details 321\n",
      "17 Random Forests and Boosting 324\n",
      "17.1 Random Forests 325\n",
      "17.2 Boosting with Squared-Error Loss 333\n",
      "17.3 Gradient Boosting 338\n",
      "17.4 Adaboost: the Original Boosting Algorithm 341\n",
      "17.5 Connections and Extensions 345\n",
      "17.6 Notes and Details 347\n",
      "18 Neural Networks and Deep Learning 351\n",
      "18.1 Neural Networks and the Handwritten Digit Problem 353\n",
      "18.2 Fitting a Neural Network 356\n",
      "18.3 Autoencoders 362\n",
      "18.4 Deep Learning 364\n",
      "18.5 Learning a Deep Network 368\n",
      "18.6 Notes and Details 371\n",
      "19 Support-Vector Machines and Kernel Methods 375\n",
      "19.1 Optimal Separating Hyperplane 376\n",
      "19.2 Soft-Margin ClassiÔ¨Åer 378\n",
      "19.3 SVM Criterion as Loss Plus Penalty 379\n",
      "19.4 Computations and the Kernel Trick 381\n",
      "19.5 Function Fitting Using Kernels 384\n",
      "19.6 Example: String Kernels for Protein ClassiÔ¨Åcation 385\n",
      "19.7 SVMs: Concluding Remarks 387\n",
      "19.8 Kernel Smoothing and Local Regression 387\n",
      "19.9 Notes and Details 390\n",
      "20 Inference After Model Selection 394\n",
      "20.1 Simultaneous ConÔ¨Ådence Intervals 395\n",
      "20.2 Accuracy After Model Selection 402\n",
      "20.3 Selection Bias 408\n",
      "20.4 Combined Bayes‚ÄìFrequentist Estimation 412\n",
      "20.5 Notes and Details 417Contents xiii\n",
      "21 Empirical Bayes Estimation Strategies 421\n",
      "21.1 Bayes Deconvolution 421\n",
      "21.2g-Modeling and Estimation 424\n",
      "21.3 Likelihood, Regularization, and Accuracy 427\n",
      "21.4 Two Examples 432\n",
      "21.5 Generalized Linear Mixed Models 437\n",
      "21.6 Deconvolution and f-Modeling 440\n",
      "21.7 Notes and Details 444\n",
      "Epilogue 446\n",
      "References 453\n",
      "Author Index 463\n",
      "Subject Index 467xivPreface\n",
      "Statistical inference is an unusually wide-ranging discipline, located as it is\n",
      "at the triple-point of mathematics, empirical science, and philosophy. The\n",
      "discipline can be said to date from 1763, with the publication of Bayes‚Äô\n",
      "rule (representing the philosophical side of the subject; the rule‚Äôs early ad-\n",
      "vocates considered it an argument for the existence of God). The most re-\n",
      "cent quarter of this 250-year history‚Äîfrom the 1950s to the present‚Äîis\n",
      "the ‚Äúcomputer age‚Äù of our book‚Äôs title, the time when computation, the tra-\n",
      "ditional bottleneck of statistical applications, became faster and easier by a\n",
      "factor of a million.\n",
      "The book is an examination of how statistics has evolved over the past\n",
      "sixty years‚Äîan aerial view of a vast subject, but seen from the height of a\n",
      "small plane, not a jetliner or satellite. The individual chapters take up a se-\n",
      "ries of inÔ¨Çuential topics‚Äîgeneralized linear models, survival analysis, the\n",
      "jackknife and bootstrap, false-discovery rates, empirical Bayes, MCMC,\n",
      "neural nets, and a dozen more‚Äîdescribing for each the key methodologi-\n",
      "cal developments and their inferential justiÔ¨Åcation.\n",
      "Needless to say, the role of electronic computation is central to our\n",
      "story. This doesn‚Äôt mean that every advance was computer-related. A land\n",
      "bridge had opened to a new continent but not all were eager to cross.\n",
      "Topics such as empirical Bayes and James‚ÄìStein estimation could have\n",
      "emerged just as well under the constraints of mechanical computation. Oth-\n",
      "ers, like the bootstrap and proportional hazards, were pureborn children of\n",
      "the computer age. Almost all topics in twenty-Ô¨Årst-century statistics are\n",
      "now computer-dependent, but it will take our small plane a while to reach\n",
      "the new millennium.\n",
      "Dictionary deÔ¨Ånitions of statistical inference tend to equate it with the\n",
      "entire discipline. This has become less satisfactory in the ‚Äúbig data‚Äù era of\n",
      "immense computer-based processing algorithms. Here we will attempt, not\n",
      "always consistently, to separate the two aspects of the statistical enterprise:\n",
      "algorithmic developments aimed at speciÔ¨Åc problem areas, for instance\n",
      "xvxvi Preface\n",
      "random forests for prediction, as distinct from the inferential arguments\n",
      "offered in their support.\n",
      "Very broadly speaking, algorithms are what statisticians do while infer-\n",
      "ence says why they do them. A particularly energetic brand of the statisti-\n",
      "cal enterprise has Ô¨Çourished in the new century, data science , emphasizing\n",
      "algorithmic thinking rather than its inferential justiÔ¨Åcation. The later chap-\n",
      "ters of our book, where large-scale prediction algorithms such as boosting\n",
      "and deep learning are examined, illustrate the data-science point of view.\n",
      "(See the epilogue for a little more on the sometimes fraught statistics/data\n",
      "science marriage.)\n",
      "There are no such subjects as Biological Inference or Astronomical In-\n",
      "ference or Geological Inference. Why do we need ‚ÄúStatistical Inference‚Äù?\n",
      "The answer is simple: the natural sciences have nature to judge the ac-\n",
      "curacy of their ideas. Statistics operates one step back from Nature, most\n",
      "often interpreting the observations of natural scientists. Without Nature to\n",
      "serve as a disinterested referee, we need a system of mathematical logic for\n",
      "guidance and correction. Statistical inference is that system, distilled from\n",
      "two and a half centuries of data-analytic experience.\n",
      "The book proceeds historically, in three parts. The great themes of clas-\n",
      "sical inference, Bayesian, frequentist, and Fisherian, reviewed in Part I,\n",
      "were set in place before the age of electronic computation. Modern practice\n",
      "has vastly extended their reach without changing the basic outlines. (An\n",
      "analogy with classical and modern literature might be made.) Part II con-\n",
      "cerns early computer-age developments, from the 1950s through the 1990s.\n",
      "As a transitional period, this is the time when it is easiest to see the ef-\n",
      "fects, or noneffects, of fast computation on the progress of statistical meth-\n",
      "odology, both in its theory and practice. Part III, ‚ÄúTwenty-First-Century\n",
      "topics,‚Äù brings the story up to the present. Ours is a time of enormously\n",
      "ambitious algorithms (‚Äúmachine learning‚Äù being the somewhat disquieting\n",
      "catchphrase). Their justiÔ¨Åcation is the ongoing task of modern statistical\n",
      "inference.\n",
      "Neither a catalog nor an encyclopedia, the book‚Äôs topics were chosen as\n",
      "apt illustrations of the interplay between computational methodology and\n",
      "inferential theory. Some missing topics that might have served just as well\n",
      "include time series, general estimating equations, causal inference, graph-\n",
      "ical models, and experimental design. In any case, there is no implication\n",
      "that the topics presented here are the only ones worthy of discussion.\n",
      "Also underrepresented are asymptotics and decision theory, the ‚Äúmath\n",
      "stat‚Äù side of the Ô¨Åeld. Our intention was to maintain a technical level of\n",
      "discussion appropriate to Masters‚Äô-level statisticians or Ô¨Årst-year PhD stu-Preface xvii\n",
      "dents. Inevitably, some of the presentation drifts into more difÔ¨Åcult waters,\n",
      "more from the nature of the statistical ideas than the mathematics. Readers\n",
      "who Ô¨Ånd our aerial view circling too long over some topic shouldn‚Äôt hesi-\n",
      "tate to move ahead in the book. For the most part, the chapters can be read\n",
      "independently of each other (though there is a connecting overall theme).\n",
      "This comment applies especially to nonstatisticians who have picked up\n",
      "the book because of interest in some particular topic, say survival analysis\n",
      "or boosting.\n",
      "Useful disciplines that serve a wide variety of demanding clients run\n",
      "the risk of losing their center. Statistics has managed, for the most part,\n",
      "to maintain its philosophical cohesion despite a rising curve of outside de-\n",
      "mand. The center of the Ô¨Åeld has in fact moved in the past sixty years, from\n",
      "its traditional home in mathematics and logic toward a more computational\n",
      "focus. Our book traces that movement on a topic-by-topic basis. An answer\n",
      "to the intriguing question ‚ÄúWhat happens next?‚Äù won‚Äôt be attempted here,\n",
      "except for a few words in the epilogue, where the rise of data science is\n",
      "discussed.Acknowledgments\n",
      "We are indebted to Cindy Kirby for her skillful work in the preparation of\n",
      "this book, and Galit Shmueli for her helpful comments on an earlier draft.\n",
      "At Cambridge University Press, a huge thank you to Steven Holt for his ex-\n",
      "cellent copy editing, Clare Dennison for guiding us through the production\n",
      "phase, and to Diana Gillooly, our editor, for her unfailing support.\n",
      "Bradley Efron\n",
      "Trevor Hastie\n",
      "Department of Statistics\n",
      "Stanford University\n",
      "May 2016\n",
      "xviiixix\n",
      "Notation\n",
      "Throughout the book the numbered ¬ésign indicates a technical note or\n",
      "reference element which is elaborated on at the end of the chapter. There,\n",
      "next to the number, the page number of the referenced location is given in\n",
      "parenthesis. For example, lowess in the notes on page 11 was referenced\n",
      "via a¬é1on page 6. Matrices such as ‚Ä†are represented in bold font, as\n",
      "are certain vectors such as y, a data vector with nelements. Most other\n",
      "vectors, such as coefÔ¨Åcient vectors, are typically not bold. We use a dark\n",
      "green typewriter font to indicate data set names such as prostate ,\n",
      "variable names such as prog from data sets, and Rcommands such as\n",
      "glmnet orlocfdr . No bibliographic references are given in the body of\n",
      "the text; important references are given in the endnotes of each chapter.Part I\n",
      "Classic Statistical Inference1\n",
      "Algorithms and Inference\n",
      "Statistics is the science of learning from experience, particularly experi-\n",
      "ence that arrives a little bit at a time: the successes and failures of a new\n",
      "experimental drug, the uncertain measurements of an asteroid‚Äôs path to-\n",
      "ward Earth. It may seem surprising that any one theory can cover such an\n",
      "amorphous target as ‚Äúlearning from experience.‚Äù In fact, there are twomain\n",
      "statistical theories, Bayesianism and frequentism, whose connections and\n",
      "disagreements animate many of the succeeding chapters.\n",
      "First, however, we want to discuss a less philosophical, more operational\n",
      "division of labor that applies to both theories: between the algorithmic and\n",
      "inferential aspects of statistical analysis. The distinction begins with the\n",
      "most basic, and most popular, statistical method, averaging. Suppose we\n",
      "have observed numbers x1;x2;:::;xnapplying to some phenomenon of\n",
      "interest, perhaps the automobile accident rates in the nD50states. The\n",
      "mean\n",
      "NxDnX\n",
      "iD1xi=n (1.1)\n",
      "summarizes the results in a single number.\n",
      "How accurate is that number? The textbook answer is given in terms of\n",
      "thestandard error ,\n",
      "bseD\"nX\n",
      "iD1.xi\u0000Nx/2ƒ±\n",
      ".n.n\u00001//#1=2\n",
      ": (1.2)\n",
      "Here averaging (1.1) is the algorithm, while the standard error provides an\n",
      "inference of the algorithm‚Äôs accuracy. It is a surprising, and crucial, aspect\n",
      "of statistical theory that the same data that supplies an estimate can also\n",
      "assess its accuracy.1\n",
      "1‚ÄúInference‚Äù concerns more than accuracy: speaking broadly, algorithms say what the\n",
      "statistician does while inference says why he or she does it.\n",
      "34 Algorithms and Inference\n",
      "Of course,bse (1.2) is itself an algorithm, which could be (and is) subject\n",
      "to further inferential analysis concerning itsaccuracy. The point is that\n",
      "the algorithm comes Ô¨Årst and the inference follows at a second level of\n",
      "statistical consideration. In practice this means that algorithmic invention\n",
      "is a more free-wheeling and adventurous enterprise, with inference playing\n",
      "catch-up as it strives to assess the accuracy, good or bad, of some hot new\n",
      "algorithmic methodology.\n",
      "If the inference/algorithm race is a tortoise-and-hare affair, then modern\n",
      "electronic computation has bred a bionic hare. There are two effects at work\n",
      "here: computer-based technology allows scientists to collect enormous data\n",
      "sets, orders of magnitude larger than those that classic statistical theory\n",
      "was designed to deal with; huge data demands new methodology, and the\n",
      "demand is being met by a burst of innovative computer-based statistical\n",
      "algorithms. When one reads of ‚Äúbig data‚Äù in the news, it is usually these\n",
      "algorithms playing the starring roles.\n",
      "Our book‚Äôs title, Computer Age Statistical Inference , emphasizes the tor-\n",
      "toise‚Äôs side of the story. The past few decades have been a golden age of\n",
      "statistical methodology. It hasn‚Äôt been, quite, a golden age for statistical\n",
      "inference, but it has not been a dark age either. The efÔ¨Çorescence of am-\n",
      "bitious new algorithms has forced an evolution (though not a revolution)\n",
      "in inference, the theories by which statisticians choose among competing\n",
      "methods. The book traces the interplay between methodology and infer-\n",
      "ence as it has developed since the 1950s, the beginning of our discipline‚Äôs\n",
      "computer age. As a preview, we end this chapter with two examples illus-\n",
      "trating the transition from classic to computer-age practice.\n",
      "1.1 A Regression Example\n",
      "Figure 1.1 concerns a study of kidney function. Data points .xi;yi/have\n",
      "been observed for nD157healthy volunteers, with xitheith volunteer‚Äôs\n",
      "age in years, and yia composite measure ‚Äú tot‚Äù of overall function. Kid-\n",
      "ney function generally declines with age, as evident in the downward scat-\n",
      "ter of the points. The rate of decline is an important question in kidney\n",
      "transplantation: in the past, potential donors past age 60 were prohibited,\n",
      "though, given a shortage of donors, this is no longer enforced.\n",
      "The solid line in Figure 1.1 is a linear regression\n",
      "yDOÀá0COÀá1x (1.3)\n",
      "Ô¨Åt to the data by least squares , that is by minimizing the sum of squared1.1 A Regression Example 5\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "****\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "*****\n",
      "*\n",
      "*\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "****\n",
      "****\n",
      "***\n",
      "*\n",
      "20 30 40 50 60 70 80 90‚àí6 ‚àí4 ‚àí2 0 2 4\n",
      "agetot\n",
      "Figure 1.1 Kidney Ô¨Åtness tot vsage for 157 volunteers. The\n",
      "line is a linear regression Ô¨Åt, showing Àô2standard errors at\n",
      "selected values of age.\n",
      "deviations\n",
      "nX\n",
      "iD1.yi\u0000Àá0\u0000Àá1xi/2(1.4)\n",
      "over all choices of .Àá0;Àá1/. The least squares algorithm, which dates back\n",
      "to Gauss and Legendre in the early 1800s, gives OÀá0D2:86 andOÀá1D\n",
      "\u00000:079 as the least squares estimates. We can read off of the Ô¨Åtted line\n",
      "an estimated value of kidney Ô¨Åtness for any chosen age. The top line of\n",
      "Table 1.1 shows estimate 1.29 at age 20, down to\u00003:43 atage 80.\n",
      "How accurate are these estimates? This is where inference comes in:\n",
      "an extended version of formula (1.2), also going back to the 1800s, pro-\n",
      "vides the standard errors, shown in line 2 of the table. The vertical bars in\n",
      "Figure 1.1 areÀôtwo standard errors, giving them about 95% chance of\n",
      "containing the true expected value of tot at each age.\n",
      "That 95% coverage depends on the validity of the linear regression model\n",
      "(1.3). We might instead try a quadratic regression yDOÀá0COÀá1xCOÀá2x2,\n",
      "or a cubic, etc., all of this being well within the reach of pre-computer\n",
      "statistical theory.6 Algorithms and Inference\n",
      "Table 1.1 Regression analysis of the kidney data; (1) linear regression\n",
      "estimates; (2) their standard errors; (3) lowess estimates; (4) their\n",
      "bootstrap standard errors.\n",
      "age 20 30 40 50 60 70 80\n",
      "1. linear regression 1.29 .50 \u0000.28\u00001.07\u00001.86\u00002.64\u00003.43\n",
      "2. std error .21 .15 .15 .19 .26 .34 .42\n",
      "3. lowess 1.66 .65 \u0000.59\u00001.27\u00001.91\u00002.68\u00003.50\n",
      "4. bootstrap std error .71 .23 .31 .32 .37 .47 .70\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "****\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "*****\n",
      "*\n",
      "*\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "****\n",
      "****\n",
      "***\n",
      "*\n",
      "20 30 40 50 60 70 80 90‚àí6 ‚àí4 ‚àí2 0 2 4\n",
      "agetot\n",
      "Figure 1.2 Local polynomial lowess(x,y,1/3) Ô¨Åt to the\n",
      "kidney-Ô¨Åtness data, with Àô2bootstrap standard deviations.\n",
      "A modern computer-based algorithm lowess produced the somewhat\n",
      "bumpy regression curve in Figure 1.2. The lowess¬é 2algorithm moves ¬é1\n",
      "its attention along the x-axis, Ô¨Åtting local polynomial curves of differing\n",
      "degrees to nearby .x;y/ points. (The 1/3 in the call3lowess(x,y,1/3)\n",
      "2Here and throughout the book, the numbered ¬ésign indicates a technical note or\n",
      "reference element which is elaborated on at the end of the chapter.\n",
      "3Here and in all our examples we are employing the language R, itself one of the key\n",
      "developments in computer-based statistical methodology.1.1 A Regression Example 7\n",
      "determines the deÔ¨Ånition of local.) Repeated passes over the x-axis reÔ¨Åne\n",
      "the Ô¨Åt, reducing the effects of occasional anomalous points. The Ô¨Åtted curve\n",
      "in Figure 1.2 is nearly linear at the right, but more complicated at the left\n",
      "where points are more densely packed. It is Ô¨Çat between ages 25 and 35,\n",
      "a potentially important difference from the uniform decline portrayed in\n",
      "Figure 1.1.\n",
      "There is no formula such as (1.2) to infer the accuracy of the lowess\n",
      "curve. Instead, a computer-intensive inferential engine, the bootstrap , was\n",
      "used to calculate the error bars in Figure 1.2. A bootstrap data set is pro-\n",
      "duced by resampling 157 pairs .xi;yi/from the original 157 with replace-\n",
      "ment , so perhaps .x1;y1/might show up twice in the bootstrap sample,\n",
      ".x2;y2/might be missing, .x3;y3/present once, etc. Applying lowess\n",
      "to the bootstrap sample generates a bootstrap replication of the original\n",
      "calculation.\n",
      "20 30 40 50 60 70 80 90‚àí4 ‚àí2 0 2 4\n",
      "agetot\n",
      "Figure 1.3 25 bootstrap replications of lowess(x,y,1/3) .\n",
      "Figure 1.3 shows the Ô¨Årst 25 (of 250) bootstrap lowess replications\n",
      "bouncing around the original curve from Figure 1.2. The variability of the\n",
      "replications at any one age, the bootstrap standard deviation , determined\n",
      "the original curve‚Äôs accuracy. How and why the bootstrap works is dis-\n",
      "cussed in Chapter 10. It has the great virtue of assessing estimation accu-8 Algorithms and Inference\n",
      "racy for anyalgorithm, no matter how complicated. The price is a hundred-\n",
      "or thousand-fold increase in computation, unthinkable in 1930, but routine\n",
      "now.\n",
      "The bottom two lines of Table 1.1 show the lowess estimates and\n",
      "their standard errors. We have paid a price for the increased Ô¨Çexibility of\n",
      "lowess , its standard errors roughly doubling those for linear regression.\n",
      "1.2 Hypothesis Testing\n",
      "Our second example concerns the march of methodology and inference\n",
      "forhypothesis testing rather than estimation: 72 leukemia patients, 47 with\n",
      "ALL (acute lymphoblastic leukemia) and 25 with AML (acute myeloid leuk-\n",
      "emia, a worse prognosis) have each had genetic activity measured for a\n",
      "panel of 7,128 genes. The histograms in Figure 1.4 compare the genetic\n",
      "activities in the two groups for gene 136.\n",
      " \n",
      "ALL scores ‚àí mean .752  \n",
      "0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.60246810\n",
      "AML scores ‚àí mean .950 \n",
      "0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.60246810\n",
      "Figure 1.4 Scores for gene 136, leukemia data. Top ALL\n",
      "(nD47), bottom AML (nD25). A two-sample t-statisticD3:01\n",
      "withp-valueD:0036 .\n",
      "TheAML group appears to show greater activity, the mean values being\n",
      "ALLD0:752 and AMLD0:950: (1.5)1.2 Hypothesis Testing 9\n",
      "Is the perceived difference genuine, or perhaps, as people like to say, ‚Äúa\n",
      "statistical Ô¨Çuke‚Äù? The classic answer to this question is via a two-sample\n",
      "t-statistic ,\n",
      "tDAML\u0000ALL\n",
      "bsd; (1.6)\n",
      "wherebsd is an estimate of the numerator‚Äôs standard deviation.4\n",
      "Dividing by bsd allows us (under Gaussian assumptions discussed in\n",
      "Chapter 5) to compare the observed value of twith a standard ‚Äúnull‚Äù dis-\n",
      "tribution, in this case a Student‚Äôs tdistribution with 70 degrees of freedom.\n",
      "We obtaintD3:01 from (1.6), which would classically be considered very\n",
      "strong evidence that the apparent difference (1.5) is genuine; in standard\n",
      "terminology, ‚Äúwith two-sided signiÔ¨Åcance level 0.0036.‚Äù\n",
      "A small signiÔ¨Åcance level (or ‚Äú p-value‚Äù) is a statement of statistical sur-\n",
      "prise: something very unusual has happened if in fact there is no difference\n",
      "in gene 136 expression levels between ALL andAML patients. We are less\n",
      "surprised by tD3:01 if gene 136 is just one candidate out of thousands\n",
      "that might have produced ‚Äúinteresting‚Äù results.\n",
      "That is the case here. Figure 1.5 shows the histogram of the two-sample\n",
      "t-statistics for the panel of 7128 genes. Now tD3:01 looks less unusual;\n",
      "400 other genes have texceeding 3.01, about 5.6% of them.\n",
      "This doesn‚Äôt mean that gene 136 is ‚ÄúsigniÔ¨Åcant at the 0.056 level.‚Äù There\n",
      "are two powerful complicating factors:\n",
      "1 Large numbers of candidates, 7128 here, will produce some large t-\n",
      "values even if there is really no difference in genetic expression between\n",
      "ALL andAML patients.\n",
      "2 The histogram implies that in this study there is something wrong with\n",
      "the theoretical null distribution (‚ÄúStudent‚Äôs twith 70 degrees of free-\n",
      "dom‚Äù), the smooth curve in Figure 1.5. It is much too narrow at the cen-\n",
      "ter, where presumably most of the genes are reporting non-signiÔ¨Åcant\n",
      "results.\n",
      "We will see in Chapter 15 that a low false-discovery rate , i.e., a low\n",
      "chance of crying wolf over an innocuous gene, requires texceeding 6.16\n",
      "in the ALL/AML study. Only 47 of the 7128 genes make the cut. False-\n",
      "discovery-rate theory is an impressive advance in statistical inference, in-\n",
      "corporating Bayesian, frequentist, and empirical Bayesian (Chapter 6) el-\n",
      "4Formally, a standard error is the standard deviation of a summary statistic, and bsd might\n",
      "better be called bse, but we will follow the distinction less than punctiliously here.10 Algorithms and Inference\n",
      " \n",
      "t statisticsFrequency\n",
      "‚àí10 ‚àí5 0 5 100100200300400500600700\n",
      "3.01\n",
      "Figure 1.5 Two-samplet-statistics for 7128 genes, leukemia\n",
      "data. The smooth curve is the theoretical null density for the\n",
      "t-statistic.\n",
      "ements. It was a necessary advance in a scientiÔ¨Åc world where computer-\n",
      "based technology routinely presents thousands of comparisons to be eval-\n",
      "uated at once.\n",
      "There is one more thing to say about the algorithm/inference statistical\n",
      "cycle. Important new algorithms often arise outside the world of profes-\n",
      "sional statisticians: neural nets, support vector machines, and boosting are\n",
      "three famous examples. None of this is surprising. New sources of data,\n",
      "satellite imagery for example, or medical microarrays, inspire novel meth-\n",
      "odology from the observing scientists. The early literature tends toward the\n",
      "enthusiastic, with claims of enormous applicability and power.\n",
      "In the second phase, statisticians try to locate the new metholodogy\n",
      "within the framework of statistical theory. In other words, they carry out\n",
      "the statistical inference part of the cycle, placing the new methodology\n",
      "within the known Bayesian and frequentist limits of performance. (Boost-\n",
      "ing offers a nice example, Chapter 17.) This is a healthy chain of events,\n",
      "good both for the hybrid vigor of the statistics profession and for the further\n",
      "progress of algorithmic technology.1.3 Notes 11\n",
      "1.3 Notes\n",
      "Legendre published the least squares algorithm in 1805, causing Gauss\n",
      "to state that he had been using the method in astronomical orbit-Ô¨Åtting\n",
      "since 1795. Given Gauss‚Äô astonishing production of major mathematical\n",
      "advances, this says something about the importance attached to the least\n",
      "squares idea. Chapter 8 includes its usual algebraic formulation, as well as\n",
      "Gauss‚Äô formula for the standard errors, line 2 of Table 1.1.\n",
      "Our division between algorithms and inference brings to mind Tukey‚Äôs\n",
      "exploratory/conÔ¨Årmatory system. However the current algorithmic world\n",
      "is often bolder in its claims than the word ‚Äúexploratory‚Äù implies, while to\n",
      "our minds ‚Äúinference‚Äù conveys something richer than mere conÔ¨Årmation.\n",
      "¬é1[p. 6] lowess was devised by William Cleveland (Cleveland, 1981) and\n",
      "is available in the R statistical computing language. It is applied to the\n",
      "kidney data in Efron (2004). The kidney data originated in the nephrology\n",
      "laboratory of Dr. Brian Myers, Stanford University, and is available from\n",
      "this book‚Äôs web site.2\n",
      "Frequentist Inference\n",
      "Before the computer age there was the calculator age, and before ‚Äúbig data‚Äù\n",
      "there were small data sets, often a few hundred numbers or fewer, labori-\n",
      "ously collected by individual scientists working under restrictive experi-\n",
      "mental constraints. Precious data calls for maximally efÔ¨Åcient statistical\n",
      "analysis. A remarkably effective theory, feasible for execution on mechan-\n",
      "ical desk calculators, was developed beginning in 1900 by Pearson, Fisher,\n",
      "Neyman, Hotelling, and others, and grew to dominate twentieth-century\n",
      "statistical practice. The theory, now referred to as classical , relied almost\n",
      "entirely on frequentist inferential ideas. This chapter sketches a quick and\n",
      "simpliÔ¨Åed picture of frequentist inference, particularly as employed in clas-\n",
      "sical applications.\n",
      "We begin with another example from Dr. Myers‚Äô nephrology laboratory:\n",
      "211 kidney patients have had their glomerular Ô¨Åltration rates measured,\n",
      "with the results shown in Figure 2.1; gfr is an important indicator of kid-\n",
      "ney function, with low values suggesting trouble. (It is a key component of\n",
      "tot in Figure 1.1.) The mean and standard error (1.1)‚Äì(1.2) are NxD54:25\n",
      "andbseD0:95, typically reported as\n",
      "54:25Àô0:95I (2.1)\n",
      "Àô0:95 denotes a frequentist inference for the accuracy of the estimate NxD\n",
      "54:25 , and suggests that we shouldn‚Äôt take the ‚Äú.25‚Äù very seriously, even\n",
      "the ‚Äú4‚Äù being open to doubt. Where the inference comes from and what\n",
      "exactly it means remains to be said.\n",
      "Statistical inference usually begins with the assumption that some prob-\n",
      "ability model has produced the observed data x, in our case the vector of\n",
      "nD211gfr measurements xD.x1;x2;:::;xn/. LetXD.X1;X2;:::;\n",
      "Xn/indicatenindependent draws from a probability distribution F, writ-\n",
      "ten\n",
      "F!X; (2.2)\n",
      "12Frequentist Inference 13\n",
      " \n",
      "gfrFrequency\n",
      "20 40 60 80 1000 5 10 15 20 25 30\n",
      "Figure 2.1 Glomerular Ô¨Åltration rates for 211 kidney patients;\n",
      "mean 54.25, standard error .95.\n",
      "Fbeing the underlying distribution of possible gfr scores here. A realiza-\n",
      "tionXDxof (2.2) has been observed, and the statistician wishes to infer\n",
      "some property of the unknown distribution F.\n",
      "Suppose the desired property is the expectation of a single random draw\n",
      "XfromF, denoted\n",
      "\u0012DEFfXg (2.3)\n",
      "(which also equals the expectation of the average NXDPXi=nof random\n",
      "vector (2.2)1). The obvious estimate of \u0012isO\u0012DNx, the sample average. If\n",
      "nwere enormous, say 1010, we would expectO\u0012to nearly equal \u0012, but oth-\n",
      "erwise there is room for error. How much error is the inferential question.\n",
      "The estimateO\u0012is calculated from xaccording to some known algorithm,\n",
      "say\n",
      "O\u0012Dt.x/; (2.4)\n",
      "t.x/in our example being the averaging function NxDPxi=n;O\u0012is a\n",
      "1The fact thatEFfNXgequalsEFfXgis a crucial, though easily proved, probabilistic\n",
      "result.14 Frequentist Inference\n",
      "realization of\n",
      "O‚ÄöDt.X/; (2.5)\n",
      "the output of t.\u0001/applied to a theoretical sample XfromF(2.2). We have\n",
      "chosent.X/, we hope, to make O‚Äöa good estimator of \u0012, the desired prop-\n",
      "erty ofF.\n",
      "We can now give a Ô¨Årst deÔ¨Ånition of frequentist inference: the accu-\n",
      "racy of an observed estimate O\u0012Dt.x/is the probabilistic accuracy of\n",
      "O‚ÄöDt.X/as an estimator of \u0012.This may seem more a tautology than a\n",
      "deÔ¨Ånition, but it contains a powerful idea: O\u0012is just a single number but O‚Äö\n",
      "takes on a range of values whose spread can deÔ¨Åne measures of accuracy.\n",
      "Bias and variance are familiar examples of frequentist inference. DeÔ¨Åne\n",
      "\u0016to be the expectation of O‚ÄöDt.X/under model (2.2),\n",
      "\u0016DEFfO‚Äög: (2.6)\n",
      "Then the bias and variance attributed to estimate O\u0012of parameter \u0012are\n",
      "biasD\u0016\u0000\u0012and varDEFn\n",
      ".O‚Äö\u0000\u0016/2o\n",
      ": (2.7)\n",
      "Again, what keeps this from tautology is the attribution to the single num-\n",
      "berO\u0012of the probabilistic properties of O‚Äöfollowing from model (2.2). If\n",
      "all of this seems too obvious to worry about, the Bayesian criticisms of\n",
      "Chapter 3 may come as a shock.\n",
      "Frequentism is often deÔ¨Åned with respect to ‚Äúan inÔ¨Ånite sequence of\n",
      "future trials.‚Äù We imagine hypothetical data sets X.1/;X.2/;X.3/;::: gen-\n",
      "erated by the same mechanism as xproviding corresponding values O‚Äö.1/;\n",
      "O‚Äö.2/,O‚Äö.3/;::: as in (2.5). The frequentist principle is then to attribute for\n",
      "O\u0012the accuracy properties of the ensemble of O‚Äövalues.2If theO‚Äös have\n",
      "empirical variance of, say, 0.04, then O\u0012is claimed to have standard error\n",
      "0:2Dp\n",
      "0:04, etc. This amounts to a more picturesque restatement of the\n",
      "previous deÔ¨Ånition.\n",
      "2.1 Frequentism in Practice\n",
      "Our working deÔ¨Ånition of frequentism is that the probabilistic properties\n",
      "of a procedure of interest are derived and then applied verbatim to the\n",
      "procedure‚Äôs output for the observed data . This has an obvious defect: it\n",
      "requires calculating the properties of estimators O‚ÄöDt.X/obtained from\n",
      "2In essence, frequentists ask themselves ‚ÄúWhat would I see if I reran the same situation\n",
      "again (and again and again. . . )?‚Äù2.1 Frequentism in Practice 15\n",
      "the true distribution F, even though Fis unknown. Practical frequentism\n",
      "uses a collection of more or less ingenious devices to circumvent the defect.\n",
      "1. The plug-in principle. A simple formula relates the standard error of\n",
      "NXDPXi=nto varF.X/, the variance of a single Xdrawn fromF,\n",
      "se\u0000NX\u0001\n",
      "D≈ívarF.X/=n¬ç1=2: (2.8)\n",
      "But having observed xD.x1;x2;:::;xn/we can estimate var F.X/with-\n",
      "out bias by\n",
      "cvarFDX\n",
      ".xi\u0000Nx/2=.n\u00001/: (2.9)\n",
      "Plugging formula (2.9) into (2.8) gives bse (1.2), the usual estimate for the\n",
      "standard error of an average Nx. In other words, the frequentist accuracy\n",
      "estimate forNxis itself estimated from the observed data.3\n",
      "2. Taylor-series approximations. StatisticsO\u0012Dt.x/more complicated\n",
      "thanNxcan often be related back to the plug-in formula by local linear\n",
      "approximations, sometimes known as the ‚Äúdelta method.‚Äù¬éFor example, ¬é1\n",
      "O\u0012DNx2hasdO\u0012=dNxD2Nx. Thinking of 2Nxas a constant gives\n",
      "se\u0000\n",
      "Nx2\u0001:D2jNxjbse; (2.10)\n",
      "withbse as in (1.2). Large sample calculations, as sample size ngoes to\n",
      "inÔ¨Ånity, validate the delta method which, fortunately, often performs well\n",
      "in small samples.\n",
      "3. Parametric families and maximum likelihood theory. Theoretical ex-\n",
      "pressions for the standard error of a maximum likelihood estimate (MLE)\n",
      "are discussed in Chapters 4 and 5, in the context of parametric families\n",
      "of distributions. These combine Fisherian theory, Taylor-series approxima-\n",
      "tions, and the plug-in principle in an easy-to-apply package.\n",
      "4. Simulation and the bootstrap. Modern computation has opened up the\n",
      "possibility of numerically implementing the ‚ÄúinÔ¨Ånite sequence of future\n",
      "trials‚Äù deÔ¨Ånition, except for the inÔ¨Ånite part. An estimate OFofF, perhaps\n",
      "the MLE, is found, and values O‚Äö.k/Dt.X.k//simulated fromOFforkD\n",
      "1;2;:::;B , sayBD1000 . The empirical standard deviation of the O‚Äös is\n",
      "then the frequentist estimate of standard error for O\u0012Dt.x/, and similarly\n",
      "with other measures of accuracy.\n",
      "This is a good description of the bootstrap, Chapter 10. (Notice that\n",
      "3The most familiar example is the observed proportion pof heads innÔ¨Çips of a coin\n",
      "having true probability \u0019: the actual standard error is ≈í\u0019.1\u0000\u0019/=n¬ç1=2but we can\n",
      "only report the plug-in estimate ≈íp.1\u0000p/=n¬ç1=2.16 Frequentist Inference\n",
      "Table 2.1 Three estimates of location for the gfr data, and their\n",
      "estimated standard errors; last two standard errors using the bootstrap,\n",
      "BD1000 .\n",
      "Estimate Standard error\n",
      "mean 54.25 .95\n",
      "25% Winsorized mean 52.61 .78\n",
      "median 52.24 .87\n",
      "here the plugging-in, of OFforF, comes Ô¨Årst rather than at the end of\n",
      "the process.) The classical methods 1‚Äì3 above are restricted to estimates\n",
      "O\u0012Dt.x/that are smoothly deÔ¨Åned functions of various sample means.\n",
      "Simulation calculations remove this restriction. Table 2.1 shows three ‚Äúlo-\n",
      "cation‚Äù estimates for the gfr data, the mean, the 25% Winsorized mean,4\n",
      "and the median, along with their standard errors, the last two computed\n",
      "by the bootstrap. A happy feature of computer-age statistical inference is\n",
      "the tremendous expansion of useful and usable statistics t.x/in the statis-\n",
      "tician‚Äôs working toolbox, the lowess algorithm in Figures 1.2 and 1.3\n",
      "providing a nice example.\n",
      "5. Pivotal statistics. A pivotal statisticO\u0012Dt.x/is one whose distri-\n",
      "bution does notdepend upon the underlying probability distribution F. In\n",
      "such a case the theoretical distribution of O‚ÄöDt.X/applies exactly toO\u0012,\n",
      "removing the need for devices 1‚Äì4 above. The classic example concerns\n",
      "Student‚Äôs two-sample t-test.\n",
      "In a two-sample problem the statistician observes two sets of numbers,\n",
      "x1D.x11;x12;:::;x1n1/x2D.x21;x22;:::;x2n2/; (2.11)\n",
      "and wishes to test the null hypothesis that they come from the same dis-\n",
      "tribution (as opposed to, say, the second set tending toward larger values\n",
      "than the Ô¨Årst). It is assumed that the distribution F1forx1isnormal , or\n",
      "Gaussian ,\n",
      "X1iind\u0018N.\u00161;\u001b2/; iD1;2;:::;n 1; (2.12)\n",
      "the notation indicating n1independent draws from a normal distribution5\n",
      "4All observations below the 25th percentile of the 211 observations are moved up to that\n",
      "point, similarly those above the 75th percentile are moved down, and Ô¨Ånally the mean is\n",
      "taken.\n",
      "5Each draw having probability density .2\u0019\u001b2/\u00001=2expf\u00000:5\u0001.x\u0000\u00161/2=\u001b2g.2.1 Frequentism in Practice 17\n",
      "with expectation \u00161and variance \u001b2. Likewise\n",
      "X2iind\u0018N.\u00162;\u001b2/ iD1;2;:::;n 2: (2.13)\n",
      "We wish to test the null hypothesis\n",
      "H0W\u00161D\u00162: (2.14)\n",
      "The obvious test statistic O\u0012D Nx2\u0000Nx1, the difference of the means, has\n",
      "distribution\n",
      "O\u0012\u0018N\u0010\n",
      "0;\u001b2\u0010\n",
      "1\n",
      "n1C1\n",
      "n2\u0011\u0011\n",
      "(2.15)\n",
      "underH0. We could plug in the unbiased estimate of \u001b2,\n",
      "O\u001b2D\"n1X\n",
      "1.x1i\u0000Nx1/2Cn2X\n",
      "1.x2i\u0000Nx2/2#,\n",
      ".n1Cn2\u00002/; (2.16)\n",
      "but Student provided a more elegant solution: instead of O\u0012, we testH0\n",
      "using the two-sample t-statistic\n",
      "tDNx2\u0000Nx1\n",
      "bsd; wherebsdDO\u001b\u0010\n",
      "1\n",
      "n1C1\n",
      "n2\u00111=2\n",
      ": (2.17)\n",
      "UnderH0,tis pivotal, having the same distribution (Student‚Äôs tdistribu-\n",
      "tion withn1Cn2\u00002degrees of freedom), no matter what the value of the\n",
      "‚Äúnuisance parameter‚Äù \u001b.\n",
      "Forn1Cn2\u00002D70, as in the leukemia example (1.5)‚Äì(1.6), Student‚Äôs\n",
      "distribution gives\n",
      "PrH0f\u00001:99\u0014t\u00141:99gD0:95: (2.18)\n",
      "The hypothesis test that rejects H0ifjtjexceeds 1.99 has probability ex-\n",
      "actly 0.05 of mistaken rejection. Similarly,\n",
      "Nx2\u0000Nx1Àô1:99\u0001bsd (2.19)\n",
      "is an exact 0.95 conÔ¨Ådence interval for the difference \u00162\u0000\u00161, covering\n",
      "the true value in 95% of repetitions of probability model (2.12)‚Äì(2.13).6\n",
      "6Occasionally, one sees frequentism deÔ¨Åned in careerist terms, e.g., ‚ÄúA statistician who\n",
      "always rejects null hypotheses at the 95% level will over time make only 5% errors of\n",
      "the Ô¨Årst kind.‚Äù This is not a comforting criterion for the statistician‚Äôs clients, who are\n",
      "interested in their own situations, not everyone else‚Äôs. Here we are only assuming\n",
      "hypothetical repetitions of the speciÔ¨Åc problem at hand.18 Frequentist Inference\n",
      "What might be called the strong deÔ¨Ånition of frequentism insists on exact\n",
      "frequentist correctness under experimental repetitions. Pivotality, unfortu-\n",
      "nately, is unavailable in most statistical situations. Our looser deÔ¨Ånition\n",
      "of frequentism, supplemented by devices such as those above,7presents a\n",
      "more realistic picture of actual frequentist practice.\n",
      "2.2 Frequentist Optimality\n",
      "The popularity of frequentist methods reÔ¨Çects their relatively modest math-\n",
      "ematical modeling assumptions: only a probability model F(more exactly\n",
      "a family of probabilities, Chapter 3) and an algorithm of choice t.x/. This\n",
      "Ô¨Çexibility is also a defect in that the principle of frequentist correctness\n",
      "doesn‚Äôt help with the choice of algorithm. Should we use the sample mean\n",
      "to estimate the location of the gfr distribution? Maybe the 25% Win-\n",
      "sorized mean would be better, as Table 2.1 suggests.\n",
      "The years 1920‚Äì1935 saw the development of two key results on fre-\n",
      "quentist optimality , that is, Ô¨Ånding the best choice oft.x/given modelF.\n",
      "The Ô¨Årst of these was Fisher‚Äôs theory of maximum likelihood estimation\n",
      "and the Fisher information bound: in parametric probability models of the\n",
      "type discussed in Chapter 4, the MLE is the optimum estimate in terms of\n",
      "minimum (asymptotic) standard error.\n",
      "In the same spirit, the Neyman‚ÄìPearson lemma provides an optimum\n",
      "hypothesis-testing algorithm. This is perhaps the most elegant of frequen-\n",
      "tist constructions. In its simplest formulation, the NP lemma assumes we\n",
      "are trying to decide between two possible probability density functions for\n",
      "the observed data x, a null hypothesis density f0.x/and an alternative\n",
      "densityf1.x/. A testing rule t.x/says which choice, 0 or 1, we will make\n",
      "having observed data x. Any such rule has two associated frequentist error\n",
      "probabilities: choosing f1when actually f0generatedx, and vice versa,\n",
      "ÀõDPrf0ft.x/D1g;\n",
      "ÀáDPrf1ft.x/D0g:(2.20)\n",
      "LetL.x/be the likelihood ratio ,\n",
      "L.x/Df1.x/=f0.x/ (2.21)\n",
      "7The list of devices is not complete. Asymptotic calculations play a major role, as do\n",
      "more elaborate combinations of pivotality and the plug-in principle; see the discussion\n",
      "of approximate bootstrap conÔ¨Ådence intervals in Chapter 11.2.2 Frequentist Optimality 19\n",
      "and deÔ¨Åne the testing rule tc.x/by\n",
      "tc.x/D(\n",
      "1if logL.x/\u0015c\n",
      "0if logL.x/<c:(2.22)\n",
      "There is one such rule for each choice of the cutoff c. The Neyman‚ÄìPearson\n",
      "lemma says that only rules of form (2.22) can be optimum; for any other\n",
      "rulet.x/there will be a rule tc.x/having smaller errors of both kinds,8\n",
      "Àõc<Àõ andÀác<Àá: (2.23)\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\n",
      "Œ±Œ≤c = .75\n",
      "Œ±  = .10,  Œ≤ = .38\n",
      "Figure 2.2 Neyman‚ÄìPearson alpha‚Äìbeta curve for f0\u0018N.0;1/ ,\n",
      "f1\u0018N.:5;1/ , and sample size nD10. Red dots correspond to\n",
      "cutoffscD2:75;1:75;:75;:::; \u00003:25.\n",
      "Figure 2.2 graphs .Àõc;Àác/as a function of the cutoff c, for the case\n",
      "wherexD.x1;x2;:::;x10/is obtained by independent sampling from a\n",
      "normal distribution, N.0;1/ forf0versusN.0:5;1/ forf1. The NP lemma\n",
      "says that any rule not of form (2.22) must have its .Àõ;Àá/ point lying above\n",
      "the curve.\n",
      "8Here we are ignoring some minor deÔ¨Ånitional difÔ¨Åculties that can occur if f0andf1are\n",
      "discrete.20 Frequentist Inference\n",
      "Frequentist optimality theory, both for estimation and for testing, an-\n",
      "chored statistical practice in the twentieth century. The larger data sets and\n",
      "more complicated inferential questions of the current era have strained the\n",
      "capabilities of that theory. Computer-age statistical inference, as we will\n",
      "see, often displays an unsettling ad hoc character. Perhaps some contem-\n",
      "porary Fishers and Neymans will provide us with a more capacious opti-\n",
      "mality theory equal to the challenges of current practice, but for now that\n",
      "is only a hope.\n",
      "Frequentism cannot claim to be a seamless philosophy of statistical in-\n",
      "ference. Paradoxes and contradictions abound within its borders, as will\n",
      "be shown in the next chapter. That being said, frequentist methods have\n",
      "a natural appeal to working scientists, an impressive history of success-\n",
      "ful application, and, as our list of Ô¨Åve ‚Äúdevices‚Äù suggests, the capacity to\n",
      "encourage clever methodology. The story that follows is not one of aban-\n",
      "donment of frequentist thinking, but rather a broadening of connections\n",
      "with other methods.\n",
      "2.3 Notes and Details\n",
      "The name ‚Äúfrequentism‚Äù seems to have been suggested by Neyman as a\n",
      "statistical analogue of Richard von Mises‚Äô frequentist theory of probability,\n",
      "the connection being made explicit in his 1977 paper, ‚ÄúFrequentist prob-\n",
      "ability and frequentist statistics.‚Äù ‚ÄúBehaviorism‚Äù might have been a more\n",
      "descriptive name9since the theory revolves around the long-run behavior\n",
      "of statisticst.x/, but in any case ‚Äúfrequentism‚Äù has stuck, replacing the\n",
      "older (sometimes disparaging) term ‚Äúobjectivism.‚Äù Neyman‚Äôs attempt at a\n",
      "complete frequentist theory of statistical inference, ‚Äúinductive behavior,‚Äù\n",
      "is not much quoted today, but can claim to be an important inÔ¨Çuence on\n",
      "Wald‚Äôs development of decision theory.\n",
      "R. A. Fisher‚Äôs work on maximum likelihood estimation is featured in\n",
      "Chapter 4. Fisher, arguably the founder of frequentist optimality theory,\n",
      "was not a pure frequentist himself, as discussed in Chapter 4 and Efron\n",
      "(1998), ‚ÄúR. A. Fisher in the 21st Century.‚Äù (Now that we are well into the\n",
      "twenty-Ô¨Årst century, the author‚Äôs talents as a prognosticator can be frequen-\n",
      "tistically evaluated.)\n",
      "¬é1[p. 15] Delta method. The delta method uses a Ô¨Årst-order Taylor series to\n",
      "approximate the variance of a function s.O\u0012/of a statisticO\u0012. SupposeO\u0012\n",
      "has mean/variance .\u0012;\u001b2/, and consider the approximation s.O\u0012/\u0019s.\u0012/C\n",
      "9That name is already spoken for in the psychology literature.2.3 Notes and Details 21\n",
      "s0.\u0012/.O\u0012\u0000\u0012/. Hence varfs.O\u0012/g\u0019js0.\u0012/j2\u001b2. We typically plug-in O\u0012for\u0012,\n",
      "and use an estimate for \u001b2.3\n",
      "Bayesian Inference\n",
      "The human mind is an inference machine: ‚ÄúIt‚Äôs getting windy, the sky is\n",
      "darkening, I‚Äôd better bring my umbrella with me.‚Äù Unfortunately, it‚Äôs not a\n",
      "very dependable machine, especially when weighing complicated choices\n",
      "against past experience. Bayes‚Äô theorem is a surprisingly simple mathemat-\n",
      "ical guide to accurate inference. The theorem (or ‚Äúrule‚Äù), now 250 years\n",
      "old, marked the beginning of statistical inference as a serious scientiÔ¨Åc sub-\n",
      "ject. It has waxed and waned in inÔ¨Çuence over the centuries, now waxing\n",
      "again in the service of computer-age applications.\n",
      "Bayesian inference, if not directly opposed to frequentism, is at least or-\n",
      "thogonal. It reveals some worrisome Ô¨Çaws in the frequentist point of view,\n",
      "while at the same time exposing itself to the criticism of dangerous overuse.\n",
      "The struggle to combine the virtues of the two philosophies has become\n",
      "more acute in an era of massively complicated data sets. Much of what\n",
      "follows in succeeding chapters concerns this struggle. Here we will review\n",
      "some basic Bayesian ideas and the ways they impinge on frequentism.\n",
      "The fundamental unit of statistical inference both for frequentists and\n",
      "for Bayesians is a family of probability densities\n",
      "FDÀö\n",
      "f\u0016.x/Ix2X;\u00162\t\n",
      "I (3.1)\n",
      "x, the observed data, is a point1in the sample space X, while the unob-\n",
      "served parameter \u0016is a point in the parameter space . The statistician\n",
      "observesxfromf\u0016.x/, and infers the value of \u0016.\n",
      "Perhaps the most familiar case is the normal family\n",
      "f\u0016.x/D1p\n",
      "2\u0019e\u00001\n",
      "2.x\u0000\u0016/2(3.2)\n",
      "1Bothxand\u0016may be scalars, vectors, or more complicated objects. Other names for the\n",
      "generic ‚Äúx‚Äù and ‚Äú\u0016‚Äù occur in speciÔ¨Åc situations, for instance xforxin Chapter 2. We\n",
      "will also call Fa ‚Äúfamily of probability distributions.‚Äù\n",
      "22Bayesian Inference 23\n",
      "(more exactly, the one-dimensional normal translation family2with vari-\n",
      "ance 1), with both Xandequaling R1, the entire real line .\u00001;1/.\n",
      "Another central example is the Poisson family\n",
      "f\u0016.x/De\u0000\u0016\u0016x=x≈†; (3.3)\n",
      "whereXis the nonnegative integers f0;1;2;:::gandis the nonnegative\n",
      "real line.0;1/. (Here the ‚Äúdensity‚Äù (3.3) speciÔ¨Åes the atoms of probability\n",
      "on the discrete points of X.)\n",
      "Bayesian inference requires one crucial assumption in addition to the\n",
      "probability family F, the knowledge of a prior density\n",
      "g.\u0016/; \u00162I (3.4)\n",
      "g.\u0016/ represents prior information concerning the parameter \u0016, available to\n",
      "the statistician before the observation of x. For instance, in an application\n",
      "of the normal model (3.2), it could be known that \u0016is positive, while past\n",
      "experience shows it never exceeding 10, in which case we might take g.\u0016/\n",
      "to be the uniform density g.\u0016/D1=10 on the interval ≈í0;10¬ç . Exactly\n",
      "what constitutes ‚Äúprior knowledge‚Äù is a crucial question we will consider\n",
      "in ongoing discussions of Bayes‚Äô theorem.\n",
      "Bayes‚Äô theorem is a rule for combining the prior knowledge in g.\u0016/ with\n",
      "the current evidence in x. Letg.\u0016jx/denote the posterior density of\u0016, that\n",
      "is, our update of the prior density g.\u0016/ after taking account of observation\n",
      "x. Bayes‚Äô rule provides a simple expression for g.\u0016jx/in terms ofg.\u0016/\n",
      "andF.\n",
      "Bayes‚Äô Rule: g.\u0016jx/Dg.\u0016/f\u0016.x/=f.x/; \u00162; (3.5)\n",
      "wheref.x/ is the marginal density ofx,\n",
      "f.x/DZ\n",
      "f\u0016.x/g.\u0016/d\u0016: (3.6)\n",
      "(The integral in (3.6) would be a sum if were discrete.) The Rule is a\n",
      "straightforward exercise in conditional probability,3and yet has far-reaching\n",
      "and sometimes surprising consequences.\n",
      "In Bayes‚Äô formula (3.5), xis Ô¨Åxed at its observed value while \u0016varies\n",
      "over, just the opposite of frequentist calculations. We can emphasize this\n",
      "2Standard notation is x\u0018N.\u0016;\u001b2/for a normal distribution with expectation \u0016and\n",
      "variance\u001b2, so (3.2) hasx\u0018N.\u0016;1/ .\n",
      "3g.\u0016jx/is the ratio ofg.\u0016/f\u0016.x/, the joint probability of the pair .\u0016;x/ , andf.x/ ,\n",
      "the marginal probability of x.24 Bayesian Inference\n",
      "by rewriting (3.5) as\n",
      "g.\u0016jx/DcxLx.\u0016/g.\u0016/; (3.7)\n",
      "whereLx.\u0016/is the likelihood function , that is,f\u0016.x/withxÔ¨Åxed and\u0016\n",
      "varying. Having computed Lx.\u0016/g.\u0016/ , the constant cxcan be determined\n",
      "numerically from the requirement that g.\u0016jx/integrate to 1, obviating the\n",
      "calculation of f.x/ (3.6).\n",
      "Note Multiplying the likelihood function by any Ô¨Åxed constant c0has no\n",
      "effect on (3.7) since c0can be absorbed into cx. So for the Poisson family\n",
      "(3.3) we can take Lx.\u0016/De\u0000\u0016\u0016x, ignoring the x≈†factor, which acts as a\n",
      "constant in Bayes‚Äô rule. The luxury of ignoring factors depending only on\n",
      "xoften simpliÔ¨Åes Bayesian calculations.\n",
      "For any two points \u00161and\u00162in, the ratio of posterior densities is, by\n",
      "division in (3.5),\n",
      "g.\u00161jx/\n",
      "g.\u00162jx/Dg.\u00161/\n",
      "g.\u00162/f\u00161.x/\n",
      "f\u00162.x/(3.8)\n",
      "(no longer involving the marginal density f.x/ ), that is, ‚Äúthe posterior odds\n",
      "ratio is the prior odds ratio times the likelihood ratio,‚Äù a memorable restate-\n",
      "ment of Bayes‚Äô rule.\n",
      "3.1 Two Examples\n",
      "A simple but genuine example of Bayes‚Äô rule in action is provided by the\n",
      "story of the Physicist‚Äôs Twins : thanks to sonograms, a physicist found out\n",
      "she was going to have twin boys. ‚ÄúWhat is the probability my twins will\n",
      "beIdentical , rather than Fraternal ?‚Äù she asked. The doctor answered that\n",
      "one-third of twin births were Identicals, and two-thirds Fraternals.\n",
      "In this situation \u0016, the unknown parameter (or ‚Äústate of nature‚Äù) is either\n",
      "Identical orFraternal with prior probability 1/3 or 2/3; X, the possible\n",
      "sonogram results for twin births, is either Same Sex orDifferent Sexes , and\n",
      "xDSame Sex was observed. (We can ignore sex since that does not affect\n",
      "the calculation.) A crucial fact is that identical twins are always same-sex\n",
      "while fraternals have probability 0.5 of same or different, so Same Sex in\n",
      "the sonogram is twice as likely if the twins are Identical. Applying Bayes‚Äô3.1 Two Examples 25\n",
      "rule in ratio form (3.8) answers the physicist‚Äôs question:\n",
      "g.IdenticaljSame/\n",
      "g.FraternaljSame/Dg.Identical/\n",
      "g.Fraternal/\u0001fIdentical.Same/\n",
      "fFraternal.Same/\n",
      "D1=3\n",
      "2=3\u00011\n",
      "1=2D1:(3.9)\n",
      "That is, the posterior odds are even, and the physicist‚Äôs twins have equal\n",
      "probabilities 0.5 of being Identical or Fraternal.4Here the doctor‚Äôs prior\n",
      "odds ratio, 2 to 1 in favor of Fraternal, is balanced out by the sonogram‚Äôs\n",
      "likelihood ratio of 2 to 1 in favor of Identical.\n",
      " ¬†Identical Twins are: Fraternal Same sex Different \n",
      "Physicist Sonogram shows: Doctor 2/3 1/3 1/3 1/3 0 1/3 b a c d \n",
      "Figure 3.1 Analyzing the twins problem.\n",
      "There are only four possible combinations of parameter \u0016and outcome\n",
      "xin the twins problem, labeled a, b, c, anddin Figure 3.1. Cell bhas\n",
      "probability 0 since Identicals cannot be of Different Sexes. Cells candd\n",
      "have equal probabilities because of the random sexes of Fraternals. Finally,\n",
      "aCbmust have total probability 1/3, and cCdtotal probability 2/3,\n",
      "according to the doctor‚Äôs prior distribution. Putting all this together, we\n",
      "can Ô¨Åll in the probabilities for all four cells, as shown. The physicist knows\n",
      "she is in the Ô¨Årst column of the table, where the conditional probabilities\n",
      "of Identical or Fraternal are equal, just as provided by Bayes‚Äô rule in (3.9).\n",
      "Presumably the doctor‚Äôs prior distribution came from some enormous\n",
      "state or national database, say three million previous twin births, one mil-\n",
      "lion Identical pairs and two million Fraternals. We deduce that cells a, c,\n",
      "anddmust have had one million entries each in the database, while cell\n",
      "bwas empty. Bayes‚Äô rule can be thought of as a big book with one page\n",
      "4They turned out to be Fraternal.26 Bayesian Inference\n",
      "for each possible outcome x. (The book has only two pages in Figure 3.1.)\n",
      "The physicist turns to the page ‚ÄúSame Sex‚Äù and sees two million previous\n",
      "twin births, half Identical and half Fraternal, correctly concluding that the\n",
      "odds are equal in her situation.\n",
      "Given any prior distribution g.\u0016/ and any family of densities f\u0016.x/,\n",
      "Bayes‚Äô rule will always provide a version of the big book. That doesn‚Äôt\n",
      "mean that the book‚Äôs contents will always be equally convincing. The prior\n",
      "for the twins problems was based on a large amount of relevant previous\n",
      "experience. Such experience is most often unavailable. Modern Bayesian\n",
      "practice uses various strategies to construct an appropriate ‚Äúprior‚Äù g.\u0016/\n",
      "in the absence of prior experience, leaving many statisticians unconvinced\n",
      "by the resulting Bayesian inferences. Our second example illustrates the\n",
      "difÔ¨Åculty.\n",
      "Table 3.1 Scores from two tests taken by 22 students, mechanics and\n",
      "vectors .\n",
      "1 2 3 4 5 6 7 8 9 10 11\n",
      "mechanics 7 44 49 59 34 46 0 32 49 52 44\n",
      "vectors 51 69 41 70 42 40 40 45 57 64 61\n",
      "12 13 14 15 16 17 18 19 20 21 22\n",
      "mechanics 36 42 5 22 18 41 48 31 42 46 63\n",
      "vectors 59 60 30 58 51 63 38 42 69 49 63\n",
      "Table 3.1 shows the scores on two tests, mechanics andvectors ,\n",
      "achieved bynD22students. The sample correlation coefÔ¨Åcient between\n",
      "the two scores isO\u0012D0:498 ,\n",
      "O\u0012D22X\n",
      "iD1.mi\u0000Nm/.vi\u0000Nv/,\"22X\n",
      "iD1.mi\u0000Nm/222X\n",
      "iD1.vi\u0000Nv/2#1=2\n",
      ";(3.10)\n",
      "withmandvshort for mechanics andvectors ,NmandNvtheir aver-\n",
      "ages. We wish to assign a Bayesian measure of posterior accuracy to the\n",
      "true correlation coefÔ¨Åcient \u0012, ‚Äútrue‚Äù meaning the correlation for the hypo-\n",
      "thetical population of all students, of which we observed only 22.\n",
      "If we assume that the joint .m;v/ distribution is bivariate normal (as\n",
      "discussed in Chapter 5), then the density of O\u0012as a function of \u0012has a\n",
      "known form,¬é ¬é13.1 Two Examples 27\n",
      "f\u0012\u0010\n",
      "O\u0012\u0011\n",
      "D.n\u00002/.1\u0000\u00122/.n\u00001/=2\u0010\n",
      "1\u0000O\u00122\u0011.n\u00004/=2\n",
      "\u0019Z1\n",
      "0dw\n",
      "\u0010\n",
      "coshw\u0000\u0012O\u0012\u0011n\u00001:\n",
      "(3.11)\n",
      "In terms of our general Bayes notation, parameter \u0016is\u0012, observation xis\n",
      "O\u0012, and family Fis given by (3.11), with both andXequaling the interval\n",
      "≈í\u00001;1¬ç. Formula (3.11) looks formidable to the human eye but not to the\n",
      "computer eye, which makes quick work of it.\n",
      "‚àí0.2 0.0 0.2 0.4 0.6 0.8 1.00.0 0.5 1.0 1.5 2.0 2.5\n",
      "Œ∏g(Œ∏|Œ∏^)\n",
      "MLE .498flat priorJeffreys\n",
      "Triangular\n",
      "‚óè\n",
      ".093 .750\n",
      "Figure 3.2 Student scores data; posterior density of correlation \u0012\n",
      "for three possible priors.\n",
      "In this case, as in the majority of scientiÔ¨Åc situations, we don‚Äôt have a\n",
      "trove of relevant past experience ready to provide a prior g.\u0012/ . One expe-\n",
      "dient, going back to Laplace, is the ‚Äúprinciple of insufÔ¨Åcient reason,‚Äù that\n",
      "is, we take\u0012to be uniformly distributed over ,\n",
      "g.\u0012/D1\n",
      "2for\u00001\u0014\u0012\u00141; (3.12)\n",
      "a ‚ÄúÔ¨Çat prior.‚Äù The solid black curve in Figure 3.2 shows the resulting poste-\n",
      "rior density (3.5), which is just the likelihood f\u0012.0:498/ plotted as a func-\n",
      "tion of\u0012(and scaled to have integral 1).28 Bayesian Inference\n",
      "Jeffreys‚Äô prior ,\n",
      "gJeff.\u0012/D1=.1\u0000\u00122/; (3.13)\n",
      "yields posterior density g.\u0012jO\u0012/shown by the dashed red curve. It suggests\n",
      "somewhat bigger values for the unknown parameter \u0012. Formula (3.13)\n",
      "arises from a theory of ‚Äúuninformative priors‚Äù discussed in the next sec-\n",
      "tion, an improvement on the principle of insufÔ¨Åcient reason; (3.13) is an\n",
      "improper density in thatR1\n",
      "\u00001g.\u0012/d\u0012D1 , but it still provides proper pos-\n",
      "terior densities when deployed in Bayes‚Äô rule (3.5).\n",
      "The dotted blue curve in Figure 3.2 is posterior density g.\u0012jO\u0012/obtained\n",
      "from the triangular-shaped prior\n",
      "g.\u0012/D1\u0000j\u0012j: (3.14)\n",
      "This is a primitive example of a shrinkage prior, one designed to favor\n",
      "smaller values of \u0012. Its effect is seen in the leftward shift of the posterior\n",
      "density. Shrinkage priors will play a major role in our discussion of large-\n",
      "scale estimation and testing problems, where we are hoping to Ô¨Ånd a few\n",
      "large effects hidden among thousands of negligible ones.\n",
      "3.2 Uninformative Prior Distributions\n",
      "Given a convincing prior distribution, Bayes‚Äô rule is easier to use and pro-\n",
      "duces more satisfactory inferences than frequentist methods. The domi-\n",
      "nance of frequentist practice reÔ¨Çects the scarcity of useful prior information\n",
      "in day-to-day scientiÔ¨Åc applications. But the Bayesian impulse is strong,\n",
      "and almost from its inception 250 years ago there have been proposals for\n",
      "the construction of ‚Äúpriors‚Äù that permit the use of Bayes‚Äô rule in the ab-\n",
      "sence of relevant experience.\n",
      "One approach, perhaps the most inÔ¨Çuential in current practice, is the\n",
      "employment of uninformative priors . ‚ÄúUninformative‚Äù has a positive con-\n",
      "notation here, implying that the use of such a prior in Bayes‚Äô rule does not\n",
      "tacitly bias the resulting inference. Laplace‚Äôs principle of insufÔ¨Åcient rea-\n",
      "son, i.e., assigning uniform prior distributions to unknown parameters, is\n",
      "an obvious attempt at this goal. Its use went unchallenged for more than a\n",
      "century, perhaps because of Laplace‚Äôs inÔ¨Çuence more than its own virtues.\n",
      "Venn (of the Venn diagram) in the 1860s, and Fisher in the 1920s, attack-\n",
      "ing the routine use of Bayes‚Äô theorem, pointed out that Laplace‚Äôs principle\n",
      "could not be applied consistently. In the student correlation example, for\n",
      "instance, a uniform prior distribution for \u0012would not be uniform if we3.2 Uninformative Prior Distributions 29\n",
      "De\u0012; posterior probabilities such as\n",
      "Prn\n",
      "\u0012 >0jO\u0012o\n",
      "DPrn\n",
      " >1jO\u0012o\n",
      "(3.15)\n",
      "was taken to be uniform a priori. Neither\n",
      "choice then could be considered uninformative.\n",
      "A more sophisticated version of Laplace‚Äôs principle was put forward by\n",
      "Jeffreys beginning in the 1930s. It depends, interestingly enough, on the\n",
      "frequentist notion of Fisher information (Chapter 4). For a one-parameter\n",
      "familyf\u0016.x/, where the parameter space is an interval of the real line\n",
      "R1, the Fisher information is deÔ¨Åned to be\n",
      "I\u0016DE\u0016(\u0012@\n",
      "@\u0016logf\u0016.x/\u00132)\n",
      ": (3.16)\n",
      "(For the Poisson family (3.3), @=@\u0016. logf\u0016.x//Dx=\u0016\u00001andI\u0016D1=\u0016.)\n",
      "The Jeffreys‚Äô prior gJeff.\u0016/is by deÔ¨Ånition\n",
      "gJeff.\u0016/DI1=2\n",
      "\u0016: (3.17)\n",
      "Because1=I\u0016equals, approximately, the variance \u001b2\n",
      "\u0016of the MLEO\u0016, an\n",
      "equivalent deÔ¨Ånition is\n",
      "gJeff.\u0016/D1=\u001b\u0016: (3.18)\n",
      "Formula (3.17) does in fact transform correctly under parameter changes,\n",
      "avoiding the Venn‚ÄìFisher criticism.¬éIt is known thatO\u0012in family (3.11) has ¬é2\n",
      "approximate standard deviation\n",
      "\u001b\u0012Dc.1\u0000\u00122/; (3.19)\n",
      "yielding Jeffreys‚Äô prior (3.13) from (3.18), the constant factor chaving no\n",
      "effect on Bayes‚Äô rule (3.5)‚Äì(3.6).\n",
      "The red triangles in Figure 3.2 indicate the ‚Äú95% credible interval‚Äù [0.093,\n",
      "0.750] for\u0012, based on Jeffreys‚Äô prior. That is, the posterior probability\n",
      "0:093\u0014\u0012\u00140:750 equals 0.95,\n",
      "Z0:750\n",
      "0:093gJeff\u0010\n",
      "\u0012jO\u0012\u0011\n",
      "d\u0012D0:95; (3.20)\n",
      "with probability 0.025 for \u0012 <0:093 or\u0012 >0:750 . It is not an accident that\n",
      "this nearly equals the standard Neyman 95% conÔ¨Ådence interval based on\n",
      "f\u0012.O\u0012/(3.11). Jeffreys‚Äô prior tends to induce this nice connection between\n",
      "the Bayesian and frequentist worlds, at least in one-parameter families.\n",
      "Multiparameter probability families, Chapter 4, make everything more30 Bayesian Inference\n",
      "difÔ¨Åcult. Suppose, for instance, the statistician observes 10 independent\n",
      "versions of the normal model (3.2), with possibly different values of \u0016,\n",
      "xiind\u0018N.\u0016i;1/ foriD1;2;:::;10; (3.21)\n",
      "in standard notation. Jeffreys‚Äô prior is Ô¨Çat for any one of the 10 problems,\n",
      "which is reasonable for dealing with them separately, but the joint Jeffreys‚Äô\n",
      "prior\n",
      "g.\u00161;\u00162;:::;\u001610/Dconstant; (3.22)\n",
      "also Ô¨Çat, can produce disastrous overall results, as discussed in Chapter 13.\n",
      "Computer-age applications are often more like (3.21) than (3.11), except\n",
      "with hundreds or thousands of cases rather than 10 to consider simultane-\n",
      "ously. Uninformative priors of many sorts, including Jeffreys‚Äô, are highly\n",
      "popular in current applications, as we will discuss. This leads to an inter-\n",
      "play between Bayesian and frequentist methodology, the latter intended to\n",
      "control possible biases in the former, exemplifying our general theme of\n",
      "computer-age statistical inference.\n",
      "3.3 Flaws in Frequentist Inference\n",
      "Bayesian statistics provides an internally consistent (‚Äúcoherent‚Äù) program\n",
      "of inference. The same cannot be said of frequentism. The apocryphal story\n",
      "of the meter reader makes the point: an engineer measures the voltages on\n",
      "a batch of 12 tubes, using a voltmeter that is normally calibrated,\n",
      "x\u0018N.\u0016;1/; (3.23)\n",
      "xbeing any one measurement and \u0016the true batch voltage. The measure-\n",
      "ments range from 82 to 99, with an average of NxD92, which he reports\n",
      "back as an unbiased estimate of \u0016.¬é ¬é3\n",
      "The next day he discovers a glitch in his voltmeter such that any volt-\n",
      "age exceeding 100 would have been reported as xD100. His frequentist\n",
      "statistician tells him that NxD92is no longer unbiased for the true expecta-\n",
      "tion\u0016since (3.23) no longer completely describes the probability family.\n",
      "(The statistician says that 92 is a little too small.) The fact that the glitch\n",
      "didn‚Äôt affect any of the actual measurements doesn‚Äôt let him off the hook;\n",
      "Nxwould not be unbiased for \u0016in future realizations of NXfrom the actual\n",
      "probability model.\n",
      "A Bayesian statistician comes to the meter reader‚Äôs rescue. For any prior\n",
      "densityg.\u0016/ , the posterior density g.\u0016jx/Dg.\u0016/f\u0016.x/=f.x/, where\n",
      "xis the vector of 12 measurements, depends only on the data xactually3.3 Flaws in Frequentist Inference 31\n",
      "observed, and not on other potential data sets Xthat might have been\n",
      "seen. The Ô¨Çat Jeffreys‚Äô prior g.\u0016/Dconstant yields posterior expectation\n",
      "NxD92for\u0016, irrespective of whether or not the glitch would have affected\n",
      "readings above 100.\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0 5 10 15 20 25 30‚àí1.5‚àí1.0‚àí0.5 0.00.51.01.52.0\n",
      "month iz value1.645\n",
      "Figure 3.3Z-values against null hypothesis \u0016D0for months 1\n",
      "through 30.\n",
      "A less contrived version of the same phenomenon is illustrated in Fig-\n",
      "ure 3.3. An ongoing experiment is being run. Each month ian independent\n",
      "normal variate is observed,\n",
      "xi\u0018N.\u0016;1/; (3.24)\n",
      "with the intention of testing the null hypothesis H0W\u0016D0versus the\n",
      "alternative\u0016>0 . The plotted points are test statistics\n",
      "ZiDiX\n",
      "jD1xj.p\n",
      "i ; (3.25)\n",
      "a ‚Äúz-value‚Äù based on all the data up to month i,\n",
      "Zi\u0018N\u0010p\n",
      "i\u0016;1\u0011\n",
      ": (3.26)\n",
      "At month 30, the scheduled end of the experiment, Z30D1:66, just ex-\n",
      "ceeding 1.645, the upper 95% point for a N.0;1/ distribution. Victory!\n",
      "The investigators get to claim ‚ÄúsigniÔ¨Åcant‚Äù rejection of H0at level 0.05.32 Bayesian Inference\n",
      "Unfortunately, it turns out that the investigators broke protocol and peek-\n",
      "ed at the data at month 20, in the hope of being able to stop an expensive\n",
      "experiment early. This proved a vain hope, Z20D0:79 not being anywhere\n",
      "near signiÔ¨Åcance, so they continued on to month 30 as originally planned.\n",
      "This means they effectively used the stopping rule ‚Äústop and declare signif-\n",
      "icance if either Z20orZ30exceeds 1.645.‚Äù Some computation shows that\n",
      "this rule had probability 0.074, not 0.05, of rejecting H0if it were true.\n",
      "Victory has turned into defeat according to the honored frequentist 0.05\n",
      "criterion.\n",
      "Once again, the Bayesian statistician is more lenient. The likelihood\n",
      "function for the full data set xD.x1;x2;:::;x30/,\n",
      "Lx.\u0016/D30Y\n",
      "iD1e\u00001\n",
      "2.xi\u0000\u0016/2; (3.27)\n",
      "is the same irrespective of whether or not the experiment might have stopped\n",
      "early. The stopping rule doesn‚Äôt affect the posterior distribution g.\u0016jx/,\n",
      "which depends on xonly through the likelihood (3.7).\n",
      " \n",
      "effect‚àísize estimatesFrequency\n",
      "‚àí4 ‚àí2 0 2 40 100 200 300 400\n",
      "gene 610\n",
      "5.29\n",
      "Figure 3.4 Unbiased effect-size estimates for 6033 genes,\n",
      "prostate cancer study. The estimate for gene 610 is x610D5:29.\n",
      "What is its effect size?\n",
      "The lenient nature of Bayesian inference can look less benign in multi-3.4 A Bayesian/Frequentist Comparison List 33\n",
      "parameter settings. Figure 3.4 concerns a prostate cancer study comparing\n",
      "52 patients with 50 healthy controls. Each man had his genetic activity\n",
      "measured for a panel of ND6033 genes. A statistic xwas computed for\n",
      "each gene,5comparing the patients with controls, say¬é ¬é4\n",
      "xi\u0018N.\u0016i;1/ iD1;2;:::;N; (3.28)\n",
      "where\u0016irepresents the true effect size for genei. Most of the genes, prob-\n",
      "ably not being involved in prostate cancer, would be expected to have effect\n",
      "sizes near 0, but the investigators hoped to spot a few large \u0016ivalues, either\n",
      "positive or negative.\n",
      "The histogram of the 6033 xivalues does in fact reveal some large val-\n",
      "ues,x610D5:29 being the winner. Question: what estimate should we\n",
      "give for\u0016610? Even though x610was individually unbiased for \u0016610, a fre-\n",
      "quentist would (correctly) worry that focusing attention on the largest of\n",
      "6033 values would produce an upward bias, and that our estimate should\n",
      "downwardly correct 5.29. ‚ÄúSelection bias,‚Äù ‚Äúregression to the mean,‚Äù and\n",
      "‚Äúthe winner‚Äôs curse‚Äù are three names for this phenomenon.\n",
      "Bayesian inference, surprisingly, is immune to selection bias.¬éIrrespec-¬é5\n",
      "tive of whether gene 610 was prespeciÔ¨Åed for particular attention or only\n",
      "came to attention as the ‚Äúwinner,‚Äù the Bayes‚Äô estimate for \u0016610given all\n",
      "the data stays the same. This isn‚Äôt obvious, but follows from the fact that\n",
      "any data-based selection process does not affect the likelihood function in\n",
      "(3.7).\n",
      "What does affect Bayesian inference is the prior g.\u0016/for the full vector\n",
      "\u0016of 6033 effect sizes. The Ô¨Çat prior, g.\u0016/constant, results in the danger-\n",
      "ous overestimateO\u0016610Dx610D5:29. A more appropriate uninformative\n",
      "prior appears as part of the empirical Bayes calculations of Chapter 15\n",
      "(and givesO\u0016610D4:11). The operative point here is that there is a price to\n",
      "be paid for the desirable properties of Bayesian inference. Attention shifts\n",
      "from choosing a good frequentist procedure to choosing an appropriate\n",
      "prior distribution. This can be a formidable task in high-dimensional prob-\n",
      "lems, the very kinds featured in computer-age inference.\n",
      "3.4 A Bayesian/Frequentist Comparison List\n",
      "Bayesians and frequentists start out on the same playing Ô¨Åeld, a family\n",
      "of probability distributions f\u0016.x/(3.1), but play the game in orthogonal\n",
      "5The statistic was the two-sample t-statistic (2.17) transformed to normality (3.28); see\n",
      "the endnotes.34 Bayesian Inference\n",
      "directions, as indicated schematically in Figure 3.5: Bayesian inference\n",
      "proceeds vertically, with xÔ¨Åxed, according to the posterior distribution\n",
      "g.\u0016jx/, while frequentists reason horizontally, with \u0016Ô¨Åxed andxvarying.\n",
      "Advantages and disadvantages accrue to both strategies, some of which are\n",
      "compared next.\n",
      "Figure 3.5 Bayesian inference proceeds vertically, given x;\n",
      "frequentist inference proceeds horizontally, given \u0016.\n",
      "\u000fBayesian inference requires a prior distribution g.\u0016/ . When past experi-\n",
      "ence provides g.\u0016/ , as in the twins example, there is every good reason to\n",
      "employ Bayes‚Äô theorem. If not, techniques such as those of Jeffreys still\n",
      "permit the use of Bayes‚Äô rule, but the results lack the full logical force\n",
      "of the theorem; the Bayesian‚Äôs right to ignore selection bias, for instance,\n",
      "must then be treated with caution.\n",
      "\u000fFrequentism replaces the choice of a prior with the choice of a method,\n",
      "or algorithm, t.x/, designed to answer the speciÔ¨Åc question at hand. This\n",
      "adds an arbitrary element to the inferential process, and can lead to meter-\n",
      "reader kinds of contradictions. Optimal choice of t.x/ reduces arbitrary\n",
      "behavior, but computer-age applications typically move outside the safe\n",
      "waters of classical optimality theory, lending an ad-hoc character to fre-\n",
      "quentist analyses.\n",
      "\u000fModern data-analysis problems are often approached via a favored meth-3.4 A Bayesian/Frequentist Comparison List 35\n",
      "odology, such as logistic regression or regression trees in the examples of\n",
      "Chapter 8. This plays into the methodological orientation of frequentism,\n",
      "which is more Ô¨Çexible than Bayes‚Äô rule in dealing with speciÔ¨Åc algorithms\n",
      "(though one always hopes for a reasonable Bayesian justiÔ¨Åcation for the\n",
      "method at hand).\n",
      "\u000fHaving chosen g.\u0016/ , only a single probability distribution g.\u0016jx/is in\n",
      "play for Bayesians. Frequentists, by contrast, must struggle to balance\n",
      "the behavior of t.x/ over a family of possible distributions, since \u0016in\n",
      "Figure 3.5 is unknown. The growing popularity of Bayesian applications\n",
      "(usually begun with uninformative priors) reÔ¨Çects their simplicity of ap-\n",
      "plication and interpretation.\n",
      "\u000fThe simplicity argument cuts both ways. The Bayesian essentially bets it\n",
      "all on the choice of his or her prior being correct, or at least not harmful.\n",
      "Frequentism takes a more defensive posture, hoping to do well, or at least\n",
      "not poorly, whatever \u0016might be.\n",
      "\u000fA Bayesian analysis answers allpossible questions at once, for example,\n",
      "estimatingEfgfrgor Prfgfr<40gor anything else relating to Figure 2.1.\n",
      "Frequentism focuses on the problem at hand, requiring different estima-\n",
      "tors for different questions. This is more work, but allows for more intense\n",
      "inspection of particular problems. In situation (2.9) for example, estima-\n",
      "tors of the formX\n",
      ".xi\u0000Nx/2=.n\u0000c/ (3.29)\n",
      "might be investigated for different choices of the constant c, hoping to\n",
      "reduce expected mean-squared error.\n",
      "\u000fThe simplicity of the Bayesian approach is especially appealing in dy-\n",
      "namic contexts, where data arrives sequentially and updating one‚Äôs beliefs\n",
      "is a natural practice. Bayes‚Äô rule was used to devastating effect before the\n",
      "2012 US presidential election, updating sequential polling results to cor-\n",
      "rectly predict the outcome in all 50 states. Bayes‚Äô theorem is an excellent\n",
      "tool in general for combining statistical evidence from disparate sources,\n",
      "the closest frequentist analog being maximum likelihood estimation.\n",
      "\u000fIn the absence of genuine prior information, a whiff of subjectivity6hangs\n",
      "over Bayesian results, even those based on uninformative priors. Classical\n",
      "frequentism claimed for itself the high ground of scientiÔ¨Åc objectivity,\n",
      "especially in contentious areas such as drug testing and approval, where\n",
      "skeptics as well as friends hang on the statistical details.\n",
      "Figure 3.5 is soothingly misleading in its schematics: \u0016andxwill\n",
      "6Here we are not discussing the important subjectivist school of Bayesian inference, of\n",
      "Savage, de Finetti, and others, covered in Chapter 13.36 Bayesian Inference\n",
      "typically be high-dimensional in the chapters that follow, sometimes very\n",
      "high-dimensional, straining to the breaking point both the frequentist and\n",
      "the Bayesian paradigms. Computer-age statistical inference at its most\n",
      "successful combines elements of the two philosophies, as for instance in\n",
      "the empirical Bayes methods of Chapter 6, and the lasso in Chapter 16.\n",
      "There are two potent arrows in the statistician‚Äôs philosophical quiver, and\n",
      "faced, say, with 1000 parameters and 1,000,000 data points, there‚Äôs no\n",
      "need to go hunting armed with just one of them.\n",
      "3.5 Notes and Details\n",
      "Thomas Bayes, if transferred to modern times, might well be employed as\n",
      "a successful professor of mathematics. Actually, he was a mid-eighteenth-\n",
      "century nonconformist English minister with substantial mathematical in-\n",
      "terests. Richard Price, a leading Ô¨Ågure of letters, science, and politics, had\n",
      "Bayes‚Äô theorem published in the 1763 Transactions of the Royal Society\n",
      "(two years after Bayes‚Äô death), his interest being partly theological, with\n",
      "the rule somehow proving the existence of God. Bellhouse‚Äôs (2004) biog-\n",
      "raphy includes some of Bayes‚Äô other mathematical accomplishments.\n",
      "Harold Jeffreys was another part-time statistician, working from his day\n",
      "job as the world‚Äôs premier geophysicist of the inter-war period (and Ô¨Åerce\n",
      "opponent of the theory of continental drift). What we called uninformative\n",
      "priors are also called noninformative orobjective . Jeffreys‚Äô brand of Bayes-\n",
      "ianism had a dubious reputation among Bayesians in the period 1950‚Äì\n",
      "1990, with preference going to subjective analysis of the type advocated\n",
      "by Savage and de Finetti. The introduction of Markov chain Monte Carlo\n",
      "methodology was the kind of technological innovation that changes philoso-\n",
      "phies. MCMC (Chapter 13), being very well suited to Jeffreys-style anal-\n",
      "ysis of Big Data problems, moved Bayesian statistics out of the textbooks\n",
      "and into the world of computer-age applications. Berger (2006) makes a\n",
      "spirited case for the objective Bayes approach.\n",
      "¬é1[p. 26] Correlation coefÔ¨Åcient density. Formula (3.11) for the correlation\n",
      "coefÔ¨Åcient density was R. A. Fisher‚Äôs debut contribution to the statistics\n",
      "literature. Chapter 32 of Johnson and Kotz (1970b) gives several equivalent\n",
      "forms. The constant cin (3.19) is often taken to be .n\u00003/\u00001=2, withnthe\n",
      "sample size.\n",
      "¬é2[p. 29] Jeffreys‚Äô prior and transformations. Suppose we change parame-\n",
      "ters from\u0016toQ\u0016in a smoothly differentiable way. The new family QfQ\u0016.x/3.5 Notes and Details 37\n",
      "satisÔ¨Åes\n",
      "@\n",
      "@Q\u0016logQfQ\u0016.x/D@\u0016\n",
      "@Q\u0016@\n",
      "@\u0016logf\u0016.x/: (3.30)\n",
      "ThenQIQ\u0016D\u0010\n",
      "@\u0016\n",
      "@Q\u0016\u00112\n",
      "I\u0016(3.16) andQgJeff.Q\u0016/DÀáÀáÀá@\u0016\n",
      "@Q\u0016ÀáÀáÀágJeff.\u0016/. But this just\n",
      "says thatgJeff.\u0016/transforms correctly to QgJeff.Q\u0016/.\n",
      "¬é3[p. 30] The meter-reader fable is taken from Edwards‚Äô (1992) book Likeli-\n",
      "hood , where he credits John Pratt. It nicely makes the point that frequentist\n",
      "inferences, which are calibrated in terms of possible observed data sets X,\n",
      "may be inappropriate for the actual observation x. This is the difference\n",
      "between working in the horizontal and vertical directions of Figure 3.5.\n",
      "¬é4[p. 33] Two-samplet-statistic. Applied to gene i‚Äôs data in the prostate\n",
      "study, the two-sample t-statisticti(2.17) has theoretical null hypothesis\n",
      "distributiont100, a Student‚Äôstdistribution with 100 degrees of freedom; xi\n",
      "in (3.28) isÀÜ\u00001.F100.ti//, whereÀÜandF100are the cumulative distribu-\n",
      "tion functions of standard normal and t100variables. Section 7.4 of Efron\n",
      "(2010) motivates approximation (3.28).\n",
      "¬é5[p. 33] Selection bias. Senn (2008) discusses the immunity of Bayesian\n",
      "inferences to selection bias and other ‚Äúparadoxes,‚Äù crediting Phil Dawid for\n",
      "the original idea. The article catches the possible uneasiness of following\n",
      "Bayes‚Äô theorem too literally in applications.\n",
      "The 22 students in Table 3.1 were randomly selected from a larger data\n",
      "set of 88 in Mardia et al. (1979) (which gave O\u0012D0:553 ). Welch and\n",
      "Peers (1963) initiated the study of priors whose credible intervals, such\n",
      "as≈í0:093;0:750¬ç in Figure 3.2, match frequentist conÔ¨Ådence intervals. In\n",
      "one-parameter problems, Jeffreys‚Äô priors provide good matches, but not\n",
      "ususally in multiparameter situations. In fact, no single multiparameter\n",
      "prior can give good matches for all one-parameter subproblems, a source of\n",
      "tension between Bayesian and frequentist methods revisited in Chapter 11.4\n",
      "Fisherian Inference and Maximum\n",
      "Likelihood Estimation\n",
      "Sir Ronald Fisher was arguably the most inÔ¨Çuential anti-Bayesian of all\n",
      "time, but that did not make him a conventional frequentist. His key data-\n",
      "analytic methods‚Äîanalysis of variance, signiÔ¨Åcance testing, and maxi-\n",
      "mum likelihood estimation‚Äîwere almost always applied frequentistically.\n",
      "Their Fisherian rationale, however, often drew on ideas neither Bayesian\n",
      "nor frequentist in nature, or sometimes the two in combination. Fisher‚Äôs\n",
      "work held a central place in twentieth-century applied statistics, and some\n",
      "of it, particularly maximum likelihood estimation, has moved forcefully\n",
      "into computer-age practice. This chapter‚Äôs brief review of Fisherian meth-\n",
      "odology sketches parts of its unique philosophical structure, while concen-\n",
      "trating on those topics of greatest current importance.\n",
      "4.1 Likelihood and Maximum Likelihood\n",
      "Fisher‚Äôs seminal work on estimation focused on the likelihood function,\n",
      "or more exactly its logarithm. For a family of probability densities f\u0016.x/\n",
      "(3.1), the log likelihood function is\n",
      "lx.\u0016/Dlogff\u0016.x/g; (4.1)\n",
      "the notation lx.\u0016/emphasizing that the parameter vector \u0016is varying\n",
      "while the observed data vector xis Ô¨Åxed. The maximum likelihood esti-\n",
      "mate (MLE) is the value of \u0016in parameter space that maximizes lx.\u0016/,\n",
      "MLEW O\u0016Darg max\n",
      "\u00162flx.\u0016/g: (4.2)\n",
      "It can happen thatO\u0016doesn‚Äôt exist or that there are multiple maximizers, but\n",
      "here we will assume the usual case where O\u0016exists uniquely. More careful\n",
      "references are provided in the endnotes.\n",
      "DeÔ¨Ånition (4.2) is extended to provide maximum likelihood estimates\n",
      "384.1 Likelihood and Maximum Likelihood 39\n",
      "for a function \u0012DT.\u0016/ of\u0016according to the simple plug-in rule\n",
      "O\u0012DT.O\u0016/; (4.3)\n",
      "most often with \u0012being a scalar parameter of particular interest, such as\n",
      "the regression coefÔ¨Åcient of an important covariate in a linear model.\n",
      "Maximum likelihood estimation came to dominate classical applied es-\n",
      "timation practice. Less dominant now, for reasons we will be investigating\n",
      "in subsequent chapters, the MLE algorithm still has iconic status, being of-\n",
      "ten the method of Ô¨Årst choice in any novel situation. There are several good\n",
      "reasons for its ubiquity.\n",
      "1 The MLE algorithm is automatic : in theory, and almost in practice, a\n",
      "single numerical algorithm produces O\u0016without further statistical input.\n",
      "This contrasts with unbiased estimation, for instance, where each new\n",
      "situation requires clever theoretical calculations.\n",
      "2 The MLE enjoys excellent frequentist properties. In large-sample situa-\n",
      "tions, maximum likelihood estimates tend to be nearly unbiased, with the\n",
      "least possible variance. Even in small samples, MLEs are usually quite\n",
      "efÔ¨Åcient, within say a few percent of the best possible performance.\n",
      "3 The MLE also has reasonable Bayesian justiÔ¨Åcation. Looking at Bayes‚Äô\n",
      "rule (3.7),\n",
      "g.\u0016jx/Dcxg.\u0016/elx.\u0016/; (4.4)\n",
      "we see thatO\u0016is the maximizer of the posterior density g.\u0016jx/if the prior\n",
      "g.\u0016/ is Ô¨Çat, that is, constant. Because the MLE depends on the family\n",
      "Fonly through the likelihood function, anomalies of the meter-reader\n",
      "type are averted.\n",
      "Figure 4.1 displays two maximum likelihood estimates for the gfr data\n",
      "of Figure 2.1. Here the data1is the vectorxD.x1;x2;:::;xn/,nD211.\n",
      "We assume that xwas obtained as a random sample of size nfrom a density\n",
      "f\u0016.x/,\n",
      "xiiid\u0018f\u0016.x/ foriD1;2;:::;n; (4.5)\n",
      "‚Äúiid‚Äù abbreviating ‚Äúindependent and identically distributed.‚Äù Two families\n",
      "are considered for the component density f\u0016.x/, the normal , with\u0016D\n",
      ".\u0012;\u001b/ ,\n",
      "f\u0016.x/D1p\n",
      "2\u0019\u001b2e\u00001\n",
      "2.x\u0000\u0012\n",
      "\u001b/2\n",
      "; (4.6)\n",
      "1Nowxis what we have been calling ‚Äú x‚Äù before, while we will henceforth use xas a\n",
      "symbol for the individual components of x.40 Fisherian Inference and MLE\n",
      " \n",
      "gfrFrequency\n",
      "20 40 60 80 100051015202530\n",
      "NormalGamma\n",
      "Figure 4.1 Glomerular Ô¨Åltration data of Figure 2.1 and two\n",
      "maximum-likelihood density estimates, normal (solid black), and\n",
      "gamma (dashed blue).\n",
      "and the gamma,2with\u0016D.\u0015;\u001b;\u0017/ ,\n",
      "f\u0016.x/D.x\u0000\u0015/\u0017\u00001\n",
      "\u001b\u0017¬Ä.\u0017/e\u0000x\u0000\u0015\n",
      "\u001b (forx\u0015\u0015, 0 otherwise) : (4.7)\n",
      "Since\n",
      "f\u0016.x/DnY\n",
      "iD1f\u0016.xi/ (4.8)\n",
      "under iid sampling, we have\n",
      "lx.\u0016/DnX\n",
      "iD1logf\u0016.xi/DnX\n",
      "iD1lxi.\u0016/: (4.9)\n",
      "Maximum likelihood estimates were found by maximizing lx.\u0016/. For the\n",
      "normal model (4.6),\n",
      "\u0010\n",
      "O\u0012;O\u001b\u0011\n",
      "D.54:3;13:7/D\u0012\n",
      "Nx;hX\n",
      ".xi\u0000Nx/2=ni1=2\u0013\n",
      ": (4.10)\n",
      "2The gamma distribution is usually deÔ¨Åned with \u0015D0as the lower limit of x. Here we\n",
      "are allowing the lower limit \u0015to vary as a free parameter.4.2 Fisher Information and the MLE 41\n",
      "There is no closed-form solution for gamma model (4.7), where numerical\n",
      "maximization gave\n",
      "\u0010\n",
      "O\u0015;O\u001b;O\u0017\u0011\n",
      "D.21:4;5:47;6:0/: (4.11)\n",
      "The plotted curves in Figure 4.1 are the two MLE densities fO\u0016.x/. The\n",
      "gamma model gives a better Ô¨Åt than the normal, but neither is really satis-\n",
      "factory. (A more ambitious maximum likelihood Ô¨Åt appears in Figure 5.7.)\n",
      "Most MLEs require numerical maximization, as for the gamma model.\n",
      "When introduced in the 1920s, maximum likelihood was criticized as com-\n",
      "putationally difÔ¨Åcult, invidious comparisons being made with the older\n",
      "method of moments, which relied only on sample moments of various\n",
      "kinds.\n",
      "There is a downside to maximum likelihood estimation that remained\n",
      "nearly invisible in classical applications: it is dangerous to rely upon in\n",
      "problems involving large numbers of parameters. If the parameter vector\n",
      "\u0016has 1000 components, each component individually may be well esti-\n",
      "mated by maximum likelihood, while the MLE O\u0012DT.O\u0016/for a quantity of\n",
      "particular interest can be grossly misleading.\n",
      "For the prostate data of Figure 3.4, model (4.6) gives MLE O\u0016iDxifor\n",
      "each of the 6033 genes. This seems reasonable, but if we are interested in\n",
      "the maximum coordinate value\n",
      "\u0012DT.\u0016/Dmax\n",
      "if\u0016ig; (4.12)\n",
      "the MLE isO\u0012D5:29, almost certainly a Ô¨Çagrant overestimate. ‚ÄúRegular-\n",
      "ized‚Äù versions of maximum likelihood estimation more suitable for high-\n",
      "dimensional applications play an important role in succeeding chapters.\n",
      "4.2 Fisher Information and the MLE\n",
      "Fisher was not the Ô¨Årst to suggest the maximum likelihood algorithm for\n",
      "parameter estimation. His paradigm-shifting work concerned the favorable\n",
      "inferential properties of the MLE, and in particular its achievement of the\n",
      "Fisher information bound. Only a brief heuristic review will be provided\n",
      "here, with more careful derivations referenced in the endnotes.\n",
      "We begin3with a one-parameter family of densities\n",
      "FDff\u0012.x/; \u00122; x2Xg; (4.13)\n",
      "3The multiparameter case is considered in the next chapter.42 Fisherian Inference and MLE\n",
      "whereis an interval of the real line, possibly inÔ¨Ånite, while the sam-\n",
      "ple space Xmay be multidimensional. (As in the Poisson example (3.3),\n",
      "f\u0012.x/can represent a discrete density, but for convenience we assume here\n",
      "the continuous case, with the probability of set AequalingR\n",
      "Af\u0012.x/dx ,\n",
      "etc.) The log likelihood function is lx.\u0012/Dlogf\u0012.x/and the MLEO\u0012D\n",
      "arg maxflx.\u0012/g, with\u0012replacing\u0016in (4.1)‚Äì(4.2) in the one-dimensional\n",
      "case.\n",
      "Dots will indicate differentiation with respect to \u0012, e.g., for the score\n",
      "function\n",
      "Plx.\u0012/D@\n",
      "@\u0012logf\u0012.x/DPf\u0012.x/=f\u0012.x/: (4.14)\n",
      "The score function has expectation 0,\n",
      "Z\n",
      "XPlx.\u0012/f\u0012.x/dxDZ\n",
      "XPf\u0012.x/dxD@\n",
      "@\u0012Z\n",
      "Xf\u0012.x/dx\n",
      "D@\n",
      "@\u00121D0;(4.15)\n",
      "where we are assuming the regularity conditions necessary for differenti-\n",
      "ating under the integral sign at the third step.\n",
      "The Fisher information I\u0012is deÔ¨Åned to be the variance of the score\n",
      "function,\n",
      "I\u0012DZ\n",
      "XPlx.\u0012/2f\u0012.x/dx; (4.16)\n",
      "the notation\n",
      "Plx.\u0012/\u0018.0;I\u0012/ (4.17)\n",
      "indicating thatPlx.\u0012/has mean 0 and variance I\u0012. The term ‚Äúinformation‚Äù is\n",
      "well chosen. The main result for maximum likelihood estimation, sketched\n",
      "next, is that the MLE O\u0012has an approximately normal distribution with\n",
      "mean\u0012and variance 1=I\u0012,\n",
      "O\u0012P \u0018N.\u0012;1=I\u0012/; (4.18)\n",
      "and that no ‚Äúnearly unbiased‚Äù estimator of \u0012can do better. In other words,\n",
      "bigger Fisher information implies smaller variance for the MLE.\n",
      "The second derivative of the log likelihood function\n",
      "Rlx.\u0012/D@2\n",
      "@\u00122logf\u0012.x/DRf\u0012.x/\n",
      "f\u0012.x/\u0000 Pf\u0012.x/\n",
      "f\u0012.x/!2\n",
      "(4.19)4.2 Fisher Information and the MLE 43\n",
      "has expectation\n",
      "E\u0012n\n",
      "Rlx.\u0012/o\n",
      "D\u0000I\u0012 (4.20)\n",
      "(theRf\u0012.x/=f\u0012.x/term having expectation 0 as in (4.15)). We can write\n",
      "\u0000Rlx.\u0012/\u0018.I\u0012;J\u0012/; (4.21)\n",
      "whereJ\u0012is the variance ofRlx.\u0012/.\n",
      "Now suppose that xD.x1;x2;:::;xn/is an iid sample from f\u0012.x/, as\n",
      "in (4.5), so that the total score function Plx.\u0012/, as in (4.9), is\n",
      "Plx.\u0012/DnX\n",
      "iD1Plxi.\u0012/; (4.22)\n",
      "and similarly\n",
      "\u0000Rlx.\u0012/DnX\n",
      "iD1\u0000Rlxi.\u0012/: (4.23)\n",
      "The MLEO\u0012based on the full sample xsatisÔ¨Åes the maximizing condition\n",
      "Plx.O\u0012/D0. A Ô¨Årst-order Taylor series gives the approximation\n",
      "0DPlx\u0010\n",
      "O\u0012\u0011:DPlx.\u0012/CRlx.\u0012/\u0010\n",
      "O\u0012\u0000\u0012\u0011\n",
      "; (4.24)\n",
      "or\n",
      "O\u0012:D\u0012CPlx.\u0012/=n\n",
      "\u0000Rlx.\u0012/=n: (4.25)\n",
      "Under reasonable regularity conditions, (4.17) and the central limit theo-\n",
      "rem imply that\n",
      "Plx.\u0012/=nP \u0018N.0;I\u0012=n/; (4.26)\n",
      "while the law of large numbers has \u0000Rlx.\u0012/=n approaching the constant I\u0012\n",
      "(4.21).\n",
      "Putting all of this together, (4.25) produces Fisher‚Äôs fundamental theo-\n",
      "rem for the MLE, that in large samples\n",
      "O\u0012P \u0018N.\u0012;1=.n I\u0012//: (4.27)\n",
      "This is the same as result (4.18) since the total Fisher information in an iid\n",
      "sample (4.5) is nI\u0012, as can be seen by taking expectations in (4.23).\n",
      "In the case of normal sampling,\n",
      "xiiid\u0018N.\u0012;\u001b2/ foriD1;2;:::;n; (4.28)44 Fisherian Inference and MLE\n",
      "with\u001b2known, we compute the log likelihood\n",
      "lx.\u0012/D\u00001\n",
      "2nX\n",
      "iD1.xi\u0000\u0012/2\n",
      "\u001b2\u0000n\n",
      "2log.2\u0019\u001b2/: (4.29)\n",
      "This gives\n",
      "Plx.\u0012/D1\n",
      "\u001b2nX\n",
      "iD1.xi\u0000\u0012/ and\u0000Rlx.\u0012/Dn\n",
      "\u001b2; (4.30)\n",
      "yielding the familiar result O\u0012DNxand, since I\u0012D1=\u001b2,\n",
      "O\u0012\u0018N.\u0012;\u001b2=n/ (4.31)\n",
      "from (4.27).\n",
      "This brings us to an aspect of Fisherian inference neither Bayesian nor\n",
      "frequentist. Fisher believed there was a ‚Äúlogic of inductive inference‚Äù that\n",
      "would produce the correct answer to any statistical question, in the same\n",
      "way ordinary logic solves deductive problems. His principal tactic was to\n",
      "logically reduce a complicated inferential question to a simple form where\n",
      "the solution should be obvious to all.\n",
      "Fisher‚Äôs favorite target for the obvious was (4.31), where a single scalar\n",
      "observationO\u0012is normally distributed around the unknown parameter of\n",
      "interest\u0012, with known variance \u001b2=n. Then everyone should agree in the\n",
      "absence of prior information that O\u0012is the best estimate of \u0012, that\u0012has\n",
      "about 95% chance of lying in the interval O\u0012Àô1:96O\u001b=pn, etc.\n",
      "Fisher was astoundingly resourceful at reducing statistical problems to\n",
      "the form (4.31). SufÔ¨Åciency, efÔ¨Åciency, conditionality, and ancillarity were\n",
      "all brought to bear, with the maximum likelihood approximation (4.27)\n",
      "being the most inÔ¨Çuential example. Fisher‚Äôs logical system is not in favor\n",
      "these days, but its conclusions remain as staples of conventional statistical\n",
      "practice.\n",
      "Suppose thatQ\u0012Dt.x/is any unbiased estimate of\u0012based on an iid\n",
      "samplexD.x1;x2;:::;xn/fromf\u0012.x/. That is,\n",
      "\u0012DE\u0012ft.x/g: (4.32)\n",
      "Then the Cram ¬¥er‚ÄìRao lower bound , described in the endnotes, says that\n",
      "the variance ofQ\u0012exceeds the Fisher information bound (4.27),¬é ¬é1\n",
      "var\u0012n\n",
      "Q\u0012o\n",
      "\u00151=.nI\u0012/: (4.33)\n",
      "A loose interpretation is that the MLE has variance at least as small as\n",
      "the best unbiased estimate of \u0012. The MLE is generally not unbiased, but4.3 Conditional Inference 45\n",
      "its bias is small (of order 1=n, compared with standard deviation of order\n",
      "1=pn), making the comparison with unbiased estimates and the Cram ¬¥er‚Äì\n",
      "Rao bound appropriate.\n",
      "4.3 Conditional Inference\n",
      "A simple example gets across the idea of conditional inference: an i.i.d.\n",
      "sample\n",
      "xiiid\u0018N.\u0012;1/; iD1;2;:::;n; (4.34)\n",
      "has produced estimate O\u0012DNx. The investigators originally disagreed on an\n",
      "affordable sample size nand Ô¨Çipped a fair coin to decide,\n",
      "nD(\n",
      "25 probability 1/2\n",
      "100 probability 1/2I(4.35)\n",
      "nD25won. Question: What is the standard deviation of Nx?\n",
      "If you answered 1=p\n",
      "25D0:2then you, like Fisher, are an advocate\n",
      "ofconditional inference . The unconditional frequentist answer says that Nx\n",
      "could have been N.\u0012;1=100/ orN.\u0012;1=25/ with equal probability, yield-\n",
      "ing standard deviation ≈í.0:01C0:04/=2¬ç1=2D0:158 . Some less obvious\n",
      "(and less trivial) examples follow in this section, and in Chapter 9, where\n",
      "conditional inference plays a central role.\n",
      "The data for a typical regression problem consists of pairs .xi;yi/,iD\n",
      "1;2;:::;n , wherexiis ap-dimensional vector of covariates for the ith\n",
      "subject andyiis a scalar response. In Figure 1.1, xiisage andyithe\n",
      "kidney Ô¨Åtness measure tot. Letxbe then\u0002pmatrix having xias itsith\n",
      "row, andythe vector of responses. A regression algorithm uses xandy\n",
      "to construct a function rx;y.x/predictingyfor any value of x, as in (1.3),\n",
      "whereOÀá0andOÀá1were obtained using least squares.\n",
      "How accurate is rx;y.x/? This question is usually answered under the\n",
      "assumption that xis Ô¨Åxed, not random: in other words, by conditioning\n",
      "on the observed value of x. The standard errors in the second line of Ta-\n",
      "ble 1.1 are conditional in this sense; they are frequentist standard deviations\n",
      "ofOÀá0COÀá1x, assuming that the 157 values for age are Ô¨Åxed as observed.\n",
      "(Acorrelation analysis between age andtot would notmake this as-\n",
      "sumption.)\n",
      "Fisher argued for conditional inference on two grounds.46 Fisherian Inference and MLE\n",
      "1More relevant inferences. The conditional standard deviation in situ-\n",
      "ation (4.35) seems obviously more relevant to the accuracy of the ob-\n",
      "servedO\u0012for estimating \u0012. It is less obvious in the regression example,\n",
      "though arguably still the case.\n",
      "2Simpler inferences. Conditional inferences are often simpler to exe-\n",
      "cute and interpret. This is the case with regression, where the statistician\n",
      "doesn‚Äôt have to worry about correlation relationships among the covari-\n",
      "ates, and also with our next example, a Fisherian classic.\n",
      "Table 4.1 shows the results of a randomized trial on 45 ulcer patients,\n",
      "comparing new andold surgical treatments. Was the new surgery signiÔ¨Å-\n",
      "cantly better? Fisher argued for carrying out the hypothesis test conditional\n",
      "on the marginals of the table .16;29;21;24/ . With the marginals Ô¨Åxed, the\n",
      "numberyin the upper left cell determines the other three cells by subtrac-\n",
      "tion. We need only test whether the number yD9is too big under the null\n",
      "hypothesis of no treatment difference, instead of trying to test the numbers\n",
      "in all four cells.4\n",
      "Table 4.1 Forty-Ô¨Åve ulcer patients randomly assigned to either new or\n",
      "old surgery, with results evaluated as either success orfailure .\n",
      "Was the new surgery signiÔ¨Åcantly better?\n",
      "success failure\n",
      "new 9 12 21\n",
      "old 7 17 24\n",
      "16 29 45\n",
      "An ancillary statistic (again, Fisher‚Äôs terminology) is one that contains\n",
      "no direct information by itself, but does determine the conditioning frame-\n",
      "work for frequentist calculations. Our three examples of ancillaries were\n",
      "the sample size n, the covariate matrix x, and the table‚Äôs marginals. ‚ÄúCon-\n",
      "tains no information‚Äù is a contentious claim. More realistically, the two ad-\n",
      "vantages of conditioning, relevance and simplicity, are thought to outweigh\n",
      "the loss of information that comes from treating the ancillary statistic as\n",
      "nonrandom. Chapter 9 makes this case speciÔ¨Åcally for standard survival\n",
      "analysis methods.\n",
      "4Section 9.3 gives the details of such tests; in the surgery example, the difference was not\n",
      "signiÔ¨Åcant.4.3 Conditional Inference 47\n",
      "Our Ô¨Ånal example concerns the accuracy of a maximum likelihood esti-\n",
      "mateO\u0012. Rather than\n",
      "O\u0012P \u0018N\u0000\n",
      "\u0012;1ƒ±\u0000\n",
      "nIO\u0012\u0001\u0001\n",
      "; (4.36)\n",
      "the plug-in version of (4.27), Fisher suggested using\n",
      "O\u0012P \u0018N.\u0012;1=I.x//; (4.37)\n",
      "whereI.x/is the observed Fisher information\n",
      "I.x/D\u0000Rlx\u0010\n",
      "O\u0012\u0011\n",
      "D\u0000@2\n",
      "@\u00122lx.\u0012/ÀáÀáÀáÀáO\u0012: (4.38)\n",
      "The expectation of I.x/isnI\u0012, so in large samples the distribution (4.37)\n",
      "converges to (4.36). Before convergence, however, Fisher suggested that\n",
      "(4.37) gives a better idea of O\u0012‚Äôs accuracy.\n",
      "As a check, a simulation was run involving i.i.d. samples xof sizenD\n",
      "20drawn from a Cauchy density\n",
      "f\u0012.x/D1\n",
      "\u00191\n",
      "1C.x\u0000\u0012/2: (4.39)\n",
      "10,000 samples xof sizenD20were drawn (with \u0012D0) and the ob-\n",
      "served information bound 1=I.x/computed for each. The 10,000 O\u0012values\n",
      "were grouped according to deciles of 1=I.x/, and the observed empirical\n",
      "variance ofO\u0012within each group was then calculated.\n",
      "This amounts to calculating a somewhat crude estimate of the condi-\n",
      "tional variance of the MLE O\u0012, given the observed information bound 1=I.x/.\n",
      "Figure 4.2 shows the results. We see that the conditional variance is close\n",
      "to1=I.x/, as Fisher predicted. The conditioning effect is quite substan-\n",
      "tial; the unconditional variance 1=nI\u0012is 0.10 here, while the conditional\n",
      "variance ranges from 0.05 to 0.20.\n",
      "The observed Fisher information I.x/acts as an approximate ancillary,\n",
      "enjoying both of the virtues claimed by Fisher: it is more relevant than the\n",
      "unconditional information nIO\u0012, and it is usually easier to calculate. Once\n",
      "O\u0012has been found, I.x/is obtained by numerical second differentiation.\n",
      "Unlike I\u0012, no probability calculations are required.\n",
      "There is a strong Bayesian current Ô¨Çowing here. A narrow peak for the\n",
      "log likelihood function, i.e., a large value of I.x/, also implies a narrow\n",
      "posterior distribution for \u0012givenx. Conditional inference, of which Fig-\n",
      "ure 4.2 is an evocative example, helps counter the central Bayesian criti-\n",
      "cism of frequentist inference: that the frequentist properties relate to data\n",
      "sets possibly much different than the one actually observed. The maximum48 Fisherian Inference and MLE\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0.05 0.10 0.15 0.20 0.250.00 0.05 0.10 0.15 0.20 0.25\n",
      "Observed Information BoundMLE variance\n",
      "Figure 4.2 Conditional variance of MLE for Cauchy samples of\n",
      "size 20, plotted versus the observed information bound 1=I.x/.\n",
      "Observed information bounds are grouped by quantile intervals\n",
      "for variance calculations (in percentages): (0‚Äì5), (5‚Äì15), :::,\n",
      "(85‚Äì95), (95‚Äì100). The broken red horizontal line is the\n",
      "unconditional variance 1=nI\u0012.\n",
      "likelihood algorithm can be interpreted both vertically and horizontally in\n",
      "Figure 3.5, acting as a connection between the Bayesian and frequentist\n",
      "worlds.\n",
      "The equivalent of result (4.37) for multiparameter families, Section 5.3,\n",
      "O\u0016P \u0018Np\u0000\n",
      "\u0016;I.x/\u00001\u0001\n",
      "; (4.40)\n",
      "plays an important role in succeeding chapters, with \u0000I.x/thep\u0002pmatrix\n",
      "of second derivatives\n",
      "I.x/D\u0000Rlx.\u0016/D\u0000\u0014@2\n",
      "@\u0016i@\u0016jlogf\u0016.x/\u0015\n",
      "O\u0016: (4.41)4.4 Permutation and Randomization 49\n",
      "4.4 Permutation and Randomization\n",
      "Fisherian methodology faced criticism for its overdependence on normal\n",
      "sampling assumptions. Consider the comparison between the 47 ALL and\n",
      "25AML patients in the gene 136 leukemia example of Figure 1.4. The two-\n",
      "samplet-statistic (1.6) had value 3.01, with two-sided signiÔ¨Åcance level\n",
      "0.0036 according to a Student- tnull distribution with 70 degrees of free-\n",
      "dom. All of this depended on the Gaussian, or normal, assumptions (2.12)‚Äì\n",
      "(2.13).\n",
      "As an alternative signiÔ¨Åcance-level calculation, Fisher suggested using\n",
      "permutations of the 72 data points. The 72 values are randomly divided\n",
      "into disjoint sets of size 47 and 25, and the two-sample t-statistic (2.17) is\n",
      "recomputed. This is done some large number Btimes, yielding permuta-\n",
      "tiont-valuest\u0003\n",
      "1;t\u0003\n",
      "2;:::;t\u0003\n",
      "B. The two-sided permutation signiÔ¨Åcance level\n",
      "for the original value tis then the proportion of the t\u0003\n",
      "ivalues exceeding t\n",
      "in absolute value,\n",
      "#fjt\u0003\n",
      "ij\u0015jtjg=B: (4.42)\n",
      "‚àí4 ‚àí2 0 2 40 200 400 600 800\n",
      "t* valuesfrequency\n",
      "||| ||| | || | | ||||| | | | | ||| | | |\n",
      "‚àí3.01 3.01original\n",
      "t‚àístatistic\n",
      "Figure 4.3 10,000 permutation t\u0003-values for testing ALL vsAML,\n",
      "for gene 136 in the leukemia data of Figure 1.3. Of these, 26\n",
      "t\u0003-values (red ticks) exceeded in absolute value the observed\n",
      "t-statistic 3.01, giving permutation signiÔ¨Åcance level 0.0026.50 Fisherian Inference and MLE\n",
      "Figure 4.3 shows the histogram of BD10,000t\u0003\n",
      "ivalues for the gene\n",
      "136 data in Figure 1.3: 26 of these exceeded tD3:01 in absolute value,\n",
      "yielding signiÔ¨Åcance level 0.0026 against the null hypothesis of no ALL/AML\n",
      "difference, close to the normal-theory signiÔ¨Åcance level 0.0036. (We were\n",
      "a little lucky here.)\n",
      "Why should we believe the permutation signiÔ¨Åcance level (4.42)? Fisher\n",
      "provided two arguments.\n",
      "\u000fSuppose we assume as a null hypothesis that the nD72observed mea-\n",
      "surementsxare an iid sample obtained from the same distributionf\u0016.x/,\n",
      "xiiid\u0018f\u0016.x/ foriD1;2;:::;n: (4.43)\n",
      "(There is no normal assumption here, say that f\u0016.x/isN.\u0012;\u001b2/.)\n",
      "Letoindicate the order statistic ofx, i.e., the 72 numbers ordered\n",
      "from smallest to largest, with their AML orALL labels removed. Then it\n",
      "can be shown that all 72≈†=.47≈†25≈†/ ways of obtaining xby dividingo\n",
      "into disjoint subsets of sizes 47 and 25 are equally likely under null hy-\n",
      "pothesis (4.43). A small value of the permutation signiÔ¨Åcance level (4.42)\n",
      "indicates that the actual division of AML/ALL measurements was notran-\n",
      "dom, but rather resulted from negation of the null hypothesis (4.43). This\n",
      "might be considered an example of Fisher‚Äôs logic of inductive inference,\n",
      "where the conclusion ‚Äúshould be obvious to all.‚Äù It is certainly an exam-\n",
      "ple of conditional inference, now with conditioning used to avoid speciÔ¨Åc\n",
      "assumptions about the sampling density f\u0016.x/.\n",
      "\u000fIn experimental situations, Fisher forcefully argued for randomization ,\n",
      "that is for randomly assigning the experimental units to the possible treat-\n",
      "ment groups. Most famously, in a clinical trial comparing drug A with\n",
      "drug B, each patient should be randomly assigned to A or B.\n",
      "Randomization greatly strengthens the conclusions of a permutation\n",
      "test. In the AML/ALL gene-136 situation, where randomization wasn‚Äôt fea-\n",
      "sible, we wind up almost certain that the AML group has systematically\n",
      "larger numbers, but cannot be certain that it is the different disease states\n",
      "causing the difference. Perhaps the AML patients are older, or heavier, or\n",
      "have more of some other characteristic affecting gene 136. Experimen-\n",
      "tal randomization almost guarantees that age, weight, etc., will be well-\n",
      "balanced between the treatment groups. Fisher‚Äôs RCT (randomized clini-\n",
      "cal trial) was and is the gold standard for statistical inference in medical\n",
      "trials.\n",
      "Permutation testing is frequentistic: a statistician following the proce-\n",
      "dure has 5% chance of rejecting a valid null hypothesis at level 0.05, etc.4.5 Notes and Details 51\n",
      "Randomization inference is somewhat different, amounting to a kind of\n",
      "forced frequentism, with the statistician imposing his or her preferred prob-\n",
      "ability mechanism upon the data. Permutation methods are enjoying a healthy\n",
      "computer-age revival, in contexts far beyond Fisher‚Äôs original justiÔ¨Åcation\n",
      "for thet-test, as we will see in Chapter 15.\n",
      "4.5 Notes and Details\n",
      "On a linear scale that puts Bayesian on the left and frequentist on the right,\n",
      "Fisherian inference winds up somewhere in the middle. Fisher rejected\n",
      "Bayesianism early on, but later criticized as ‚Äúwooden‚Äù the hard-line fre-\n",
      "quentism of the Neyman‚ÄìWald decision-theoretic school. Efron (1998) lo-\n",
      "cates Fisher along the Bayes‚Äìfrequentist scale for several different criteria;\n",
      "see in particular Figure 1 of that paper.\n",
      "Bayesians, of course, believe there is only one true logic of inductive in-\n",
      "ference. Fisher disagreed. His most ambitious attempt to ‚Äúenjoy the Bayes-\n",
      "ian omelette without breaking the Bayesian eggs‚Äù5wasÔ¨Åducial inference .\n",
      "The simplest example concerns the normal translation model x\u0018N.\u0012;1/ ,\n",
      "where\u0012\u0000xhas a standard N.0;1/ distribution, the Ô¨Åducial distribution of\n",
      "\u0012givenxthen being N.x;1/ . Among Fisher‚Äôs many contributions, Ô¨Ådu-\n",
      "cial inference was the only outright popular bust. Nevertheless the idea has\n",
      "popped up again in the current literature under the name ‚ÄúconÔ¨Ådence dis-\n",
      "tribution;‚Äù see Efron (1993) and Xie and Singh (2013). A brief discussion\n",
      "appears in Chapter 11.\n",
      "¬é1[p. 44] For an unbiased estimator Q\u0012Dt.x/(4.32), we have\n",
      "Z\n",
      "Xt.x/Plx.\u0012/f\u0012.x/dxDZ\n",
      "Xt.x/Pf\u0012.x/dxD@\n",
      "@\u0012Z\n",
      "Xt.x/f\u0012.x/dx\n",
      "D@\n",
      "@\u0012\u0012D1:\n",
      "(4.44)\n",
      "HereXisXn, the sample space of xD.x1;x2;:::;xn/, and we are as-\n",
      "suming the conditions necessary for differentiating under the integral sign;\n",
      "(4.44) givesR\n",
      ".t.x/\u0000\u0012/Plx.\u0012/f\u0012.x/dxD1(sincePlx.\u0012/has expectation\n",
      "5Attributed to the important Bayesian theorist L. J. Savage.52 Fisherian Inference and MLE\n",
      "0), and then, applying the Cauchy‚ÄìSchwarz inequality ,\n",
      "\u0014Z\n",
      "X.t.x/\u0000\u0012/Plx.\u0012/f\u0012.x/dx\u00152\n",
      "\u0014\u0014Z\n",
      "X.t.x/\u0000\u0012/2f\u0012.x/dx\u0015\u0014Z\n",
      "XPlx.\u0012/2f\u0012.x/dx\u0015\n",
      ";(4.45)\n",
      "or\n",
      "1\u0014var\u0012n\n",
      "Q\u0012o\n",
      "I\u0012: (4.46)\n",
      "This veriÔ¨Åes the Cram ¬¥er‚ÄìRao lower bound (4.33): the optimal variance for\n",
      "an unbiased estimator is one over the Fisher information.\n",
      "Optimality results are a sign of scientiÔ¨Åc maturity. Fisher information\n",
      "and its estimation bound mark the transition of statistics from a collection\n",
      "of ad-hoc techniques to a coherent discipline. (We have lost some ground\n",
      "recently, where, as discussed in Chapter 1, ad-hoc algorithmic coinages\n",
      "have outrun their inferential justiÔ¨Åcation.) Fisher‚Äôs information bound was\n",
      "a major mathematical innovation, closely related to and predating, Heisen-\n",
      "berg‚Äôs uncertainty principle and Shannon‚Äôs information bound; see Dembo\n",
      "et al. (1991).\n",
      "Unbiased estimation has strong appeal in statistical applications, where\n",
      "‚Äúbiased,‚Äù its opposite, carries a hint of self-interested data manipulation.\n",
      "In large-scale settings, such as the prostate study of Figure 3.4, one can,\n",
      "however, strongly argue for biased estimates. We saw this for gene 610,\n",
      "where the usual unbiased estimate O\u0016610D5:29 is almost certainly too\n",
      "large. Biased estimation will play a major role in our subsequent chapters.\n",
      "Maximum likelihood estimation is effectively unbiased in most situa-\n",
      "tions. Under repeated sampling, the expected mean squared error\n",
      "MSEDE\u001a\u0010\n",
      "O\u0012\u0000\u0012\u00112\u001b\n",
      "DvarianceCbias2(4.47)\n",
      "has order-of-magnitude variance DO.1=n/ and bias2DO.1=n2/, the\n",
      "latter usually becoming negligible as sample size nincreases. (Important\n",
      "exceptions, where bias issubstantial, can occur if O\u0012DT.O\u0016/whenO\u0016is\n",
      "high-dimensional, as in the James‚ÄìStein situation of Chapter 7.) Section\n",
      "10 of Efron (1975) provides a detailed analysis.\n",
      "Section 9.2 of Cox and Hinkley (1974) gives a careful and wide-ranging\n",
      "account of the MLE and Fisher information. Lehmann (1983) covers the\n",
      "same ground, somewhat more technically, in his Chapter 6.5\n",
      "Parametric Models and Exponential Families\n",
      "We have been reviewing classic approaches to statistical inference‚Äîfre-\n",
      "quentist, Bayesian, and Fisherian‚Äîwith an eye toward examining their\n",
      "strengths and limitations in modern applications. Putting philosophical dif-\n",
      "ferences aside, there is a common methodological theme in classical statis-\n",
      "tics: a strong preference for low-dimensional parametric models; that is, for\n",
      "modeling data-analysis problems using parametric families of probability\n",
      "densities (3.1),\n",
      "FDÀö\n",
      "f\u0016.x/Ix2X;\u00162\t\n",
      "; (5.1)\n",
      "where the dimension of parameter \u0016is small, perhaps no greater than 5\n",
      "or 10 or 20. The inverted nomenclature ‚Äúnonparametric‚Äù suggests the pre-\n",
      "dominance of classical parametric methods.\n",
      "Two words explain the classic preference for parametric models: math-\n",
      "ematical tractability. In a world of sliderules and slow mechanical arith-\n",
      "metic, mathematical formulation, by necessity, becomes the computational\n",
      "tool of choice. Our new computation-rich environment has unplugged the\n",
      "mathematical bottleneck, giving us a more realistic, Ô¨Çexible, and far-reach-\n",
      "ing body of statistical techniques. But the classic parametric families still\n",
      "play an important role in computer-age statistics, often assembled as small\n",
      "parts of larger methodologies (as with the generalized linear models of\n",
      "Chapter 8). This chapter1presents a brief review of the most widely used\n",
      "parametric models, ending with an overview of exponential families, the\n",
      "great connecting thread of classical theory and a player of continuing im-\n",
      "portance in computer-age applications.\n",
      "1This chapter covers a large amount of technical material for use later, and may be\n",
      "reviewed lightly at Ô¨Årst reading.\n",
      "5354 Parametric Models\n",
      "5.1 Univariate Families\n",
      "Univariate parametric families, in which the sample space Xof observation\n",
      "xis a subset of the real line R1, are the building blocks of most statistical\n",
      "analyses. Table 5.1 names and describes the Ô¨Åve most familiar univariate\n",
      "families: normal, Poisson, binomial, gamma, and beta. (The chi-squared\n",
      "distribution with ndegrees of freedom \u001f2\n",
      "nis also included since it is dis-\n",
      "tributed as2\u0001Gam.n=2;1/ .) The normal distribution N.\u0016;\u001b2/is a shifted\n",
      "and scaled version of the N.0;1/ distribution2used in (3.27),\n",
      "N.\u0016;\u001b2/\u0018\u0016C\u001bN.0;1/: (5.2)\n",
      "Table 5.1 Five familiar univariate densities, and their sample spaces X,\n",
      "parameter spaces , and expectations and variances; chi-squared\n",
      "distribution with ndegrees of freedom is 2Gam.n=2;1/ .\n",
      "Name ,Density XExpectation,\n",
      "Notation Variance\n",
      "Normal1\n",
      "\u001bp\n",
      "2\u0019e\u00001\n",
      "2.x\u0000\u0016\n",
      "\u001b/2\n",
      "R1\u00162R1\u0016\n",
      "N.\u0016;\u001b2/ \u001b2>0 \u001b2\n",
      "Poisson e\u0000\u0016\u0016x\n",
      "x≈†f0;1;:::g\u0016>0\u0016\n",
      "Poi.\u0016/ \u0016\n",
      "Binomial n≈†\n",
      "x≈†.n\u0000x/≈†\u0019x.1\u0000\u0019/n\u0000xf0;1;:::;ng0<\u0019<1n\u0019\n",
      "Bi.n;\u0019/ n\u0019.1 \u0000\u0019/\n",
      "Gammax\u0017\u00001e\u0000x=\u001b\n",
      "\u001b\u0017¬Ä.\u0017/x\u00150\u0017>0 \u001b\u0017\n",
      "Gam.\u0017;\u001b/ \u001b>0 \u001b2\u0017\n",
      "Beta ¬Ä.\u00171C\u00172/\n",
      "¬Ä.\u00171/¬Ä.\u00172/x\u00171\u00001.1\u0000x/\u00172\u000010\u0014x\u00141\u00171>0 \u00171=.\u00171C\u00172/\n",
      "Be.\u00171;\u00172/ \u0017 2>0\u00171\u00172\n",
      ".\u00171C\u00172/2.\u00171C\u00172C1/\n",
      "Relationships abound among the table‚Äôs families. For instance, indepen-\n",
      "dent gamma variables Gam .\u00171;\u001b/and Gam.\u00172;\u001b/yield a beta variate ac-\n",
      "cording to\n",
      "Be.\u00171;\u00172/\u0018Gam.\u00171;\u001b/\n",
      "Gam.\u00171;\u001b/CGam.\u00172;\u001b/: (5.3)\n",
      "The binomial and Poisson are particularly close cousins. A Bi .n;\u0019/ distri-\n",
      "bution (the number of heads in nindependent Ô¨Çips of a coin with probabil-\n",
      "2The notation in (5.2) indicates that if X\u0018N.\u0016;\u001b2/andY\u0018N.0;1/ thenXand\n",
      "\u0016C\u001bYhave the same distribution.5.2 The Multivariate Normal Distribution 55\n",
      "0.00 0.05 0.10 0.15 0.20\n",
      "xf(x)\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "01234567891011121314151617‚óèBinomial: (6, 2.19)\n",
      "Poisson:  (6, 2.45)\n",
      "Figure 5.1 Comparison of the binomial distribution Bi .30;0:2/\n",
      "(black lines) with the Poisson Poi .6/(red dots). In the legend we\n",
      "show the mean and standard deviation for each distribution.\n",
      "ity of heads\u0019) approaches a Poi .n\u0019/ distribution,\n",
      "Bi.n;\u0019/P \u0018Poi.n\u0019/ (5.4)\n",
      "asngrows large and \u0019small, the notationP \u0018indicating approximate equal-\n",
      "ity of the two distributions. Figure 5.1 shows the approximation already\n",
      "working quite effectively for nD30and\u0019D0:2.\n",
      "The Ô¨Åve families in Table 5.1 have Ô¨Åve different sample spaces, making\n",
      "them appropriate in different situations. Beta distributions, for example,\n",
      "are natural candidates for modeling continuous data on the unit interval\n",
      "≈í0;1¬ç . Choices of the two parameters .\u00171;\u00172/provide a variety of possible\n",
      "shapes, as illustrated in Figure 5.2. Later we will discuss general exponen-\n",
      "tial families, unavailable in classical theory, that greatly expand the catalog\n",
      "of possible shapes.\n",
      "5.2 The Multivariate Normal Distribution\n",
      "Classical statistics produced a less rich catalog of multivariate distribu-\n",
      "tions, ones where the sample space Xexists in Rp,p-dimensional Eu-56 Parametric Models\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.0\n",
      "xf(x)( ŒΩ1,ŒΩ2 )\n",
      "( 8, 4)\n",
      "( 2, 4)\n",
      "(.5,.5)\n",
      "Figure 5.2 Three beta densities, with .\u00171;\u00172/indicated.\n",
      "clidean space, p > 1 . By far the greatest amount of attention focused on\n",
      "the multivariate normal distribution.\n",
      "A random vector xD.x1;x2;:::;xp/0;normally distributed or not, has\n",
      "mean vector\n",
      "\u0016DEfxgD\u0000\n",
      "Efx1g;Efx2g;:::;Efxpg\u00010(5.5)\n",
      "andp\u0002pcovariance matrix3\n",
      "‚Ä†DEÀö\n",
      ".x\u0000\u0016/.x\u0000\u0016/0\t\n",
      "D\u0000\n",
      "EÀö\n",
      ".xi\u0000\u0016i/.xj\u0000\u0016j/\t\u0001\n",
      ": (5.6)\n",
      "(The outer product uv0of vectorsuandvis the matrix having elements\n",
      "uivj.) We will use the convenient notation\n",
      "x\u0018.\u0016;‚Ä†/ (5.7)\n",
      "for (5.5) and (5.6), reducing to the familiar form x\u0018.\u0016;\u001b2/in the uni-\n",
      "variate case.\n",
      "Denoting the entries of ‚Ä†by\u001bij, foriandjequaling1;2;:::;p , the\n",
      "diagonal elements are variances,\n",
      "\u001biiDvar.xi/: (5.8)\n",
      "3The notation‚Ä†D.\u001bij/deÔ¨Ånes theijth element of a matrix.5.2 The Multivariate Normal Distribution 57\n",
      "The off-diagonal elements relate to the correlations between the coordi-\n",
      "nates ofx,\n",
      "cor.xi;xj/D\u001bijp\u001bii\u001bjj: (5.9)\n",
      "The multivariate normal distribution extends the univariate deÔ¨Ånition\n",
      "N.\u0016;\u001b2/in Table 5.1. To begin with, let zD.z1;z2;:::;zp/0be a vector\n",
      "ofpindependent N.0;1/ variates, with probability density function\n",
      "f.z/D.2\u0019/\u0000p\n",
      "2e\u00001\n",
      "2Pp\n",
      "1z2\n",
      "iD.2\u0019/\u0000p\n",
      "2e\u00001\n",
      "2z0z(5.10)\n",
      "according to line 1 of Table 5.1.\n",
      "The multivariate normal family is obtained by linear transformations of\n",
      "z: let\u0016be ap-dimensional vector and Tap\u0002pnonsingular matrix, and\n",
      "deÔ¨Åne the random vector\n",
      "xD\u0016CTz: (5.11)\n",
      "Following the usual rules of probability transformations yields the density\n",
      "ofx,\n",
      "f\u0016;‚Ä†.x/D.2\u0019/\u0000p=2\n",
      "j‚Ä†j1=2e\u00001\n",
      "2.x\u0000\u0016/0‚Ä†\u00001.x\u0000\u0016/; (5.12)\n",
      "where‚Ä†is thep\u0002psymmetric positive deÔ¨Ånite matrix\n",
      "‚Ä†DTT0(5.13)\n",
      "andj‚Ä†jits determinant;¬éf\u0016;‚Ä†.x/, thep-dimensional multivariate normal ¬é1\n",
      "distribution with mean \u0016and covariance ‚Ä†, is denoted\n",
      "x\u0018Np.\u0016;‚Ä†/: (5.14)\n",
      "Figure 5.3 illustrates the bivariate normal distribution with \u0016D.0;0/0\n",
      "and‚Ä†having\u001b11D\u001b22D1and\u001b12D0:5(so cor.x1;x2/D0:5). The\n",
      "bell-shaped mountain on the left is a plot of density (5.12). The right panel\n",
      "shows a scatterplot of 2000 points drawn from this distribution. Concentric\n",
      "ellipses illustrate curves of constant density,\n",
      ".x\u0000\u0016/0‚Ä†\u00001.x\u0000\u0016/Dconstant: (5.15)\n",
      "Classical multivariate analysis was the study of the multivariate normal\n",
      "distribution, both of its probabilistic and statistical properties. The notes\n",
      "reference some important (and lengthy) multivariate texts. Here we will\n",
      "just recall a couple of results useful in the chapters to follow.58 Parametric Models\n",
      "**\n",
      "*\n",
      "*****\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "***\n",
      "****\n",
      "***\n",
      "*\n",
      "****\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "****\n",
      "*\n",
      "***\n",
      "* ***\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*****\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "** ***\n",
      "***\n",
      "***\n",
      "* *\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "***\n",
      "****\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "* * **\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "* *** **\n",
      "****\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*****\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "****\n",
      "***\n",
      "*\n",
      "**\n",
      "****\n",
      "* ****\n",
      "**\n",
      "**\n",
      "**** *\n",
      "*\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "** *\n",
      "*\n",
      "***\n",
      "***\n",
      "**\n",
      "*\n",
      "* **\n",
      "*\n",
      "**\n",
      "**\n",
      "* ***\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "**** *\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "* **\n",
      "*\n",
      "****\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "* **\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "*\n",
      "* ***\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "** ***\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "* *\n",
      "***\n",
      "**\n",
      "*\n",
      "*\n",
      "****\n",
      "*\n",
      "***\n",
      "*\n",
      "*\n",
      "* **\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "******\n",
      "**\n",
      "***\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "**\n",
      "* **\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "****\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "*****\n",
      "**\n",
      "*****\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "***\n",
      "*\n",
      "*\n",
      "**\n",
      "** **\n",
      "*\n",
      "***\n",
      "*\n",
      "*\n",
      "*\n",
      "***\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*****\n",
      "*\n",
      "**\n",
      "****\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "*\n",
      "*****\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "*** *\n",
      "**\n",
      "****\n",
      "*\n",
      "* **\n",
      "**\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "** **\n",
      "***\n",
      "* **\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "****\n",
      "**\n",
      "** * **\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "****\n",
      "*\n",
      "*****\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "*****\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "** **\n",
      "* *\n",
      "**\n",
      "***\n",
      "* *\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "****\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "** ****\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "****\n",
      "**\n",
      "*****\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "*\n",
      "**\n",
      "** ***\n",
      "***\n",
      "**\n",
      "****\n",
      "*** *\n",
      "***\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "* *\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "** **\n",
      "*\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "***\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*** *\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "*****\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "*** **\n",
      "***\n",
      "*\n",
      "*** **\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "****\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "****\n",
      "**\n",
      "* ***\n",
      "** *\n",
      "****\n",
      "*\n",
      "***\n",
      "*\n",
      "***\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "* * **\n",
      "**\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "* *\n",
      "** **\n",
      "***\n",
      "*\n",
      "****\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "*\n",
      "***\n",
      "***\n",
      "***\n",
      "*\n",
      "*\n",
      "**\n",
      "*****\n",
      "*\n",
      "****\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "****\n",
      "****\n",
      "* *\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "******\n",
      "*** *\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "******\n",
      "* **\n",
      "***\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "*\n",
      "*****\n",
      "* *****\n",
      "*\n",
      "***\n",
      "*\n",
      "****\n",
      "*** **\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "****\n",
      "***\n",
      "*\n",
      "**\n",
      "****\n",
      "**\n",
      "***\n",
      "******\n",
      "*\n",
      "****\n",
      "****\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "***\n",
      "****\n",
      "** * *\n",
      "***\n",
      "**\n",
      "***\n",
      "****\n",
      "***\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "***\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "* *\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "** **\n",
      "*\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "** ***\n",
      "*\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "** *\n",
      "** *\n",
      "**\n",
      "****\n",
      "*\n",
      "*\n",
      "**** *\n",
      "*\n",
      "****\n",
      "**\n",
      "**** *\n",
      "** **\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*****\n",
      "*\n",
      "* **\n",
      "*\n",
      "**\n",
      "*\n",
      "****\n",
      "***\n",
      "*\n",
      "*\n",
      "**\n",
      "****\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "****\n",
      "*\n",
      "***\n",
      "***\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "***\n",
      "*\n",
      "*** *\n",
      "**\n",
      "** *\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "**\n",
      "*\n",
      "* **\n",
      "*\n",
      "***\n",
      "***\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "* *\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "**\n",
      "*** **\n",
      "*\n",
      "**\n",
      "*\n",
      "****\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "*****\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "* ***\n",
      "***\n",
      "***\n",
      "**\n",
      "*****\n",
      "*\n",
      "****\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "***\n",
      "*\n",
      "*\n",
      "*\n",
      "** *\n",
      "**\n",
      "***\n",
      "*\n",
      "** *\n",
      "*****\n",
      "*\n",
      "***\n",
      "** *\n",
      "*\n",
      "***\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "‚àí2 ‚àí1 0 1 2‚àí2 ‚àí1 0 1 2\n",
      "x1x1\n",
      "x2\n",
      "x2\n",
      "Figure 5.3 Left: bivariate normal density, with var .x1/D\n",
      "var.x2/D1and cor.x1;x2/D0:5.Right: sample of 2000\n",
      ".x1;x2/pairs from this bivariate normal density.\n",
      "Suppose that xD.x1;x2;:::;xp/0is partitioned into\n",
      "x.1/D.x1;x2;:::;xp1/0andx.2/D.xp1C1;xp1C2;:::;xp1Cp2/0;\n",
      "(5.16)\n",
      "p1Cp2Dp, with\u0016and‚Ä†similarly partitioned,\n",
      " \n",
      "x.1/\n",
      "x.2/!\n",
      "\u0018Np  \n",
      "\u0016.1/\n",
      "\u0016.2/!\n",
      ";\u0012‚Ä†11‚Ä†12\n",
      "‚Ä†21‚Ä†22\u0013!\n",
      "(5.17)\n",
      "(so‚Ä†11isp1\u0002p1,‚Ä†12isp1\u0002p2, etc.). Then the conditional distribution\n",
      "ofx.2/givenx.1/is itself normal,¬é ¬é2\n",
      "x.2/jx.1/\u0018Np2\u0000\n",
      "\u0016.2/C‚Ä†21‚Ä†\u00001\n",
      "11.x.1/\u0000\u0016.1//;‚Ä†22\u0000‚Ä†21‚Ä†\u00001\n",
      "11‚Ä†12\u0001\n",
      ":\n",
      "(5.18)\n",
      "Ifp1Dp2D1, then (5.18) reduces to\n",
      "x2jx1\u0018N\u0012\n",
      "\u00162C\u001b12\n",
      "\u001b11.x1\u0000\u00161/; \u001b22\u0000\u001b2\n",
      "12\n",
      "\u001b11\u0013\n",
      "I (5.19)\n",
      "here\u001b12=\u001b11is familiar as the linear regression coefÔ¨Åcient of x2as a func-\n",
      "tion ofx1, while\u001b2\n",
      "12=\u001b11\u001b22equals cor.x1;x2/2, the squared proportion\n",
      "R2of the variance of x2explained by x1. Hence we can write the (unex-\n",
      "plained) variance term in (5.19) as \u001b22.1\u0000R2/.\n",
      "Bayesian statistics also makes good use of the normal family. It helps to\n",
      "begin with the univariate case x\u0018N.\u0016;\u001b2/, where now we assume that5.3 Fisher‚Äôs Information Bound 59\n",
      "the expectation vector itself has a normal prior distribution N.M;A/ :\n",
      "\u0016\u0018N.M;A/ andxj\u0016\u0018N.\u0016;\u001b2/: (5.20)\n",
      "Bayes‚Äô theorem and some algebra show that the posterior distribution of \u0016\n",
      "having observed xis normal,¬é ¬é3\n",
      "\u0016jx\u0018N\u0012\n",
      "MCA\n",
      "AC\u001b2.x\u0000M/;A\u001b2\n",
      "AC\u001b2\u0013\n",
      ": (5.21)\n",
      "The posterior expectation O\u0016BayesDMC.A=.AC\u001b2//.x\u0000M/is ashrink-\n",
      "age estimator of\u0016: if, say,Aequals\u001b2, thenO\u0016BayesDMC.x\u0000M/=2\n",
      "is shrunk half the way back from the unbiased estimate O\u0016Dxtoward the\n",
      "prior meanM, while the posterior variance \u001b2=2ofO\u0016Bayes is only one-half\n",
      "that ofO\u0016.\n",
      "The multivariate version of the Bayesian setup (5.20) is\n",
      "\u0016\u0018Np.M;A/andxj\u0016\u0018Np.\u0016;‚Ä†/; (5.22)\n",
      "now withMand\u0016p-vectors, andAand‚Ä†positive deÔ¨Ånite p\u0002pmatrices.\n",
      "As indicated in the notes, the posterior distribution of \u0016givenxis then\n",
      "\u0016jx\u0018Np\u0000\n",
      "MCA.AC‚Ä†/\u00001.x\u0000M/;A.AC‚Ä†/\u00001‚Ä†\u0001\n",
      ";(5.23)\n",
      "which reduces to (5.21) when pD1.\n",
      "5.3 Fisher‚Äôs Information Bound for Multiparameter Families\n",
      "The multivariate normal distribution plays its biggest role in applications\n",
      "as a large-sample approximation for maximum likelihood estimates. We\n",
      "suppose that the parametric family of densities ff\u0016.x/g, normal or not, is\n",
      "smoothly deÔ¨Åned in terms of its p-dimensional parameter vector \u0016. (In\n",
      "terms of (5.1), is a subset of Rp.)\n",
      "The MLE deÔ¨Ånitions and results are direct analogues of the single-param-\n",
      "eter calculations beginning at (4.14) in Chapter 4. The score functionPlx.\u0016/\n",
      "is now deÔ¨Åned as the gradient of log ff\u0016.x/g,\n",
      "Plx.\u0016/Dr\u0016Àö\n",
      "logf\u0016.x/\t\n",
      "D\u0012\n",
      ":::;@logf\u0016.x/\n",
      "@\u0016i;:::\u00130\n",
      "; (5.24)\n",
      "thep-vector of partial derivatives of log f\u0016.x/with respect to the coordi-\n",
      "nates of\u0016. It has mean zero,\n",
      "E\u0016n\n",
      "Plx.\u0016/o\n",
      "D0D.0;0;0;:::;0/0: (5.25)60 Parametric Models\n",
      "By deÔ¨Ånition, the Fisher information matrix I\u0016for\u0016is thep\u0002pcovari-\n",
      "ance matrix ofPlx.\u0016/; using outer product notation,\n",
      "I\u0016DE\u0016n\n",
      "Plx.\u0016/Plx.\u0016/0o\n",
      "D\u0012\n",
      "E\u0016\u001a@logf\u0016.x/\n",
      "@\u0016i@logf\u0016.x/\n",
      "@\u0016j\u001b\u0013\n",
      ":(5.26)\n",
      "The key result is that the MLE O\u0016Darg max\u0016ff\u0016.x/ghas an approxi-\n",
      "mately normal distribution with covariance matrix I\u00001\n",
      "\u0016,\n",
      "O\u0016P \u0018Np.\u0016;I\u00001\n",
      "\u0016/: (5.27)\n",
      "Approximation (5.27) is justiÔ¨Åed by large-sample arguments, say with x\n",
      "an iid sample in Rp,.x1;x2;:::;xn/,ngoing to inÔ¨Ånity.\n",
      "Suppose the statistician is particularly interested in \u00161, the Ô¨Årst coordi-\n",
      "nate of\u0016. Let\u0016.2/D.\u00162;\u00163;:::;\u0016p/denote the other p\u00001coordinates\n",
      "of\u0016, which are now ‚Äúnuisance parameters‚Äù as far as the estimation of \u00161\n",
      "goes. According to (5.27), the MLE O\u00161, which is the Ô¨Årst coordinate of O\u0016,\n",
      "has\n",
      "O\u00161P \u0018N\u0000\n",
      "\u00161;.I\u00001\n",
      "\u0016/11\u0001\n",
      "; (5.28)\n",
      "where the notation indicates the upper leftmost entry of I\u00001\n",
      "\u0016.\n",
      "We can partition the information matrix I\u0016into the two parts corre-\n",
      "sponding to\u00161and\u0016.2/,\n",
      "I\u0016D\u0012I\u001611I\u00161.2/\n",
      "I\u0016.2/1I\u0016.22/\u0013\n",
      "(5.29)\n",
      "(withI\u00161.2/DI0\n",
      "\u0016.2/1of dimension 1\u0002.p\u00001/andI\u0016.22/.p\u00001/\u0002.p\u00001/).\n",
      "The endnotes show that¬é ¬é4\n",
      ".I\u00001\n",
      "\u0016/11D\u0000I\u001611\u0000I\u00161.2/I\u00001\n",
      "\u0016.22/I\u0016.2/1\u0001\u00001: (5.30)\n",
      "The subtracted term on the right side of (5.30) is nonnegative, implying\n",
      "that\n",
      ".I\u00001\n",
      "\u0016/11\u0015I\u00001\n",
      "\u001611: (5.31)\n",
      "If\u0016.2/were known to the statistician, rather than requiring estimation,\n",
      "thenf\u00161\u0016.2/.x/would be a one-parameter family, with Fisher information\n",
      "I\u001611for estimating \u00161, giving\n",
      "O\u00161P \u0018N.\u00161;I\u00001\n",
      "\u001611/: (5.32)5.4 The Multinomial Distribution 61\n",
      "Comparing (5.28) with (5.32), (5.31) shows that the variance of the MLE\n",
      "O\u00161must always increase4in the presence of nuisance parameters.¬é¬é5\n",
      "Maximum likelihood, and in fact any form of unbiased or nearly unbi-\n",
      "ased estimation, pays a nuisance tax for the presence of ‚Äúother‚Äù parameters.\n",
      "Modern applications often involve thousands of others ; think of regression\n",
      "Ô¨Åts with too many predictors. In some circumstances, biased estimation\n",
      "methods can reverse the situation, using the others to actually improve esti-\n",
      "mation of a target parameter; see Chapter 6 on empirical Bayes techniques,\n",
      "and Chapter 16 on `1regularized regression models.\n",
      "5.4 The Multinomial Distribution\n",
      "Second in the small catalog of well-known classic multivariate distribu-\n",
      "tions is the multinomial. The multinomial applies to situations in which\n",
      "the observations take on only a Ô¨Ånite number of discrete values, say Lof\n",
      "them. The2\u00022ulcer surgery of Table 4.1 is repeated in Table 5.2, now with\n",
      "the cells labeled 1;2;3; and4. Here there are LD4possible outcomes\n",
      "for each patient: ( new,success ), (new,failure ), (old,success ),\n",
      "(old,failure ).\n",
      "Table 5.2 The ulcer study of Table 4.1, now with the cells numbered 1\n",
      "through 4 as shown.\n",
      "success failure\n",
      "new19212\n",
      "old37417\n",
      "A numbernof cases has been observed, nD45in Table 5.2. Let xD\n",
      ".x1;x2;:::;xL/be the vector of counts for the Lpossible outcomes,\n",
      "xlD#fcases having outcome lg; (5.33)\n",
      "xD.9;12;7;17/0for the ulcer data. It is convenient to code the outcomes\n",
      "in terms of the coordinate vectors elof lengthL,\n",
      "elD.0;0;:::;0;1;0;:::;0/0; (5.34)\n",
      "with a 1 in the lth place.\n",
      "4Unless I\u00161.2/ is a vector of zeros, a condition that amounts to approximate\n",
      "independence ofO\u00161andO\u0016.2/.62 Parametric Models\n",
      "Figure 5.4 The simplex S3is an equilateral triangle set at an\n",
      "angle to the coordinate axes in R3.\n",
      "The multinomial probability model assumes that the ncases are inde-\n",
      "pendent of each other, with each case having probability \u0019lfor outcome\n",
      "el,\n",
      "\u0019lDPrfelg; lD1;2;:::;L: (5.35)\n",
      "Let\n",
      "\u0019D.\u00191;\u00192;:::;\u0019L/0(5.36)\n",
      "indicate the vector of probabilities. The count vector xthen follows the\n",
      "multinomial distribution ,\n",
      "f\u0019.x/Dn≈†\n",
      "x1≈†x2≈†:::xL≈†LY\n",
      "lD1\u0019xl\n",
      "l; (5.37)\n",
      "denoted\n",
      "x\u0018MultL.n;\u0019/ (5.38)\n",
      "(fornobservations, Loutcomes, probability vector \u0019).\n",
      "The parameter space for\u0019is the simplex SL,\n",
      "SLD(\n",
      "\u0019W\u0019l\u00150andLX\n",
      "lD1\u0019lD1)\n",
      ": (5.39)\n",
      "Figure 5.4 shows S3, an equilateral triangle sitting at an angle to the coordi-\n",
      "nate axese1;e2;ande3. The midpoint of the triangle \u0019D.1=3;1=3;1=3/5.4 The Multinomial Distribution 63\n",
      "corresponds to a multinomial distribution putting equal probability on the\n",
      "three possible outcomes.\n",
      " ¬†004\t\n",
      " ¬†013\t\n",
      "Figure 5.5 Sample space Xforx\u0018Mult3.4;\u0019/; numbers\n",
      "indicate.x1;x2;x3/.\n",
      "The sample space Xforxis the subset of nSL(the set of nonnegative\n",
      "vectors summing to n) having integer components. Figure 5.5 illustrates\n",
      "the casenD4andLD3, now with the triangle of Figure 5.4 multiplied\n",
      "by 4 and set Ô¨Çat on the page. The point 121 indicates xD.1;2;1/ , with\n",
      "probability12\u0001\u00191\u00192\n",
      "2\u00193according to (5.37), etc.\n",
      "In the dichotomous case,LD2, the multinomial distribution reduces\n",
      "to the binomial, with .\u00191;\u00192/equaling.\u0019;1\u0000\u0019/in line 3 of Table 5.1,\n",
      "and.x1;x2/equaling.x;n\u0000x/. The mean vector and covariance matrix\n",
      "of MultL.n;\u0019/, for any value of L, are¬é ¬é6\n",
      "x\u0018\u0000\n",
      "n\u0019;n\u0002\n",
      "diag.\u0019/\u0000\u0019\u00190\u0003\u0001\n",
      "(5.40)\n",
      "(diag.\u0019/is the diagonal matrix with diagonal elements \u0019l), so var.xl/D\n",
      "n\u0019l.1\u0000\u0019l/and covariance .xl;xj/D\u0000n\u0019l\u0019j; (5.40) generalizes the\n",
      "binomial mean and variance .n\u0019;n\u0019.1\u0000\u0019//.\n",
      "There is a useful relationship between the multinomial distribution and\n",
      "the Poisson. Suppose S1;S2;:::;SLare independent Poissons having pos-\n",
      "sibly different parameters,\n",
      "Slind\u0018Poi.\u0016l/; lD1;2;:::;L; (5.41)\n",
      "or, more concisely,\n",
      "S\u0018Poi.\u0016/ (5.42)\n",
      "withSD.S1;S2;:::;SL/0and\u0016D.\u00161;\u00162;:::;\u0016L/0, the independence64 Parametric Models\n",
      "being assumed in notation (5.42). Then the conditional distribution of S\n",
      "given the sum SCDPSlis multinomial,¬é ¬é7\n",
      "SjSC\u0018MultL.SC;\u0016=\u0016C/; (5.43)\n",
      "\u0016CDP\u0016l.\n",
      "Going in the other direction, suppose N\u0018Poi.n/. Then the uncondi-\n",
      "tional or marginal distribution of Mult L.N;\u0019/is Poisson,\n",
      "MultL.N;\u0019/\u0018Poi.n\u0019/ ifN\u0018Poi.n/: (5.44)\n",
      "Calculations involving x\u0018MultL.n;\u0019/are sometimes complicated by\n",
      "the multinomial‚Äôs correlations. The approximation xP \u0018Poi.n\u0019/removes\n",
      "the correlations and is usually quite accurate if nis large.\n",
      "There is one more important thing to say about the multinomial family: it\n",
      "contains alldistributions on a sample space Xcomposed of Ldiscrete cat-\n",
      "egories. In this sense it is a model for nonparametric inference on X. The\n",
      "nonparametric bootstrap calculations of Chapter 10 use the multinomial in\n",
      "this way. Nonparametrics, and the multinomial, have played a larger role\n",
      "in the modern environment of large, difÔ¨Åcult to model, data sets.\n",
      "5.5 Exponential Families\n",
      "Classic parametric families dominated statistical theory and practice for a\n",
      "century and more, with an enormous catalog of their individual properties‚Äî\n",
      "means, variances, tail areas, etc.‚Äîbeing compiled. A surprise, though a\n",
      "slowly emerging one beginning in the 1930s, was that all of them were\n",
      "examples of a powerful general construction: exponential families . What\n",
      "follows here is a brief introduction to the basic theory, with further devel-\n",
      "opment to come in subsequent chapters.\n",
      "To begin with, consider the Poisson family, line 2 of Table 5.1. The ratio\n",
      "of Poisson densities at two parameter values \u0016and\u00160is\n",
      "f\u0016.x/\n",
      "f\u00160.x/De\u0000.\u0016\u0000\u00160/\u0012\u0016\n",
      "\u00160\u0013x\n",
      "; (5.45)\n",
      "which can be re-expressed as\n",
      "f\u0016.x/DeÀõx\u0000 .Àõ/f\u00160.x/; (5.46)\n",
      "where we have deÔ¨Åned\n",
      "ÀõDlogf\u0016=\u00160gand .Àõ/D\u00160.eÀõ\u00001/: (5.47)\n",
      "Looking at (5.46), we can describe the Poisson family in three steps.5.5 Exponential Families 65\n",
      "1 Start with any one Poisson distribution f\u00160.x/.\n",
      "2 For any value of \u0016>0 letÀõDlogf\u0016=\u00160gand calculate\n",
      "Qf\u0016.x/DeÀõxf\u00160.x/ forxD0;1;2;:::: (5.48)\n",
      "3 Finally, divideQf\u0016.x/by exp. .Àõ// to get the Poisson density f\u0016.x/.\n",
      "In other words, we ‚Äútilt‚Äù f\u00160.x/with the exponential factor eÀõxto get\n",
      "Qf\u0016.x/, and then renormalize Qf\u0016.x/to sum to 1. Notice that (5.46) gives\n",
      "exp.\u0000 .Àõ// as the renormalizing constant since\n",
      "e .Àõ/D1X\n",
      "0eÀõxf\u00160.x/: (5.49)\n",
      "0 5 10 15 20 25 300.00 0.05 0.10 0.15 0.20\n",
      "xf(x)\n",
      "‚óè‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "Figure 5.6 Poisson densities for \u0016D3;6;9;12;15;18 ; heavy\n",
      "green curve with dots for \u0016D12.\n",
      "Figure 5.6 graphs the Poisson density f\u0016.x/for\u0016D3;6;9;12;15;18 .\n",
      "Each Poisson density is a renormalized exponential tilt of any other Poisson\n",
      "density. So for instance f6.x/is obtained from f12.x/via the tilteÀõxwith\n",
      "ÀõDlogf6=12gD\u00000:693 .5\n",
      "5Alternate expressions for f\u0016.x/as an exponential family are available, for example\n",
      "exp.Àõx\u0000 .Àõ//f0.x/, whereÀõDlog\u0016, .Àõ/Dexp.Àõ/, andf0.x/D1=x≈† . (It\n",
      "isn‚Äôt necessary for f0.x/to be a member of the family.)66 Parametric Models\n",
      "The Poisson is a one-parameter exponential family , in thatÀõandxin\n",
      "expression (5.46) are one-dimensional. A p-parameter exponential family\n",
      "has the form\n",
      "fÀõ.x/DeÀõ0y\u0000 .Àõ/f0.x/ forÀõ2A; (5.50)\n",
      "whereÀõandyarep-vectors and Ais contained in Rp. HereÀõis the\n",
      "‚Äúcanonical‚Äù or ‚Äúnatural‚Äù parameter vector and yDt.x/ is the ‚ÄúsufÔ¨Åcient\n",
      "statistic‚Äù vector. The normalizing function  .Àõ/ , which makes fÀõ.x/inte-\n",
      "grate (or sum) to one, satisÔ¨Åes\n",
      "e .Àõ/DZ\n",
      "XeÀõ0yf0.x/dx; (5.51)\n",
      "and it can be shown that the parameter space Afor which the integral is\n",
      "Ô¨Ånite is a convex set¬éinRp. As an example, the gamma family on line 4 ¬é8\n",
      "of Table 5.1 is a two-parameter exponential family, with ÀõandyDt.x/\n",
      "given by\n",
      ".Àõ1;Àõ2/D\u0012\n",
      "\u00001\n",
      "\u001b;\u0017\u0013\n",
      ";.y1;y2/D.x;logx/; (5.52)\n",
      "and\n",
      " .Àõ/D\u0017log\u001bClog¬Ä.\u0017/\n",
      "D\u0000Àõ2logf\u0000Àõ1gClogf¬Ä.Àõ2/g:(5.53)\n",
      "The parameter space AisfÀõ1<0andÀõ2>0g.\n",
      "Why are we interested in exponential tilting rather than some other trans-\n",
      "formational form? The answer has to do with repeated sampling. Suppose\n",
      "xD.x1;x2;:::;xn/is an iid sample from a p-parameter exponential\n",
      "family (5.50). Then, letting yiDt.xi/denote the sufÔ¨Åcient vector corre-\n",
      "sponding toxi,\n",
      "fÀõ.x/DnY\n",
      "iD1eÀõ0yi\u0000 .Àõ/f0.xi/\n",
      "Den.Àõ0Ny\u0000 .Àõ//f0.x/;(5.54)\n",
      "whereNyDPn\n",
      "1yi=n. This is still a p-parameter exponential family, now\n",
      "with natural parameter nÀõ, sufÔ¨Åcient statistic Ny, and normalizer n .Àõ/ .\n",
      "No matter how large nmay be, the statistician can still compress all the\n",
      "inferential information into a p-dimensional statistic Ny. Only exponential\n",
      "families enjoy this property.\n",
      "Even though they were discovered and developed in quite different con-\n",
      "texts, and at quite different times, all of the distributions discussed in this5.5 Exponential Families 67\n",
      "chapter exist in exponential families. This isn‚Äôt quite the coincidence it\n",
      "seems. Mathematical tractability was the prized property of classic para-\n",
      "metric distributions, and tractability was greatly facilitated by exponential\n",
      "structure, even if that structure went unrecognized.\n",
      "In one-parameter exponential families, the normalizer  .Àõ/ is also known\n",
      "as the cumulant generating function . Derivatives of  .Àõ/ yield the cumu-\n",
      "lants ofy,6the Ô¨Årst two giving the mean and variance¬é ¬é9\n",
      "P .Àõ/DEÀõfygandR .Àõ/DvarÀõfyg: (5.55)\n",
      "Similarly, inp-parametric families\n",
      "P .Àõ/D.:::@ =@Àõ j:::/0DEÀõfyg (5.56)\n",
      "and\n",
      "R .Àõ/D\u0012@2 .Àõ/\n",
      "@Àõj@Àõk\u0013\n",
      "DcovÀõfyg: (5.57)\n",
      "Thep-dimensional expectation parameter , denoted\n",
      "ÀáDEÀõfyg; (5.58)\n",
      "is a one-to-one function of the natural parameter Àõ. LetVÀõindicate the\n",
      "p\u0002pcovariance matrix,\n",
      "VÀõDcovÀõfyg: (5.59)\n",
      "Then thep\u0002pderivative matrix of Àáwith respect to Àõis\n",
      "dÀá\n",
      "dÀõD.@Àáj=@Àõk/DVÀõ; (5.60)\n",
      "this following from (5.56)‚Äì(5.57), the inverse mapping being dÀõ=dÀáD\n",
      "V\u00001\n",
      "Àõ. As a one-parameter example, the Poisson in Table 5.1 has ÀõDlog\u0016,\n",
      "ÀáD\u0016,yDx, anddÀá=dÀõD1=.dÀõ=dÀá/D\u0016DVÀõ.\n",
      "The maximum likelihood estimate for the expectation parameter Àáis\n",
      "simplyy(orNyunder repeated sampling (5.54)), which makes it immediate\n",
      "to calculate in most situations.¬éLess immediate is the MLE for the natural ¬é10\n",
      "parameterÀõ: the one-to-one mapping ÀáDP .Àõ/ (5.56) has inverse ÀõD\n",
      "P \u00001.Àá/, so\n",
      "OÀõDP \u00001.y/; (5.61)\n",
      "6The simpliÔ¨Åed dot notation leads to more compact expressions: P .Àõ/Dd .Àõ/=dÀõ\n",
      "andR .Àõ/Dd2 .Àõ/=dÀõ2.68 Parametric Models\n",
      "e.g.,OÀõDlogyfor the Poisson. The trouble is that P \u00001.\u0001/is usually un-\n",
      "available in closed form. Numerical approximation algorithms are neces-\n",
      "sary to calculateOÀõin most cases.\n",
      "All of the classic exponential families have closed-form expressions for\n",
      " .Àõ/ (andfÀõ.x/), yielding pleasant formulas for the mean Àáand covar-\n",
      "ianceVÀõ, (5.56)‚Äì(5.57). Modern computational technology allows us to\n",
      "work with general exponential families, designed for speciÔ¨Åc tasks, with-\n",
      "out concern for mathematical tractability.\n",
      " \n",
      "gfrFrequency\n",
      "20 40 60 80 100051015202530\n",
      "GammaExponential Family\n",
      "Figure 5.7 A seven-parameter exponential family Ô¨Åt to the gfr\n",
      "data of Figure 2.1 (solid) compared with gamma Ô¨Åt of Figure 4.1\n",
      "(dashed).\n",
      "As an example we again consider Ô¨Åtting the gfr data of Figure 2.1.\n",
      "For our exponential family of possible densities we take f0.x/\u00111, and\n",
      "sufÔ¨Åcient statistic vector\n",
      "y.x/D.x;x2;:::;x7/; (5.62)\n",
      "soÀõ0yin (5.50) can represent all 7th-order polynomials in x, the gfr\n",
      "measurement.7(Stopping at power 2 gives the N.\u0016;\u001b2/family, which we\n",
      "already know Ô¨Åts poorly from Figure 4.1.) The heavy curve in Figure 5.7\n",
      "shows the MLE Ô¨Åt fOÀõ.x/now following the gfr histogram quite closely.\n",
      "Chapter 10 discusses ‚ÄúLindsey‚Äôs method,‚Äù a simpliÔ¨Åed algorithm for cal-\n",
      "culating the MLEOÀõ.\n",
      "7Any intercept in the polynomial is absorbed into the  .Àõ/ term in (5.57).5.6 Notes and Details 69\n",
      "A more exotic example concerns the generation of random graphs on a\n",
      "Ô¨Åxed set ofNnodes. Each possible graph has a certain total number Eof\n",
      "edges, andTof triangles. A popular choice for generating such graphs is\n",
      "the two-parameter exponential family having yD.E;T/ , so that larger\n",
      "values ofÀõ1andÀõ2yield more connections.\n",
      "5.6 Notes and Details\n",
      "The notion of sufÔ¨Åcient statistics , ones that contain all available inferen-\n",
      "tial information, was perhaps Fisher‚Äôs happiest contribution to the classic\n",
      "corpus. He noticed that in the exponential family form (5.50), the fact that\n",
      "the parameter Àõinteracts with the data xonly through the factor exp .Àõ0y/\n",
      "makesy.x/ sufÔ¨Åcient for estimating Àõ. In 1935‚Äì36, a trio of authors, work-\n",
      "ing independently in different countries, Pitman, Darmois, and Koopmans,\n",
      "showed that exponential families are the only ones that enjoy Ô¨Åxed-dimen-\n",
      "sional sufÔ¨Åcient statistics under repeated independent sampling. Until the\n",
      "late 1950s such distributions were called Pitman‚ÄìDarmois‚ÄìKoopmans fam-\n",
      "ilies, the long name suggesting infrequent usage.\n",
      "Generalized linear models, Chapter 8, show the continuing impact of\n",
      "sufÔ¨Åciency on statistical practice. Peter Bickel has pointed out that data\n",
      "compression , a lively topic in areas such as image transmission, is a mod-\n",
      "ern, less stringent, version of sufÔ¨Åciency.\n",
      "Our only nonexponential family so far was (4.39), the Cauchy transla-\n",
      "tional model. Efron and Hinkley (1978) analyze the Cauchy family in terms\n",
      "ofcurved exponential families , a generalization of model (5.50).\n",
      "Properties of classical distributions (lots of properties and lots of distri-\n",
      "butions) are covered in Johnson and Kotz‚Äôs invaluable series of reference\n",
      "books, 1969‚Äì1972. Two classic multivariate analysis texts are Anderson\n",
      "(2003) and Mardia et al. (1979).\n",
      "¬é1[p. 57] Formula (5.12) .FromzDT\u00001.x\u0000\u0016/we havedz=dxDT\u00001\n",
      "and\n",
      "f\u0016;‚Ä†.x/Df.z/jT\u00001jD.2\u0019/\u0000p\n",
      "2jT\u00001je\u00001\n",
      "2.x\u0000\u0016/0T\u000010T\u00001.x\u0000\u0016/;(5.63)\n",
      "so (5.12) follows from TT0D‚Ä†andjTjDj‚Ä†j1=2.\n",
      "¬é2[p. 58] Formula (5.18) .Let∆íD‚Ä†\u00001be partitioned as in (5.17). Then\n",
      "\u0012∆í11∆í12\n",
      "∆í21∆í22\u0013\n",
      "D \u0000\n",
      "‚Ä†11\u0000‚Ä†12‚Ä†\u00001\n",
      "22‚Ä†21\u0001\u00001\u0000‚Ä†\u00001\n",
      "11‚Ä†12∆í22\n",
      "\u0000‚Ä†\u00001\n",
      "22‚Ä†21∆í11\u0000\n",
      "‚Ä†22\u0000‚Ä†21‚Ä†\u00001\n",
      "11‚Ä†12\u0001\u00001!\n",
      ";\n",
      "(5.64)\n",
      "direct multiplication showing that ∆í‚Ä†DI, the identity matrix. If ‚Ä†is70 Parametric Models\n",
      "symmetric then ∆í21D∆í0\n",
      "12. By redeÔ¨Åning xto bex\u0000\u0016we can set\u0016.1/\n",
      "and\u0016.2/equal to zero in (5.18). The quadratic form in the exponent of\n",
      "(5.12) is\n",
      ".x0\n",
      ".1/;x0\n",
      ".2//∆í\u0000\n",
      "x.1/;x.2/\u0001\n",
      "Dx0\n",
      ".2/∆í22x.2/C2x0\n",
      ".1/∆í12x.2/Cx0\n",
      ".1/∆í11x.1/:\n",
      "(5.65)\n",
      "But, using (5.64), this matches the quadratic form from (5.18),\n",
      "\u0000\n",
      "x.2/\u0000‚Ä†21‚Ä†\u00001\n",
      "11x.1/\u00010∆í22\u0000\n",
      "x.2/\u0000‚Ä†21‚Ä†\u00001\n",
      "11x.1/\u0001\n",
      "(5.66)\n",
      "except for an added term that does notinvolvex.2/. For a multivariate nor-\n",
      "mal distribution, this is sufÔ¨Åcient to show that the conditional distribution\n",
      "ofx.2/givenx.1/is indeed (5.18) (see ¬é3).\n",
      "¬é3[p. 59] Formulas (5.21) and(5.23) .Suppose that the continuous univariate\n",
      "random variable zhas density of the form\n",
      "f.z/Dc0e\u00001\n",
      "2Q.z/; whereQ.z/Daz2C2bzCc1; (5.67)\n",
      "a;b;c0andc1constants,a>0 . Then, by ‚Äúcompleting the square,‚Äù\n",
      "f.z/Dc2e\u00001\n",
      "2a.z\u0000b\n",
      "a/2\n",
      "; (5.68)\n",
      "and we see that z\u0018N.b=a;1=a/ . The key point is that form (5.67) spec-\n",
      "iÔ¨Åeszas normal, with mean and variance uniquely determined by aand\n",
      "b. The multivariate version of this fact was used in the derivation of for-\n",
      "mula (5.18).\n",
      "By redeÔ¨Åning \u0016andxas\u0016\u0000Mandx\u0000M, we can take MD0in\n",
      "(5.21). Setting BDA=.AC\u001b2/, density (5.21) for \u0016jxis of form (5.67),\n",
      "with\n",
      "Q.\u0016/D\u00162\n",
      "B\u001b2\u00002x\u0016\n",
      "\u001b2CBx2\n",
      "\u001b2: (5.69)\n",
      "But Bayes‚Äô rule says that the density of \u0016jxis proportional to g.\u0016/f\u0016.x/,\n",
      "also of form (5.67), now with\n",
      "Q.\u0016/D\u00121\n",
      "AC1\n",
      "\u001b2\u0013\n",
      "\u00162\u00002x\u0016\n",
      "\u001b2Cx2\n",
      "\u001b2: (5.70)\n",
      "A little algebra shows that the quadratic and linear coefÔ¨Åcients of \u0016match\n",
      "in (5.69)‚Äì(5.70), verifying (5.21).\n",
      "We verify the multivariate result (5.23) using a different argument. The\n",
      "2pvector.\u0016;x/0has joint distribution\n",
      "N\u0012\u0012M\n",
      "M\u0013\n",
      ";\u0012A A\n",
      "A AC‚Ä†\u0013\u0013\n",
      ": (5.71)5.6 Notes and Details 71\n",
      "Now we employ (5.18) and a little manipulation to get (5.23).\n",
      "¬é4[p. 60] Formula (5.30) .This is the matrix identity (5.64), now with ‚Ä†\n",
      "equaling I\u0016.\n",
      "¬é5[p. 61] Multivariate Gaussian and nuisance parameters. The cautionary\n",
      "message here‚Äîthat increasing the number of unknown nuisance parame-\n",
      "ters decreases the accuracy of the estimate of interest‚Äîcan be stated more\n",
      "positively: if some nuisance parameters are actually known, then the MLE\n",
      "of the parameter of interest becomes more accurate. Suppose, for example,\n",
      "we wish to estimate \u00161from a sample of size nin a bivariate normal model\n",
      "x\u0018N2.\u0016;‚Ä†/(5.14). The MLENx1has variance\u001b11=nin notation (5.19).\n",
      "But if\u00162is known then the MLE of \u00161becomesNx1\u0000.\u001b12=\u001b22/.Nx2\u0000\u00162/\n",
      "with variance .\u001b11=n/\u0001.1\u0000\u001a2/,\u001abeing the correlation \u001b12=p\u001b11\u001b22.\n",
      "¬é6[p. 63] Formula (5.40) .xDPn\n",
      "iD1xi, where thexiare iid observations\n",
      "having PrfxiDeigD\u0019l, as in (5.35). The mean and covariance of each\n",
      "xiare\n",
      "EfxigDLX\n",
      "1\u0019lelD\u0019 (5.72)\n",
      "and\n",
      "covfxigDEfxix0\n",
      "ig\u0000EfxigEfx0\n",
      "igDX\n",
      "\u0019lele0\n",
      "l\u0000\u0019\u00190\n",
      "Ddiag.\u0019/\u0000\u0019\u00190:(5.73)\n",
      "Formula (5.40) follows from EfxgDPEfxigand covfxgDPcovfxig.\n",
      "¬é7[p. 64] Formula (5.43) .The densities of S(5.42) andSCDPSlare\n",
      "f\u0016.S/DLY\n",
      "lD1e\u0000\u0016l\u0016Sl\n",
      "l=Sl≈†andf\u0016C.SC/De\u0000\u0016C\u0016SC\n",
      "C=SC≈†:(5.74)\n",
      "The conditional density of SgivenSCis the ratio\n",
      "f\u0016.SjSC/D \n",
      "SC≈†\n",
      "QL\n",
      "1Sl≈†!LY\n",
      "lD1\u0012\u0016l\n",
      "\u0016C\u0013Sl\n",
      "; (5.75)\n",
      "which is (5.43).\n",
      "¬é8[p. 66] Formula (5.51) and the convexity of A.SupposeÀõ1andÀõ2are any\n",
      "two points in A, i.e., values of Àõhaving the integral in (5.51) Ô¨Ånite. For any\n",
      "value ofcin the interval ≈í0;1¬ç , and any value of y, we have\n",
      "ceÀõ0\n",
      "1yC.1\u0000c/eÀõ0\n",
      "2y\u0015e≈ícÀõ1C.1\u0000c/Àõ2¬ç0y(5.76)\n",
      "because of the convexity in cof the function on the right (veriÔ¨Åed by show-\n",
      "ing that its second derivative is positive). Integrating both sides of (5.76)72 Parametric Models\n",
      "overXwith respect to f0.x/shows that the integral on the right must be\n",
      "Ô¨Ånite: that is, cÀõ1C.1\u0000c/Àõ2is inA, verifyingA‚Äôs convexity.\n",
      "¬é9[p. 67] Formula (5.55) .In the univariate case, differentiating both sides of\n",
      "(5.51) with respect to Àõgives\n",
      "P .Àõ/e .Àõ/DZ\n",
      "XyeÀõyf0.x/dxI (5.77)\n",
      "dividing bye .Àõ/shows thatP .Àõ/DEÀõfyg. Differentiating (5.77) again\n",
      "gives\n",
      "\u0000R .Àõ/CP .Àõ/2\u0001\n",
      "e .Àõ/DZ\n",
      "Xy2eÀõyf0.x/dx; (5.78)\n",
      "or\n",
      "R .Àõ/DEÀõfy2g\u0000EÀõfyg2DvarÀõfyg: (5.79)\n",
      "Successive derivatives of  .Àõ/ yield the higher cumulants of y, its skew-\n",
      "ness, kurtosis, etc.\n",
      "¬é10[p. 67] MLE forÀá.The gradient with respect to Àõof logfÀõ.y/(5.50) is\n",
      "rÀõ\u0000\n",
      "Àõ0y\u0000 .Àõ/\u0001\n",
      "Dy\u0000P .Àõ/Dy\u0000EÀõfy\u0003g; (5.80)\n",
      "(5.56), where y\u0003represents a hypothetical realization y.x\u0003/drawn from\n",
      "fÀõ.\u0001/. We achieve the MLE OÀõatrOÀõD0, or\n",
      "EOÀõfy\u0003gDy: (5.81)\n",
      "In other words the MLE OÀõis the value of Àõthat makes the expectation\n",
      "EÀõfy\u0003gmatch the observed y. Thus (5.58) implies that the MLE of pa-\n",
      "rameterÀáisy.Part II\n",
      "Early Computer-Age Methods6\n",
      "Empirical Bayes\n",
      "The constraints of slow mechanical computation molded classical statistics\n",
      "into a mathematically ingenious theory of sharply delimited scope. Emerg-\n",
      "ing after the Second World War, electronic computation loosened the com-\n",
      "putational stranglehold, allowing a more expansive and useful statistical\n",
      "methodology.\n",
      "Some revolutions start slowly. The journals of the 1950s continued to\n",
      "emphasize classical themes: pure mathematical development typically cen-\n",
      "tered around the normal distribution. Change came gradually, but by the\n",
      "1990s a new statistical technology, computer enabled, was Ô¨Årmly in place.\n",
      "Key developments from this period are described in the next several chap-\n",
      "ters. The ideas, for the most part, would not startle a pre-war statistician,\n",
      "but their computational demands, factors of 100 or 1000 times those of\n",
      "classical methods, would. More factors of a thousand lay ahead, as will be\n",
      "told in Part III, the story of statistics in the twenty-Ô¨Årst century.\n",
      "Empirical Bayes methodology, this chapter‚Äôs topic, has been a particu-\n",
      "larly slow developer despite an early start in the 1940s. The roadblock here\n",
      "was not so much the computational demands of the theory as a lack of ap-\n",
      "propriate data sets. Modern scientiÔ¨Åc equipment now provides ample grist\n",
      "for the empirical Bayes mill, as will be illustrated later in the chapter, and\n",
      "more dramatically in Chapters 15‚Äì21.\n",
      "6.1 Robbins‚Äô Formula\n",
      "Table 6.1 shows one year of claims data for a European automobile insur-\n",
      "ance company; 7840 of the 9461 policy holders made no claims during the\n",
      "year, 1317 made a single claim, 239 made two claims each, etc., with Ta-\n",
      "ble 6.1 continuing to the one person who made seven claims. Of course the\n",
      "insurance company is concerned about the claims each policy holder will\n",
      "make in the next year.\n",
      "Bayes‚Äô formula seems promising here. We suppose that xk, the number\n",
      "7576 Empirical Bayes\n",
      "Table 6.1 Countsyxof number of claims xmade in a single year by\n",
      "9461 automobile insurance policy holders. Robbins‚Äô formula (6.7)\n",
      "estimates the number of claims expected in a succeeding year, for instance\n",
      "0:168 for a customer in the xD0category. Parametric maximum\n",
      "likelihood analysis based on a gamma prior gives less noisy estimates.\n",
      "Claimsx 0 1 2 3 4 5 6 7\n",
      "Countsyx 7840 1317 239 42 14 4 4 1\n",
      "Formula (6.7) .168 .363 .527 1.33 1.43 6.00 1.75\n",
      "Gamma MLE .164 .398 .633 .87 1.10 1.34 1.57\n",
      "of claims to be made in a single year by policy holder k, follows a Poisson\n",
      "distribution with parameter \u0012k,\n",
      "PrfxkDxgDp\u0012k.x/De\u0000\u0012k\u0012x\n",
      "k=x≈†; (6.1)\n",
      "forxD0;1;2;3;::: ;\u0012kis the expected value of xk. A good customer,\n",
      "from the company‚Äôs point of view, has a small value of \u0012k, though in any\n",
      "one year his or her actual number of accidents xkwill vary randomly ac-\n",
      "cording to probability density (6.1).\n",
      "Suppose we knew the prior density g.\u0012/ for the customers‚Äô \u0012values.\n",
      "Then Bayes‚Äô rule (3.5) would yield\n",
      "Ef\u0012jxgDR1\n",
      "0\u0012p\u0012.x/g.\u0012/d\u0012R1\n",
      "0p\u0012.x/g.\u0012/d\u0012(6.2)\n",
      "for the expected value of \u0012of a customer observed to make xclaims in a\n",
      "single year. This would answer the insurance company‚Äôs question of what\n",
      "number of claims Xto expect the next year from the same customer, since\n",
      "Ef\u0012jxgis alsoEfXjxg(\u0012being the expectation of X).\n",
      "Formula (6.2) is just the ticket if the prior g.\u0012/ is known to the company,\n",
      "but what if it is not? A clever rewriting of (6.2) provides a way forward.\n",
      "Using (6.1), (6.2) becomes\n",
      "Ef\u0012jxgDR1\n",
      "0\u0002\n",
      "e\u0000\u0012\u0012xC1=x≈†\u0003\n",
      "g.\u0012/d\u0012R1\n",
      "0\u0002\n",
      "e\u0000\u0012\u0012x=x≈†\u0003\n",
      "g.\u0012/d\u0012\n",
      "D.xC1/R1\n",
      "0\u0002\n",
      "e\u0000\u0012\u0012xC1=.xC1/≈†\u0003\n",
      "g.\u0012/d\u0012R1\n",
      "0\u0002\n",
      "e\u0000\u0012\u0012x=x≈†\u0003\n",
      "g.\u0012/d\u0012:(6.3)6.1 Robbins‚Äô Formula 77\n",
      "Themarginal density ofx, integratingp\u0012.x/over the prior g.\u0012/ , is\n",
      "f.x/DZ1\n",
      "0p\u0012.x/g.\u0012/d\u0012DZ1\n",
      "0h\n",
      "e\u0000\u0012\u0012x=x≈†i\n",
      "g.\u0012/d\u0012: (6.4)\n",
      "Comparing (6.3) with (6.4) gives Robbins‚Äô formula ,\n",
      "Ef\u0012jxgD.xC1/f.xC1/=f.x/: (6.5)\n",
      "The surprising and gratifying fact is that, even with no knowledge of the\n",
      "prior density g.\u0012/ , the insurance company can estimate Ef\u0012jxg(6.2) from\n",
      "formula (6.5). The obvious estimate of the marginal density f.x/ is the\n",
      "proportion of total counts in category x,\n",
      "Of.x/Dyx=N; withNDP\n",
      "xyx;the total count ; (6.6)\n",
      "Of.0/D7840=9461 ,Of.1/D1317=9461 , etc. This yields an empirical\n",
      "version of Robbins‚Äô formula,\n",
      "OEf\u0012jxgD.xC1/Of.xC1/ƒ±Of.x/D.xC1/yxC1=yx; (6.7)\n",
      "the Ô¨Ånal expression not requiring N. Table 6.1 givesOEf\u0012j0gD0:168 :\n",
      "customers who made zero claims in one year had expectation 0.168 of a\n",
      "claim the next year; those with one claim had expectation 0.363, and so on.\n",
      "Robbins‚Äô formula came as a surprise1to the statistical world of the\n",
      "1950s: the expectation Ef\u0012kjxkgfor a single customer, unavailable without\n",
      "the priorg.\u0012/ , somehow becomes available in the context of a large study.\n",
      "The terminology empirical Bayes is apt here: Bayesian formula (6.5) for a\n",
      "single subject is estimated empirically (i.e., frequentistically) from a col-\n",
      "lection of similar cases. The crucial point, and the surprise, is that large\n",
      "data sets of parallel situations carry within them their own Bayesian in-\n",
      "formation . Large parallel data sets are a hallmark of twenty-Ô¨Årst-century\n",
      "scientiÔ¨Åc investigation, promoting the popularity of empirical Bayes meth-\n",
      "ods.\n",
      "Formula (6.7) goes awry at the right end of Table 6.1, where it is destabi-\n",
      "lized by small count numbers. A parametric approach gives more depend-\n",
      "able results: now we assume that the prior density g.\u0012/ for the customers‚Äô\n",
      "\u0012kvalues has a gamma form (Table 5.1)\n",
      "g.\u0012/D\u0012\u0017\u00001e\u0000\u0012=\u001b\n",
      "\u001b\u0017¬Ä.\u0017/; for\u0012\u00150; (6.8)\n",
      "but with parameters \u0017and\u001bunknown. Estimates .O\u0017;O\u001b/are obtained by\n",
      "1Perhaps it shouldn‚Äôt have; estimation methods similar to (6.7) were familiar in the\n",
      "actuarial literature.78 Empirical Bayes\n",
      "maximum likelihood Ô¨Åtting to the counts yx, yielding a parametrically es-\n",
      "timated marginal density¬é ¬é1\n",
      "Of.x/DfO\u0017;O\u001b.x/; (6.9)\n",
      "or equivalentlyOyxDNfO\u0017;O\u001b.x/.\n",
      "0 1 2 3 4 5 6 70 2 4 6 8 10\n",
      "claimslog(counts)‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè ‚óè\n",
      "‚óè\n",
      "Figure 6.1 Auto accident data; log(counts) vs claims for 9461\n",
      "auto insurance policies. The dashed line is a gamma MLE Ô¨Åt.\n",
      "The bottom row of Table 6.1 gives parametric estimates EO\u0017;O\u001bf\u0012jxgD\n",
      ".xC1/OyxC1=Oyx, which are seen to be less eccentric for large x. Figure 6.1\n",
      "compares (on the log scale) the raw counts yxwith their parametric cousins\n",
      "Oyx.\n",
      "6.2 The Missing-Species Problem\n",
      "The very Ô¨Årst empirical Bayes success story related to the butterÔ¨Çy data of\n",
      "Table 6.2. Even in the midst of World War II Alexander Corbet, a leading\n",
      "naturalist, had been trapping butterÔ¨Çies for two years in Malaysia (then\n",
      "Malaya): 118 species were so rare that he had trapped only one specimen\n",
      "each, 74 species had been trapped twice each, Table 6.2 going on to show\n",
      "that 44 species were trapped three times each, and so on. Some of the more6.2 The Missing-Species Problem 79\n",
      "common species had appeared hundreds of times each, but of course Corbet\n",
      "was interested in the rarer specimens.\n",
      "Table 6.2 ButterÔ¨Çy data; number yof species seen xtimes each in two\n",
      "years of trapping; 118 species trapped just once, 74 trapped twice each,\n",
      "etc.\n",
      "x 1 2 3 4 5 6 7 8 9 10 11 12\n",
      "y 118 74 44 24 29 22 20 19 20 15 12 14\n",
      "x 13 14 15 16 17 18 19 20 21 22 23 24\n",
      "y 6 12 6 9 9 6 10 10 11 5 3 3\n",
      "Corbet then asked a seemingly impossible question: if he trapped for one\n",
      "additional year, how many new species would he expect to capture? The\n",
      "question relates to the absent entry in Table 6.2, xD0, the species that\n",
      "haven‚Äôt been seen yet. Do we really have any evidence at all for answering\n",
      "Corbet? Fortunately he asked the right man: R. A. Fisher, who produced a\n",
      "surprisingly satisfying solution for the ‚Äúmissing-species problem.‚Äù\n",
      "Suppose there are Sspecies in all, seen or unseen, and that xk, the num-\n",
      "ber of times species kis trapped in one time unit,2follows a Poisson dis-\n",
      "tribution with parameter \u0012kas in (6.1),\n",
      "xk\u0018Poi.\u0012k/; forkD1;2;:::;S: (6.10)\n",
      "The entries in Table 6.2 are\n",
      "yxD#fxkDxg; forxD1;2;:::;24; (6.11)\n",
      "the number of species trapped exactly xtimes each.\n",
      "Now consider a further trapping period of ttime units,tD1=2in Cor-\n",
      "bet‚Äôs question, and let xk.t/be the number of times species kis trapped in\n",
      "the new period. Fisher‚Äôs key assumption is that\n",
      "xk.t/\u0018Poi.\u0012kt/ (6.12)\n",
      "independently ofxk. That is, any one species is trapped independently over\n",
      "time3at a rate proportional to its parameter \u0012k.\n",
      "The probability that species kisnotseen in the initial trapping period\n",
      "2One time unit equals two years in Corbet‚Äôs situation.\n",
      "3This is the deÔ¨Ånition of a Poisson process .80 Empirical Bayes\n",
      "butisseen in the new period, that is xkD0andxk.t/>0 , is\n",
      "e\u0000\u0012k\u0010\n",
      "1\u0000e\u0000\u0012kt\u0011\n",
      "; (6.13)\n",
      "so thatE.t/ , the expected number of new species seen in the new trapping\n",
      "period, is\n",
      "E.t/DSX\n",
      "kD1e\u0000\u0012k\u0010\n",
      "1\u0000e\u0000\u0012kt\u0011\n",
      ": (6.14)\n",
      "It is convenient to write (6.14) as an integral,\n",
      "E.t/DSZ1\n",
      "0e\u0000\u0012\u0010\n",
      "1\u0000e\u0000\u0012t\u0011\n",
      "g.\u0012/d\u0012; (6.15)\n",
      "whereg.\u0012/ is the ‚Äúempirical density‚Äù putting probability 1=S on each of\n",
      "the\u0012kvalues. (Later we will think of g.\u0012/ as a continuous prior density on\n",
      "the possible\u0012kvalues.)\n",
      "Expanding1\u0000e\u0000\u0012tgives\n",
      "E.t/DSZ1\n",
      "0e\u0000\u0012\u0002\n",
      "\u0012t\u0000.\u0012t/2=2≈†C.\u0012t/3=3≈†\u0000\u0001\u0001\u0001\u0003\n",
      "g.\u0012/d\u0012: (6.16)\n",
      "Notice that the expected value exofyxis the sum of the probabilities of\n",
      "being seen exactly xtimes in the initial period,\n",
      "exDEfyxgDSX\n",
      "kD1e\u0000\u0012k\u0012x\n",
      "k=x≈†\n",
      "DSZ1\n",
      "0h\n",
      "e\u0000\u0012\u0012x=x≈†i\n",
      "g.\u0012/d\u0012:(6.17)\n",
      "Comparing (6.16) with (6.17) provides a surprising result,\n",
      "E.t/De1t\u0000e2t2Ce3t3\u0000\u0001\u0001\u0001: (6.18)\n",
      "We don‚Äôt know the exvalues but, as in Robbins‚Äô formula, we can esti-\n",
      "mate them by the yxvalues, yielding an answer to Corbet‚Äôs question,\n",
      "OE.t/Dy1t\u0000y2t2Cy3t3\u0000\u0001\u0001\u0001: (6.19)\n",
      "Corbet speciÔ¨Åed tD1=2, so4\n",
      "OE.1=2/D118.1=2/\u000074.1=2/2C44.1=2/3\u0000\u0001\u0001\u0001\n",
      "D45:2:(6.20)\n",
      "4This may have been discouraging; there were no new trapping results reported.6.2 The Missing-Species Problem 81\n",
      "Table 6.3 Expectation (6.19) and its standard error (6.21) for the number\n",
      "of new species captured in tadditional fractional units of trapping time.\n",
      "t0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n",
      "E.t/ 0 11.10 20.96 29.79 37.79 45.2 52.1 58.9 65.6 71.6 75.0\n",
      "bsd.t/ 0 2.24 4.48 6.71 8.95 11.2 13.4 15.7 17.9 20.1 22.4\n",
      "Formulas (6.18) and (6.19) do not require the butterÔ¨Çies to arrive inde-\n",
      "pendently. If we are willing to add the assumption that the xk‚Äôs are mutually\n",
      "independent, we can calculate¬é ¬é2\n",
      "bsd.t/D 24X\n",
      "xD1yxt2x!1=2\n",
      "(6.21)\n",
      "as an approximate standard error for OE.t/ . Table 6.3 showsOE.t/ andbsd.t/\n",
      "fortD0;0:1;0:2;:::;1 ; in particular,\n",
      "OE.0:5/D45:2Àô11:2: (6.22)\n",
      "Formula (6.19) becomes unstable for t > 1 . This is our price for sub-\n",
      "stituting the nonparametric estimates yxforexin (6.18). Fisher actually\n",
      "answered Corbet using a parametric empirical Bayes model in which the\n",
      "priorg.\u0012/ for the Poisson parameters \u0012k(6.12) was assumed to be of the\n",
      "gamma form (6.8). It can be shown¬éthat thenE.t/ (6.15) is given by ¬é3\n",
      "t/\u0000\u0017gƒ±1f1\u0000.1C\n",
      "\u0017/; (6.23)\n",
      "D\u001b=.1C\u001b/. TakingOe1Dy1, maximum likelihood estimation\n",
      "gave\n",
      "O\u0017D0:104 andO\u001bD89:79: (6.24)\n",
      "Figure 6.2 shows that the parametric estimate of E.t/ (6.23) usingOe1,\n",
      "O\u0017, andO\u001bis just slightly greater than the nonparametric estimate (6.19) over\n",
      "the range0\u0014t\u00141. Fisher‚Äôs parametric estimate, however, gives reason-\n",
      "able results for t >1 ,OE.2/D123for instance, for a future trapping period\n",
      "of 2 units (4 years). ‚ÄúReasonable‚Äù does not necessarily mean dependable.\n",
      "The gamma prior is a mathematical convenience, not a fact of nature; pro-\n",
      "jections into the far future fall into the category of educated guessing.\n",
      "The missing-species problem encompasses more than butterÔ¨Çies. There\n",
      "are 884,647 words in total in the recognized Shakespearean canon, of which\n",
      "14,376 are so rare they appear just once each, 4343 appear twice each, etc.,82 Empirical Bayes\n",
      "0.0 0.2 0.4 0.6 0.8 1.00 20 40 60 80\n",
      "time tE^(t) Gamma model  \n",
      "E^(2) = 123\n",
      "E^(4) = 176\n",
      "E^(8) = 233\n",
      "Figure 6.2 ButterÔ¨Çy data; expected number of new species in t\n",
      "units of additional trapping time. Nonparametric Ô¨Åt (solid) Àô1\n",
      "standard deviation; gamma model (dashed).\n",
      "Table 6.4 Shakespeare‚Äôs word counts; 14,376 distinct words appeared\n",
      "once each in the canon, 4343 distinct words twice each, etc. The canon\n",
      "has 884,647 words in total, counting repeats.\n",
      "1 2 3 4 5 6 7 8 9 10\n",
      "0C 14376 4343 2292 1463 1043 837 638 519 430 364\n",
      "10C 305 259 242 223 187 181 179 130 127 128\n",
      "20C 104 105 99 112 93 74 83 76 72 63\n",
      "30C 73 47 56 59 53 45 34 49 45 52\n",
      "40C 49 41 30 35 37 21 41 30 28 19\n",
      "50C 25 19 28 27 31 19 19 22 23 14\n",
      "60C 30 19 21 18 15 10 15 14 11 16\n",
      "70C 13 12 10 16 18 11 8 15 12 7\n",
      "80C 13 12 11 8 10 11 7 12 9 8\n",
      "90C 4 7 6 7 10 10 15 7 7 5\n",
      "as in Table 6.4, which goes on to the Ô¨Åve words appearing 100 times each.\n",
      "All told, 31,534 distinct words appear (including those that appear more\n",
      "than 100 times each), this being the observed size of Shakespeare‚Äôs vocab-\n",
      "ulary. But what of the words Shakespeare knew but didn‚Äôt use? These are\n",
      "the ‚Äúmissing species‚Äù in Table 6.4.6.2 The Missing-Species Problem 83\n",
      "Suppose another quantity of previously unknown Shakespeare manu-\n",
      "scripts was discovered, comprising 884647\u0001twords (sotD1would rep-\n",
      "resent a new canon just as large as the old one). How many previously\n",
      "unseen distinct words would we expect to discover?\n",
      "Employing formulas (6.19) and (6.21) gives\n",
      "11430Àô178 (6.25)\n",
      "for the expected number of distinct new words if tD1. This is a very con-\n",
      "servative lower bound on how many words Shakespeare knew but didn‚Äôt\n",
      "use. We can imagine trising toward inÔ¨Ånity, revealing ever more unseen\n",
      "vocabulary. Formula (6.19) fails for t > 1 , and Fisher‚Äôs gamma assump-\n",
      "tion is just that, but more elaborate empirical Bayes calculations give a Ô¨Årm\n",
      "lower bound of 35;000Con Shakespeare‚Äôs unseen vocabulary, exceeding\n",
      "the visible portion!\n",
      "Missing mass is an easier version of the missing-species problem, in\n",
      "which we only ask for the proportion of the total sum of \u0012kvalues corre-\n",
      "sponding to the species that went unseen in the original trapping period,\n",
      "MDX\n",
      "unseen\u0012k\u001eX\n",
      "all\u0012k: (6.26)\n",
      "The numerator has expectation\n",
      "X\n",
      "all\u0012ke\u0000\u0012kDSZ1\n",
      "0\u0012e\u0000\u0012g.\u0012/d\u0012De1 (6.27)\n",
      "as in (6.17), while the expectation of the denominator is\n",
      "X\n",
      "all\u0012kDX\n",
      "allEfxsgDE(X\n",
      "allxs)\n",
      "DEfNg; (6.28)\n",
      "whereNis the total number of butterÔ¨Çies trapped. The obvious missing-\n",
      "mass estimate is then\n",
      "OMDy1=N: (6.29)\n",
      "For the Shakespeare data,\n",
      "OMD14376=884647D0:016: (6.30)\n",
      "We have seen most of Shakespeare‚Äôs vocabulary, as weighted by his usage,\n",
      "though not by his vocabulary count.\n",
      "All of this seems to live in the rareÔ¨Åed world of mathematical abstrac-\n",
      "tion, but in fact some previously unknown Shakespearean work might have84 Empirical Bayes\n",
      "been discovered in 1985. A short poem, ‚ÄúShall I die?,‚Äù was found in the\n",
      "archives of the Bodleian Library and, controversially, attributed to Shake-\n",
      "speare by some but not all experts.\n",
      "The poem of 429 words provided a new ‚Äútrapping period‚Äù of length only\n",
      "tD429=884647D4:85\u000110\u00004; (6.31)\n",
      "and a prediction from (6.19) of\n",
      "EftgD6:97 (6.32)\n",
      "new ‚Äúspecies,‚Äù i.e., distinct words not appearing in the canon. In fact there\n",
      "were nine such words in the poem. Similar empirical Bayes predictions\n",
      "for the number of words appearing once each in the canon, twice each,\n",
      "etc., showed reasonable agreement with the poem‚Äôs counts, but not enough\n",
      "to stiÔ¨Çe doubters. ‚ÄúShall I die?‚Äù is currently grouped with other canonical\n",
      "apocrypha by a majority of experts.\n",
      "6.3 A Medical Example\n",
      "The reader may have noticed that our examples so far have not been par-\n",
      "ticularly computer intensive; all of the calculations could have been (and\n",
      "originally were) done by hand.5This section discusses a medical study\n",
      "where the empirical Bayes analysis is more elaborate.\n",
      "Cancer surgery sometimes involves the removal of surrounding lymph\n",
      "nodes as well as the primary target at the site. Figure 6.3 concerns ND844\n",
      "surgeries, each reporting\n",
      "nD# nodes removed and xD# nodes found positive ; (6.33)\n",
      "‚Äúpositive‚Äù meaning malignant. The ratios\n",
      "pkDxk=nk; kD1;2;:::;N; (6.34)\n",
      "are described in the histogram. A large proportion of them, 340=844 or\n",
      "40%, were zero, the remainder spreading unevenly between zero and one.\n",
      "The denominators nkranged from 1 to 69, with a mean of 19 and standard\n",
      "deviation of 11.\n",
      "We suppose that each patient has some true probability of a node being\n",
      "5Not so collecting the data. Corbet‚Äôs work was pre-computer but Shakespeare‚Äôs word\n",
      "counts were done electronically. Twenty-Ô¨Årst-century scientiÔ¨Åc technology excels at the\n",
      "production of the large parallel-structured data sets conducive to empirical Bayes\n",
      "analysis.6.3 A Medical Example 85\n",
      " \n",
      "p = x/nFrequency\n",
      "0.0 0.2 0.4 0.6 0.8 1.00 20 40 60 80 100*340\n",
      "Figure 6.3 Nodes study; ratio pDx=n for 844 patients; nD\n",
      "number of nodes removed, xDnumber positive.\n",
      "positive, say probability \u0012kfor patientk, and that his or her nodal results\n",
      "occur independently of each other, making xkbinomial,\n",
      "xk\u0018Bi.nk;\u0012k/: (6.35)\n",
      "This givespkDxk=nkwith mean and variance\n",
      "pk\u0018.\u0012k;\u0012k.1\u0000\u0012k/=nk/; (6.36)\n",
      "so that\u0012kis estimated more accurately when nkis large.\n",
      "A Bayesian analysis would begin with the assumption of a prior density\n",
      "g.\u0012/ for the\u0012kvalues,\n",
      "\u0012k\u0018g.\u0012/; forkD1;2;:::;ND844: (6.37)\n",
      "We don‚Äôt know g.\u0012/ , but the parallel nature of the nodes data set‚Äî844\n",
      "similar cases‚Äîsuggests an empirical Bayes approach. As a Ô¨Årst try for the\n",
      "nodes study, we assume that log fg.\u0012/gis a fourth-degree polynomial in \u0012,\n",
      "logfgÀõ.\u0012/gDa0C4X\n",
      "jD1Àõj\u0012jI (6.38)86 Empirical Bayes\n",
      "gÀõ.\u0012/is determined by the parameter vector ÀõD.Àõ1;Àõ2;Àõ3;Àõ4/since,\n",
      "givenÀõ,a0can be calculated from the requirement that\n",
      "Z1\n",
      "0gÀõ.\u0012/d\u0012D1DZ1\n",
      "0exp(\n",
      "a0C4X\n",
      "1Àõj\u0012j)\n",
      "d\u0012: (6.39)\n",
      "For a given choice of Àõ, letfÀõ.xk/be the marginal probability of the\n",
      "observed value xkfor patientk,\n",
      "fÀõ.xk/DZ1\n",
      "0 \n",
      "nk\n",
      "xk!\n",
      "\u0012xk.1\u0000\u0012/nk\u0000xkgÀõ.\u0012/d\u0012: (6.40)\n",
      "The maximum likelihood estimate of Àõis the maximizer\n",
      "OÀõDarg max\n",
      "Àõ(NX\n",
      "kD1logfÀõ.xk/)\n",
      ": (6.41)\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.000.020.040.060.080.100.12\n",
      "Œ∏g^(Œ∏)¬± sd\n",
      "Figure 6.4 Estimated prior density g.\u0012/ for the nodes study;\n",
      "59% of patients have \u0012\u00140:2, 7% have\u0012\u00150:8.\n",
      "Figure 6.4 graphs gOÀõ.\u0012/, the empirical Bayes estimate for the prior dis-\n",
      "tribution of the \u0012kvalues. The huge spike at zero in Figure 6.3 is now\n",
      "reduced: Prf\u0012k\u00140:01gD0:12 compared with the 38% of the pkvalues6.3 A Medical Example 87\n",
      "less than 0.01. Small \u0012values are still the rule though, for instance\n",
      "Z0:20\n",
      "0gOÀõ.\u0012/d\u0012D0:59 compared withZ1:00\n",
      "0:80gOÀõ.\u0012/d\u0012D0:07: (6.42)\n",
      "The vertical bars in Figure 6.4 indicate Àôone standard error for the es-\n",
      "timation ofg.\u0012/ . The curve seems to have been estimated very accurately,\n",
      "at least if we assume the adequacy of model (6.37). Chapter 21 describes\n",
      "the computations involved in Figure 6.4.\n",
      "The posterior distribution of \u0012kgivenxkandnkis estimated according\n",
      "to Bayes‚Äô rule (3.5) to be\n",
      "Og.\u0012jxk;nk/DgOÀõ.\u0012/ \n",
      "nk\n",
      "xk!\n",
      "\u0012xk.1\u0000\u0012/nk\u0000xk\u001e\n",
      "fOÀõ.xk/; (6.43)\n",
      "withfOÀõ.xk/from (6.40).\n",
      "0.0 0.2 0.4 0.6 0.8 1.00 2 4 6\n",
      "Œ∏g(Œ∏ | x, n)x=7 n=32x=17 n=18\n",
      "x=3 n=6\n",
      "0.5\n",
      "Figure 6.5 Empirical Bayes posterior densities of \u0012for three\n",
      "patients, given xDnumber of positive nodes, nDnumber of\n",
      "nodes.\n",
      "Figure 6.5 graphsOg.\u0012jxk;nk/for three choices of .xk;nk/:.7;32/ ,.3;6/ ,\n",
      "and.17;18/ . If we take\u0012\u00150:50 as indicating poor prognosis (and sug-\n",
      "gesting more aggressive follow-up therapy), then the Ô¨Årst patient is almost\n",
      "surely on safe ground, the third patient almost surely needs more follow-up\n",
      "therapy and the situation of the second is uncertain.88 Empirical Bayes\n",
      "6.4 Indirect Evidence 1\n",
      "A good deÔ¨Ånition of a statistical argument is one in which many small\n",
      "pieces of evidence, often contradictory, are combined to produce an overall\n",
      "conclusion. In the clinical trial of a new drug, for instance, we don‚Äôt expect\n",
      "the drug to cure every patient, or the placebo to always fail, but eventually\n",
      "perhaps we will obtain convincing evidence of the new drug‚Äôs efÔ¨Åcacy.\n",
      "The clinical trial is collecting direct statistical evidence, in which each\n",
      "subject‚Äôs success or failure bears directly upon the question of interest. Di-\n",
      "rect evidence, interpreted by frequentist methods, was the dominant mode\n",
      "of statistical application in the twentieth century, being strongly connected\n",
      "to the idea of scientiÔ¨Åc objectivity.\n",
      "Bayesian inference provides a theoretical basis for incorporating indi-\n",
      "rectevidence, for example the doctor‚Äôs prior experience with twin sexes in\n",
      "Section 3.1. The assertion of a prior density g.\u0012/ amounts to a claim for\n",
      "the relevance of past data to the case at hand.\n",
      "Empirical Bayes removes the Bayes scaffolding. In place of a reassuring\n",
      "priorg.\u0012/ , the statistician must put his or her faith in the relevance of the\n",
      "‚Äúother‚Äù cases in a large data set to the case of direct interest. For the second\n",
      "patient in Figure 6.5, the direct estimate of his \u0012value isO\u0012D3=6D0:50.\n",
      "The empirical Bayes estimate is a little less,\n",
      "O\u0012EBDZ1\n",
      "0\u0012Og.\u0012jxkD3;nkD6/D0:446: (6.44)\n",
      "A small difference, but we will see bigger ones in succeeding chapters.\n",
      "The changes in twenty-Ô¨Årst-century statistics have largely been demand\n",
      "driven, responding to the massive data sets enabled by modern scientiÔ¨Åc\n",
      "equipment. Philosophically, as opposed to methodologically, the biggest\n",
      "change has been the increased acceptance of indirect evidence, especially\n",
      "as seen in empirical Bayes and objective (‚Äúuninformative‚Äù) Bayes appli-\n",
      "cations. False-discovery rates , Chapter 15, provide a particularly striking\n",
      "shift from direct to indirect evidence in hypothesis testing. Indirect evi-\n",
      "dence in estimation is the subject of our next chapter.\n",
      "6.5 Notes and Details\n",
      "Robbins (1956) introduced the term ‚Äúempirical Bayes‚Äù as well as rule (6.7)\n",
      "as part of a general theory of empirical Bayes estimation. 1956 was also the\n",
      "publication year for Good and Toulmin‚Äôs solution (6.19) to the missing-\n",
      "species problem. Good went out of his way to credit his famous Bletchley6.5 Notes and Details 89\n",
      "colleague Alan Turing for some of the ideas. The auto accident data is taken\n",
      "from Table 3.1 of Carlin and Louis (1996), who provide a more complete\n",
      "discussion. Empirical Bayes estimates such as 11430 in (6.25) do not de-\n",
      "pend on independence among the ‚Äúspecies,‚Äù but accuracies such as Àô178\n",
      "do; and similarly for the error bars in Figures 6.2 and 6.4.\n",
      "Corbet‚Äôs enormous efforts illustrate the difÔ¨Åculties of amassing large\n",
      "data sets in pre-computer times. Dependable data is still hard to come by,\n",
      "but these days it is often the statistician‚Äôs job to pry it out of enormous\n",
      "databases. Efron and Thisted (1976) apply formula (6.19) to the Shake-\n",
      "speare word counts, and then use linear programming methods to bound\n",
      "Shakespeare‚Äôs unseen vocabulary from below at 35,000 words. (Shake-\n",
      "speare was actually less ‚Äúwordy‚Äù than his contemporaries, Marlow and\n",
      "Donne.) ‚ÄúShall I die,‚Äù the possibly Shakespearean poem recovered in 1985,\n",
      "is analyzed by a variety of empirical Bayes techniques in Thisted and Efron\n",
      "(1987). Comparisons are made with other Elizabethan authors, none of\n",
      "whom seem likely candidates for authorship.\n",
      "The Shakespeare word counts are from Spevack‚Äôs (1968) concordance.\n",
      "(The Ô¨Årst concordance was compiled by hand in the mid 1800s, listing\n",
      "every word Shakespeare wrote and where it appeared, a full life‚Äôs labor.)\n",
      "The nodes example, Figure 6.3, is taken from Gholami et al. (2015).\n",
      "¬é1[p. 78] Formula (6.9) .For any positive numbers canddwe have\n",
      "Z1\n",
      "0\u0012c\u00001e\u0000\u0012=dd\u0012Ddc¬Ä.c/; (6.45)\n",
      "so combining gamma prior (6.8) with Poisson density (6.1) gives marginal\n",
      "density\n",
      "f\u0017;\u001b.x/DR1\n",
      "d\u0012\u0017Cx\u00001e\u0000\u0012=\n",
      "\u001b\u0017¬Ä.\u0017/x≈†\n",
      "\u0017Cx¬Ä.\u0017Cx/\n",
      "\u001b\u0017¬Ä.\u0017/x≈†;(6.46)\n",
      "D\u001b=.1C\u001b/. Assuming independence among the counts yx\n",
      "(which is exactly true if the customers act independently of each other and\n",
      "N, the total number of them, is itself Poisson), the log likelihood function\n",
      "for the accident data is\n",
      "xmaxX\n",
      "xD0yxlogff\u0017;\u001b.x/g: (6.47)\n",
      "Herexmaxis some notional upper bound on the maximum possible number90 Empirical Bayes\n",
      "of accidents for a single customer; since yxD0forx > 7 the choice of\n",
      "xmaxis irrelevant. The values .O\u0017;O\u001b/in (6.8) maximize (6.47).\n",
      "¬é2[p. 81] Formula (6.21) .IfNDPyx, the total number trapped, is assumed\n",
      "to be Poisson, and if the Nobserved values xkare mutually independent,\n",
      "then a useful property of the Poisson distribution implies that the counts yx\n",
      "are themselves approximately independent Poisson variates\n",
      "yxind\u0018Poi.ex/; forxD0;1;2;:::; (6.48)\n",
      "in notation (6.17). Formula (6.19) and var fyxgDexthen give\n",
      "varn\n",
      "OE.t/o\n",
      "DX\n",
      "x\u00151ext2x: (6.49)\n",
      "Substitutingyxforexproduces (6.21). Section 11.5 of Efron (2010) shows\n",
      "that (6.49) is an upper bound on var fOE.t/gifNis considered Ô¨Åxed rather\n",
      "than Poisson.\n",
      "¬é3[p. 81] Formula (6.23) .Combining the case xD1in (6.17) with (6.15)\n",
      "yields\n",
      "E.t/De1\u0002R1\n",
      "0e\u0000\u0012g.\u0012/d\u0012\u0000R1\n",
      "0e\u0000\u0012.1Ct/g.\u0012/d\u0012\u0003\n",
      "R1\n",
      "0\u0012e\u0000\u0012g.\u0012/d\u0012: (6.50)\n",
      "Substituting the gamma prior (6.8) for g.\u0012/ , and using (6.45) three times,\n",
      "gives formula (6.23).7\n",
      "James‚ÄìStein Estimation and Ridge\n",
      "Regression\n",
      "If Fisher had lived in the era of ‚Äúapps,‚Äù maximum likelihood estimation\n",
      "might have made him a billionaire. Arguably the twentieth century‚Äôs most\n",
      "inÔ¨Çuential piece of applied mathematics, maximum likelihood continues to\n",
      "be a prime method of choice in the statistician‚Äôs toolkit. Roughly speaking,\n",
      "maximum likelihood provides nearly unbiased estimates of nearly mini-\n",
      "mum variance, and does so in an automatic way.\n",
      "That being said, maximum likelihood estimation has shown itself to be\n",
      "an inadequate and dangerous tool in many twenty-Ô¨Årst-century applica-\n",
      "tions. Again speaking roughly, unbiasedness can be an unaffordable luxury\n",
      "when there are hundreds or thousands of parameters to estimate at the same\n",
      "time.\n",
      "The James‚ÄìStein estimator made this point dramatically in 1961, and\n",
      "made it in the context of just a few unknown parameters, not hundreds or\n",
      "thousands. It begins the story of shrinkage estimation , in which deliberate\n",
      "biases are introduced to improve overall performance, at a possible danger\n",
      "to individual estimates. Chapters 7 and 21 will carry on the story in its\n",
      "modern implementations.\n",
      "7.1 The James‚ÄìStein Estimator\n",
      "Suppose we wish to estimate a single parameter \u0016from observation xin\n",
      "the Bayesian situation\n",
      "\u0016\u0018N.M;A/ andxj\u0016\u0018N.\u0016;1/; (7.1)\n",
      "in which case \u0016has posterior distribution\n",
      "\u0016jx\u0018N.MCB.x\u0000M/;B/ ≈íBDA=.AC1/¬ç (7.2)\n",
      "as given in (5.21) (where we take \u001b2D1for convenience). The Bayes\n",
      "estimator of\u0016,\n",
      "O\u0016BayesDMCB.x\u0000M/; (7.3)\n",
      "9192 James‚ÄìStein Estimation and Ridge Regression\n",
      "has expected squared error\n",
      "En\u0000\n",
      "O\u0016Bayes\u0000\u0016\u00012o\n",
      "DB; (7.4)\n",
      "compared with 1 for the MLE O\u0016MLEDx,\n",
      "En\u0000\n",
      "O\u0016MLE\u0000\u0016\u00012o\n",
      "D1: (7.5)\n",
      "If, say,AD1in (7.1) then BD1=2andO\u0016Bayeshas only half the risk of\n",
      "the MLE.\n",
      "The same calculation applies to a situation where we have Nindepen-\n",
      "dent versions of (7.1), say\n",
      "\u0016D.\u00161;\u00162;:::;\u0016N/0andxD.x1;x2;:::;xN/0; (7.6)\n",
      "with\n",
      "\u0016i\u0018N.M;A/ andxij\u0016i\u0018N.\u0016i;1/; (7.7)\n",
      "independently for iD1;2;:::;N . (Notice that the \u0016idiffer from each\n",
      "other, and that this situation is not the same as (5.22)‚Äì(5.23).) Let O\u0016Bayes\n",
      "indicate the vector of individual Bayes estimates O\u0016Bayes\n",
      "iDMCB.xi\u0000M/,\n",
      "O\u0016BayesDMCB.x\u0000M/;\u0002\n",
      "MD.M;M;:::;M/0\u0003\n",
      "; (7.8)\n",
      "and similarly\n",
      "O\u0016MLEDx:\n",
      "Using (7.4) the total squared error risk of O\u0016Bayesis\n",
      "2oBayes\u0000\u0016\n",
      "DE(NX\n",
      "iD1\u0010\n",
      "O\u0016Bayes\n",
      "i\u0000\u0016i\u00112)\n",
      "DN\u0001B (7.9)\n",
      "compared with\n",
      "2oMLE\u0000\u0016\n",
      "DN: (7.10)\n",
      "Again,O\u0016Bayeshas onlyBtimes the risk ofO\u0016MLE.\n",
      "This is Ô¨Åne if we know MandA(or equivalently MandB) in (7.1). If\n",
      "not, we might try to estimate them from xD.x1;x2;:::;xN/. Marginally,\n",
      "(7.7) gives\n",
      "xiind\u0018N.M;AC1/: (7.11)\n",
      "ThenOMDNxis an unbiased estimate of M. Moreover,\n",
      "OBD1\u0000.N\u00003/=S\"\n",
      "SDNX\n",
      "iD1.xi\u0000Nx/2#\n",
      "(7.12)7.1 The James‚ÄìStein Estimator 93\n",
      "unbiasedly estimates B, as long asN >3 .¬éThe James‚ÄìStein estimator is ¬é1\n",
      "the plug-in version of (7.3),\n",
      "O\u0016JS\n",
      "iDOMCOB\u0010\n",
      "xi\u0000OM\u0011\n",
      "foriD1;2;:::;N; (7.13)\n",
      "or equivalentlyO\u0016JSDOMCOB.x\u0000OM/, withOMD.OM;OM;:::;OM/0.\n",
      "At this point the terminology ‚Äúempirical Bayes‚Äù seems especially apt:\n",
      "Bayesian model (7.7) leads to the Bayes estimator (7.8), which itself is\n",
      "estimated empirically (i.e., frequentistically) from all the data x, and then\n",
      "applied to the individual cases. Of course O\u0016JScannot perform as well as\n",
      "the actual Bayes‚Äô rule O\u0016Bayes, but the increased risk is surprisingly modest.\n",
      "The expected squared risk of O\u0016JSunder model (7.7) is¬é ¬é2\n",
      "2oJS\u0000\u0016\n",
      "DNBC3.1\u0000B/: (7.14)\n",
      "If, say,ND20andAD1, then (7.14) equals 11.5, compared with true\n",
      "Bayes risk 10 from (7.9), much less than risk 20 for O\u0016MLE.\n",
      "A defender of maximum likelihood might respond that none of this is\n",
      "surprising: Bayesian model (7.7) speciÔ¨Åes the parameters \u0016ito be clustered\n",
      "more or less closely around a central point M, whileO\u0016MLEmakes no such\n",
      "assumption, and cannot be expected to perform as well. Wrong! Removing\n",
      "the Bayesian assumptions does not rescue O\u0016MLE, as James and Stein proved\n",
      "in 1961:\n",
      "James‚ÄìStein Theorem Suppose that\n",
      "xij\u0016i\u0018N.\u0016i;1/ (7.15)\n",
      "independently for iD1;2;:::;N , withN\u00154. Then\n",
      "2oJS\u0000\u0016\n",
      "2oMLE\u0000\u0016\n",
      "(7.16)\n",
      "for all choices of \u00162RN.(The expectations in (7.16) are with \u0016Ô¨Åxed\n",
      "andxvarying according to (7.15).)\n",
      "In the language of decision theory, equation (7.16) says that O\u0016MLEis\n",
      "inadmissible :¬éits total squared error risk exceeds that of O\u0016JSno matter¬é3\n",
      "what\u0016may be. This is a strong frequentist form of defeat for O\u0016MLE, not\n",
      "depending on Bayesian assumptions.\n",
      "The James‚ÄìStein theorem came as a rude shock to the statistical world\n",
      "of 1961. First of all, the defeat came on MLE‚Äôs home Ô¨Åeld: normal observa-\n",
      "tions with squared error loss. Fisher‚Äôs ‚Äúlogic of inductive inference,‚Äù Chap-\n",
      "ter 4, claimed thatO\u0016MLEDxwas the obviously correct estimator in the uni-\n",
      "variate case, an assumption tacitly carried forward to multiparameter linear94 James‚ÄìStein Estimation and Ridge Regression\n",
      "regression problems, where versions of O\u0016MLEwere predominant. There are\n",
      "still some good reasons for sticking with O\u0016MLEin low-dimensional prob-\n",
      "lems, as discussed in Section 7.4. But shrinkage estimation, as exempliÔ¨Åed\n",
      "by the James‚ÄìStein rule, has become a necessity in the high-dimensional\n",
      "situations of modern practice.\n",
      "7.2 The Baseball Players\n",
      "The James‚ÄìStein theorem doesn‚Äôt say by how much O\u0016JSbeatsO\u0016MLE. If the\n",
      "improvement were inÔ¨Ånitesimal nobody except theorists would be inter-\n",
      "ested. In favorable situations the gains can in fact be substantial, as sug-\n",
      "gested by (7.14). One such situation appears in Table 7.1. The batting av-\n",
      "erages1of 18 Major League players have been observed over the 1970 sea-\n",
      "son. The column labeled MLE reports the player‚Äôs observed average over\n",
      "his Ô¨Årst 90 at bats; TRUTH is the average over the remainder of the 1970\n",
      "season (370 further at bats on average). We would like to predict TRUTH\n",
      "from the early-season observations.\n",
      "The column labeled JSin Table 7.1 is from a version of the James‚Äì\n",
      "Stein estimator applied to the 18 MLE numbers. We suppose that each\n",
      "player‚Äôs MLE valuepi(his batting average in the Ô¨Årst 90 tries) is a binomial\n",
      "proportion,\n",
      "pi\u0018Bi.90;Pi/=90: (7.17)\n",
      "HerePiis his true average , how he would perform over an inÔ¨Ånite number\n",
      "of tries; TRUTHiis itself a binomial proportion, taken over an average of\n",
      "370 more tries per player.\n",
      "At this point there are two ways to proceed. The simplest uses a normal\n",
      "approximation to (7.17),\n",
      "piP \u0018N.Pi;\u001b2\n",
      "0/; (7.18)\n",
      "where\u001b2\n",
      "0is the binomial variance\n",
      "\u001b2\n",
      "0D Np.1\u0000Np/=90; (7.19)\n",
      "withNpD0:254 the average of the pivalues. Letting xiDpi=\u001b0, applying\n",
      "(7.13), and transforming back to OpJS\n",
      "iD\u001b0O\u0016JS\n",
      "i, gives James‚ÄìStein estimates\n",
      "OpJS\n",
      "iD NpC\u0014\n",
      "1\u0000.N\u00003/\u001b2\n",
      "0P.pi\u0000Np/2\u0015\n",
      ".pi\u0000Np/: (7.20)\n",
      "1Batting averageD# hits=# at bats, that is, the success rate. For example, Player 1 hits\n",
      "successfully 31 times in his Ô¨Årst 90 tries, for batting average 31=90D0:345 . This data\n",
      "is based on 1970 Major League performances, but is partly artiÔ¨Åcial; see the endnotes.7.2 The Baseball Players 95\n",
      "Table 7.1 Eighteen baseball players; MLE is batting average in Ô¨Årst 90 at\n",
      "bats; TRUTH is average in remainder of 1970 season; James‚ÄìStein\n",
      "estimator JSis based on arcsin transformation of MLEs. Sum of squared\n",
      "errors for predicting TRUTH :MLE .0425, JS.0218.\n",
      "Player MLE JS TRUTH x\n",
      "1 .345 .283 .298 11.96\n",
      "2 .333 .279 .346 11.74\n",
      "3 .322 .276 .222 11.51\n",
      "4 .311 .272 .276 11.29\n",
      "5 .289 .265 .263 10.83\n",
      "6 .289 .264 .273 10.83\n",
      "7 .278 .261 .303 10.60\n",
      "8 .255 .253 .270 10.13\n",
      "9 .244 .249 .230 9.88\n",
      "10 .233 .245 .264 9.64\n",
      "11 .233 .245 .264 9.64\n",
      "12 .222 .242 .210 9.40\n",
      "13 .222 .241 .256 9.39\n",
      "14 .222 .241 .269 9.39\n",
      "15 .211 .238 .316 9.14\n",
      "16 .211 .238 .226 9.14\n",
      "17 .200 .234 .285 8.88\n",
      "18 .145 .212 .200 7.50\n",
      "A second approach begins with the arcsin transformation\n",
      "xiD2.nC0:5/1=2sin\u00001\"\u0012npiC0:375\n",
      "nC0:75\u00131=2#\n",
      "; (7.21)\n",
      "nD90(column labeled xin Table 7.1), a classical device that produces\n",
      "approximate normal deviates of variance 1,\n",
      "xiP \u0018N.\u0016i;1/; (7.22)\n",
      "where\u0016iis transformation (7.21) applied to TRUTHi. Using (7.13) gives\n",
      "O\u0016JS\n",
      "i, which is Ô¨Ånally inverted back to the binomial scale,\n",
      "OpJS\n",
      "iD1\n",
      "nh\n",
      ".nC0:75/\u0010\n",
      "sin\u0010O\u0016JS\n",
      "i\n",
      "2p\n",
      "nC0:5\u0011\u00112\n",
      "\u00000:375i\n",
      "(7.23)\n",
      "Formulas (7.20) and (7.23) yielded nearly the same estimates for the\n",
      "baseball players; the JScolumn in Table 7.1 is from (7.23). James and\n",
      "Stein‚Äôs theorem requires normality, but the James‚ÄìStein estimator often96 James‚ÄìStein Estimation and Ridge Regression\n",
      "works perfectly well in less ideal situations. That is the case in Table 7.1:\n",
      "18X\n",
      "iD1.MLEi\u0000TRUTHi/2D0:0425 while18X\n",
      "iD1.JSi\u0000TRUTHi/2D0:0218:\n",
      "(7.24)\n",
      "In other words, the James‚ÄìStein estimator reduced total predictive squared\n",
      "error by about 50%.\n",
      "0.15 0.20 0.25 0.30 0.35 \n",
      "‚óè‚óè‚óè‚óè ‚óè‚óè‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè MLE\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè JAMES‚àíSTEIN\n",
      "‚óè ‚óè ‚óè ‚óè‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè TRUE\n",
      "Batting averages\n",
      "Figure 7.1 Eighteen baseball players; top line MLE, middle\n",
      "James‚ÄìStein, bottom true values. Only 13 points are visible, since\n",
      "there are ties.\n",
      "The James‚ÄìStein rule describes a shrinkage estimator , each MLE value\n",
      "xibeing shrunk by factor OBtoward the grand mean OMDNx(7.13). (OBD\n",
      "0:34 in (7.20).) Figure 7.1 illustrates the shrinking process for the baseball\n",
      "players.\n",
      "To see why shrinking might make sense, let us return to the original\n",
      "Bayes model (7.8) and take MD0for simplicity, so that the xiare\n",
      "marginally N.0;AC1/(7.11). Even though each xiis unbiased for its\n",
      "parameter\u0016i, as a group they are ‚Äúoverdispersed,‚Äù\n",
      "E(NX\n",
      "iD1x2\n",
      "i)\n",
      "DN.AC1/compared with E(NX\n",
      "iD1\u00162\n",
      "i)\n",
      "DNA: (7.25)\n",
      "The sum of squares of the MLEs exceeds that of the true values by expected\n",
      "amountN; shrinkage improves group estimation by removing the excess.7.3 Ridge Regression 97\n",
      "In fact the James‚ÄìStein rule overshrinks the data, as seen in the bottom\n",
      "two lines of Figure 7.1, a property it inherits from the underlying Bayes\n",
      "model: the Bayes estimates O\u0016Bayes\n",
      "iDBxihave\n",
      "E(NX\n",
      "iD1\u0010\n",
      "O\u0016Bayes\n",
      "i\u00112)\n",
      "DNB2.AC1/DNAA\n",
      "AC1; (7.26)\n",
      "overshrinking E.P\u00162\n",
      "i/DNAby factorA=.AC1/. We could use the\n",
      "less extreme shrinking rule Q\u0016iDp\n",
      "Bxi, which gives the correct expected\n",
      "sum of squares NA, but a larger expected sum of squared estimation errors\n",
      "EfP.Q\u0016i\u0000\u0016i/2jxg.\n",
      "The most extreme shrinkage rule would be ‚Äúall the way,‚Äù that is, to\n",
      "O\u0016NULL\n",
      "iDNx foriD1;2;:::;N; (7.27)\n",
      "NULL indicating that in a classical sense we have accepted the null hy-\n",
      "pothesis of no differences among the \u0016ivalues. (This gaveP.Pi\u0000Np/2D\n",
      "0:0266 for the baseball data (7.24).) The James‚ÄìStein estimator is a data-\n",
      "based rule for compromising between the null hypothesis of no differences\n",
      "and the MLE‚Äôs tacit assumption of no relationship at all among the \u0016i\n",
      "values. In this sense it blurs the classical distinction between hypothesis\n",
      "testing and estimation.\n",
      "7.3 Ridge Regression\n",
      "Linear regression, perhaps the most widely used estimation technique, is\n",
      "based on a version of O\u0016MLE. In the usual notation, we observe an n-dimen-\n",
      "sional vectoryD.y1;y2;:::;yn/0from the linear model\n",
      "yDXÀáC\u000f: (7.28)\n",
      "HereXis a knownn\u0002pstructure matrix ,Àáis an unknown p-dimensional\n",
      "parameter vector, while the noise vector\u000fD.\u000f1;\u000f2;:::;\u000fn/0has its com-\n",
      "ponents uncorrelated and with constant variance \u001b2,\n",
      "\u000f\u0018.0;\u001b2I/; (7.29)\n",
      "whereIis then\u0002nidentity matrix. Often \u000fis assumed to be multivariate\n",
      "normal,\n",
      "\u000f\u0018Nn.0;\u001b2I/; (7.30)\n",
      "but that is not required for most of what follows.98 James‚ÄìStein Estimation and Ridge Regression\n",
      "Theleast squares estimate OÀá, going back to Gauss and Legendre in the\n",
      "early 1800s, is the minimizer of the total sum of squared errors,\n",
      "OÀáDarg min\n",
      "ÀáÀö\n",
      "ky\u0000XÀák2\t\n",
      ": (7.31)\n",
      "It is given by\n",
      "OÀáDS\u00001X0y; (7.32)\n",
      "whereSis thep\u0002pinner product matrix\n",
      "SDX0XI (7.33)\n",
      "OÀáis unbiased for Àáand has covariance matrix \u001b2S\u00001,\n",
      "OÀá\u0018\u0000\n",
      "Àá;\u001b2S\u00001\u0001\n",
      ": (7.34)\n",
      "In the normal case (7.30) OÀáis the MLE of Àá. Before 1950 a great deal\n",
      "of effort went into designing matrices Xsuch thatS\u00001could be feasibly\n",
      "calculated, which is now no longer a concern.\n",
      "A great advantage of the linear model is that it reduces the number of\n",
      "unknown parameters to p(orpC1including\u001b2), no matter how large n\n",
      "may be. In the kidney data example of Section 1.1, nD157whilepD2.\n",
      "In modern applications, however, phas grown larger and larger, sometimes\n",
      "into the thousands or more, as we will see in Part III, causing statisticians\n",
      "again to confront the limitations of high-dimensional unbiased estimation.\n",
      "Ridge regression is a shrinkage method designed to improve the estima-\n",
      "tion ofÀáin linear models. By transformations¬éwe can standardize (7.28) ¬é4\n",
      "so that the columns of Xeach have mean 0 and sum of squares 1, that is,\n",
      "SiiD1 foriD1;2;:::;p: (7.35)\n",
      "(This puts the regression coefÔ¨Åcients Àá1;Àá2;:::;Àápon comparable scales.)\n",
      "For convenience, we also assume NyD0. A ridge regression estimate OÀá.\u0015/\n",
      "is deÔ¨Åned, for \u0015\u00150, to be\n",
      "OÀá.\u0015/D.SC\u0015I/\u00001X0yD.SC\u0015I/\u00001SOÀá (7.36)\n",
      "(using (7.32));OÀá.\u0015/ is a shrunken version of OÀá, the bigger\u0015the more\n",
      "extreme the shrinkage: OÀá.0/DOÀáwhileOÀá.1/equals the vector of zeros.\n",
      "Ridge regression effects can be quite dramatic. As an example, con-\n",
      "sider the diabetes data, partially shown in Table 7.2, in which 10 prediction\n",
      "variables measured at baseline‚Äî age,sex,bmi (body mass index), map\n",
      "(mean arterial blood pressure), and six blood serum measurements‚Äîhave7.3 Ridge Regression 99\n",
      "Table 7.2 First 7 ofnD442patients in the diabetes study; we wish to\n",
      "predict disease progression at one year ‚Äúprog‚Äù from the 10 baseline\n",
      "measurements age,sex, . . . ,glu.\n",
      "age sex bmi map tc ldl hdl tch ltg glu prog\n",
      "59 1 32.1 101 157 93.2 38 4 2.11 87 151\n",
      "48 0 21.6 87 183 103.2 70 3 1.69 69 75\n",
      "72 1 30.5 93 156 93.6 41 4 2.03 85 141\n",
      "24 0 25.3 84 198 131.4 40 5 2.12 89 206\n",
      "50 0 23.0 101 192 125.4 52 4 1.86 80 135\n",
      "23 0 22.6 89 139 64.8 61 2 1.82 68 97\n",
      "36 1 22.0 90 160 99.6 50 3 1.72 82 138\n",
      ":::::::::::::::::::::::::::::::::\n",
      "been obtained for nD442patients. We wish to use the 10 variables to pre-\n",
      "dictprog , a quantitative assessment of disease progression one year after\n",
      "baseline. In this case Xis the442\u000210matrix of standardized predictor\n",
      "variables, and yisprog with its mean subtracted off.\n",
      "‚àí500 0 500\n",
      "ŒªŒ≤^(Œª)\n",
      "0.00 0.05 0.15 0.20 0.25 0.1‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "age\n",
      "sexbmi\n",
      "map\n",
      "tcldl\n",
      "hdltchltg\n",
      "glu\n",
      "Figure 7.2 Ridge coefÔ¨Åcient trace for the standardized diabetes\n",
      "data.100 James‚ÄìStein Estimation and Ridge Regression\n",
      "Table 7.3 Ordinary least squares estimate OÀá.0/ compared with ridge\n",
      "regression estimate OÀá.0:1/ with\u0015D0:1. The columns sd(0) and sd(0.1)\n",
      "are their estimated standard errors. (Here \u001bwas taken to be 54.1, the\n",
      "usual OLS estimate based on model (7.28).)\n",
      "OÀá.0/OÀá.0:1/ sd(0) sd(0.1)\n",
      "age\u000010.0 1.3 59.7 52.7\n",
      "sex\u0000239.8\u0000207.2 61.2 53.2\n",
      "bmi 519.8 489.7 66.5 56.3\n",
      "map 324.4 301.8 65.3 55.7\n",
      "tc\u0000792.2\u000083.5 416.2 43.6\n",
      "ldl 476.7\u000070.8 338.6 52.4\n",
      "hdl 101.0\u0000188.7 212.3 58.4\n",
      "tch 177.1 115.7 161.3 70.8\n",
      "ltg 751.3 443.8 171.7 58.4\n",
      "glu 67.6 86.7 65.9 56.6\n",
      "Figure 7.2 vertically plots the 10 coordinates of OÀá.\u0015/ as the ridge pa-\n",
      "rameter\u0015increases from 0 to 0.25. Four of the coefÔ¨Åcients change rapidly\n",
      "at Ô¨Årst. Table 7.3 compares OÀá.0/ , that is the usual estimate OÀá, withOÀá.0:1/ .\n",
      "Positive coefÔ¨Åcients predict increased disease progression. Notice that ldl,\n",
      "the ‚Äúbad cholesterol‚Äù measurement, goes from being a strongly positive\n",
      "predictor inOÀáto a mildly negative one in OÀá.0:1/ .\n",
      "There is a Bayesian rationale for ridge regression. Assume that the noise\n",
      "vector\u000fis normal as in (7.30), so that\n",
      "OÀá\u0018Np\u0000\n",
      "Àá;\u001b2S\u00001\u0001\n",
      "(7.37)\n",
      "rather than just (7.34). Then the Bayesian prior\n",
      "Àá\u0018Np\u0012\n",
      "0;\u001b2\n",
      "\u0015I\u0013\n",
      "(7.38)\n",
      "makes\n",
      "En\n",
      "ÀájOÀáo\n",
      "D.SC\u0015I/\u00001SOÀá; (7.39)\n",
      "the same as the ridge regression estimate OÀá.\u0015/ (using (5.23) with MD0,\n",
      "AD.\u001b2=\u0015/I, and‚Ä†D.S=\u001b2/\u00001). Ridge regression amounts to an\n",
      "increased prior belief that Àálies near0.\n",
      "The last two columns of Table 7.3 compare the standard deviations¬éof ¬é5\n",
      "OÀáandOÀá.0:1/ . Ridging has greatly reduced the variability of the estimated7.3 Ridge Regression 101\n",
      "regression coefÔ¨Åcients. This does notguarantee that the corresponding es-\n",
      "timate of\u0016DXÀá,\n",
      "O\u0016.\u0015/DXOÀá.\u0015/; (7.40)\n",
      "will be more accurate than the ordinary least squares estimate O\u0016DXOÀá.\n",
      "We have (deliberately) introduced bias, and the squared bias term coun-\n",
      "teracts some of the advantage of reduced variability. The Cpcalculations\n",
      "of Chapter 12 suggest that the two effects nearly offset each other for the\n",
      "diabetes data. However, if interest centers on the coefÔ¨Åcients of Àá, then\n",
      "ridging can be crucial, as Table 7.3 emphasizes.\n",
      "By current standards, pD10is a small number of predictors. Data sets\n",
      "withpin the thousands, and more, will show up in Part III. In such situa-\n",
      "tions the scientist is often looking for a few interesting predictor variables\n",
      "hidden in a sea of uninteresting ones: the prior belief is that most of the Àái\n",
      "values lie near zero. Biasing the maximum likelihood estimates OÀáitoward\n",
      "zero then becomes a necessity.\n",
      "There is still another way to motivate the ridge regression estimator\n",
      "OÀá.\u0015/ :\n",
      "OÀá.\u0015/Darg min\n",
      "Àáfky\u0000XÀák2C\u0015kÀák2g: (7.41)\n",
      "Differentiating the term in brackets with respect to Àáshows thatOÀá.\u0015/D\n",
      ".SC\u0015I/\u00001X0yas in (7.36). If \u0015D0then (7.41) describes the ordinary\n",
      "least squares algorithm; \u0015 > 0 penalizes choices ofÀáhavingkÀáklarge,\n",
      "biasingOÀá.\u0015/ toward the origin.\n",
      "Various terminologies are used to describe algorithms such as (7.41): pe-\n",
      "nalized least squares ;penalized likelihood ;maximized a-posteriori proba-\n",
      "bility (MAP);¬éand, generically, regularization describes almost any method ¬é6\n",
      "that tamps down statistical variability in high-dimensional estimation or\n",
      "prediction problems.\n",
      "A wide variety of penalty terms are in current use, the most inÔ¨Çuential\n",
      "one involving the ‚Äú `1norm‚ÄùkÀák1DPp\n",
      "1jÀájj,\n",
      "QÀá.\u0015/Darg min\n",
      "Àáfky\u0000XÀák2C\u0015kÀák1g; (7.42)\n",
      "the so-called lasso estimator, Chapter 16. Despite the Bayesian provenance,\n",
      "most regularization research is carried out frequentistically, with various\n",
      "penalty terms investigated for their probabilistic behavior regarding esti-\n",
      "mation, prediction, and variable selection.\n",
      "If we apply the James‚ÄìStein rule to the normal model (7.37), we get a\n",
      "different shrinkage rule¬éforOÀá, sayQÀáJS, ¬é7102 James‚ÄìStein Estimation and Ridge Regression\n",
      "QÀáJSD\"\n",
      "1\u0000.p\u00002/\u001b2\n",
      "OÀá0SOÀá#\n",
      "OÀá: (7.43)\n",
      "LettingQ\u0016JSDXQÀáJSbe the corresponding estimator of \u0016DEfygin\n",
      "(7.28), the James‚ÄìStein Theorem guarantees that\n",
      "2oJS\u0000\u0016\n",
      "<p\u001b2(7.44)\n",
      "no matter what Àáis, as long as p\u00153.2There is no such guarantee for\n",
      "ridge regression, and no foolproof way to choose the ridge parameter \u0015.\n",
      "On the other hand, QÀáJSdoes not stabilize the coordinate standard devia-\n",
      "tions, as in the sd(0.1) column of Table 7.3. The main point here is that at\n",
      "present there is no optimality theory for shrinkage estimation. Fisher pro-\n",
      "vided an elegant theory for optimal unbiased estimation. It remains to be\n",
      "seen whether biased estimation can be neatly codiÔ¨Åed.\n",
      "7.4 Indirect Evidence 2\n",
      "There is a downside to shrinkage estimation, which we can examine by\n",
      "returning to the baseball data of Table 7.1. One thousand simulations were\n",
      "run, each one generating simulated batting averages\n",
      "p\u0003\n",
      "i\u0018Bi.90;TRUTHi/=90 iD1;2;:::;18: (7.45)\n",
      "These gave corresponding James‚ÄìStein (JS) estimates (7.20), with \u001b2\n",
      "0D\n",
      "Np\u0003.1\u0000Np\u0003/=90.\n",
      "Table 7.4 shows the root mean square error for the MLE and JS estimates\n",
      "over 1000 simulations for each of the 18 players,\n",
      "2\n",
      "41\n",
      "10001000X\n",
      "jD1.p\u0003\n",
      "ij\u0000TRUTHi/23\n",
      "51=2\n",
      "and2\n",
      "41\n",
      "10001000X\n",
      "jD1.Op\u0003JS\n",
      "ij\u0000TRUTHi/23\n",
      "51=2\n",
      "(7.46)\n",
      "As foretold by the James‚ÄìStein Theorem, the JS estimates are easy victors\n",
      "in terms of total squared error (summing over all 18 players). However,\n",
      "Op\u0003JS\n",
      "iloses toOp\u0003MLE\n",
      "iDp\u0003\n",
      "ifor 4 of the 18 players, losing badly in the case\n",
      "of player 2.\n",
      "Histograms comparing the 1000 simulations of p\u0003\n",
      "iwith those ofOp\u0003JS\n",
      "i\n",
      "for player 2 appear in Figure 7.3. Strikingly, all 1000 of the Op\u0003JS\n",
      "2jvalues lie\n",
      "2Of course we are assuming \u001b2is known in (7.43); if it is estimated, some of the\n",
      "improvement erodes away.7.4 Indirect Evidence 2 103\n",
      "Table 7.4 Simulation study comparing root mean square errors for MLE\n",
      "and JS estimators (7.20) as estimates of TRUTH . Total mean square errors\n",
      ".0384 ( MLE) and .0235 ( JS). Asterisks indicate four players for whom\n",
      "rmsJS exceeded rmsMLE ; these have two largest and two smallest\n",
      "TRUTH values (player 2 is Clemente). Column rmsJS1 is for the limited\n",
      "translation version of JSthat bounds shrinkage to within one standard\n",
      "deviation of the MLE.\n",
      "Player TRUTH rmsMLE rmsJS rmsJS1\n",
      "1 .298 .046 .033 .032\n",
      "2 .346* .049 .077 .056\n",
      "3 .222 .044 .042 .038\n",
      "4 .276 .048 .015 .023\n",
      "5 .263 .047 .011 .020\n",
      "6 .273 .046 .014 .021\n",
      "7 .303 .047 .037 .035\n",
      "8 .270 .049 .012 .022\n",
      "9 .230 .044 .034 .033\n",
      "10 .264 .047 .011 .021\n",
      "11 .264 .047 .012 .020\n",
      "12 .210* .043 .053 .044\n",
      "13 .256 .045 .014 .020\n",
      "14 .269 .048 .012 .021\n",
      "15 .316* .048 .049 .043\n",
      "16 .226 .045 .038 .036\n",
      "17 .285 .046 .022 .026\n",
      "18 .200* .043 .062 .048\n",
      "below TRUTH2D0:346 . Player 2 could have had a legitimate complaint if\n",
      "the James‚ÄìStein estimate were used to set his next year‚Äôs salary.\n",
      "The four losing cases for Op\u0003JS\n",
      "iare the players with the two largest and\n",
      "two smallest values of the TRUTH . Shrinkage estimators work against cases\n",
      "that are genuinely outstanding (in a positive or negative sense). Player 2\n",
      "was Roberto Clemente. A better informed Bayesian, that is, a baseball fan,\n",
      "would know that Clemente had led the league in batting over the previ-\n",
      "ous several years, and shouldn‚Äôt be thrown into a shrinkage pool with 17\n",
      "ordinary hitters.\n",
      "Of course the James‚ÄìStein estimates were more accurate for 14 of the\n",
      "18 players. Shrinkage estimation tends to produce better results in general ,\n",
      "at the possible expense of extreme cases. Nobody cares much about Cold\n",
      "War batting averages, but if the context were the efÔ¨Åcacies of 18 new anti-\n",
      "cancer drugs the stakes would be higher.104 James‚ÄìStein Estimation and Ridge Regression\n",
      " \n",
      "p^Frequency\n",
      "0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55050100150200250300350\n",
      "Truth 0.346\n",
      "p^ MLE\n",
      "p^ James‚àíStein\n",
      "Figure 7.3 Comparing MLE estimates (solid) with JS estimates\n",
      "(line) for Clemente; 1000 simulations, 90 at bats each.\n",
      "Compromise methods are available. The rmsJS1 column of Table 7.4\n",
      "refers to a limited translation version ofOpJS\n",
      "iin which shrinkage is not al-\n",
      "lowed to diverge more than one \u001b0unit fromOpi; in formulaic terms,\n",
      "OpJS1\n",
      "iDminÀö\n",
      "max\u0000\n",
      "OpJS\n",
      "i;Opi\u0000\u001b0\u0001\n",
      ";OpiC\u001b0\t\n",
      ": (7.47)\n",
      "This mitigates the Clemente problem while still gaining most of the shrink-\n",
      "age advantages.\n",
      "The use of indirect evidence amounts to learning from the experience\n",
      "of others , each batter learning from the 17 others in the baseball exam-\n",
      "ples. ‚ÄúWhich others?‚Äù is a key question in applying computer-age methods.\n",
      "Chapter 15 returns to the question in the context of false-discovery rates.\n",
      "7.5 Notes and Details\n",
      "The Bayesian motivation emphasized in Chapters 6 and 7 is anachronistic:\n",
      "originally the work emerged mainly from frequentist considerations and\n",
      "was justiÔ¨Åed frequentistically, as in Robbins (1956). Stein (1956) proved\n",
      "the inadmissibility of O\u0016MLE, the neat version of O\u0016JSappearing in James\n",
      "and Stein (1961) (Willard James was Stein‚Äôs graduate student); O\u0016JSis it-\n",
      "self inadmissible, being everywhere improvable by changing OBin (7.13)7.5 Notes and Details 105\n",
      "to max.OB;0/ . This in turn is inadmissible, but further gains tend to the\n",
      "minuscule.\n",
      "In a series of papers in the early 1970s, Efron and Morris emphasized\n",
      "the empirical Bayes motivation of the James‚ÄìStein rule, Efron and Morris\n",
      "(1972) giving the limited translation version (7.47). The baseball data in its\n",
      "original form appears in Table 1.1 of Efron (2010). Here the original 45 at\n",
      "bats recorded for each player have been artiÔ¨Åcially augmented by adding\n",
      "45 binomial draws, Bi .45;TRUTHi/for playeri. This gives a somewhat\n",
      "less optimistic view of the James‚ÄìStein rule‚Äôs performance.\n",
      "‚ÄúStein‚Äôs paradox in statistics,‚Äù Efron and Morris‚Äô title for their 1977 Sci-\n",
      "entiÔ¨Åc American article, catches the statistics world‚Äôs sense of discomfort\n",
      "with the James‚ÄìStein theorem. Why should our estimate for Player A go\n",
      "up or down depending on the other players‚Äô performances? This is the\n",
      "question of direct versus indirect evidence, raised again in the context of\n",
      "hypothesis testing in Chapter 15. Unbiased estimation has great scientiÔ¨Åc\n",
      "appeal, so the argument is by no means settled.\n",
      "Ridge regression was introduced into the statistics literature by Hoerl\n",
      "and Kennard (1970). It appeared previously in the numerical analysis liter-\n",
      "ature as Tikhonov regularization.\n",
      "¬é1[p. 93] Formula (7.12) .IfZhas a chi-squared distribution with \u0017degrees\n",
      "of freedom,Z\u0018\u001f2\n",
      "\u0017(that is,Z\u0018Gam.\u0017=2;2/ in Table 5.1), it has density\n",
      "f.z/Dz\u0017=2\u00001e\u0000z=2\n",
      "2\u0017=2¬Ä.\u0017=2/forz\u00150; (7.48)\n",
      "yielding\n",
      "E\u001a1\n",
      "z\u001b\n",
      "DZ1\n",
      "0z\u0017=2\u00002e\u0000z=2\n",
      "2\u0017=2¬Ä.\u0017=2/dzD2\u0017=2\u00001\n",
      "2\u0017=2¬Ä.\u0017=2\u00001/\n",
      "¬Ä.\u0017=2/D1\n",
      "\u0017\u00002:(7.49)\n",
      "But standard results, starting from (7.11), show that S\u0018.AC1/\u001f2\n",
      "N\u00001.\n",
      "With\u0017DN\u00001in (7.49),\n",
      "E\u001aN\u00003\n",
      "S\u001b\n",
      "D1\n",
      "AC1; (7.50)\n",
      "verifying (7.12).\n",
      "¬é2[p. 93] Formula (7.14) .First consider the simpler situation where Min\n",
      "(7.11) is known to equal zero, in which case the James‚ÄìStein estimator is\n",
      "O\u0016JS\n",
      "iDOBxi withOBD1\u0000.N\u00002/=S; (7.51)\n",
      "whereSDPN\n",
      "1x2\n",
      "i. For convenient notation let\n",
      "OCD1\u0000OBD.N\u00002/=S andCD1\u0000BD1=.AC1/: (7.52)106 James‚ÄìStein Estimation and Ridge Regression\n",
      "The conditional distribution \u0016ijx\u0018N.Bxi;B/gives\n",
      "En\u0000\n",
      "O\u0016JS\n",
      "i\u0000\u0016i\u00012ÀáÀáÀáxo\n",
      "DBC.OC\u0000C/2x2\n",
      "i; (7.53)\n",
      "and, adding over the Ncoordinates,\n",
      "2ÀáÀáÀáxo\n",
      "DNBC.OC\u0000C/2S: (7.54)\n",
      "The marginal distribution S\u0018.AC1/\u001f2\n",
      "Nand (7.49) yields, after a little\n",
      "calculation,\n",
      "En\n",
      ".OC\u0000C/2So\n",
      "D2.1\u0000B/; (7.55)\n",
      "and so\n",
      "2oJS\u0000\u0016\n",
      "DNBC2.1\u0000B/: (7.56)\n",
      "By orthogonal transformations, in situation (7.7), where Mis not as-\n",
      "sumed to be zero, O\u0016JScan be represented as the sum of two parts: a JS\n",
      "estimate inN\u00001dimensions but with MD0as in (7.51), and a MLE\n",
      "estimate of the remaining one coordinate. Using (7.56) this gives\n",
      "2oJS\u0000\u0016\n",
      "D.N\u00001/BC2.1\u0000B/C1\n",
      "DNBC3.1\u0000B/;(7.57)\n",
      "which is (7.14).\n",
      "¬é3[p. 93] The James‚ÄìStein Theorem. Stein (1981) derived a simpler proof of\n",
      "the JS Theorem that appears in Section 1.2 of Efron (2010).\n",
      "¬é4[p. 98] Transformations to form (7.35) .The linear regression model (7.28)\n",
      "isequivariant under scale changes of the variables xj. What this means\n",
      "is that the space of Ô¨Åts using linear combinations of the xjis the same as\n",
      "the space of linear combinations using scaled versions QxjDxj=sj, with\n",
      "sj>0. Furthermore, the least squares Ô¨Åts are the same, and the coefÔ¨Åcient\n",
      "estimates map in the obvious way:OQÀájDsjOÀáj.\n",
      "Not so for ridge regression. Changing the scales of the columns of X\n",
      "will generally lead to different Ô¨Åts. Using the penalty version (7.41) of\n",
      "ridge regression, we see that the penalty term kÀák2DP\n",
      "jÀá2\n",
      "jtreats all the\n",
      "coefÔ¨Åcients as equals. This penalty is most natural if all the variables are\n",
      "measured on the same scale. Hence we typically use for sjthe standard\n",
      "deviation of variable xj, which leads to (7.35). Furthermore, with ridge\n",
      "regression we typically do not penalize the intercept. This can be achieved7.5 Notes and Details 107\n",
      "bycentering and scaling each of the variables, QxjD.xj\u00001Nxj/=sj, where\n",
      "NxjDnX\n",
      "iD1xij=n andsjD\u00141\n",
      "nX\n",
      ".xij\u0000Nxj/2\u00151=2\n",
      "; (7.58)\n",
      "with1then-vector of 1s. We now work with QXD.Qx1;Qx2;:::;Qxp/rather\n",
      "thanX, and the intercept is estimated separately as Ny.\n",
      "¬é5[p. 100] Standard deviations in Table 7.3. From the Ô¨Årst equality in (7.36)\n",
      "we calculate the covariance matrix of OÀá.\u0015/ to be\n",
      "Cov\u0015D\u001b2.SC\u0015I/\u00001S.SC\u0015I/\u00001: (7.59)\n",
      "The entries sd(0.1) in Table 7.3 are square roots of the diagonal elements\n",
      "of Cov\u0015, substituting the ordinary least squares estimate O\u001bD54:1 for\u001b2.\n",
      "¬é6[p. 101] Penalized likelihood and MAP . With\u001b2Ô¨Åxed and known in the\n",
      "normal linear model y\u0018Nn.XÀá;\u001b2I/, minimizingky\u0000XÀák2is the\n",
      "same as maximizing the log density function\n",
      "logfÀá.y/D\u00001\n",
      "2ky\u0000XÀák2Cconstant: (7.60)\n",
      "In this sense, the term \u0015kÀák2in (7.41) penalizes the likelihood log fÀá.y/\n",
      "connected with Àáin proportion to the magnitude kÀák2. Under the prior\n",
      "distribution (7.38), the log posterior density of Àágiveny(the log of (3.5))\n",
      "is\n",
      "\u00001\n",
      "2\u001b2Àö\n",
      "ky\u0000XÀák2C\u0015kÀák2\t\n",
      "; (7.61)\n",
      "plus a term that doesn‚Äôt depend on Àá. That makes the maximizer of (7.41)\n",
      "also the maximizer of the posterior density of Àágiveny, or the MAP.\n",
      "D.S1=2=\u001b/OÀáindOula (7.43) .Let\n",
      "(7.37), whereS1=2is a matrix square root of S,.S1=2/2DS. Then\n",
      ";I/; (7.62)\n",
      "and theMD0form of the James‚ÄìStein rule (7.51) is\n",
      "JSD\u0014\n",
      "1\u0000p\u00002\n",
      "k2\u0015\n",
      ": (7.63)\n",
      "Transforming back to the Àáscale gives (7.43).8\n",
      "Generalized Linear Models and Regression\n",
      "Trees\n",
      "Indirect evidence is not the sole property of Bayesians. Regression models\n",
      "are the frequentist method of choice for incorporating the experience of\n",
      "‚Äúothers.‚Äù As an example, Figure 8.1 returns to the kidney Ô¨Åtness data of\n",
      "Section 1.1. A potential new donor, aged 55, has appeared, and we wish\n",
      "to assess his kidney Ô¨Åtness without subjecting him to an arduous series of\n",
      "medical tests. Only one of the 157 previously tested volunteers was age\n",
      "55, his tot score being\u00000:01 (the upper large dot in Figure 8.1). Most\n",
      "applied statisticians, though, would prefer to read off the height of the least\n",
      "squares regression line at age D55(the green dot on the regression line),\n",
      "dtotD\u00001:46. The former is the only direct evidence we have, while the\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "****\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "*****\n",
      "*\n",
      "*\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "****\n",
      "****\n",
      "***\n",
      "*\n",
      "20 30 40 50 60 70 80 90‚àí6 ‚àí4 ‚àí2 0 2 4\n",
      "Agetot\n",
      "‚óè‚óè\n",
      "55\n",
      "Figure 8.1 Kidney data; a new volunteer donor is aged 55.\n",
      "Which prediction is preferred for his kidney function?\n",
      "1088.1 Logistic Regression 109\n",
      "regression line lets us incorporate indirect evidence for age 55 from all 157\n",
      "previous cases.\n",
      "Increasingly aggressive use of regression techniques is a hallmark of\n",
      "modern statistical practice, ‚Äúaggressive‚Äù applying to the number and type\n",
      "of predictor variables, the coinage of new methodology, and the sheer size\n",
      "of the target data sets. Generalized linear models, this chapter‚Äôs main topic,\n",
      "have been the most pervasively inÔ¨Çuential of the new methods. The chapter\n",
      "ends with a brief review of regression trees, a completely different regres-\n",
      "sion methodology that will play an important role in the prediction algo-\n",
      "rithms of Chapter 17.\n",
      "8.1 Logistic Regression\n",
      "An experimental new anti-cancer drug called Xilathon is under devel-\n",
      "opment. Before human testing can begin, animal studies are needed to de-\n",
      "termine safe dosages. To this end, a bioassay or dose‚Äìresponse experiment\n",
      "was carried out: 11 groups of nD10mice each were injected with in-\n",
      "creasing amounts of Xilathon , dosages coded11;2;:::;11 .\n",
      "Let\n",
      "yiD# mice dying in ith group: (8.1)\n",
      "The points in Figure 8.2 show the proportion of deaths\n",
      "piDyi=10; (8.2)\n",
      "lethality generally increasing with dose. The counts yiare modeled as in-\n",
      "dependent binomials,\n",
      "yiind\u0018Bi.ni;\u0019i/ foriD1;2;:::;N; (8.3)\n",
      "ND11and allniequaling 10 here; \u0019iis the true death rate in group\n",
      "i, estimated unbiasedly by pi, the direct evidence for \u0019i. The regression\n",
      "curve in Figure 8.2 uses allthe doses to give a better picture of the true\n",
      "dose‚Äìresponse relation.\n",
      "Logistic regression is a specialized technique for regression analysis of\n",
      "count or proportion data. The logit parameter\u0015is deÔ¨Åned as\n",
      "\u0015Dlogn\u0019\n",
      "1\u0000\u0019o\n",
      "; (8.4)\n",
      "1Dose would usually be labeled on a log scale, each one, say, 50% larger than its\n",
      "predecessor.110 GLMs and Regression Trees\n",
      "‚óè ‚óè ‚óè‚óè‚óè ‚óè\n",
      "‚óè‚óè ‚óè‚óè ‚óè\n",
      "DoseProportion of deaths\n",
      "12345678910110.00 0.25 0.50 0.75 1.00\n",
      "‚óè‚óèLD50 = 5.69\n",
      "Figure 8.2 Dose‚Äìresponse study; groups of 10 mice exposed to\n",
      "increasing doses of experimental drug. The points are the\n",
      "observed proportions that died in each group. The Ô¨Åtted curve is\n",
      "the maximum-likelihoood estimate of the linear logistic\n",
      "regression model. The open circle on the curve is the LD50, the\n",
      "estimated dose for 50% mortality.\n",
      "with\u0015increasing from\u00001 to1as\u0019increases from 0 to 1. A linear lo-\n",
      "gistic regression dose‚Äìresponse analysis begins with binomial model (8.3),\n",
      "and assumes that the logit is a linear function of dose,\n",
      "\u0015iDlog\u001a\u0019i\n",
      "1\u0000\u0019i\u001b\n",
      "DÀõ0CÀõ1xi: (8.5)\n",
      "Maximum likelihood gives estimates .OÀõ0;OÀõ1/, and Ô¨Åtted curve\n",
      "O\u0015.x/DOÀõ0COÀõ1x: (8.6)\n",
      "Since the inverse transformation of (8.4) is\n",
      "\u0019D\u0010\n",
      "1Ce\u0000\u0015\u0011\u00001\n",
      "(8.7)\n",
      "we obtain from (8.6) the linear logistic regression curve\n",
      "O\u0019.x/D\u0010\n",
      "1Ce\u0000.OÀõ0COÀõ1x/\u0011\u00001\n",
      "(8.8)\n",
      "pictured in Figure 8.2.\n",
      "Table 8.1 compares the standard deviation of the estimated regression8.1 Logistic Regression 111\n",
      "Table 8.1 Standard deviation estimates for O\u0019.x/ in Figure 8.1. The Ô¨Årst\n",
      "row is for the linear logistic regression Ô¨Åt (8.8) ; the second row is based\n",
      "on the individual binomial estimates pi.\n",
      "x 1 2 3 4 5 6 7 8 9 10 11\n",
      "sdO\u0019.x/ .015 .027 .043 .061 .071 .072 .065 .050 .032 .019 .010\n",
      "sdpi .045 .066 .094 .126 .152 .157 .138 .106 .076 .052 .035\n",
      "curve (8.8) at xD1;2;:::;11 (as discussed in the next section) with\n",
      "the usual binomial standard deviation estimate ≈ípi.1\u0000pi/=10¬ç1=2obtained\n",
      "by considering the 11 doses separately.2Regression has reduced error by\n",
      "better than 50%, the price being possible bias if model (8.5) goes seriously\n",
      "wrong.\n",
      "One advantage of the logit transformation is that \u0015isn‚Äôt restricted to the\n",
      "range≈í0;1¬ç , so model (8.5) never verges on forbidden territory. A better\n",
      "reason has to do with the exploitation of exponential family properties. We\n",
      "can rewrite the density function for Bi .n;y/ as\n",
      " \n",
      "n\n",
      "y!\n",
      "\u0019y.1\u0000\u0019/n\u0000yDe\u0015y\u0000n .\u0015/ \n",
      "n\n",
      "y!\n",
      "(8.9)\n",
      "with\u0015the logit parameter (8.4) and\n",
      " .\u0015/Dlogf1Ce\u0015gI (8.10)\n",
      "(8.9) is a one-parameter exponential family3as described in Section 5.5,\n",
      "with\u0015the natural parameter, called Àõthere.\n",
      "LetyD.y1;y2;:::;yN/denote the full data set, ND11in Figure 8.2.\n",
      "Using (8.5), (8.9), and the independence of the yigives the probability\n",
      "density ofyas a function of .Àõ0;Àõ1/,\n",
      "fÀõ0;Àõ1.y/DNY\n",
      "iD1e\u0015iyi\u0000ni .\u0015i/ \n",
      "ni\n",
      "yi!\n",
      "DeÀõ0S0CÀõ1S1\u0001e\u0000PN\n",
      "1ni .Àõ0CÀõ1xi/\u0001NY\n",
      "iD1 \n",
      "ni\n",
      "yi!\n",
      ";(8.11)\n",
      "2For the separate-dose standard error, piwas taken equal to the Ô¨Åtted value from the\n",
      "curve in Figure 8.2.\n",
      "3It is not necessary for f\u00160.x/in (5.46) on page 64 to be a probability density function,\n",
      "only that it not depend on the parameter \u0016.112 GLMs and Regression Trees\n",
      "where\n",
      "S0DNX\n",
      "iD1yiandS1DNX\n",
      "iD1xiyi: (8.12)\n",
      "Formula (8.11) expresses fÀõ0;Àõ1.y/as the product of three factors,\n",
      "fÀõ0;Àõ1.y/DgÀõ0;Àõ1.S0;S1/h.Àõ0;Àõ1/j.y/; (8.13)\n",
      "only the Ô¨Årst of which involves both the parameters and the data. This im-\n",
      "plies that.S0;S1/is asufÔ¨Åcient statistic :¬éno matter how large Nmight be ¬é1\n",
      "(later we will have Nin the thousands), just the two numbers .S0;S1/con-\n",
      "tain all of the experiment‚Äôs information. Only the logistic parameterization\n",
      "(8.4) makes this happen.4\n",
      "A more intuitive picture of logistic regression depends on D.pi;O\u0019i/, the\n",
      "deviance between an observed proportion pi(8.2) and an estimate O\u0019i,\n",
      "D.pi;O\u0019i/D2ni\u0014\n",
      "pilog\u0012pi\n",
      "O\u0019i\u0013\n",
      "C.1\u0000pi/log\u00121\u0000pi\n",
      "1\u0000O\u0019i\u0013\u0015\n",
      ":(8.14)\n",
      "The deviance5is zero ifO\u0019iDpi, otherwise it increases as O\u0019ideparts\n",
      "further frompi.\n",
      "The logistic regression MLE value .OÀõ0;OÀõ1/also turns out to be the\n",
      "choice of.Àõ0;Àõ1/minimizing the total deviance between the Npointspi\n",
      "and their corresponding estimates O\u0019iD\u0019OÀõ0;OÀõ1.xi/(8.8):\n",
      ".OÀõ0;OÀõ1/Darg min\n",
      ".Àõ0;Àõ1/NX\n",
      "iD1D.pi;\u0019Àõ0;Àõ1.xi//: (8.15)\n",
      "The solid line in Figure 8.2 is the linear logistic curve coming closest to\n",
      "the 11 points, when distance is measured by total deviance. In this way the\n",
      "200-year-old notion of least squares is generalized to binomial regression,\n",
      "as discussed in the next section. A more sophisticated notion of distance\n",
      "between data and models is one of the accomplishments of modern statis-\n",
      "tics.\n",
      "Table 8.2 reports on the data for a more structured logistic regression\n",
      "analysis. Human muscle cell colonies were infused with mouse nuclei in\n",
      "Ô¨Åve different ratios, cultured over time periods ranging from one to Ô¨Åve\n",
      "4Where the name ‚Äúlogistic regression‚Äù comes from is explained in the endnotes, along\n",
      "with a description of its nonexponential family predecessor probit analysis .\n",
      "5Deviance is analogous to squared error in ordinary regression theory, as discussed in\n",
      "what follows. It is twice the ‚ÄúKullback‚ÄìLeibler distance,‚Äù the preferred name in the\n",
      "information-theory literature.8.1 Logistic Regression 113\n",
      "Table 8.2 Cell infusion data; human cell colonies infused with mouse\n",
      "nuclei in Ô¨Åve ratios over 1 to 5 days and observed to see whether they did\n",
      "or did not thrive. Green numbers are estimates O\u0019ijfrom the logistic\n",
      "regression model. For example, 5 of 31 colonies in the lowest ratio/days\n",
      "category thrived, with observed proportion 5=31D0:16, and logistic\n",
      "regression estimate O\u001911D0:11:\n",
      "Time\n",
      "1 2 3 4 5\n",
      "15/31 3/28 20/45 24/47 29/35\n",
      ".11 .25 .42 .54 .75\n",
      "215/77 36/78 43/71 56/71 66/74\n",
      ".24 .45 .64 .74 .88\n",
      "Ratio 348/126 68/116 145/171 98/119 114/129\n",
      ".38 .62 .77 .85 .93\n",
      "429/92 35/52 57/85 38/50 72/77\n",
      ".32 .56 .73 .81 .92\n",
      "511/53 20/52 20/48 40/55 52/61\n",
      ".18 .37 .55 .67 .84\n",
      "days, and observed to see whether they thrived. For example, of the 126\n",
      "colonies having the third ratio and shortest time period, 48 thrived.\n",
      "Let\u0019ijdenote the true probability of thriving for ratio iduring time\n",
      "periodj, and\u0015ijits logit logf\u0019ij=.1\u0000\u0019ij/g. A two-way additive logistic\n",
      "regression was Ô¨Åt to the data,6\n",
      "\u0015ijD\u0016CÀõiCÀáj; iD1;2;:::;5; jD1;2;:::;5: (8.16)\n",
      "The green numbers in Table 8.2 show the maximum likelihood estimates\n",
      "O\u0019ijD1.\u0014\n",
      "1Ce\u0000\u0010\n",
      "O\u0016COÀõiCOÀáj\u0011\u0015\n",
      ": (8.17)\n",
      "Model (8.16) has nine free parameters (taking into account the con-\n",
      "straintsPÀõiDPÀájD0necessary to avoid deÔ¨Ånitional difÔ¨Åculties)\n",
      "compared with just two in the dose‚Äìresponse experiment. The count can\n",
      "easily go much higher these days.\n",
      "Table 8.3 reports on a 57-variable logistic regression applied to the spam\n",
      "data. A researcher (named George) labeled ND4601 of his email mes-\n",
      "6Using the statistical computing language R; see the endnotes.114 GLMs and Regression Trees\n",
      "Table 8.3 Logistic regression analysis of the spam data, model (8.17) ;\n",
      "estimated regression coefÔ¨Åcients, standard errors, and zDestimate=se,\n",
      "for 57 keyword predictors. The notation char$ means the relative\n",
      "number of times $appears, etc. The last three entries measure\n",
      "characteristics such as length of capital-letter strings. The word george\n",
      "is special, since the recipient of the email is named George, and the goal\n",
      "here is to build a customized spam Ô¨Ålter.\n",
      "Estimate se z-value Estimate se z-value\n",
      "intercept\u000012.27 1.99\u00006.16 lab\u00001.48 .89\u00001.66\n",
      "make\u0000.12 .07\u00001.68 labs \u0000.15 .14\u00001.05\n",
      "address\u0000.19 .09\u00002.10 telnet \u0000.07 .19\u0000.35\n",
      "all .06 .06 1.03 857 .84 1.08 .78\n",
      "3d 3.14 2.10 1.49 data \u0000.41 .17\u00002.37\n",
      "our .38 .07 5.52 415 .22 .53 .42\n",
      "over .24 .07 3.53 85\u00001.09 .42\u00002.61\n",
      "remove .89 .13 6.85 technology .37 .12 2.99\n",
      "internet .23 .07 3.39 1999 .02 .07 .26\n",
      "order .20 .08 2.58 parts \u0000.13 .09\u00001.41\n",
      "mail .08 .05 1.75 pm \u0000.38 .17\u00002.26\n",
      "receive\u0000.05 .06\u0000.86 direct \u0000.11 .13\u0000.84\n",
      "will\u0000.12 .06\u00001.87 cs\u000016.27 9.61\u00001.69\n",
      "people\u0000.02 .07\u0000.35 meeting\u00002.06 .64\u00003.21\n",
      "report .05 .05 1.06 original \u0000.28 .18\u00001.55\n",
      "addresses .32 .19 1.70 project \u0000.98 .33\u00002.97\n",
      "free .86 .12 7.13 re \u0000.80 .16\u00005.09\n",
      "business .43 .10 4.26 edu\u00001.33 .24\u00005.43\n",
      "email .06 .06 1.03 table \u0000.18 .13\u00001.40\n",
      "you .14 .06 2.32 conference\u00001.15 .46\u00002.49\n",
      "credit .53 .27 1.95 char; \u0000.31 .11\u00002.92\n",
      "your .29 .06 4.62 char( \u0000.05 .07\u0000.75\n",
      "font .21 .17 1.24 char \u0000.07 .09\u0000.78\n",
      "000 .79 .16 4.76 char! .28 .07 3.89\n",
      "money .19 .07 2.63 char$ 1.31 .17 7.55\n",
      "hp\u00003.21 .52\u00006.14 char# 1.03 .48 2.16\n",
      "hpl\u0000.92 .39\u00002.37 cap.ave .38 .60 .64\n",
      "george\u000039.62 7.12\u00005.57 cap.long 1.78 .49 3.62\n",
      "650 .24 .11 2.24 cap.tot .51 .14 3.75\n",
      "sages as either spam orham (nonspam7), say\n",
      "yiD(\n",
      "1if emailiisspam\n",
      "0if emailiisham(8.18)\n",
      "7‚ÄúHam‚Äù refers to ‚Äúnonspam‚Äù or good email; this is a playful connection to the processed8.1 Logistic Regression 115\n",
      "(40% of the messages were spam ). ThepD57predictor variables repre-\n",
      "sent the most frequently used words and tokens in George‚Äôs corpus of email\n",
      "(excluding trivial words such as articles), and are in fact the relative fre-\n",
      "quencies of these chosen words in each email (standardized by the length\n",
      "of the email). The goal of the study was to predict whether future emails\n",
      "arespam orham using these keywords; that is, to build a customized spam\n",
      "Ô¨Ålter .\n",
      "Letxijdenote the relative frequency of keyword jin emaili, and\u0019i\n",
      "represent the probability that email iisspam . Letting\u0015ibe the logit trans-\n",
      "form logf\u0019i=.1\u0000\u0019i/g, we Ô¨Åt the additive logistic model\n",
      "\u0015iDÀõ0C57X\n",
      "jD1Àõjxij: (8.19)\n",
      "Table 8.3 showsOÀõifor each word‚Äîfor example, \u00000:12 formake ‚Äîas well\n",
      "as the estimated standard error and the z-value : estimate=se.\n",
      "It looks like certain words, such as free andyour , are good spam\n",
      "predictors. However, the table as a whole has an unstable appearance, with\n",
      "occasional very large estimates OÀõiaccompanied by very large standard de-\n",
      "viations.8The dangers of high-dimensional maximum likelihood estima-\n",
      "tion are apparent here. Some sort of shrinkage estimation is called for, as\n",
      "discussed in Chapter 16.\n",
      "\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001\n",
      "Regression analysis, either in its classical form or in modern formula-\n",
      "tions, requires covariate information xto put the various cases into some\n",
      "sort of geometrical relationship. Given such information, regression is the\n",
      "statistician‚Äôs most powerful tool for bringing ‚Äúother‚Äù results to bear on a\n",
      "case of primary interest: for instance, the age-55 volunteer in Figure 8.1.\n",
      "Empirical Bayes methods do not require covariate information but may\n",
      "be improvable if it exists. If, for example, the player‚Äôs age were an impor-\n",
      "tant covariate in the baseball example of Table 7.1, we might Ô¨Årst regress\n",
      "the MLE values on age, and then shrink them toward the regression line\n",
      "rather than toward the grand mean Npas in (7.20). In this way, two different\n",
      "sorts of indirect evidence would be brought to bear on the estimation of\n",
      "each player‚Äôs ability.\n",
      "spam that was fake ham during WWII, and has been adopted by the machine-learning\n",
      "community.\n",
      "8The4601\u000257Xmatrix.xij/was standardized, so disparate scalings are not the\n",
      "cause of these discrepancies. Some of the features have mostly ‚Äúzero‚Äù observations,\n",
      "which may account for their unstable estimation.116 GLMs and Regression Trees\n",
      "8.2 Generalized Linear Models9\n",
      "Logistic regression is a special case of generalized linear models (GLMs),\n",
      "a key 1970s methodology having both algorithmic and inferential inÔ¨Çu-\n",
      "ence. GLMs extend ordinary linear regression, that is least squares curve-\n",
      "Ô¨Åtting, to situations where the response variables are binomial, Poisson,\n",
      "gamma, beta, or in fact any exponential family form.\n",
      "We begin with a one-parameter exponential family,\n",
      "n\n",
      ".\u0015/f0.y/; \u00152∆ío\n",
      "; (8.20)\n",
      "as in (5.46) (now with Àõandxreplaced by\u0015andy, and .Àõ/ replaced by\n",
      ".\u0015/ , for clearer notation in what follows). Here \u0015is the natural parameter\n",
      "andythesufÔ¨Åcient statistic , both being one-dimensional in usual applica-\n",
      "tions;\u0015takes its values in an interval of the real line. Each coordinate yi\n",
      "of an observed data set yD.y1;y2;:::;yi;:::;yN/0is assumed to come\n",
      "from a member of family (8.20),\n",
      "yi\u0018f\u0015i.\u0001/independently for iD1;2;:::;N: (8.21)\n",
      "Table 8.4 lists \u0015andyfor the Ô¨Årst four families in Table 5.1, as well as\n",
      "their deviance and normalizing functions.\n",
      "By itself, model (8.21) requires Nparameters\u00151;\u00152;:::;\u0015N, usually\n",
      "too many for effective individual estimation. A key GLM tactic is to specify\n",
      "the\u0015s in terms of a linear regression equation. Let Xbe anN\u0002p‚Äústructure\n",
      "matrix,‚Äù with ith row sayx0\n",
      "i, andÀõan unknown vector of pparameters;\n",
      "theN-vector\u0015D.\u00151;\u00152;:::;\u0015N/0is then speciÔ¨Åed by\n",
      "\u0015DXÀõ: (8.22)\n",
      "In the dose‚Äìresponse experiment of Figure 8.2 and model (8.5), XisN\u00022\n",
      "withith row.1;xi/and parameter vector ÀõD.Àõ0;Àõ1/.\n",
      "The probability density function fÀõ.y/of the data vector yis\n",
      "fÀõ.y/DNY\n",
      "iD1f\u0015i.yi/DePN\n",
      ".\u0015i//NY\n",
      "iD1f0.yi/; (8.23)\n",
      "which can be written as\n",
      "fÀõ.y/DeÀõ0z\u0000 .Àõ/f0.y/; (8.24)\n",
      "9Some of the more technical points raised in this section are referred to in later chapters,\n",
      "and can be scanned or omitted at Ô¨Årst reading.8.2 Generalized Linear Models 117\n",
      "Table 8.4 Exponential family form for Ô¨Årst four cases in Table 5.1;\n",
      "natural parameter \u0015, sufÔ¨Åcient statistic y, deviance (8.31) between family\n",
      ".\u0015/ .rsf1andf2,D.f1;f2/, and normalizing function \n",
      ".\u0015/ D.f 1;f2/ \n",
      "1.Normal \u0016=\u001b2x\u0010\n",
      "\u00161\u0000\u00162\n",
      "\u001b\u00112\n",
      "\u001b2\u00152=2\n",
      "N.\u0016;\u001b2/,\n",
      "\u001b2known\n",
      "2.Poisson log\u0016 x 2\u0016 1h\u0010\n",
      "\u00162\n",
      "\u00161\u00001\u0011\n",
      "\u0000log\u00162\n",
      "\u00161i\n",
      "e\u0015\n",
      "Poi.\u0016/\n",
      "3.Binomial log\u0019\n",
      "1\u0000\u0019x 2nh\n",
      "\u00191log\u00191\n",
      "\u00192C.1\u0000\u00191/log1\u0000\u00191\n",
      "1\u0000\u00192i\n",
      "nlog.1Ce\u0015/\n",
      "Bi.n;\u0019/\n",
      "4.Gamma\u00001=\u001b x 2\u0017h\u0010\n",
      "\u001b1\n",
      "\u001b2\u00001\u0011\n",
      "\u0000log\u001b1\n",
      "\u001b2i\n",
      "\u0000\u0017log.\u0000\u0015/\n",
      "Gam.\u0017;\u001b/ ,\n",
      "\u0017known\n",
      "where\n",
      "zDX0yand .Àõ/DNX\n",
      ".x0\n",
      "iÀõ/; (8.25)\n",
      "ap-parameter exponential family (5.50), with natural parameter vector Àõ\n",
      "and sufÔ¨Åcient statistic vector z. The main point is that all the information\n",
      "from ap-parameter GLM is summarized in the p-dimensional vector z,\n",
      "no matter how large Nmay be, making it easier both to understand and to\n",
      "analyze.\n",
      "We have now reduced the N-parameter model (8.20)‚Äì(8.21) to the p-\n",
      "parameter exponential family (8.24), with pusually much smaller than N,\n",
      "in this way avoiding the difÔ¨Åculties of high-dimensional estimation. The\n",
      "moments of the one-parameter constituents (8.20) determine the estimation\n",
      "properties in model (8.22)‚Äì(8.24). Let .\u0016\u0015;\u001b2\n",
      "\u0015/denote the expectation and\n",
      "variance of univariate density f\u0015.y/(8.20),\n",
      "y\u0018.\u0016\u0015;\u001b2\n",
      "\u0015/; (8.26)\n",
      "for instance.\u0016\u0015;\u001b2\n",
      "\u0015/D.e\u0015;e\u0015/for the Poisson. The N-vectoryobtained\n",
      "from GLM (8.22) then has mean vector and covariance matrix\n",
      "y\u0018.\u0016.Àõ/;‚Ä†.Àõ//; (8.27)118 GLMs and Regression Trees\n",
      "where\u0016.Àõ/is the vector with ith component \u0016\u0015iwith\u0015iDx0\n",
      "iÀõ, and‚Ä†.Àõ/\n",
      "is theN\u0002Ndiagonal matrix having diagonal elements \u001b2\n",
      "\u0015i.\n",
      "The maximum likelihood estimate OÀõof the parameter vector Àõcan be\n",
      "shown to satisfy the simple equation¬é ¬é2\n",
      "X0≈íy\u0000\u0016.OÀõ/¬çD0: (8.28)\n",
      "For the normal case where yi\u0018N.\u0016i;\u001b2/in (8.21), that is, for ordinary\n",
      "linear regression, \u0016.OÀõ/DXOÀõand (8.28) becomes X0.y\u0000XOÀõ/D0, with\n",
      "the familiar solution\n",
      "OÀõD.X0X/\u00001X0yI (8.29)\n",
      "otherwise,\u0016.Àõ/is a nonlinear function of Àõ, and (8.28) must be solved\n",
      "by numerical iteration. This is made easier by the fact that, for GLMs,\n",
      "logfÀõ.y/, the likelihood function we wish to maximize, is a concave func-\n",
      "tion ofÀõ. The MLEOÀõhas approximate expectation and covariance¬é ¬é3\n",
      "OÀõP \u0018.Àõ;\u0000\n",
      "X0‚Ä†.Àõ/X\u0001\u00001/; (8.30)\n",
      "similar to the exact OLS result OÀõ\u0018.Àõ;\u001b\u00002.X0X/\u00001/.¬é ¬é4\n",
      "Generalizing the binomial deÔ¨Ånition (8.14), the deviance between den-\n",
      "sitiesf1.y/andf2.y/is deÔ¨Åned to be\n",
      "D.f1;f2/D2Z\n",
      "Yf1.y/log\u001af1.y/\n",
      "f2.y/\u001b\n",
      "dy; (8.31)\n",
      "the integral (or sum for discrete distributions) being over their common\n",
      "sample space Y.D.f1;f2/is always nonnegative, equaling zero only if\n",
      "f1andf2are the same; in general D.f1;f2/does not equal D.f2;f1/.\n",
      "Deviance does not depend on how the two densities are named, for example\n",
      "(8.14) having the same expression as the Binomial entry in Table 8.4.\n",
      "In what follows it will sometimes be useful to label the family (8.20) by\n",
      "itsexpectation parameter \u0016DE\u0015fygrather than by the natural parameter\n",
      "\u0015:\n",
      ".\u0015/f0.y/; (8.32)\n",
      "meaning the same thing as (8.20), only the names attached to the individ-\n",
      "ual family members being changed. In this notation it is easy to show a\n",
      "fundamental result sometimes known as\n",
      "Hoeffding‚Äôs Lemma¬éThe maximum likelihood estimate of \u0016giveny ¬é5\n",
      "isyitself, and the log likelihood logf\u0016.y/decreases from its maximum\n",
      "logfy.y/by an amount that depends on the deviance D.y;\u0016/ ,\n",
      "f\u0016.y/Dfy.y/e\u0000D.y;\u0016/=2: (8.33)8.2 Generalized Linear Models 119\n",
      "Returning to the GLM framework (8.21)‚Äì(8.22), parameter vector Àõ\n",
      "gives\u0015.Àõ/DXÀõ, which in turn gives the vector of expectation param-\n",
      "eters\n",
      "\u0016.Àõ/D.:::\u0016i.Àõ/:::/0; (8.34)\n",
      "for instance\u0016i.Àõ/Dexpf\u0015i.Àõ/gfor the Poisson family. Multiplying Hoeff-\n",
      "ding‚Äôs lemma (8.33) over the NcasesyD.y1;y2;:::;yN/0yields\n",
      "fÀõ.y/DNY\n",
      "iD1f\u0016i.Àõ/.yi/D\"NY\n",
      "iD1fyi.yi/#\n",
      "e\u0000PN\n",
      "1D.yi;\u0016i.Àõ//: (8.35)\n",
      "This has an important consequence: the MLEOÀõis the choice of Àõthat\n",
      "minimizes the total deviancePN\n",
      "1D.yi;\u0016i.Àõ//. As in Figure 8.2, GLM\n",
      "maximum likelihood Ô¨Åtting is ‚Äúleast total deviance‚Äù in the same way that\n",
      "ordinary linear regression is least sum of squares.\n",
      "\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001\n",
      "The inner circle of Figure 8.3 represents normal theory, the preferred\n",
      "venue of classical applied statistics. Exact inferences‚Äî t-tests,Fdistribu-\n",
      "tions, most of multivariate analysis‚Äîwere feasible within the circle. Out-\n",
      "side the circle was a general theory based mainly on asymptotic (large-\n",
      "sample) approximations involving Taylor expansions and the central limit\n",
      "theorem.\n",
      " \n",
      "NORMAL THEORY\n",
      "(exact calculations)EXPONENTIAL FAMILIES\n",
      "(partly exact)GENERAL THEORY\n",
      "(asymptotics)Figure 8.3. Three levels of statistical modeling\n",
      "Figure 8.3 Three levels of statistical modeling.\n",
      "A few useful exact results lay outside the normal theory circle, relating120 GLMs and Regression Trees\n",
      "to a few special families: the binomial, Poisson, gamma, beta, and others\n",
      "less well known. Exponential family theory, the second circle in Figure 8.3,\n",
      "uniÔ¨Åed the special cases into a coherent whole. It has a ‚Äúpartly exact‚Äù Ô¨Ça-\n",
      "vor, with some ideal counterparts to normal theory‚Äîconvex likelihood sur-\n",
      "faces, least deviance regression‚Äîbut with some approximations necessary,\n",
      "as in (8.30). Even the approximations, though, are often more convincing\n",
      "than those of general theory, exponential families‚Äô Ô¨Åxed-dimension sufÔ¨Å-\n",
      "cient statistics making the asymptotics more transparent.\n",
      "Logistic regression has banished its predecessors (such as probit anal-\n",
      "ysis) almost entirely from the Ô¨Åeld, and not only because of estimating\n",
      "efÔ¨Åciencies and computational advantages (which are actually rather mod-\n",
      "est), but also because it is seen as a clearer analogue to ordinary least\n",
      "squares, our 200-year-old dependable standby. GLM research development\n",
      "has been mostly frequentist, but with a substantial admixture of likelihood-\n",
      "based reasoning, and a hint of Fisher‚Äôs ‚Äúlogic of inductive inference.‚Äù\n",
      "Helping the statistician choose between competing methodologies is the\n",
      "job of statistical inference. In the case of generalized linear models the\n",
      "choice has been made, at least partly, in terms of aesthetics as well as phi-\n",
      "losophy.\n",
      "8.3 Poisson Regression\n",
      "The third most-used member of the GLM family, after normal theory least\n",
      "squares and logistic regression, is Poisson regression. Nindependent Pois-\n",
      "son variates are observed,\n",
      "yiind\u0018Poi.\u0016i/; iD1;2;:::;N; (8.36)\n",
      "where\u0015iDlog\u0016iis assumed to follow a linear model,\n",
      "\u0015.Àõ/DXÀõ; (8.37)\n",
      "whereXis a knownN\u0002pstructure matrix and Àõan unknownp-vector\n",
      "of regression coefÔ¨Åcients. That is, \u0015iDx0\n",
      "iÀõforiD1;2;:::;N , wherex0\n",
      "i\n",
      "is theith row ofX.\n",
      "In the chapters that follow we will see Poisson regression come to the\n",
      "rescue in what at Ô¨Årst appear to be awkward data-analytic situations. Here\n",
      "we will settle for an example involving density estimation from a spatially\n",
      "truncated sample.\n",
      "Table 8.5 shows galaxy counts¬éfrom a small portion of the sky: 487 ¬é6\n",
      "galaxies have had their redshifts rand apparent magnitudes mmeasured.8.3 Poisson Regression 121\n",
      "Table 8.5 Counts for a truncated sample of 487 galaxies, binned by\n",
      "redshift and magnitude.\n",
      "redshift (farther)\u0000!\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n",
      "18 1 6 6 3 1 4 6 8 8 20 10 7 16 9 4\n",
      "17 3 2 3 4 0 5 7 6 6 7 5 7 6 8 5\n",
      "16 3 2 3 3 3 2 9 9 6 3 5 4 5 2 1\n",
      "15 1 1 4 3 4 3 2 3 8 9 4 3 4 1 1\n",
      "14 1 3 2 3 3 4 5 7 6 7 3 4 0 0 1\n",
      "13 3 2 4 5 3 6 4 3 2 2 5 1 0 0 0\n",
      "12 2 0 2 4 5 4 2 3 3 0 1 2 0 0 1\n",
      "\" 11 4 1 1 4 7 3 3 1 2 0 1 1 0 0 0\n",
      "magnitude 10 1 0 0 2 2 2 1 2 0 0 0 1 2 0 0\n",
      "(dimmer) 9 1 1 0 2 2 2 0 0 0 0 1 0 0 0 0\n",
      "8 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0\n",
      "7 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "6 0 0 3 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "5 0 3 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "4 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "3 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "2 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "Distance from earth is an increasing function of r, while apparent bright-\n",
      "ness is a decreasing function10ofm. In this survey, counts were limited to\n",
      "galaxies having\n",
      "1:22\u0014r\u00143:32 and17:2\u0014m\u001421:5; (8.38)\n",
      "the upper limit reÔ¨Çecting the difÔ¨Åculty of measuring very dim galaxies.\n",
      "The range of log rhas been divided into 15 equal intervals and likewise\n",
      "18 equal intervals for m. Table 8.5 gives the counts of the 487 galaxies in\n",
      "the18\u000215D270bins. (The lower right corner of the table is empty be-\n",
      "cause distant galaxies always appear dim.) The multinomial/Poisson con-\n",
      "nection (5.44) helps motivate model (8.36), picturing the table as a multi-\n",
      "nomial observation on 270 categories, in which the sample size Nwas\n",
      "itself Poisson.\n",
      "We can imagine Table 8.5 as a small portion of a much more extensive\n",
      "table, hypothetically available if the data were nottruncated. Experience\n",
      "suggests that we might then Ô¨Åt an appropriate bivariate normal density to\n",
      "the data, as in Figure 5.3. It seems like it might be awkward to Ô¨Åt part of a\n",
      "bivariate normal density to truncated data, but Poisson regression offers an\n",
      "easy solution.\n",
      "10An object of the second magnitude is less bright than one of the Ô¨Årst, and so on, a\n",
      "classiÔ¨Åcation system owing to the Greeks.122 GLMs and Regression Trees\n",
      "Letrbe the 270-vector listing the values of rin each bin of the table\n",
      "(in column order), and likewise mfor the 270mvalues‚Äîfor instance mD\n",
      ".18;17;:::;1/ repeated 15 times‚Äîand deÔ¨Åne the 270\u00025matrixXas\n",
      "XD≈ír;m;r2;rm;m2¬ç; (8.39)\n",
      "wherer2is the vector whose components are the square of r‚Äôs, etc. The\n",
      "log density of a bivariate normal distribution in .r;m/ is of the form Àõ1rC\n",
      "Àõ2mCÀõ3r2CÀõ4rmCÀõ5m2, agreeing with log \u0016iDx0\n",
      "iÀõas speciÔ¨Åed\n",
      "by (8.39). We can use a Poisson GLM, with yitheith bin‚Äôs count, to es-\n",
      "timate the portion of our hypothesized bivariate normal distribution in the\n",
      "truncation region (8.38).\n",
      "DimmerFartherCount\n",
      "DimmerFartherDensity\n",
      "Figure 8.4 Leftgalaxy data; binned counts. Right Poisson GLM\n",
      "density estimate.\n",
      "The left panel of Figure 8.4 is a perspective picture of the raw counts\n",
      "in Table 8.5. On the right is the Ô¨Åtted density from the Poisson regression.\n",
      "Irrespective of density estimation, Poisson regression has done a useful job\n",
      "of smoothing the raw bin counts.\n",
      "Contours of equal value of the Ô¨Åtted log density\n",
      "OÀõ0COÀõ1rCOÀõ2mCOÀõ3r2COÀõ4rmCOÀõ5m2(8.40)\n",
      "are shown in Figure 8.5. One can imagine the contours as truncated por-\n",
      "tions of ellipsoids, of the type shown in Figure 5.3. The right panel of\n",
      "Figure 8.4 makes it clear that we are nowhere near the center of the hypo-\n",
      "thetical bivariate normal density, which must lie well beyond our dimness\n",
      "limit.8.3 Poisson Regression 123\n",
      "‚àí1.5 ‚àí1.0 ‚àí0.5 0.0‚àí21 ‚àí20 ‚àí19 ‚àí18 ‚àí17\n",
      "FartherDimmer\n",
      " 0.5  1  1.5 \n",
      " 2  2.5  3 \n",
      " 3.5  4 \n",
      " 4.5 \n",
      " 5  5.5 \n",
      " 6  6.5  7  7.5  8  8.5  9 ‚óè\n",
      "Figure 8.5 Contour curves for Poisson GLM density estimate for\n",
      "the galaxy data. The red dot shows the point of maximum density.\n",
      "The Poisson deviance residual Zbetween an observed count yand a\n",
      "Ô¨Åtted valueO\u0016is\n",
      "ZDsign.y\u0000O\u0016/D.y;O\u0016/1=2; (8.41)\n",
      "withDthe Poisson deviance from Table 8.4. Zjk, the deviance residual\n",
      "between the count yijin theijth bin of Table 8.5 and the Ô¨Åtted value O\u0016jk\n",
      "from the Poisson GLM, was calculated for all 270 bins. Standard frequen-\n",
      "tist GLM theory says that SDP\n",
      "jkZ2\n",
      "jkshould be about 270 if the bivari-\n",
      "ate normal model (8.39) is correct.11Actually the Ô¨Åt was poor: SD610.\n",
      "In practice we might try adding columns to Xin (8.39), e.g., rm2or\n",
      "r2m2, improving the Ô¨Åt where it was worst, near the boundaries of the ta-\n",
      "ble. Chapter 12 demonstrates some other examples of Poisson density esti-\n",
      "mation. In general, Poisson GLMs reduce density estimation to regression\n",
      "model Ô¨Åtting, a familiar and Ô¨Çexible inferential technology.\n",
      "11This is a modern version of the classic chi-squared goodness-of-Ô¨Åt test.124 GLMs and Regression Trees\n",
      "8.4 Regression Trees\n",
      "The data setdfor a regression problem typically consists of Npairs.xi;yi/,\n",
      "dDf.xi;yi/; iD1;2;:::;Ng; (8.42)\n",
      "wherexiis a vector of predictors , or ‚Äúcovariates,‚Äù taking its value in some\n",
      "spaceX, andyiis the response , assumed to be univariate in what follows.\n",
      "The regression algorithm, perhaps a Poisson GLM, inputs dand outputs\n",
      "arulerd.x/: for any value of xinX,rd.x/produces an estimate Oyfor a\n",
      "possible future value of y,\n",
      "OyDrd.x/: (8.43)\n",
      "In the logistic regression example (8.8), rd.x/isO\u0019.x/ .\n",
      "There are three principal uses for the rule rd.x/.\n",
      "1 For prediction : Given a new observation of x, but not of its correspond-\n",
      "ingy, we useOyDrd.x/to predicty. In the spam example, the 57\n",
      "keywords of an incoming message could be used to predict whether or\n",
      "not it is spam.12(See Chapter 12.)\n",
      "2 For estimation : The rulerd.x/describes a ‚Äúregression surface‚Äù OSover\n",
      "X,\n",
      "OSDfrd.x/; x2Xg: (8.44)\n",
      "The right panel of Figure 8.4 shows OSfor the galaxy example. OScan be\n",
      "thought of as estimating S, the true regression surface, often deÔ¨Åned in\n",
      "the form of conditional expectation,\n",
      "SDfEfyjxg; x2Xg: (8.45)\n",
      "(In a dichotomous situation where yis coded as 0 or 1, SDfPrfyD\n",
      "1jxg; x2Xg.)\n",
      "For estimation, but not necessarily for prediction, we want OSto accu-\n",
      "rately portray S. The right panel of Figure 8.4 shows the estimated galaxy\n",
      "density still increasing monotonically in dimmer at the top end of the\n",
      "truncation region, but not so in farther , perhaps an important clue for\n",
      "directing future search counts.13The Ô¨Çat region in the kidney function re-\n",
      "gression curve of Figure 1.2 makes almost no difference to prediction, but\n",
      "is of scientiÔ¨Åc interest if accurate.\n",
      "12Prediction of dichotomous outcomes is often called ‚ÄúclassiÔ¨Åcation.‚Äù\n",
      "13Physicists call a regression-based search for new objects ‚Äúbump hunting.‚Äù8.4 Regression Trees 125\n",
      "3 For explanation : The 10 predictors for the diabetes data of Section 7.3,\n",
      "age,sex,bmi,. . . , were selected by the researcher in the hope of ex-\n",
      "plaining the etiology of diabetes progression. The relative contribution\n",
      "of the different predictors to rd.x/is then of interest. How the regression\n",
      "surface is composed is of prime concern in this use, but not in use 1 or 2\n",
      "above.\n",
      "The three different uses of rd.x/raise different inferential questions.\n",
      "Use 1 calls for estimates of prediction error. In a dichotomous situation\n",
      "such as the spam study, we would want to know both error probabilities\n",
      "PrfOyDspamjyDhamgand PrfOyDhamjyDspamg:(8.46)\n",
      "For estimation, the accuracy of rd.x/as a function of x, perhaps in stan-\n",
      "dard deviation terms,\n",
      "sd.x/Dsd.Oyjx/; (8.47)\n",
      "would tell how closely OSapproximates S. Use 3, explanation, requires\n",
      "more elaborate inferential tools, saying for example which of the regression\n",
      "coefÔ¨ÅcientsÀõiin (8.19) can safely be set to zero.\n",
      "|t1t2\n",
      "t3t4\n",
      "R1R1\n",
      "R2R2\n",
      "R3R3\n",
      "R4R4\n",
      "R5R5\n",
      "X1X1 X1\n",
      "X2X2X2\n",
      "X1‚â§t1\n",
      "X2‚â§t2 X1‚â§t3\n",
      "X2‚â§t4\n",
      "Figure 8.6 Lefta hypothetical regression tree based on two\n",
      "predictorsX1andX2.Right corresponding regression surface.\n",
      "Regression trees use a simple but intuitively appealing technique to form\n",
      "a regression surface: recursive partitioning. The left panel of Figure 8.6\n",
      "illustrates the method for a hypothetical situation involving two predictor\n",
      "variables,X1andX2(e.g.,randmin the galaxy example). At the top of126 GLMs and Regression Trees\n",
      "the tree, the sample population of Ncases has been split into two groups:\n",
      "those withX1equal to or less than value t1go to the left, those with X1>\n",
      "t1to the right. The leftward group is itself then divided into two groups\n",
      "depending on whether or not X2\u0014t2. The division stops there, leaving\n",
      "twoterminal nodes R1andR2. On the tree‚Äôs right side, two other splits\n",
      "give terminal nodes R3,R4, andR5.\n",
      "A prediction valueOyRjis attached to each terminal node Rj. The predic-\n",
      "tionOyapplying to a new observation xD.x1;x2/is calculated by starting\n",
      "xat the top of the tree and following the splits downward until a terminal\n",
      "node, and its attached prediction OyRj, is reached. The corresponding re-\n",
      "gression surfaceOSis shown in the right panel of Figure 8.6 (here the OyRj\n",
      "happen to be in ascending order).\n",
      "Various algorithmic rules are used to decide which variable to split and\n",
      "which splitting value tto take at each step of the tree‚Äôs construction. Here\n",
      "is the most common method: suppose at step kof the algorithm, group kof\n",
      "Nkcases remains to be split, those cases having mean and sum of squares\n",
      "mkDX\n",
      "i2groupkyi=Nkands2\n",
      "kDX\n",
      "i2groupk.yi\u0000mk/2: (8.48)\n",
      "Dividing group kinto group k;leftand group k;rightproduces means mk;leftand\n",
      "mk;right, and corresponding sums of squares s2\n",
      "k;leftands2\n",
      "k;right. The algorithm\n",
      "proceeds by choosing the splitting variable Xkand the threshold tkto min-\n",
      "imize\n",
      "s2\n",
      "k;leftCs2\n",
      "k;right: (8.49)\n",
      "In other words, it splits group kinto two groups that are as different from\n",
      "each other as possible.¬é ¬é7\n",
      "Cross-validation estimates of prediction error, Chapter 12, are used to\n",
      "decide when the splitting process should stop. If group kis not to be further\n",
      "divided, it becomes terminal node Rk, with prediction value OyRkDmk.\n",
      "None of this would be feasible without electronic computation, but even\n",
      "quite large prediction problems can be short work for modern computers.\n",
      "Figure 8.7 shows a regression tree analysis14of the spam data, Ta-\n",
      "ble 8.3. There are seven terminal nodes, labeled 0 or 1 for decision ham\n",
      "orspam . The leftmost node, say R1, is a 0, and contains 2462 ham cases\n",
      "and 275 spam (compared with 2788 and 1813 in the full data set). Starting\n",
      "at the top of the tree, R1is reached if it has a low proportion of $ symbols\n",
      "14Using the R program rpart , in classiÔ¨Åcation mode, employing a different splitting rule\n",
      "than the version based on (8.49).8.4 Regression Trees 127\n",
      "|char$< ‚àí0.0826\n",
      "remove< ‚àí0.1513\n",
      "char!< 0.1335\n",
      "capruntot< ‚àí0.3757\n",
      "free< 0.7219hp>=‚àí0.08945\n",
      "0\n",
      "2462/275\n",
      "0\n",
      "129/321\n",
      "1/201\n",
      "33/1891\n",
      "30/3000\n",
      "63/71\n",
      "70/990Figure 8.7 . Regression Tree, Spam Data: 0=nonspam,  1=spam,\n",
      "Error Rates: nonspam 5.2%,  spam 17.4%\n",
      "Captions indicate leftward (nonspam) moves\n",
      "Figure 8.7 Regression tree on the spam data;0Dham,1D\n",
      "spam . Error rates: ham 5.2%, spam 17.4%. Captions indicate\n",
      "leftward (ham) moves.\n",
      "char$ , a low proportion of the word remove , and a low proportion of\n",
      "exclamation marks char! .\n",
      "Regression trees are easy to interpret (‚ÄúToo many dollar signs means\n",
      "spam!‚Äù) seemingly suiting them for use 3, explanation. Unfortunately, they\n",
      "are also easy to overinterpret, with a reputation for being unstable in prac-\n",
      "tice. Discontinuous regression surfaces OS, as in Figure 8.6, disqualify them\n",
      "for use 2, estimation. Their principal use in what follows will be as key\n",
      "parts of prediction algorithms, use 1. The tree in Figure 8.6 has apparent\n",
      "error rates (8.46) of 5.2% and 17.4%. This can be much improved upon\n",
      "by ‚Äúbagging‚Äù (bootstrap aggregation), Chapters 17 and 20, and by other\n",
      "computer-intensive techniques.\n",
      "Compared with generalized linear models, regression trees represent a\n",
      "break from classical methodology that is more stark. First of all, they are\n",
      "totally nonparametric; bigger but less structured data sets have promoted\n",
      "nonparametrics in twenty-Ô¨Årst-century statistics. Regression trees are more\n",
      "computer-intensive and less efÔ¨Åcient than GLMs but, as will be seen in Part\n",
      "III, the availability of massive data sets and modern computational equip-128 GLMs and Regression Trees\n",
      "ment has diminished the appeal of efÔ¨Åciency in favor of easy assumption-\n",
      "free application.\n",
      "8.5 Notes and Details\n",
      "Computer-age algorithms depend for their utility on statistical computing\n",
      "languages. After a period of evolution, the language S(Becker et al. , 1988)\n",
      "and its open-source successor R(R Core Team, 2015), have come to dom-\n",
      "inate applied practice.15Generalized linear models are available from a\n",
      "single Rcommand, e.g.,\n",
      "glm(y\u0018X,family=binomial)\n",
      "for logistic regression (Chambers and Hastie, 1993), and similarly for re-\n",
      "gression trees and hundreds of other applications.\n",
      "The classic version of bioassay, probit analysis , assumes that each test\n",
      "animal has its own lethal dose level X, and that the population distribution\n",
      "ofXis normal,\n",
      "PrfX\u0014xgDÀÜ.Àõ0CÀõ1x/ (8.50)\n",
      "for unknown parameters .Àõ0;Àõ1/and standard normal cdf ÀÜ. Then the\n",
      "number of animals dying at dose xis binomial Bi .nx;\u0019x/as in (8.3), with\n",
      "\u0019xDÀÜ.Àõ0CÀõ1x/, or\n",
      "ÀÜ\u00001.\u0019x/DÀõ0CÀõ1x: (8.51)\n",
      "Replacing the standard normal cdf ÀÜ.z/ with the logistic cdf 1=.1Ce\u0000z/\n",
      "(which resembles ÀÜ), changes (8.51) into logistic regression (8.5). The\n",
      "usual goal of bioassay was to estimate ‚ÄúLD50,‚Äù the dose lethal to 50%\n",
      "of the test population; it is indicated by the open circle in Figure 8.2.\n",
      "Cox (1970), the classic text on logistic regression, lists Berkson (1944)\n",
      "as an early practitioner. Wedderburn (1974) is credited with generalized\n",
      "linear models in McCullagh and Nelder‚Äôs inÔ¨Çuential text of that name, Ô¨Årst\n",
      "edition 1983; Birch (1964) developed an important and suggestive special\n",
      "case of GLM theory.\n",
      "The twenty-Ô¨Årst century has seen an efÔ¨Çorescence of computer-based re-\n",
      "gression techniques, as described extensively in Hastie et al. (2009). The\n",
      "discussion of regression trees here is taken from their Section 9.2, including\n",
      "our Figure 8.6. They use the spam data as a central example; it is publicly\n",
      "15Previous computer packages such as SAS and SPSS continue to play a major role in\n",
      "application areas such as the social sciences, biomedical statistics, and the\n",
      "pharmaceutical industry.8.5 Notes and Details 129\n",
      "available at ftp.ics.uci.edu . Breiman et al. (1984) propelled regres-\n",
      "sion trees into wide use with their CART algorithm.\n",
      "¬é1[p. 112] SufÔ¨Åciency as in (8.13) .The Fisher‚ÄìNeyman criterion says that if\n",
      "fÀõ.x/DhÀõ.S.x//g.x/, wheng.\u0001/does not depend on Àõ, thenS.x/is\n",
      "sufÔ¨Åcient for Àõ.\n",
      "¬é2[p. 118] Equation (8.28) .From (8.24)‚Äì(8.25) we have the log likelihood\n",
      "function\n",
      "lÀõ.y/DÀõ0z\u0000 .Àõ/ (8.52)\n",
      "with sufÔ¨Åcient statistic zDX0yand .Àõ/DPN\n",
      ".x0\n",
      "iÀõ/. Differentiat-\n",
      "ing with respect to Àõ,\n",
      "PlÀõ.y/Dz\u0000P .Àõ/DX0y\u0000X0\u0016.Àõ/; (8.53)\n",
      ".x0D\u0016\u0015(5.55), soPd d\n",
      "iÀõ/Dx0\n",
      "i\u0016i.Àõ/. But\n",
      "(8.53) saysPlÀõ.y/DX0.y\u0000\u0016.Àõ//, verifying the MLE equation (8.28).\n",
      "¬é3[p. 118] Concavity of the log likelihood. From (8.53), the second derivative\n",
      "matrixRlÀõ.y/with respect to Àõis\n",
      "\u0000R .Àõ/D\u0000 covÀõ.z/; (8.54)\n",
      "(5.57)‚Äì(5.59). But zDX0yhas\n",
      "covÀõ.z/DX0‚Ä†.Àõ/X; (8.55)\n",
      "a positive deÔ¨Ånite p\u0002pmatrix, verifying the concavity of lÀõ.y/(which in\n",
      "fact applies to any exponential family, not only GLMs).\n",
      "¬é4[p. 118] Formula (8.30) .The sufÔ¨Åcient statistic zhas mean vector and co-\n",
      "variance matrix\n",
      "z\u0018.Àá;VÀõ/; (8.56)\n",
      "withÀáDEÀõfzg(5.58) andVÀõDX0‚Ä†.Àõ/X(8.55). Using (5.60), the\n",
      "Ô¨Årst-order Taylor series for OÀõas a function of zis\n",
      "OÀõ:DÀõCV\u00001\n",
      "Àõ.z\u0000Àá/: (8.57)\n",
      "Taken literally, (8.57) gives (8.30). In the OLS formula, we have \u001b\u00002rather\n",
      "than\u001b2since the natural parameter Àõfor the Normal entry in Table 8.4 is\n",
      "\u0016=\u001b2.\n",
      "¬é5[p. 118] Formula (8.33) .This formula, attributed to Hoeffding (1965), is a\n",
      "key result in the interpretation of GLM Ô¨Åtting. Applying deÔ¨Ånition (8.31)130 GLMs and Regression Trees\n",
      "to family (8.32) gives\n",
      "1\n",
      ".\u00152/¬çg\u00152/DE\u00151f.\u00151\u0000\u00152/y\u0000≈í\n",
      ".\u00152/¬ç:(8.58)\n",
      "If\u00151is the MLEO\u0015then\u00161Dy(from the maximum likelihood equation\n",
      ".\u0015/Dy\u0000\u0016\u0015), giving16P\n",
      "1\n",
      "2D\u0010\n",
      "O\u0015;\u0015\u0011\n",
      "D\u0010\n",
      "O\u0015\u0000\u0015\u0011\n",
      "y\u0000h\n",
      "\u0010\n",
      "O\u0015\u0011\n",
      ".\u0015/i\n",
      "(8.59)\n",
      "for any choice of \u0015. But the right-hand side of (8.59) is \u0000log≈íf\u0015.y/=fy.y/¬ç,\n",
      "verifying (8.33).\n",
      "¬é6[p. 120] Table 8.5. The galaxy counts are from Loh and Spillar‚Äôs 1988\n",
      "redshift survey, as discussed in Efron and Petrosian (1992).\n",
      "¬é7[p. 126] Criteria (8.49) .Abbreviating ‚Äúleft‚Äù and ‚Äúright‚Äù by landr, we\n",
      "have\n",
      "s2\n",
      "kDs2\n",
      "klCs2\n",
      "krCNklNkr\n",
      "Nk.mkl\u0000mkr/2; (8.60)\n",
      "withNklandNkrthe subgroup sizes, showing that minimizing (8.49) is\n",
      "the same as maximizing the last term in (8.60). Intuitively, a good split is\n",
      "one that makes the left and right groups as different as possible, the ideal\n",
      "being all 0s on the left and all 1s on the right, making the terminal nodes\n",
      "‚Äúpure.‚Äù\n",
      "16In some casesO\u0015is undeÔ¨Åned; for example, when yD0for a Poisson response,\n",
      "O\u0015Dlog.y/which is undeÔ¨Åned. But, in (8.59), we assume that O\u0015yD0. Similarly for\n",
      "binaryyand the binomial family.9\n",
      "Survival Analysis and the EM Algorithm\n",
      "Survival analysis had its roots in governmental and actuarial statistics,\n",
      "spanning centuries of use in assessing life expectancies, insurance rates,\n",
      "and annuities. In the 20 years between 1955 and 1975, survival analysis\n",
      "was adapted by statisticians for application to biomedical studies. Three\n",
      "of the most popular post-war statistical methodologies emerged during\n",
      "this period: the Kaplan‚ÄìMeier estimate, the log-rank test,1and Cox‚Äôs pro-\n",
      "portional hazards model, the succession showing increased computational\n",
      "demands along with increasingly sophisticated inferential justiÔ¨Åcation. A\n",
      "connection with one of Fisher‚Äôs ideas on maximum likelihood estimation\n",
      "leads in the last section of this chapter to another statistical method that has\n",
      "‚Äúgone platinum,‚Äù the EM algorithm.\n",
      "9.1 Life Tables and Hazard Rates\n",
      "An insurance company‚Äôs life table appears in Table 9.1, showing its number\n",
      "of clients (that is, life insurance policy holders) by age, and the number of\n",
      "deaths during the past year in each age group,2for example Ô¨Åve deaths\n",
      "among the 312 clients aged 59. The column labeled OSis of great interest\n",
      "to the company‚Äôs actuaries, who have to set rates for new policy holders.\n",
      "It is an estimate of survival probability: probability 0.893 of a person aged\n",
      "30 (the beginning of the table) surviving past age 59, etc. OSis calculated\n",
      "according to an ancient but ingenious algorithm.\n",
      "LetXrepresent a typical lifetime, so\n",
      "fiDPrfXDig (9.1)\n",
      "1Also known as the Mantel‚ÄìHaenszel or Cochran‚ÄìMantel‚ÄìHaenszel test.\n",
      "2The insurance company is Ô¨Åctitious but the deaths yare based on the true 2010 rates for\n",
      "US men, per Social Security Administration data.\n",
      "131132 Survival Analysis and the EM Algorithm\n",
      "Table 9.1 Insurance company life table; at each age, nDnumber of\n",
      "policy holders, yDnumber of deaths,OhDhazard ratey=n,OSD\n",
      "survival probability estimate (9.6) .\n",
      "Agen yOhOS Agen yOhOS\n",
      "30 116 0 .000 1.000 60 231 1 .004 .889\n",
      "31 44 0 .000 1.000 61 245 5 .020 .871\n",
      "32 95 0 .000 1.000 62 196 5 .026 .849\n",
      "33 97 0 .000 1.000 63 180 4 .022 .830\n",
      "34 120 0 .000 1.000 64 170 2 .012 .820\n",
      "35 71 1 .014 .986 65 114 0 .000 .820\n",
      "36 125 0 .000 .986 66 185 5 .027 .798\n",
      "37 122 0 .000 .986 67 127 2 .016 .785\n",
      "38 82 0 .000 .986 68 127 5 .039 .755\n",
      "39 113 0 .000 .986 69 158 2 .013 .745\n",
      "40 79 0 .000 .986 70 100 3 .030 .723\n",
      "41 90 0 .000 .986 71 155 4 .026 .704\n",
      "42 154 0 .000 .986 72 92 1 .011 .696\n",
      "43 103 0 .000 .986 73 90 1 .011 .689\n",
      "44 144 0 .000 .986 74 110 2 .018 .676\n",
      "45 192 2 .010 .976 75 122 5 .041 .648\n",
      "46 153 1 .007 .969 76 138 8 .058 .611\n",
      "47 179 1 .006 .964 77 46 0 .000 .611\n",
      "48 210 0 .000 .964 78 75 4 .053 .578\n",
      "49 259 2 .008 .956 79 69 6 .087 .528\n",
      "50 225 2 .009 .948 80 95 4 .042 .506\n",
      "51 346 1 .003 .945 81 124 6 .048 .481\n",
      "52 370 2 .005 .940 82 67 7 .104 .431\n",
      "53 568 4 .007 .933 83 112 12 .107 .385\n",
      "54 1081 8 .007 .927 84 113 8 .071 .358\n",
      "55 1042 2 .002 .925 85 116 12 .103 .321\n",
      "56 1094 10 .009 .916 86 124 17 .137 .277\n",
      "57 597 4 .007 .910 87 110 21 .191 .224\n",
      "58 359 1 .003 .908 88 63 9 .143 .192\n",
      "59 312 5 .016 .893 89 79 10 .127 .168\n",
      "is the probability of dying at age i, and\n",
      "SiDX\n",
      "j\u0015ifjDPrfX\u0015ig (9.2)\n",
      "is the probability of surviving past age i\u00001. The hazard rate at ageiis by9.1 Life Tables and Hazard Rates 133\n",
      "deÔ¨Ånition\n",
      "hiDfi=SiDPrfXDijX\u0015ig; (9.3)\n",
      "the probability of dying at age igiven survival past age i\u00001.\n",
      "A crucial observation is that the probability Sijof surviving past age j\n",
      "given survival past age i\u00001is the product of surviving each intermediate\n",
      "year,\n",
      "SijDjY\n",
      "kDi.1\u0000hk/DPrfX >jjX\u0015igI (9.4)\n",
      "Ô¨Årst you have to survive year i, probability1\u0000hi; then yeariC1, proba-\n",
      "bility1\u0000hiC1, etc., up to year j, probability1\u0000hj. Notice thatSi(9.2)\n",
      "equalsS1;i\u00001.\n",
      "OSin Table 9.1 is an estimate of SijforiD30. First, each hiwas\n",
      "estimated as the binomial proportion of the number of deaths yiamong the\n",
      "niclients,\n",
      "OhiDyi=ni; (9.5)\n",
      "and then we set\n",
      "OS30;jDjY\n",
      "kD30\u0010\n",
      "1\u0000Ohk\u0011\n",
      ": (9.6)\n",
      "The insurance company doesn‚Äôt have to wait 50 years to learn the proba-\n",
      "bility of a 30-year-old living past 80 (estimated to be 0.506 in the table).\n",
      "One year‚Äôs data sufÔ¨Åces.3\n",
      "Hazard rates are more often described in terms of a continuous positive\n",
      "random variable T(often called ‚Äútime‚Äù), having density function f.t/ and\n",
      "‚Äúreverse cdf,‚Äù or survival function,\n",
      "S.t/DZ1\n",
      "tf.x/dxDPrfT\u0015tg: (9.7)\n",
      "The hazard rate\n",
      "h.t/Df.t/=S.t/ (9.8)\n",
      "satisÔ¨Åes\n",
      "h.t/dt:DPrfT2.t;tCdt/jT\u0015tg (9.9)\n",
      "fordt!0, in analogy with (9.3). The analog of (9.4) is¬é ¬é1\n",
      "3Of course the estimates can go badly wrong if the hazard rates change over time.134 Survival Analysis and the EM Algorithm\n",
      "PrfT\u0015t1jT\u0015t0gDexp\u001a\n",
      "\u0000Zt1\n",
      "t0h.x/dx\u001b\n",
      "(9.10)\n",
      "so in particular the reverse cdf (9.7) is given by\n",
      "S.t/Dexp\u001a\n",
      "\u0000Zt\n",
      "0h.x/dx\u001b\n",
      ": (9.11)\n",
      "A one-sided exponential density\n",
      "f.t/D.1=c/e\u0000t=cfort\u00150 (9.12)\n",
      "hasS.t/Dexpf\u0000t=cgand constant hazard rate\n",
      "h.t/D1=c: (9.13)\n",
      "The name ‚Äúmemoryless‚Äù is quite appropriate for density (9.12): having\n",
      "survived to any time t, the probability of surviving dtunits more is always\n",
      "the same, about 1\u0000dt=c , no matter what tis. If human lifetimes were\n",
      "exponential there wouldn‚Äôt be old or young people, only lucky or unlucky\n",
      "ones.\n",
      "9.2 Censored Data and the Kaplan‚ÄìMeier Estimate\n",
      "Table 9.2 reports the survival data from a randomized clinical trial run by\n",
      "NCOG (the Northern California Oncology Group) comparing two treat-\n",
      "ments for head and neck cancer: ArmA, chemotherapy, versus ArmB,\n",
      "chemotherapy plus radiation. The response for each patient is survival time\n",
      "in days. TheCsign following some entries indicates censored data , that is,\n",
      "survival times known only to exceed the reported value. These are patients\n",
      "‚Äúlost to followup,‚Äù mostly because the NCOG experiment ended with some\n",
      "of the patients still alive.\n",
      "This is what the experimenters hoped to see of course, but it compli-\n",
      "cates the comparison. Notice that there is more censoring in ArmB. In\n",
      "the absence of censoring we could run a simple two-sample test, maybe\n",
      "Wilcoxon‚Äôs test, to see whether the more aggressive treatment of ArmB\n",
      "was increasing the survival times. Kaplan‚ÄìMeier curves provide a graph-\n",
      "ical comparison that takes proper account of censoring. (The next section\n",
      "describes an appropriate censored data two-sample test.) Kaplan‚ÄìMeier\n",
      "curves have become familiar friends to medical researchers, a lingua franca\n",
      "for reporting clinical trial results.\n",
      "Life table methods are appropriate for censored data. Table 9.3 puts the\n",
      "ArmAresults into the same form as the insurance study of Table 9.1, now9.2 Censored data and Kaplan‚ÄìMeier 135\n",
      "Table 9.2 Censored survival times in days, from two arms of the NCOG\n",
      "study of head/neck cancer.\n",
      "ArmA: Chemotherapy\n",
      "7 34 42 63 64 74+ 83 84 91\n",
      "108 112 129 133 133 139 140 140 146\n",
      "149 154 157 160 160 165 173 176 185+\n",
      "218 225 241 248 273 277 279+ 297 319+\n",
      "405 417 420 440 523 523+ 583 594 1101\n",
      "1116+ 1146 1226+ 1349+ 1412+ 1417\n",
      "ArmB: ChemotherapyCRadiation\n",
      "37 84 92 94 110 112 119 127 130\n",
      "133 140 146 155 159 169+ 173 179 194\n",
      "195 209 249 281 319 339 432 469 519\n",
      "528+ 547+ 613+ 633 725 759+ 817 1092+ 1245+\n",
      "1331+ 1557 1642+ 1771+ 1776 1897+ 2023+ 2146+ 2297+\n",
      "with the time unit being months. Of the 51 patients enrolled4inArmA,\n",
      "y1D1was observed to die in the Ô¨Årst month after treatment; this left 50 at\n",
      "risk,y2D2of whom died in the second month; y3D5of the remaining\n",
      "48 died in their third month after treatment, and one was lost to followup,\n",
      "this being noted in the lcolumn of the table, leaving n4D40patients ‚Äúat\n",
      "risk‚Äù at the beginning of month 5, etc.\n",
      "OShere is calculated as in (9.6) except starting at time 1 instead of 30.\n",
      "There is nothing wrong with this estimate, but binning the NCOG survival\n",
      "data by months is arbitrary. Why not go down to days, as the data was\n",
      "originally presented in Table 9.2? A Kaplan‚ÄìMeier survival curve is the\n",
      "limit of life table survival estimates as the time unit goes to zero.\n",
      "Observations zifor censored data problems are of the form\n",
      "ziD.ti;di/; (9.14)\n",
      "wheretiequals the observed survival time while diindicates whether or\n",
      "not there was censoring,\n",
      "diD(\n",
      "1if death observed\n",
      "0if death not observed(9.15)\n",
      "4The patients were enrolled at different calendar times, as they entered the study, but for\n",
      "each patient ‚Äútime zero‚Äù in the table is set at the beginning of his or her treatment.136 Survival Analysis and the EM Algorithm\n",
      "Table 9.3 ArmAof the NCOG head/neck cancer study, binned by month;\n",
      "nDnumber at risk, yDnumber of deaths, lDlost to followup, hD\n",
      "hazard ratey=n;OSDlife table survival estimate.\n",
      "Monthn y l h OS Monthn y l h OS\n",
      "1 51 1 0 .020 .980 25 7 0 0 .000 .184\n",
      "2 50 2 0 .040 .941 26 7 0 0 .000 .184\n",
      "3 48 5 1 .104 .843 27 7 0 0 .000 .184\n",
      "4 42 2 0 .048 .803 28 7 0 0 .000 .184\n",
      "5 40 8 0 .200 .642 29 7 0 0 .000 .184\n",
      "6 32 7 0 .219 .502 30 7 0 0 .000 .184\n",
      "7 25 0 1 .000 .502 31 7 0 0 .000 .184\n",
      "8 24 3 0 .125 .439 32 7 0 0 .000 .184\n",
      "9 21 2 0 .095 .397 33 7 0 0 .000 .184\n",
      "10 19 2 1 .105 .355 34 7 0 0 .000 .184\n",
      "11 16 0 1 .000 .355 35 7 0 0 .000 .184\n",
      "12 15 0 0 .000 .355 36 7 0 0 .000 .184\n",
      "13 15 0 0 .000 .355 37 7 1 1 .143 .158\n",
      "14 15 3 0 .200 .284 38 5 1 0 .200 .126\n",
      "15 12 1 0 .083 .261 39 4 0 0 .000 .126\n",
      "16 11 0 0 .000 .261 40 4 0 0 .000 .126\n",
      "17 11 0 0 .000 .261 41 4 0 1 .000 .126\n",
      "18 11 1 1 .091 .237 42 3 0 0 .000 .126\n",
      "19 9 0 0 .000 .237 43 3 0 0 .000 .126\n",
      "20 9 2 0 .222 .184 44 3 0 0 .000 .126\n",
      "21 7 0 0 .000 .184 45 3 0 1 .000 .126\n",
      "22 7 0 0 .000 .184 46 2 0 0 .000 .126\n",
      "23 7 0 0 .000 .184 47 2 1 1 .500 .063\n",
      "24 7 0 0 .000 .184\n",
      "(sodiD0corresponds to aCin Table 9.2). Let\n",
      "t.1/<t.2/<t.3/<:::<t.n/ (9.16)\n",
      "denote the ordered survival times,5censored or not, with corresponding\n",
      "indicatord.k/fort.k/. The Kaplan‚ÄìMeier estimate for survival probability\n",
      "S.j/DPrfX >t.j/gis then¬éthe life table estimate ¬é2\n",
      "OS.j/DY\n",
      "k\u0014j\u0012n\u0000k\n",
      "n\u0000kC1\u0013d.k/\n",
      ": (9.17)\n",
      "5Assuming no ties among the survival times, which is convenient but not crucial for what\n",
      "follows.9.2 Censored data and Kaplan‚ÄìMeier 137\n",
      "OSjumps downward at death times tj, and is constant between observed\n",
      "deaths.\n",
      "0 200 400 600 800 1000 1200 14000.00.20.40.60.81.0\n",
      "DaysSurvivalArm A: chemotherapy only\n",
      "Arm B: chemotherapy + radiation\n",
      "Figure 9.1 NCOG Kaplan‚ÄìMeier survival curves; lower ArmA\n",
      "(chemotherapy only); upper ArmB(chemotherapyCradiation).\n",
      "Vertical lines indicate approximate 95% conÔ¨Ådence intervals.\n",
      "The Kaplan‚ÄìMeier curves for both arms of the NCOG study are shown\n",
      "in Figure 9.1. ArmB, the more aggressive treatment, looks better: its 50%\n",
      "survival estimate occurs at 324 days, compared with 182 days for ArmA.\n",
      "The answer to the inferential question‚Äîis Breally better than Aor is this\n",
      "just random variability?‚Äîis less clear-cut.\n",
      "The accuracy ofOS.j/can be estimated from Greenwood‚Äôs formula¬éfor¬é3\n",
      "its standard deviation (now back in life table notation),\n",
      "sd\u0010\n",
      "OS.j/\u0011\n",
      "DOS.j/2\n",
      "4X\n",
      "k\u0014jyk\n",
      "nk.nk\u0000yk/3\n",
      "51=2\n",
      ": (9.18)\n",
      "The vertical bars in Figure 9.1 are approximate 95% conÔ¨Ådence limits for\n",
      "the two curves based on Greenwood‚Äôs formula. They overlap enough to cast\n",
      "doubt on the superiority of ArmBat any one choice of ‚Äúdays,‚Äù but the two-\n",
      "sample test of the next section, which compares survival at all timepoints,\n",
      "will provide more deÔ¨Ånitive evidence.\n",
      "Life tables and the Kaplan‚ÄìMeier estimate seem like a textbook example\n",
      "of frequentist inference as described in Chapter 2: a useful probabilistic138 Survival Analysis and the EM Algorithm\n",
      "result is derived (9.4), and then implemented by the plug-in principle (9.6).\n",
      "There is more to the story though, as discussed below.\n",
      "Life table curves are nonparametric, in the sense that no particular re-\n",
      "lationship is assumed between the hazard rates hi. A parametric approach\n",
      "can greatly improve the curves‚Äô accuracy.¬éReverting to the life table form ¬é4\n",
      "of Table 9.3, we assume that the death counts ykare independent binomi-\n",
      "als,\n",
      "ykind\u0018Bi.nk;hk/; (9.19)\n",
      "and that the logits \u0015kDlogfhk=.1\u0000hk/gsatisfy some sort of regression\n",
      "equation\n",
      "\u0015DXÀõ; (9.20)\n",
      "as in (8.22). A cubic regression for instance would set xkD.1;k;k2;k3/0\n",
      "for thekth row ofX, withX47\u00024for Table 9.3.\n",
      "0 10 20 30 400.00 0.05 0.10 0.15\n",
      "MonthsDeaths per MonthArm A: chemotherapy only\n",
      "Arm B: chemotherapy + radiation\n",
      "Figure 9.2 Parametric hazard rate estimates for the NCOG study.\n",
      "ArmA, black curve, has about 2.5 times higher hazard than\n",
      "ArmBfor all times more than a year after treatment. Standard\n",
      "errors shown at 15 and 30 months.\n",
      "The parametric hazard-rate estimates in Figure 9.2 were instead based\n",
      "on a ‚Äúcubic-linear spline,‚Äù\n",
      "xkD\u0000\n",
      "1;k;.k\u000011/2\n",
      "\u0000;.k\u000011/3\n",
      "\u0000\u00010; (9.21)\n",
      "where.k\u000011/\u0000equalsk\u000011fork\u001411, and 0 fork\u001511. The vector9.3 The Log-Rank Test 139\n",
      "\u0015DXÀõdescribes a curve that is cubic for k\u001411, linear fork\u001511,\n",
      "and joined smoothly at 11. The logistic regression maximum likelihood\n",
      "estimateOÀõproduced hazard rate curves\n",
      "OhkD1.\u0010\n",
      "1Ce\u0000x0\n",
      "kOÀõ\u0011\n",
      "(9.22)\n",
      "as in (8.8). The black curve in Figure 9.2 traces OhkforArmA, while the\n",
      "red curve is that for ArmB, Ô¨Åt separately.\n",
      "Comparison in terms of hazard rates is more informative than the sur-\n",
      "vival curves of Figure 9.1. Both arms show high initial hazards, peaking at\n",
      "Ô¨Åve months, and then a long slow decline.6ArmBhazard is always below\n",
      "ArmA, in a ratio of about 2.5 to 1 after the Ô¨Årst year. Approximate 95%\n",
      "conÔ¨Ådence limits, obtained as in (8.30), don‚Äôt overlap, indicating superior-\n",
      "ity of ArmBat 15 and 30 months after treatment.\n",
      "In addition to its frequentist justiÔ¨Åcation, survival analysis takes us into\n",
      "the Fisherian realm of conditional inference, Section 4.3. The yk‚Äôs in model\n",
      "(9.19) are considered conditionally on thenk‚Äôs, effectively treating the nk\n",
      "values in Table 9.3 as ancillaries , that is as Ô¨Åxed constants, by themselves\n",
      "containing no statistical information about the unknown hazard rates. We\n",
      "will examine this tactic more carefully in the next two sections.\n",
      "9.3 The Log-Rank Test\n",
      "A randomized clinical trial, interpreted by a two-sample test, remains the\n",
      "gold standard of medical experimentation. Interpretation usually involves\n",
      "Student‚Äôs two-sample t-test or its nonparametric cousin Wilcoxon‚Äôs test,\n",
      "but neither of these is suitable for censored data. The log-rank test¬é¬é5\n",
      "employs an ingenious extension of life tables for the nonparametric two-\n",
      "sample comparison of censored survival data.\n",
      "Table 9.4 compares the results of the NCOG study for the Ô¨Årst six months7\n",
      "after treatment. At the beginning8of month 1 there were 45 patients ‚Äúat\n",
      "risk‚Äù in ArmB, none of whom died, compared with 51 at risk and 1 death\n",
      "inArmA. This left 45 at risk in ArmBat the beginning of month 2, and\n",
      "50 in ArmA, with 1 and 2 deaths during the month respectively. (Losses\n",
      "6The cubic‚Äìlinear spline (9.21) is designed to show more detail in the early months,\n",
      "where there is more available patient data and where hazard rates usually change more\n",
      "quickly.\n",
      "7A month is deÔ¨Åned here as 365/12=30.4 days.\n",
      "8The ‚Äúbeginning of month 1‚Äù is each patient‚Äôs initial treatment time, at which all 45\n",
      "patients ever enrolled in ArmBwere at risk, that is, available for observation.140 Survival Analysis and the EM Algorithm\n",
      "Table 9.4 Life table comparison for the Ô¨Årst six months of the NCOG\n",
      "study. For example, at the beginning of the sixth month after treatment,\n",
      "there were 33 remaining ArmBpatients, of whom 4 died during the\n",
      "month, compared with 32 at risk and 7 dying in ArmA. The conditional\n",
      "expected number of deaths in ArmA, assuming the null hypothesis of\n",
      "equal hazard rates in both arms, was 5.42, using expression (9.24) .\n",
      "MonthArmB Arm A Expected number\n",
      "ArmAdeathsAt risk Died At risk Died\n",
      "1 45 0 51 1 .53\n",
      "2 45 1 50 2 1.56\n",
      "3 44 1 48 5 3.13\n",
      "4 43 5 42 2 3.46\n",
      "5 38 5 40 8 6.67\n",
      "6 33 4 32 7 5.42\n",
      "to followup were assumed to occur at the endof each month; there was 1\n",
      "such at the end of month 3, reducing the number at risk in ArmAto 42 for\n",
      "month 4.)\n",
      "The month 6 data is displayed in two-by-two tabular form in Table 9.5,\n",
      "showing the notation used in what follows: nAfor the number at risk in\n",
      "ArmA,ndfor the number of deaths, etc.; yindicates the number of ArmA\n",
      "deaths. If the marginal totals nA;nB;nd;andnsare given, then ydeter-\n",
      "mines the other three table entries by subtraction, so we are not losing any\n",
      "information by focusing on y.\n",
      "Table 9.5 Two-by-two display of month-6 data for the NCOG study.Eis\n",
      "the expected number of ArmAdeaths assuming the null hypothesis of\n",
      "equal hazard rates (last column of Table 9.4).\n",
      "Died Survived\n",
      "ArmAyD725nAD32ED5:42\n",
      "ArmB 4 29nBD33\n",
      "ndD11 nsD54nD65\n",
      "Consider the null hypothesis that the hazard rates (9.3) for month 6 are9.3 The Log-Rank Test 141\n",
      "the same in ArmAandArmB,\n",
      "H0.6/WhA6DhB6: (9.23)\n",
      "UnderH0.6/,yhas meanEand variance V,\n",
      "EDnAnd=n\n",
      "VDnAnBndnsƒ±\u0002\n",
      "n2.n\u00001/\u0003\n",
      ";(9.24)\n",
      "as calculated according to the hypergeometric distribution .¬éED5:42 and¬é6\n",
      "VD2:28 in Table 9.5.\n",
      "We can form a two-by-two table for each of the ND47months of the\n",
      "NCOG study, calculating yi;Ei, andVifor monthi. The log-rank statistic\n",
      "Zis then deÔ¨Åned to be¬é ¬é7\n",
      "ZDNX\n",
      "iD1.yi\u0000Ei/, NX\n",
      "iD1Vi!1=2\n",
      ": (9.25)\n",
      "The idea here is simple but clever. Each month we test the null hypothesis\n",
      "of equal hazard rates\n",
      "H0.i/WhAiDhBi: (9.26)\n",
      "The numerator yi\u0000Eihas expectation 0 under H0.i/, but, ifhAiis greater\n",
      "thanhBi, that is, if treatment B is superior, then the numerator has a pos-\n",
      "itive expectation. Adding up the numerators gives us power to detect a\n",
      "general superiority of treatment B over A, against the null hypothesis of\n",
      "equal hazard rates, hAiDhBifor alli.\n",
      "For the NCOG study, binned by months,\n",
      "NX\n",
      "iD1yiD42;NX\n",
      "iD1EiD32:9;NX\n",
      "iD1ViD16:0; (9.27)\n",
      "giving log-rank test statistic\n",
      "ZD2:27: (9.28)\n",
      "Asymptotic calculations based on the central limit theorem suggest\n",
      "ZP \u0018N.0;1/ (9.29)\n",
      "under the null hypothesis that the two treatments are equally effective, i.e.,\n",
      "thathAiDhBiforiD1;2;:::;N . In the usual interpretation, ZD\n",
      "2:27 is signiÔ¨Åcant at the one-sided 0.012 level, providing moderately strong\n",
      "evidence in favor of treatment B.\n",
      "An impressive amount of inferential guile goes into the log-rank test.142 Survival Analysis and the EM Algorithm\n",
      "1 Working with hazard rates instead of densities or cdfs is essential for\n",
      "survival data.\n",
      "2 Conditioning at each period on the numbers at risk, nAandnBin Ta-\n",
      "ble 9.5, Ô¨Ånesses the difÔ¨Åculties of censored data; censoring only changes\n",
      "the at-risk numbers in future periods.\n",
      "3 Also conditioning on the number of deaths and survivals, ndandns\n",
      "in Table 9.5, leaves only the univariate statisticyto interpret at each\n",
      "period, which is easily done through the null hypothesis of equal hazard\n",
      "rates (9.26).\n",
      "4 Adding the discrepancies yi\u0000Eiin the numerator of (9.25) (rather than\n",
      "say, adding the individual ZvaluesZiD.yi\u0000Ei/=V1=2\n",
      "i, or adding the\n",
      "Z2\n",
      "ivalues) accrues power for the natural alternative hypothesis ‚Äú hAi>\n",
      "hBifor alli,‚Äù while avoiding destabilization from small values of Vi.\n",
      "Each of the four tactics had been used separately in classical applica-\n",
      "tions. Putting them together into the log-rank test was a major inferential\n",
      "accomplishment, foreshadowing a still bigger step forward, the propor-\n",
      "tional hazards model , our subject in the next section.\n",
      "Conditional inference takes on an aggressive form in the log-rank test.\n",
      "LetDiindicate all the data except yiavailable at the end of the ith period.\n",
      "For month 6 in the NCOG study,D6includes all data for months 1‚Äì5 in\n",
      "Table 9.4, and the marginals nA;nB;nd;andnsin Table 9.5, but not the y\n",
      "value for month 6. The key assumption is that, under the null hypothesis of\n",
      "equal hazard rates (9.26),\n",
      "yijDiind\u0018.Ei;Vi/; (9.30)\n",
      "‚Äúind‚Äù here meaning that the yi‚Äôs can be treated as independent quantities\n",
      "with means and variances (9.24). In particular, we can add the variances\n",
      "Vito get the denominator of (9.25). (A ‚Äúpartial likelihood‚Äù argument, de-\n",
      "scribed in the endnotes, justiÔ¨Åes adding the variances.)\n",
      "The purpose of all this Fisherian conditioning is to simplify the infer-\n",
      "ence: the conditional distribution yijDidepends only on the hazard rates\n",
      "hAiandhBi; ‚Äúnuisance parameters,‚Äù relating to the survival times and cen-\n",
      "soring mechanism of the data in Table 9.2, are hidden away. There is a price\n",
      "to pay in testing power, though usually a small one. The lost-to-followup\n",
      "valueslin Table 9.3 have been ignored, even though they might contain\n",
      "useful information, say if all the early losses occurred in one arm.9.4 The Proportional Hazards Model 143\n",
      "9.4 The Proportional Hazards Model\n",
      "The Kaplan‚ÄìMeier estimator is a one-sample device, dealing with data\n",
      "coming from a single distribution. The log-rank test makes two-sample\n",
      "comparisons. Proportional hazards ups the ante to allow for a full regres-\n",
      "sion analysis of censored data. Now the individual data points ziare of the\n",
      "form\n",
      "ziD.ci;ti;di/; (9.31)\n",
      "wheretianddiare observed survival time and censoring indicator, as in\n",
      "(9.14)‚Äì(9.15), and ciis a known1\u0002pvector of covariates whose effect\n",
      "on survival we wish to assess. Both of the previous methods are included\n",
      "here: for the log-rank test, ciindicates treatment, say ciequals 0 or 1 for\n",
      "ArmAorArmB, whileciis absent for Kaplan‚ÄìMeier.\n",
      "Table 9.6 Pediatric cancer data, Ô¨Årst 20 of 1620 children. Sex 1Dmale,\n",
      "2Dfemale; race 1Dwhite, 2Dnonwhite; age in years; entryD\n",
      "calendar date of entry in days since July 1, 2001; farDhome distance\n",
      "from treatment center in miles; tDsurvival time in days; dD1if death\n",
      "observed, 0 if not.\n",
      "sex race age entry far t d\n",
      "1 1 2.50 710 108 325 0\n",
      "2 1 10.00 1866 38 1451 0\n",
      "2 2 18.17 2531 100 221 0\n",
      "2 1 3.92 2210 100 2158 0\n",
      "1 1 11.83 875 78 760 0\n",
      "2 1 11.17 1419 0 168 0\n",
      "2 1 5.17 1264 28 2976 0\n",
      "2 1 10.58 670 120 1833 0\n",
      "1 1 1.17 1518 73 131 0\n",
      "2 1 6.83 2101 104 2405 0\n",
      "1 1 13.92 1239 0 969 0\n",
      "1 1 5.17 518 117 1894 0\n",
      "1 1 2.50 1849 99 193 1\n",
      "1 1 .83 2758 38 1756 0\n",
      "2 1 15.50 2004 12 682 0\n",
      "1 1 17.83 986 65 1835 0\n",
      "2 1 3.25 1443 58 2993 0\n",
      "1 1 10.75 2807 42 1616 0\n",
      "1 2 18.08 1229 23 1302 0\n",
      "2 2 5.83 2727 23 174 1144 Survival Analysis and the EM Algorithm\n",
      "Medical studies regularly produce data of form (9.31). An example, the\n",
      "pediatric cancer data, is partially listed in Table 9.6. The Ô¨Årst 20 of nD\n",
      "1620 cases are shown. There are Ô¨Åve explanatory covariates (deÔ¨Åned in the\n",
      "table‚Äôs caption): sex,race ,age at entry, calendar date of entry into\n",
      "the study, and far, the distance of the child‚Äôs home from the treatment\n",
      "center. The response variable tis survival in days from time of treatment\n",
      "until death. Happily, only 160 of the children were observed to die ( dD\n",
      "1). Some left the study for various reasons, but most of the dD0cases\n",
      "were those children still alive at the end of the study period. Of particular\n",
      "interest was the effect of far on survival. We wish to carry out a regression\n",
      "analysis of this heavily censored data set.\n",
      "The proportional hazards model assumes that the hazard rate hi.t/for\n",
      "theith individual (9.8) is\n",
      "hi.t/Dh0.t/ec0\n",
      "iÀá: (9.32)\n",
      "Hereh0.t/is a baseline hazard (which we need not specify) and Àáis an\n",
      "unknownp-parameter vector we want to estimate. For concise notation,\n",
      "let\n",
      "\u0012iDec0\n",
      "iÀáI (9.33)\n",
      "model (9.32) says that individual i‚Äôs hazard is a constant nonnegative factor\n",
      "\u0012itimes the baseline hazard. Equivalently, from (9.11), the ith survival\n",
      "functionSi.t/is a power of the baseline survival function S0.t/,\n",
      "Si.t/DS0.t/\u0012i: (9.34)\n",
      "Larger values of \u0012ilead to more quickly declining survival curves, i.e., to\n",
      "worse survival (as in (9.11)).\n",
      "LetJbe the number of observed deaths, JD160here, occurring at\n",
      "times\n",
      "T.1/<T.2/<:::<T .J/; (9.35)\n",
      "again for convenience assuming no ties.9Just before time T.j/there is a\n",
      "risk set of individuals still under observation, whose indices we denote by\n",
      "Rj,\n",
      "RjDfiWti\u0015T.j/g: (9.36)\n",
      "Letijbe the index of the individual observed to die at time T.j/. The\n",
      "key to proportional hazards regression is the following result.\n",
      "9More precisely, assuming only one event, a death, occurred at T.j/, with none of the\n",
      "other individuals being lost to followup at exact time T.j/.9.4 The Proportional Hazards Model 145\n",
      "Lemma¬éUnder the proportional hazards model (9.32) , the conditional ¬é8\n",
      "probability, given the risk set Rj, that individual iinRjis the one ob-\n",
      "served to die at time T.j/is\n",
      "PrfijDijRjgDec0\n",
      "iÀá\u001eX\n",
      "k2Rjec0\n",
      "kÀá: (9.37)\n",
      "To put it in words, given that one person dies at time T.j/, the probability\n",
      "it is individual iis proportional to exp.c0\n",
      "iÀá/, among the set of individuals\n",
      "at risk.\n",
      "For the purpose of estimating the parameter vector Àáin model (9.32),\n",
      "we multiply factors (9.37) to form the partial likelihood\n",
      "L.Àá/DJY\n",
      "jD10\n",
      "@ec0\n",
      "ijÀá\u001eX\n",
      "k2Rjec0\n",
      "kÀá1\n",
      "A: (9.38)\n",
      "L.Àá/ is then treated as an ordinary likelihood function, yielding an approx-\n",
      "imately unbiased MLE-like estimate\n",
      "OÀáDarg max\n",
      "ÀáfL.Àá/g; (9.39)\n",
      "with an approximate covariance obtained from the second-derivative ma-\n",
      "trix ofl.Àá/DlogL.Àá/ ,¬éas in Section 4.3, ¬é9\n",
      "OÀáP \u0018\u0012\n",
      "Àá;h\n",
      "\u0000Rl\u0010\n",
      "OÀá\u0011i\u00001\u0013\n",
      ": (9.40)\n",
      "Table 9.7 shows the proportional hazards analysis of the pediatric can-\n",
      "cer data, with the covariates age,entry , and far standardized to have\n",
      "mean 0 and standard deviation 1 for the 1620 cases.10Neither sex nor\n",
      "race seems to make much difference. We see that age is a mildly signif-\n",
      "icant factor, with older children doing better (i.e., the estimated regression\n",
      "coefÔ¨Åcient is negative). However, the dramatic effects are date of entry\n",
      "andfar. Individuals who entered the study later survived longer‚Äîperhaps\n",
      "the treatment protocol was being improved‚Äîwhile children living farther\n",
      "away from the treatment center did worse.\n",
      "JustiÔ¨Åcation of the partial likelihood calculations is similar to that for\n",
      "the log-rank test, but there are some important differences, too: the pro-\n",
      "portional hazards model is semiparametric (‚Äúsemi‚Äù because we don‚Äôt have\n",
      "to specifyh0.t/in (9.32)), rather than nonparametric as before; and the\n",
      "10Table 9.7 was obtained using the Rprogram coxph .146 Survival Analysis and the EM Algorithm\n",
      "Table 9.7 Proportional hazards analysis of pediatric cancer data ( age,\n",
      "entry andfar standardized). Age signiÔ¨Åcantly negative, older children\n",
      "doing better; entry very signiÔ¨Åcantly negative, showing hazard rate\n",
      "declining with calendar date of entry; far very signiÔ¨Åcantly positive,\n",
      "indicating worse results for children living farther away from the\n",
      "treatment center. Last two columns show limits of approximate 95%\n",
      "conÔ¨Ådence intervals for exp.Àá/.\n",
      "Àá sdz-valuep-value exp.Àá/ Lower Upper\n",
      "sex\u0000.023 .160\u0000.142 .887 .98 .71 1.34\n",
      "race .282 .169 1.669 .095 1.33 .95 1.85\n",
      "age\u0000.235 .088\u00002.664 .008 .79 .67 .94\n",
      "entry\u0000.460 .079\u00005.855 .000 .63 .54 .74\n",
      "far .296 .072 4.117 .000 1.34 1.17 1.55\n",
      "emphasis on likelihood has increased the Fisherian nature of the inference,\n",
      "moving it further away from pure frequentism. Still more Fisherian is the\n",
      "emphasis on likelihood inference in (9.38)‚Äì(9.40), rather than the direct\n",
      "frequentist calculations of (9.24)‚Äì(9.25).\n",
      "The conditioning argument here is less obvious than that for the Kaplan‚Äì\n",
      "Meier estimate or the log-rank test. Has its convenience possibly come at\n",
      "too high a price? In fact it can be shown that inference based on the partial\n",
      "likelihood is highly efÔ¨Åcient, assuming of course the correctness of the\n",
      "proportional hazards model (9.32).\n",
      "9.5 Missing Data and the EM Algorithm\n",
      "Censored data, the motivating factor for survival analysis, can be thought\n",
      "of as a special case of a more general statistical topic, missing data . What‚Äôs\n",
      "missing, in Table 9.2 for example, are the actual survival times for the\n",
      "Ccases, which are known only to exceed the tabled values. If the data\n",
      "were notmissing, we could use standard statistical methods, for instance\n",
      "Wilcoxon‚Äôs test, to compare the two arms of the NCOG study. The EM algo-\n",
      "rithm is an iterative technique for solving missing-data inferential problems\n",
      "using only standard methods.\n",
      "A missing-data situation is shown in Figure 9.3: nD40points have\n",
      "been independently sampled from a bivariate normal distribution (5.12),9.5 Missing Data and the EM Algorithm 147\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè ‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè\n",
      "0 1 2 3 4 5‚àí0.5 0.00.51.01.52.0\n",
      "x1x2\n",
      "‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè\n",
      "Figure 9.3 Forty points from a bivariate normal distribution, the\n",
      "last 20 withx2missing (circled).\n",
      "means.\u00161;\u00162/, variances.\u001b2\n",
      "1;\u001b2\n",
      "2/, and correlation \u001a,\n",
      " \n",
      "x1i\n",
      "x2i!\n",
      "ind\u0018N2  \n",
      "\u00161\n",
      "\u00162!\n",
      ";\u0012\u001b2\n",
      "1\u001b1\u001b2\u001a\n",
      "\u001b1\u001b2\u001a \u001b2\n",
      "2\u0013!\n",
      ": (9.41)\n",
      "However, the second coordinates of the last 20 points have been lost. These\n",
      "are represented by the circled points in Figure 9.3, with their x2values\n",
      "arbitrarily set to 0.\n",
      "We wish to Ô¨Ånd the maximum likelihood estimate of the parameter vec-\n",
      "tor\u0012D.\u00161;\u00162,\u001b1;\u001b2;\u001a/. The standard maximum likelihood estimates\n",
      "O\u00161D40X\n",
      "iD1x1i=40;O\u00162D40X\n",
      "iD1x2i=40;\n",
      "O\u001b1D\"40X\n",
      "iD1.x1i\u0000O\u00161/2=40#1=2\n",
      ";O\u001b2D\"40X\n",
      "iD1.x2i\u0000O\u00162/2=40#1=2\n",
      ";\n",
      "O\u001aD\"40X\n",
      "iD1.x1i\u0000O\u00161/.x2i\u0000O\u00162/=40#,\n",
      ".O\u001b1O\u001b2/;\n",
      "(9.42)148 Survival Analysis and the EM Algorithm\n",
      "are unavailable for \u00162,\u001b2, and\u001abecause of the missing data.\n",
      "The EM algorithm begins by Ô¨Ålling in the missing data in some way, say\n",
      "by settingx2iD0for the 20 missing values, giving an artiÔ¨Åcially complete\n",
      "data set data.0/. Then it proceeds as follows.\n",
      "\u000fThe standard method (9.42) is applied to the Ô¨Ålled-in data.0/to produce\n",
      "O\u0012.0/D.O\u0016.0/\n",
      "1;O\u0016.0/\n",
      "2;O\u001b.0/\n",
      "1;O\u001b.0/\n",
      "2;O\u001a.0//; this is the M (‚Äúmaximizing‚Äù) step.11\n",
      "\u000fEach of the missing values is replaced by its conditional expectation\n",
      "(assuming\u0012DO\u0012.0/) given the nonmissing data; this is the E (‚Äúexpecta-\n",
      "tion‚Äù) step. In our case the missing values x2iare replaced by\n",
      "O\u0016.0/\n",
      "2CO\u001a.0/O\u001b.0/\n",
      "2\n",
      "O\u001b.0/\n",
      "1\u0010\n",
      "x1i\u0000O\u0016.0/\n",
      "1\u0011\n",
      ": (9.43)\n",
      "\u000fThe E and M steps are repeated, at the jth stage giving a new artiÔ¨Åcially\n",
      "complete data set data.j/and an updated estimate O\u0012.j/. The iteration\n",
      "stops whenkO\u0012.jC1/\u0000O\u0012.j/kis suitably small.\n",
      "Table 9.8 shows the EM algorithm at work on the bivariate normal ex-\n",
      "ample of Figure 9.3. In exponential families the algorithm is guaranteed to\n",
      "converge to the MLE O\u0012based on just the observed data o; moreover, the\n",
      "likelihoodfO\u0012.j/.o/increases with every step j. (The convergence can be\n",
      "sluggish, as it is here for O\u001b2andO\u001a.)\n",
      "The EM algorithm ultimately derives from the fake-data principle , a\n",
      "property of maximum likelihood estimation going back to Fisher that can\n",
      "only brieÔ¨Çy be summarized here.¬éLetxD.o;u/represent the ‚Äúcomplete ¬é10\n",
      "data,‚Äù of which ois observed while uis unobserved or missing. Write the\n",
      "density forxas\n",
      "f\u0012.x/Df\u0012.o/f\u0012.ujo/; (9.44)\n",
      "and letO\u0012.o/be the MLE of \u0012based just ono.\n",
      "Suppose we now generate simulations of uby sampling from the condi-\n",
      "tional distribution fO\u0012.o/.ujo/,\n",
      "u\u0003k\u0018fO\u0012.o/.ujo/ forkD1;2;:::;K (9.45)\n",
      "(the stars indicating creation by the statistician and not by observation),\n",
      "giving fake complete-data values x\u0003kD.o;u\u0003k/. Let\n",
      "data\u0003Dfx\u00031;x\u00032;:::;x\u0003Kg; (9.46)\n",
      "11In this example,O\u0016.0/\n",
      "1andO\u001b.0/\n",
      "1are available as the complete-data estimates in (9.42),\n",
      "and, as in Table 9.8, stay the same in subsequent steps of the algorithm.9.5 Missing Data and the EM Algorithm 149\n",
      "Table 9.8 EM algorithm for estimating means, standard deviations, and\n",
      "the correlation of the bivariate normal distribution that gave the data in\n",
      "Figure 9.3.\n",
      "Step\u00161\u00162\u001b1\u001b2\u001a\n",
      "1 1.86 .463 1.08 .738 .162\n",
      "2 1.86 .707 1.08 .622 .394\n",
      "3 1.86 .843 1.08 .611 .574\n",
      "4 1.86 .923 1.08 .636 .679\n",
      "5 1.86 .971 1.08 .667 .736\n",
      "6 1.86 1.002 1.08 .694 .769\n",
      "7 1.86 1.023 1.08 .716 .789\n",
      "8 1.86 1.036 1.08 .731 .801\n",
      "9 1.86 1.045 1.08 .743 .808\n",
      "10 1.86 1.051 1.08 .751 .813\n",
      "11 1.86 1.055 1.08 .756 .816\n",
      "12 1.86 1.058 1.08 .760 .819\n",
      "13 1.86 1.060 1.08 .763 .820\n",
      "14 1.86 1.061 1.08 .765 .821\n",
      "15 1.86 1.062 1.08 .766 .822\n",
      "16 1.86 1.063 1.08 .767 .822\n",
      "17 1.86 1.064 1.08 .768 .823\n",
      "18 1.86 1.064 1.08 .768 .823\n",
      "19 1.86 1.064 1.08 .769 .823\n",
      "20 1.86 1.064 1.08 .769 .823\n",
      "whose notional likelihoodQK\n",
      "1f\u0012.x\u0003k/yields MLEO\u0012\u0003. It then turns out\n",
      "thatO\u0012\u0003goes toO\u0012.o/asKgoes to inÔ¨Ånity. In other words, maximum likeli-\n",
      "hood estimation is self-consistent : generating artiÔ¨Åcial data from the MLE\n",
      "densityfO\u0012.o/.ujo/doesn‚Äôt change the MLE. Moreover, any value O\u0012.0/not\n",
      "equal to the MLEO\u0012.o/cannot be self-consistent: carrying through (9.45)‚Äì\n",
      "(9.46) usingfO\u0012.0/.ujo/leads to hypothetical MLE O\u0012.1/havingfO\u0012.1/.o/ >\n",
      "fO\u0012.0/.o/, etc., a more general version of the EM algorithm.12\n",
      "Modern technology allows social scientists to collect huge data sets,\n",
      "perhaps hundreds of responses for each of thousands or even millions of\n",
      "individuals. Inevitably, some entries of the individual responses will be\n",
      "missing. Imputation amounts to employing some version of the fake-data\n",
      "principle to Ô¨Åll in the missing values. Imputation‚Äôs goal goes beyond Ô¨Ånd-\n",
      "12Simulation (9.45) is unnecessary in exponential families, where at each stage data\u0003can\n",
      "be replaced by .o;E.j/.ujo//, withE.j/indicating expectation with respect to O\u0012.j/,\n",
      "as in (9.43).150 Survival Analysis and the EM Algorithm\n",
      "ing the MLE, to the creation of graphs, conÔ¨Ådence intervals, histograms,\n",
      "and more, using only convenient, standard complete-data methods.\n",
      "Finally, returning to survival analysis, the Kaplan‚ÄìMeier estimate (9.17)\n",
      "is itself self-consistent.¬éConsider the ArmAcensored observation 74C ¬é11\n",
      "in Table 9.2. We know that that patient‚Äôs survival time exceeded 74. Sup-\n",
      "pose we distribute his probability mass ( 1=51 of the ArmAsample) to the\n",
      "right, in accordance with the conditional distribution for x > 74 deÔ¨Åned\n",
      "by the ArmAKaplan‚ÄìMeier survival curve. It turns out that redistributing\n",
      "all the censored cases does not change the original Kaplan‚ÄìMeier survival\n",
      "curve; Kaplan‚ÄìMeier is self-consistent, leading to its identiÔ¨Åcation as the\n",
      "‚Äúnonparametric MLE‚Äù of a survival function.\n",
      "9.6 Notes and Details\n",
      "The progression from life tables, Kaplan‚ÄìMeier curves, and the log-rank\n",
      "test to proportional hazards regression was modest in its computational\n",
      "demands, until the Ô¨Ånal step. Kaplan‚ÄìMeier curves lie within the capabil-\n",
      "ities of mechanical calculators. Not so for proportional hazards, which is\n",
      "emphatically a child of the computer age. As the algorithms grew more in-\n",
      "tricate, their inferential justiÔ¨Åcation deepened in scope and sophistication.\n",
      "This is a pattern we also saw in Chapter 8, in the progression from bioassay\n",
      "to logistic regression to generalized linear models, and will reappear as we\n",
      "move from the jackknife to the bootstrap in Chapter 10.\n",
      "Censoring is not the same as truncation. For the truncated galaxy data\n",
      "of Section 8.3, we learn of the existence of a galaxy only if it falls into\n",
      "the observation region (8.38). The censored individuals in Table 9.2 are\n",
      "known to exist, but with imperfect knowledge of their lifetimes. There is a\n",
      "version of the Kaplan‚ÄìMeier curve applying to truncated data, which was\n",
      "developed in the astronomy literature by Lynden-Bell (1971).\n",
      "The methods of this chapter apply to data that is left-truncated as well\n",
      "as right-censored. In a survival time study of a new HIV drug, for instance,\n",
      "subjectimight not enter the study until some time \u001ciafter his or her initial\n",
      "diagnosis, in which case tiwould be left-truncated at \u001ci, as well as possibly\n",
      "later right-censored. This only modiÔ¨Åes the composition of the various risk\n",
      "sets. However, other missing-data situations, e.g., left- andright-censoring,\n",
      "require more elaborate, less elegant, treatments.\n",
      "¬é1[p. 133] Formula (9.10) .Let the interval ≈ít0;t1¬çbe partitioned into a large\n",
      "number of subintervals of length dt, withtkthe midpoint of subinterval k.9.6 Notes and Details 151\n",
      "As in (9.4), using (9.9),\n",
      "PrfT\u0015t1jT\u0015t0g:DY\n",
      ".1\u0000h.ti/dt/\n",
      "DexpnX\n",
      "log.1\u0000h.ti/dt/o\n",
      ":Dexpn\n",
      "\u0000X\n",
      "h.ti/dto\n",
      ";(9.47)\n",
      "which, asdt!0, goes to (9.10).\n",
      "¬é2[p. 136] Kaplan‚ÄìMeier estimate. In the life table formula (9.6) (with kD\n",
      "1), let the time unit be small enough to make each bin contain at most one\n",
      "valuet.k/(9.16). Then at t.k/,\n",
      "Oh.k/Dd.k/\n",
      "n\u0000kC1; (9.48)\n",
      "giving expression (9.17).\n",
      "¬é3[p. 137] Greenwood‚Äôs formula (9.18) .In the life table formulation of Sec-\n",
      "tion 9.1, (9.6) gives\n",
      "logOSjDjX\n",
      "1log\u0010\n",
      "1\u0000Ohk\u0011\n",
      ": (9.49)\n",
      "FromnkOhkind\u0018Bi.nk;hk/we get\n",
      "varn\n",
      "logOSjo\n",
      "DjX\n",
      "1varn\n",
      "log\u0010\n",
      "1\u0000Ohk\u0011o:DjX\n",
      "1varOhk\n",
      ".1\u0000hk/2\n",
      "DjX\n",
      "1hk\n",
      "1\u0000hk1\n",
      "nk;(9.50)\n",
      "where we have used the delta-method approximation var flogXg:DvarfXg=\n",
      "EfXg2. Plugging in hkDyk=nkyields\n",
      "varn\n",
      "logOSjo:DjX\n",
      "1yk\n",
      "nk.nk\u0000yk/: (9.51)\n",
      "Then the inverse approximation var fXgDEfXg2varflogXggives Green-\n",
      "wood‚Äôs formula (9.18).\n",
      "The censored data situation of Section 9.2 does not enjoy independence\n",
      "between theOhkvalues. However, successive conditional independence, given\n",
      "thenkvalues, is enough to verify the result, as in the partial likelihood cal-\n",
      "culations below. Note : the conÔ¨Ådence intervals in Figure 9.1 were obtained152 Survival Analysis and the EM Algorithm\n",
      "by exponentiating the intervals,\n",
      "logOSjÀô1:96h\n",
      "varn\n",
      "logOSjoi1=2\n",
      ": (9.52)\n",
      "¬é4[p. 138] Parametric life tables analysis. Figure 9.2 and the analysis behind\n",
      "it is developed in Efron (1988), where it is called ‚Äúpartial logistic regres-\n",
      "sion‚Äù in analogy with partial likelihood.\n",
      "¬é5[p. 139] The log-rank test. This chapter featured an all-star cast, includ-\n",
      "ing four of the most referenced papers of the post-war era: Kaplan and\n",
      "Meier (1958), Cox (1972) on proportional hazards, Dempster et al. (1977)\n",
      "codifying and naming the EM algorithm, and Mantel and Haenszel (1959)\n",
      "on the log-rank test. (Cox (1958) gives a careful, and early, analysis of\n",
      "the Mantel‚ÄìHaenszel idea.) The not very helpful name ‚Äúlog-rank‚Äù does\n",
      "at least remind us that the test depends only on the ranks of the survival\n",
      "times, and will give the same result if all the observed survival times tiare\n",
      "monotonically transformed, say to exp .ti/ort1=2\n",
      "i. It is often referred to as\n",
      "the Mantel‚ÄìHaenszel or Cochran‚ÄìMantel‚ÄìHaenszel test in older literature.\n",
      "Kaplan‚ÄìMeier and proportional hazards are also rank-based procedures.\n",
      "¬é6[p. 141] Hypergeometric distribution. Hypergeometric calculations, as for\n",
      "Table 9.5, are often stated as follows: nmarbles are placed in an urn, nA\n",
      "labeled A and nBlabeled B;ndmarbles are drawn out at random; yis\n",
      "the number of these labeled A. Elementary (but not simple) calculations\n",
      "then produce the conditional distribution of ygiven the table‚Äôs marginals\n",
      "nA;nB;n;nd;andns,\n",
      "PrfyjmarginalsgD \n",
      "nA\n",
      "y! \n",
      "nB\n",
      "nd\u0000y!\u001e \n",
      "n\n",
      "nd!\n",
      "(9.53)\n",
      "for\n",
      "max.nA\u0000ns;0/\u0014y\u0014min.nd;nA/;\n",
      "and expressions (9.24) for the mean and variance. If nAandnBgo to inÔ¨Ån-\n",
      "ity such thatnA=n!pAandnB=n!1\u0000pA, thenV!ndpA.1\u0000pA/,\n",
      "the variance of y\u0018Bi.nd;pA/.\n",
      "¬é7[p. 141] Log-rank statistic Z(9.25) .Why is.PN\n",
      "1Vi/1=2the correct de-\n",
      "nominator for Z? LetuiDyi\u0000Eiin (9.30), soZ‚Äôs numerator isPN\n",
      "1ui,\n",
      "with\n",
      "uijDi\u0018.0;Vi/ (9.54)\n",
      "under the null hypothesis of equal hazard rates. This implies that, uncon-\n",
      "ditionally,EfuigD0. Forj < i ,ujis a function of Di(sinceyjand9.6 Notes and Details 153\n",
      "Ejare), soEfujuijDigD0, and, again unconditionally, EfujuigD0.\n",
      "Therefore, assuming equal hazard rates,\n",
      "E NX\n",
      "1ui!2\n",
      "DE NX\n",
      "1u2\n",
      "i!\n",
      "DNX\n",
      "1varfuig\n",
      ":DNX\n",
      "1Vi:(9.55)\n",
      "The last approximation, replacing unconditional variances var fuigwith\n",
      "conditional variances Vi, is justiÔ¨Åed in Crowley (1974), as is the asymp-\n",
      "totic normality (9.29).\n",
      "¬é8[p. 145] Lemma (9.37) .Fori2Rj, the probability pithat death occurs in\n",
      "the inÔ¨Ånitesimal interval .T.j/;T.j/CdT/ ishi.T.j//dT , so\n",
      "piDh0.T.j//ec0\n",
      "iÀádT; (9.56)\n",
      "and the probability of event Aithat individual idies while the others don‚Äôt\n",
      "is\n",
      "PiDpiY\n",
      "k2Rj\u0000i.1\u0000pk/: (9.57)\n",
      "But theAiare disjoint events, so, given that [Aihas occurred, the proba-\n",
      "bility that it is individual iwho died is\n",
      "Pi\u001eX\n",
      "RjPj:DeciÀá\u001eX\n",
      "k2RjeckÀá; (9.58)\n",
      "this becoming exactly (9.37) as dT!0.\n",
      "¬é9[p. 145] Partial likelihood (9.40) .Cox (1975) introduced partial likelihood\n",
      "as inferential justiÔ¨Åcation for the proportional hazards model, which had\n",
      "been questioned in the literature. Let Djindicate all the observable infor-\n",
      "mation available just before time T.j/(9.35), including all the death or loss\n",
      "times for individuals having ti<T.j/. (Notice thatDjdetermines the risk\n",
      "setRj.) By successive conditioning we write the full likelihood f\u0012.data/\n",
      "as\n",
      "f\u0012.data/Df\u0012.D1/f\u0012.i1jR1/f\u0012.D2jD1/f\u0012.i2jR2/:::\n",
      "DJY\n",
      "jD1f\u0012.DjjDj\u00001/JY\n",
      "jD1f\u0012.ijjRj/:(9.59)\n",
      "Letting\u0012D.Àõ;Àá/ , whereÀõis a nuisance parameter vector having to do154 Survival Analysis and the EM Algorithm\n",
      "with the occurrence and timing of events between observed deaths,\n",
      "fÀõ;Àá.data/D2\n",
      "4JY\n",
      "jD1fÀõ;Àá.DjjDj\u00001/3\n",
      "5L.Àá/; (9.60)\n",
      "whereL.Àá/ is the partial likelihood (9.38).\n",
      "The proportional hazards model simply ignores the bracketed factor in\n",
      "(9.60);l.Àá/DlogL.Àá/ is treated as a genuine likelihood, maximized to\n",
      "giveOÀá, and assigned covariance matrix .\u0000Rl.OÀá//\u00001as in Section 4.3. Efron\n",
      "(1977) shows this tactic is highly efÔ¨Åcient for the estimation of Àá.\n",
      "¬é10[p. 148] Fake-data principle. For any two values of the parameters \u00121and\n",
      "\u00122deÔ¨Åne\n",
      "l\u00121.\u00122/DZ\n",
      "≈ílogf\u00122.o;u/¬çf\u00121.ujo/du; (9.61)\n",
      "this being the limit as K!1 of\n",
      "l\u00121.\u00122/Dlim\n",
      "K!11\n",
      "KKX\n",
      "kD1logf\u00122.o;u\u0003k/; (9.62)\n",
      "the fake-data log likelihood (9.46) under \u00122, if\u00121were the true value of \u0012.\n",
      "Usingf\u0012.o;u/Df\u0012.o/f\u0012.ujo/, deÔ¨Ånition (9.61) gives\n",
      "l\u00121.\u00122/\u0000l\u00121.\u00121/Dlog\u0012f\u00122.o/\n",
      "f\u00121.o/\u0013\n",
      "CZ\n",
      "log\u0012f\u00122.ujo/\n",
      "f\u00121.ujo/\u0013\n",
      "f\u00121.ujo/\n",
      "Dlog\u0012f\u00122.o/\n",
      "f\u00121.o/\u0013\n",
      "\u00001\n",
      "2D.f\u00121.ujo/;f\u00122.ujo//;(9.63)\n",
      "withDthe deviance (8.31), which is always positive unless ujohas the\n",
      "same distribution under \u00121and\u00122, which we will assume doesn‚Äôt happen.\n",
      "Suppose we begin the EM algorithm at \u0012D\u00121and Ô¨Ånd the value \u00122\n",
      "maximizingl\u00121.\u0012/. Thenl\u00121.\u00122/ > l\u00121.\u00121/andD > 0 impliesf\u00122.o/ >\n",
      "f\u00121.o/in (9.63); that is, we have increased the likelihood of the observed\n",
      "data. Now take \u00121DO\u0012Darg max\u0012f\u0012.o/. Then the right side of (9.63) is\n",
      "negative, implying lO\u0012.O\u0012/>lO\u0012.\u00122/for any\u00122not equaling\u00121DO\u0012. Putting\n",
      "this together,13successively computing \u00121;\u00122;\u00123;::: by fake-data MLE\n",
      "calculations increases f\u0012.o/at every step, and the only stable point of the\n",
      "algorithm is at \u0012DO\u0012.o/.\n",
      "¬é11[p. 150] Kaplan‚ÄìMeier self-consistency. This property was veriÔ¨Åed in Efron\n",
      "(1967), where the name was coined.\n",
      "13Generating the fake data is equivalent to the E step of the algorithm, the M step being\n",
      "the maximization of l\u0012j.\u0012/.10\n",
      "The Jackknife and the Bootstrap\n",
      "A central element of frequentist inference is the standard error . An algo-\n",
      "rithm has produced an estimate of a parameter of interest, for instance the\n",
      "meanNxD0:752 for the 47 ALL scores in the top panel of Figure 1.4.\n",
      "How accurate is the estimate? In this case, formula (1.2) for the standard\n",
      "deviation1of a sample mean gives estimated standard error\n",
      "bseD0:040; (10.1)\n",
      "so one can‚Äôt take the third digit of NxD0:752 very seriously, and even the\n",
      "5 is dubious.\n",
      "Direct standard error formulas like (1.2) exist for various forms of aver-\n",
      "aging, such as linear regression (7.34), and for hardly anything else. Tay-\n",
      "lor series approximations (‚Äúdevice 2‚Äù of Section 2.1) extend the formulas\n",
      "to smooth functions of averages, as in (8.30). Before computers, applied\n",
      "statisticians needed to be Taylor series experts in laboriously pursuing the\n",
      "accuracy of even moderately complicated statistics.\n",
      "The jackknife (1957) was a Ô¨Årst step toward a computation-based, non-\n",
      "formulaic approach to standard errors. The bootstrap (1979) went further\n",
      "toward automating a wide variety of inferential calculations, including stan-\n",
      "dard errors. Besides sparing statisticians the exhaustion of tedious routine\n",
      "calculations the jackknife and bootstrap opened the door for more com-\n",
      "plicated estimation algorithms, which could be pursued with the assurance\n",
      "that their accuracy would be easily assessed. This chapter focuses on stan-\n",
      "dard errors, with more adventurous bootstrap ideas deferred to Chapter 11.\n",
      "We end with a brief discussion of accuracy estimation for robust statistics.\n",
      "1We will use the terms ‚Äústandard error‚Äù and ‚Äústandard deviation‚Äù interchangeably.\n",
      "155156 The Jackknife and the Bootstrap\n",
      "10.1 The Jackknife Estimate of Standard Error\n",
      "The basic applications of the jackknife apply to one-sample problems , where\n",
      "the statistician has observed an independent and identically distributed (iid)\n",
      "samplexD.x1;x2;:::;xn/0from an unknown probability distribution F\n",
      "on some space X,\n",
      "xiiid\u0018F foriD1;2;:::;n: (10.2)\n",
      "Xcan be anything: the real line, the plane, a function space.2Areal-valued\n",
      "statisticO\u0012has been computed by applying some algorithm s.\u0001/tox,\n",
      "O\u0012Ds.x/; (10.3)\n",
      "and we wish to assign a standard error to O\u0012. That is, we wish to estimate\n",
      "the standard deviation of O\u0012Ds.x/under sampling model (10.2).\n",
      "Letx.i/be the sample with xiremoved,\n",
      "x.i/D.x1;x2;:::;xi\u00001;xiC1;:::;xn/0; (10.4)\n",
      "and denote the corresponding value of the statistic of interest as\n",
      "O\u0012.i/Ds.x.i//: (10.5)\n",
      "Then the jackknife estimate of standard error forO\u0012is\n",
      "bsejackD\"\n",
      "n\u00001\n",
      "nnX\n",
      "1\u0010\n",
      "O\u0012.i/\u0000O\u0012.\u0001/\u00112#1=2\n",
      ";withO\u0012.\u0001/DnX\n",
      "1O\u0012.i/=n: (10.6)\n",
      "In the case whereO\u0012is the meanNxof real values x1;x2;:::;xn(i.e.,X\n",
      "is an interval of the real line), O\u0012.i/is their average excluding xi, which can\n",
      "be expressed as\n",
      "O\u0012.i/D.nNx\u0000xi/=.n\u00001/: (10.7)\n",
      "Equation (10.7) gives O\u0012.\u0001/DNx,O\u0012.i/\u0000O\u0012.\u0001/D.Nx\u0000xi/=.n\u00001/, and\n",
      "bsejackD\"nX\n",
      "iD1.xi\u0000Nx/2=.n.n\u00001//#1=2\n",
      "; (10.8)\n",
      "exactly the same as the classic formula (1.2). This is no coincidence. The\n",
      "fudge factor.n\u00001/=n in deÔ¨Ånition (10.6) was inserted to make bsejackagree\n",
      "with (1.2) whenO\u0012isNx.\n",
      "2IfXis an interval of the real line we might take Fto be the usual cumulative\n",
      "distribution function, but here we will just think of Fas any full description of the\n",
      "probability distribution for an xionX.10.1 The Jackknife Estimate of Standard Error 157\n",
      "The advantage of bsejackis that deÔ¨Ånition (10.6) can be applied in an au-\n",
      "tomatic way to anystatisticO\u0012Ds.x/. All that is needed is an algorithm\n",
      "that computes s.\u0001/for the deleted data sets x.i/. Computer power is being\n",
      "substituted for theoretical Taylor series calculations. Later we will see that\n",
      "the underlying inferential ideas‚Äîplug-in estimation of frequentist standard\n",
      "errors‚Äîhaven‚Äôt changed, only their implementation.\n",
      "As an example, consider the kidney function data set of Section 1.1. Here\n",
      "the data consists of nD157points.xi;yi/, withxDage andyDtot in\n",
      "Figure 1.1. (So the generic xiin (10.2) now represents the pair .xi;yi/, and\n",
      "Fdescribes a distribution in the plane.) Suppose we are interested in the\n",
      "correlation between age and tot, estimated by the usual sample correlation\n",
      "O\u0012Ds.x/,\n",
      "s.x/DnX\n",
      "iD1.xi\u0000Nx/.yi\u0000Ny/,\"nX\n",
      "1.xi\u0000Nx/2nX\n",
      "1.yi\u0000Ny/2#1=2\n",
      ";(10.9)\n",
      "computed to beO\u0012D\u00000:572 for the kidney data.\n",
      "Applying (10.6) gave bsejackD0:058 for the accuracy of O\u0012. Nonpara-\n",
      "metric bootstrap computations, Section 10.2, also gave estimated standard\n",
      "error 0.058. The classic Taylor series formula looks quite formidable in this\n",
      "case,\n",
      "bsetaylorD(O\u00122\n",
      "4n\u0014O\u001640\n",
      "O\u00162\n",
      "20CO\u001604\n",
      "O\u00162\n",
      "02C2O\u001622\n",
      "O\u001620O\u001602C4O\u001622\n",
      "O\u00162\n",
      "11\u00004O\u001631\n",
      "O\u001611O\u001620\u00004O\u001613\n",
      "O\u001611O\u001602\u0015)1=2\n",
      "(10.10)\n",
      "where\n",
      "O\u0016hkDnX\n",
      "iD1.xi\u0000Nx/h.yi\u0000Ny/k=n: (10.11)\n",
      "It gavebseD0:057 .\n",
      "It is worth emphasizing some features of the jackknife formula (10.6).\n",
      "\u000fIt is nonparametric; no special form of the underlying distribution F\n",
      "need be assumed.\n",
      "\u000fIt is completely automatic: a single master algorithm can be written that\n",
      "inputs the data set xand the function s.x/, and outputs bsejack.\n",
      "\u000fThe algorithm works with data sets of size n\u00001, notn. There is a hidden\n",
      "assumption of smooth behavior across sample sizes. This can be worri-\n",
      "some for statistics like the sample median that have a different deÔ¨Ånition\n",
      "for odd and even sample size.158 The Jackknife and the Bootstrap\n",
      "\u000fThe jackknife standard error is upwardly biased as an estimate of the\n",
      "true standard error.¬é ¬é1\n",
      "\u000fThe connection of the jackknife formula (10.6) with Taylor series meth-\n",
      "ods is closer than it appears. We can write\n",
      "bsejackD\u0014Pn\n",
      "1D2\n",
      "i\n",
      "n2\u00151=2\n",
      "; whereDiDO\u0012.i/\u0000O\u0012.\u0001/\n",
      "1=p\n",
      "n.n\u00001/:(10.12)\n",
      "As discussed in Section 10.3, the Diare approximate directional deriva-\n",
      "tives, measures of how fast the statistic s.x/is changing as we decrease\n",
      "the weight on data point xi. So se2\n",
      "jackis proportional to the sum of\n",
      "squared derivatives of s.x/in thencomponent directions. Taylor series\n",
      "expressions such as (10.10) amount to doing the derivatives by formula\n",
      "rather than numerically.\n",
      "‚àí4 ‚àí2 0 2\n",
      "Agetot\n",
      "2025303540455055606570758085 25\n",
      "Figure 10.1 Thelowess curve for the kidney data of\n",
      "Figure 1.2. Vertical bars indicate Àô2standard errors: jackknife\n",
      "(10.6) blue dashed; bootstrap (10.16) red solid. The jackknife\n",
      "greatly overestimates variability at age 25.\n",
      "The principal weakness of the jackknife is its dependence on local deriva-\n",
      "tives. Unsmooth statistics s.x/, such as the kidney data lowess curve in\n",
      "Figure 1.2, can result in erratic behavior for bsejack. Figure 10.1 illustrates\n",
      "the point. The dashed blue vertical bars indicate Àô2jackknife standard er-10.2 The Nonparametric Bootstrap 159\n",
      "rors for the lowess curve evaluated at ages 20;25;:::;85 . For the most\n",
      "part these agree with the dependable bootstrap standard errors, solid red\n",
      "bars, described in Section 10.2. But things go awry at age 25, where the\n",
      "local derivatives greatly overstate the sensitivity of the lowess curve to\n",
      "global changes in the sample x.\n",
      "10.2 The Nonparametric Bootstrap\n",
      "From the point of view of the bootstrap, the jackknife was a halfway house\n",
      "between classical methodology and a full-throated use of electronic com-\n",
      "putation. (The term ‚Äúcomputer-intensive statistics‚Äù was coined to describe\n",
      "the bootstrap.) The frequentist standard error of an estimate O\u0012Ds.x/is,\n",
      "ideally, the standard deviation we would observe by repeatedly sampling\n",
      "new versions of xfromF. This is impossible since Fis unknown. Instead,\n",
      "the bootstrap (‚Äúingenious device‚Äù number 4 in Section 2.1) substitutes an\n",
      "estimateOFforFand then estimates the frequentist standard error by direct\n",
      "simulation, a feasible tactic only since the advent of electronic computa-\n",
      "tion.\n",
      "The bootstrap estimate of standard error for a statistic O\u0012Ds.x/com-\n",
      "puted from a random sample xD.x1;x2;:::;xn/(10.2) begins with the\n",
      "notion of a bootstrap sample\n",
      "x\u0003D.x\u0003\n",
      "1;x\u0003\n",
      "2;:::;x\u0003\n",
      "n/; (10.13)\n",
      "where eachx\u0003\n",
      "iis drawn randomly with equal probability and with replace-\n",
      "ment fromfx1;x2;:::;xng. Each bootstrap sample provides a bootstrap\n",
      "replication of the statistic of interest,3\n",
      "O\u0012\u0003Ds.x\u0003/: (10.14)\n",
      "Some large number Bof bootstrap samples are independently drawn\n",
      "(BD500in Figure 10.1). The corresponding bootstrap replications are\n",
      "calculated, say\n",
      "O\u0012\u0003bDs.x\u0003b/ forbD1;2;:::;B: (10.15)\n",
      "The resulting bootstrap estimate of standard error for O\u0012is the empirical\n",
      "3The star notation x\u0003is intended to avoid confusion with the original data x, which stays\n",
      "Ô¨Åxed in bootstrap computations, and likewise O\u0012\u0003vis-a-visO\u0012.160 The Jackknife and the Bootstrap\n",
      "standard deviation of the O\u0012\u0003bvalues,\n",
      "bsebootD\"BX\n",
      "bD1\u0010\n",
      "O\u0012\u0003b\u0000O\u0012\u0003\u0001\u00112.\n",
      ".B\u00001/#1=2\n",
      ";withO\u0012\u0003\u0001DBX\n",
      "bD1O\u0012\u0003bƒ±\n",
      "B:\n",
      "(10.16)\n",
      "Motivation for bsebootbegins by noting that O\u0012is obtained in two steps:\n",
      "Ô¨Årstxis generated by iid sampling from probability distribution F, and\n",
      "thenO\u0012is calculated from xaccording to algorithm s.\u0001/,\n",
      "Fiid\u0000!xs\u0000!O\u0012: (10.17)\n",
      "We don‚Äôt know F, but we can estimate it by the empirical probability dis-\n",
      "tributionOFthat puts probability 1=n on each point xi(e.g., weight 1=157\n",
      "on each point .xi;yi/in Figure 1.2). Notice that a bootstrap sample x\u0003\n",
      "(10.13) is an iid sample drawn from OF, since then each x\u0003independently\n",
      "has equal probability of being any member of fx1;x2;:::;xng. It can be\n",
      "shown thatOFmaximizes the probability of obtaining the observed sample\n",
      "xunder all possible choices of Fin (10.2), i.e., it is the nonparametric\n",
      "MLE ofF.\n",
      "Bootstrap replications O\u0012\u0003are obtained by a process analogous to (10.17),\n",
      "OFiid\u0000!x\u0003s\u0000!O\u0012\u0003: (10.18)\n",
      "In the real world (10.17) we only get to see the single value O\u0012, but the boot-\n",
      "strap world (10.18) is more generous: we can generate as many bootstrap\n",
      "replicationsO\u0012\u0003bas we want, or have time for, and directly estimate their\n",
      "variability as in (10.16). The fact that OFapproachesFasngrows large\n",
      "suggests, correctly in most cases, that bsebootapproaches the true standard\n",
      "error ofO\u0012.\n",
      "The true standard deviation of O\u0012, i.e., its standard error, can be thought\n",
      "of as a function of the probability distribution Fthat generates the data,\n",
      "say Sd.F/. Hypothetically, Sd .F/inputsFand outputs the standard devi-\n",
      "ation ofO\u0012, which we can imagine being evaluated by independently run-\n",
      "ning (10.17) some enormous number of times N, and then computing the\n",
      "empirical standard deviation of the resulting O\u0012values,\n",
      "Sd.F/D2\n",
      "4NX\n",
      "jD1\u0010\n",
      "O\u0012.j/\u0000O\u0012.\u0001/\u00112.\n",
      ".N\u00001/3\n",
      "51=2\n",
      ";withO\u0012.\u0001/DNX\n",
      "1O\u0012.j/ƒ±\n",
      "N:\n",
      "(10.19)10.2 The Nonparametric Bootstrap 161\n",
      "The bootstrap standard error of O\u0012is the plug-in estimate\n",
      "bsebootDSd.OF/: (10.20)\n",
      "More exactly, Sd .OF/is the ideal bootstrap estimate of standard error, what\n",
      "we would get by letting the number of bootstrap replications Bgo to in-\n",
      "Ô¨Ånity. In practice we have to stop at some Ô¨Ånite value of B, as discussed in\n",
      "what follows.\n",
      "As with the jackknife, there are several important points worth empha-\n",
      "sizing aboutbseboot.\n",
      "\u000fIt is completely automatic. Once again, a master algorithm can be writ-\n",
      "ten that inputs the data xand the function s.\u0001/, and outputs bseboot.\n",
      "\u000fWe have described the one-sample nonparametric bootstrap . Parametric\n",
      "and multisample versions will be taken up later.\n",
      "\u000fBootstrapping ‚Äúshakes‚Äù the original data more violently than jackknif-\n",
      "ing, producing nonlocal deviations of x\u0003fromx. The bootstrap is more\n",
      "dependable than the jackknife for unsmooth statistics since it doesn‚Äôt\n",
      "depend on local derivatives.\n",
      "\u000fBD200is usually sufÔ¨Åcient¬éfor evaluating bseboot. Larger values, 1000 ¬é2\n",
      "or 2000, will be required for the bootstrap conÔ¨Ådence intervals of Chap-\n",
      "ter 11.\n",
      "\u000fThere is nothing special about standard errors. We could just as well\n",
      "use the bootstrap replications to estimate the expected absolute error\n",
      "EfjO\u0012\u0000\u0012jg, or any other accuracy measure.\n",
      "\u000fFisher‚Äôs MLE formula (4.27) is applied in practice via\n",
      "bseÔ¨ÅsherD.nIO\u0012/\u00001=2; (10.21)\n",
      "that is, by plugging in O\u0012for\u0012after a theoretical calculation of se. The\n",
      "bootstrap operates in the same way at (10.20), though the plugging in is\n",
      "done before rather than after the calculation. The connection with Fishe-\n",
      "rian theory is more obvious for the parametric bootstrap of Section 10.4.\n",
      "The jackknife is a completely frequentist device, both in its assumptions\n",
      "and in its applications (standard errors and biases). The bootstrap is also\n",
      "basically frequentist, but with a touch of the Fisherian as in the relation\n",
      "with (10.21). Its versatility has led to applications in a variety of estima-\n",
      "tion and prediction problems, with even some Bayesian connections.¬é¬é3\n",
      "Unusual applications can also pop up for the jackknife; see the jackknife-\n",
      "after-bootstrap comment in the chapter endnotes.¬é ¬é4\n",
      "From a classical point of view, the bootstrap is an incredible computa-\n",
      "tional spendthrift. Classical statistics was fashioned to minimize the hard162 The Jackknife and the Bootstrap\n",
      "labor of mechanical computation. The bootstrap seems to go out of its way\n",
      "to multiply it, by factors of BD200or 2000 or more. It is nice to re-\n",
      "port that all this computational largesse can have surprising data analytic\n",
      "payoffs.\n",
      "Table 10.1 Correlation matrix for the student score data. The eigenvalues\n",
      "are 3.463, 0.660, 0.447, 0.234, and 0.197. The eigenratio statistic\n",
      "O\u0012D0:693 , and its bootstrap standard error estimate is 0.075\n",
      "(BD2000 ).\n",
      "mechanics vectors algebra analytics statistics\n",
      "mechanics 1.00 .50 .76 .65 .54\n",
      "vectors .50 1.00 .59 .51 .38\n",
      "algebra .76 .59 1.00 .76 .67\n",
      "analysis .65 .51 .76 1.00 .74\n",
      "statistics .54 .38 .67 .74 1.00\n",
      "The 22 students of Table 3.1 actually each took Ô¨Åve tests, mechanics ,\n",
      "vectors ,algebra ,analytics , andstatistics . Table 10.1 shows\n",
      "the sample correlation matrix and also its eigenvalues. The ‚Äúeigenratio‚Äù\n",
      "statistic,\n",
      "O\u0012Dlargest eigenvalue =sum eigenvalues ; (10.22)\n",
      "measures how closely the Ô¨Åve scores can be predicted by a single linear\n",
      "combination, essentially an IQ score for each student: O\u0012D0:693 here,\n",
      "indicating strong predictive power for the IQ score. How accurate is 0.693?\n",
      "BD2000 bootstrap replications (10.15) yielded bootstrap standard er-\n",
      "ror estimate (10.16) bsebootD0:075 . (This was 10 times more bootstraps\n",
      "than necessary for bseboot, but will be needed for Chapter 11‚Äôs bootstrap con-\n",
      "Ô¨Ådence interval calculations.) The jackknife (10.6) gave a bigger estimate,\n",
      "bsejackD0:083 .\n",
      "Standard errors are usually used to suggest approximate conÔ¨Ådence in-\n",
      "tervals, oftenO\u0012Àô1:96bse for 95% coverage. These are based on an assump-\n",
      "tion of normality for O\u0012. The histogram of the 2000 bootstrap replications of\n",
      "O\u0012, as seen in Figure 10.2, disabuses belief in even approximate normality.\n",
      "Compared with classical methods, a massive amount of computation has\n",
      "gone into the histogram, but this will pay off in Chapter 11 with more ac-\n",
      "curate conÔ¨Ådence limits. We can claim a double reward here for bootstrap\n",
      "methods: much wider applicability and improved inferences. The bootstrap10.3 Resampling Plans 163\n",
      " \n",
      "Œ∏^‚àóFrequency\n",
      "0.4 0.5 0.6 0.7 0.8 0.90 50 100 150\n",
      "Standard Error\n",
      "Bootstrap .075\n",
      "Jackknife .083\n",
      "Figure 10.2 Histogram of BD2000 bootstrap replications O\u0012\u0003\n",
      "for the eigenratio statistic (10.22) for the student score data. The\n",
      "vertical black line is at O\u0012D:693. The long left tail shows that\n",
      "normality is a dangerous assumption in this case.\n",
      "histogram‚Äîinvisible to classical statisticians‚Äînicely illustrates the advan-\n",
      "tages of computer-age statistical inference.\n",
      "10.3 Resampling Plans\n",
      "There is a second way to think about the jackknife and the bootstrap:\n",
      "as algorithms that reweight, or resample , the original data vector xD\n",
      ".x1;x2;:::;xn/0. At the price of a little more abstraction, resampling con-\n",
      "nects the two algorithms and suggests a class of other possibilities.\n",
      "Aresampling vector PD.P1;P2;:::;Pn/0is by deÔ¨Ånition a vector of\n",
      "nonnegative weights summing to 1,\n",
      "PD.P1;P2;:::;Pn/0withPi\u00150andnX\n",
      "iD1PiD1: (10.23)\n",
      "That is,Pis a member of the simplex Sn(5.39). Resampling plans operate\n",
      "by holding the original data set xÔ¨Åxed, and seeing how the statistic of\n",
      "interestO\u0012changes as the weight vector Pvaries across Sn.164 The Jackknife and the Bootstrap\n",
      "We denote the value of O\u0012for a vector putting weight Pionxias\n",
      "O\u0012\u0003DS.P/; (10.24)\n",
      "the star notation now indicating any reweighting, not necessarily from boot-\n",
      "strapping;O\u0012Ds.x/describes the behavior of O\u0012in the real world (10.17),\n",
      "whileO\u0012\u0003DS.P/describes it in the resampling world. For the sample\n",
      "means.x/D Nx, we haveS.P/DPn\n",
      "1Pixi. The unbiased estimate of\n",
      "variances.x/DPn\n",
      "i.xi\u0000Nx/2=.n\u00001/can be seen to have\n",
      "S.P/Dn\n",
      "n\u000012\n",
      "4nX\n",
      "iD1Pix2\n",
      "i\u0000 nX\n",
      "iD1Pixi!23\n",
      "5: (10.25)\n",
      "P0 P(1) P(2) \n",
      "P(3) (3,0,0)  \n",
      "(2,1,0)  (1,2,0)  (0,3,0)  (1,1,1) (2,0,1) (0,2,1) (1,0,2) (0,1,2) (0,0,3) \n",
      "0.5 1.0 1.5 0.0 ‚Äì 0.5 ‚Äì 1.0 ‚Äì 1.5 0.5 1.0 1.5 0.0 ‚Äì 0.5 2.0 \n",
      "Figure 10.3 Resampling simplex for sample size nD3. The\n",
      "center point is P0(10.26); the green circles are the jackknife\n",
      "pointsP.i/(10.28); triples indicate bootstrap resampling numbers\n",
      ".N1;N2;N3/(10.29). The bootstrap probabilities are6=27 for\n",
      "P0,1=27 for each corner point, and 3=27 for each of the six\n",
      "starred points.\n",
      "Letting\n",
      "P0D.1;1;:::;1/0=n; (10.26)10.3 Resampling Plans 165\n",
      "the resampling vector putting equal weight on each value xi, we require in\n",
      "the deÔ¨Ånition of S.\u0001/that\n",
      "S.P0/Ds.x/DO\u0012; (10.27)\n",
      "the original estimate. The ith jackknife value O\u0012.i/(10.5) corresponds to\n",
      "resampling vector\n",
      "P.i/D.1;1;:::;1;0;1;:::;1/0=.n\u00001/; (10.28)\n",
      "with 0 in the ith place. Figure 10.3 illustrates the resampling simplex S3\n",
      "applying to sample size nD3, with the center point being P0and the open\n",
      "circles the three possible jackknife vectors P.i/.\n",
      "WithnD3sample pointsfx1;x2;x3gthere are only 10 distinct boot-\n",
      "strap vectors (10.13), also shown in Figure 10.3. Let\n",
      "NiD#fx\u0003\n",
      "jDxig; (10.29)\n",
      "the number of bootstrap draws in x\u0003equalingxi. The triples in the Ô¨Åg-\n",
      "ure are.N1;N2;N3/, for example .1;0;2/ forx\u0003havingx1once andx3\n",
      "twice.4The bootstrap resampling vectors are of the form\n",
      "P\u0003D.N1;N2;:::;Nn/0=n; (10.30)\n",
      "where theNiare nonnegative integers summing to n. According to deÔ¨Å-\n",
      "nition (10.13) of bootstrap sampling, the vector ND.N1;N2;:::;Nn/0\n",
      "follows a multinomial distribution (5.38) with ndraws onnequally likely\n",
      "categories,\n",
      "N\u0018Multn.n;P0/: (10.31)\n",
      "This gives bootstrap probability (5.37)\n",
      "n≈†\n",
      "N1≈†N2≈†:::Nn≈†1\n",
      "nn(10.32)\n",
      "onP\u0003(10.30).\n",
      "Figure 10.3 is misleading in that the jackknife vectors P.i/appear only\n",
      "slightly closer to P0than are the bootstrap vectors P\u0003. Asngrows large\n",
      "they are, in fact, an order of magnitude closer. Subtracting (10.26) from\n",
      "(10.28) gives Euclidean distance\n",
      "kP.i/\u0000P0kD1.p\n",
      "n.n\u00001/: (10.33)\n",
      "4A hidden assumption of deÔ¨Ånition (10.24) is that O\u0012Ds.x/has the same value for any\n",
      "permutation ofx, so for instance s.x1;x3;x3/Ds.x3;x1;x3/DS.1=3;0;2=3/ .166 The Jackknife and the Bootstrap\n",
      "For the bootstrap, notice that Niin (10.29) has a binomial distribution,\n",
      "Ni\u0018Bi\u0012\n",
      "n;1\n",
      "n\u0013\n",
      "; (10.34)\n",
      "with mean 1 and variance .n\u00001/=n . ThenP\u0003\n",
      "iDNi=nhas mean and vari-\n",
      "ance.1=n;.n\u00001/=n3/. Adding over the ncoordinates gives the expected\n",
      "root mean square distance for bootstrap vector P\u0003,\n",
      "\u0000\n",
      "EkP\u0003\u0000P0k2\u00011=2Dp\n",
      ".n\u00001/=n2; (10.35)\n",
      "an order of magnitudepntimes further than (10.33).\n",
      "The function S.P/has approximate directional derivative\n",
      "DiDS.P.i//\u0000S.P0/\n",
      "kP.i/\u0000P0k(10.36)\n",
      "in the direction from P0towardP.i/(measured along the dashed lines\n",
      "in Figure 10.3). Dimeasures the slope of function S.P/atP0, in the\n",
      "direction ofP.i/. Formula (10.12) shows bsejackas proportional to the root\n",
      "mean square of the slopes.\n",
      "IfS.P/is alinear function ofP, as it is for the sample mean, it turns\n",
      "out thatbsejackequalsbseboot(except for the fudge factor .n\u00001/=n in (10.6)).\n",
      "Most statistics are not linear, and then the local jackknife resamples may\n",
      "provide a poor approximation to the full resampling behavior of S.P/. This\n",
      "was the case at one point in Figure 10.1.\n",
      "With only 10 possible resampling points P\u0003, we can easily evaluate the\n",
      "ideal bootstrap standard error estimate\n",
      "bsebootD\"10X\n",
      "kD1pk\u0010\n",
      "O\u0012\u0003k\u0000O\u0012\u0003\u0001\u00112#1=2\n",
      ";O\u0012\u0003\u0001D10X\n",
      "kD1pkO\u0012\u0003k; (10.37)\n",
      "withO\u0012\u0003kDS.Pk/andpkthe probability from (10.32) (listed in Fig-\n",
      "ure 10.3). This rapidly becomes impractical. The number of distinct boot-\n",
      "strap samples for npoints turns out to be\n",
      " \n",
      "2n\u00001\n",
      "n!\n",
      ": (10.38)\n",
      "FornD10this is already 92,378, while nD20gives6:9\u00021010distinct\n",
      "possible resamples. Choosing BvectorsP\u0003at random, which is what al-\n",
      "gorithm (10.13)‚Äì(10.15) effectively is doing, makes the un-ideal bootstrap\n",
      "standard error estimate (10.16) almost as accurate as (10.37) for Bas small\n",
      "as 200 or even less.10.3 Resampling Plans 167\n",
      "The luxury of examining the resampling surface provides a major advan-\n",
      "tage to modern statisticians, both in inference and methodology. A variety\n",
      "of other resampling schemes have been proposed, a few of which follow.\n",
      "The InÔ¨Ånitesimal Jackknife\n",
      "Looking at Figure 10.3 again, the vector\n",
      "Pi.\u000f/D.1\u0000\u000f/P0C\u000fP.i/DP0C\u000f.P.i/\u0000P0/ (10.39)\n",
      "lies proportion \u000fof the way from P0toP.i/. Then\n",
      "QDiDlim\n",
      "\u000f!0S.Pi.\u000f//\u0000S.P0/\n",
      "\u000fkP.i/\u0000P0k(10.40)\n",
      "exactly deÔ¨Ånes the direction derivative at P0in the direction of P.i/. The\n",
      "inÔ¨Ånitesimal jackknife estimate of standard error is\n",
      "bseIJD nX\n",
      "iD1QD2\n",
      "iƒ±\n",
      "n2!1=2\n",
      "; (10.41)\n",
      "usually evaluated numerically by setting \u000fto some small value in (10.40)‚Äì\n",
      "(10.41) (rather than \u000fD1in (10.12)). We will meet the inÔ¨Ånitesimal jack-\n",
      "knife again in Chapters 17 and 20.\n",
      "Multisample Bootstrap\n",
      "The median difference between the AML and the ALL scores in Figure 1.4\n",
      "is\n",
      "mediffD0:968\u00000:733D0:235: (10.42)\n",
      "How accurate is 0.235? An appropriate form of bootstrapping draws 25\n",
      "times with replacement from the 25 AML patients, 47 times with replace-\n",
      "ment from the 47 ALL patients, and computes mediff\u0003as the difference\n",
      "between the medians of the two bootstrap samples. (Drawing one boot-\n",
      "strap sample of size 72 from all the patients would result in random sample\n",
      "sizes for the AML\u0003=ALL\u0003groups, adding inappropriate variability to the\n",
      "frequentist standard error estimate.)\n",
      "A histogram of BD500mediff\u0003values appears in Figure 10.4. They\n",
      "givebsebootD0:074 . The estimate (10.42) is 3.18 bse units above zero, agree-\n",
      "ing surprisingly well with the usual two-sample t-statistic 3.01 (based on\n",
      "mean differences), and its permutation histogram Figure 4.3. Permutation\n",
      "testing can be considered another form of resampling.168 The Jackknife and the Bootstrap\n",
      " \n",
      "mediff*Frequency\n",
      "0.0 0.1 0.2 0.3 0.4 0.5010203040506070\n",
      "Figure 10.4 BD500bootstrap replications for the median\n",
      "difference between the AML andALL scores in Figure 1.4, giving\n",
      "bsebootD0:074 . The observed value mediffD0:235 (vertical\n",
      "black line) is more than 3 standard errors above zero.\n",
      "Moving Blocks Bootstrap\n",
      "SupposexD.x1;x2;:::;xn/, instead of being an iid sample (10.2), is a\n",
      "time series. That is, the xvalues occur in a meaningful order, perhaps with\n",
      "nearby observations highly correlated with each other. Let Bmbe the set of\n",
      "contiguous blocks of length m, for example\n",
      "B3Df.x1;x2;x3/;.x2;x3;x4/;:::;.xn\u00002;xn\u00001;xn/g: (10.43)\n",
      "Presumably,mis chosen large enough that correlations between xiandxj,\n",
      "jj\u0000ij> m , are neglible. The moving block bootstrap Ô¨Årst selects n=m\n",
      "blocks from Bm, and assembles them in random order to construct a boot-\n",
      "strap samplex\u0003. Having constructed Bsuch samples, bsebootis calculated\n",
      "as in (10.15)‚Äì(10.16).\n",
      "The Bayesian Bootstrap\n",
      "LetG1;G2;:::;Gnbe independent one-sided exponential variates (de-\n",
      "noted Gam(1,1) in Table 5.1), each having density exp .\u0000x/forx > 0 .10.4 The Parametric Bootstrap 169\n",
      "The Bayesian bootstrap uses resampling vectors\n",
      "P\u0003D.G1;G2;:::;Gn/,nX\n",
      "1Gi: (10.44)\n",
      "It can be shown that P\u0003is then uniformly distributed over the resampling\n",
      "simplex Sn; fornD3, uniformly distributed over the triangle in Fig-\n",
      "ure 10.3. Prescription (10.44) is motivated by assuming a Jeffreys-style\n",
      "uninformative prior distribution (Section 3.2) on the unknown distribution\n",
      "F(10.2).\n",
      "Distribution (10.44) for P\u0003has mean vector and covariance matrix\n",
      "P\u0003\u0018\u0014\n",
      "P0;1\n",
      "nC1\u0000\n",
      "diag.P0/\u0000P0P0\n",
      "0\u0001\u0015\n",
      ": (10.45)\n",
      "This is almost identical to the mean and covariance of bootstrap resamples\n",
      "P\u0003\u0018Multn.n,P0/=n,\n",
      "P\u0003\u0018\u0014\n",
      "P0;1\n",
      "n\u0000\n",
      "diag.P0/\u0000P0P0\n",
      "0\u0001\u0015\n",
      "; (10.46)\n",
      "(5.40). The Bayesian bootstrap and the ordinary bootstrap tend to agree, at\n",
      "least for smoothly deÔ¨Åned statistics O\u0012\u0003DS.P\u0003/.\n",
      "There was some Bayesian disparagement of the bootstrap when it Ô¨Årst\n",
      "appeared because of its blatantly frequentist take on estimation accuracy.\n",
      "And yet connections like (10.45)‚Äì(10.46) have continued to pop up, as we\n",
      "will see in Chapter 13.\n",
      "10.4 The Parametric Bootstrap\n",
      "In our description (10.18) of bootstrap resampling,\n",
      "OFiid\u0000!x\u0003\u0000!O\u0012\u0003; (10.47)\n",
      "there is no need to insist that OFbe the nonparametric MLE of F. Suppose\n",
      "we are willing to assume that the observed data vector xcomes from a\n",
      "parametric family Fas in (5.1),\n",
      "FDÀö\n",
      "f\u0016.x/; \u00162\t\n",
      ": (10.48)\n",
      "LetO\u0016be the MLE of \u0016. The bootstrap parametric resamples from fO\u0016.\u0001/,\n",
      "fO\u0016\u0000!x\u0003\u0000!O\u0012\u0003; (10.49)\n",
      "and proceeds as in (10.14)‚Äì(10.16) to calculate bseboot.170 The Jackknife and the Bootstrap\n",
      "As an example, suppose that xD.x1;x2;:::;xn/is an iid sample of\n",
      "sizenfrom a normal distribution,\n",
      "xiiid\u0018N.\u0016;1/; iD1;2;:::;n: (10.50)\n",
      "ThenO\u0016DNx, and a parametric bootstrap sample is x\u0003D.x\u0003\n",
      "1;x\u0003\n",
      "2;:::;x\u0003\n",
      "n/,\n",
      "where\n",
      "x\u0003\n",
      "iiid\u0018N.Nx;1/; iD1;2;:::;n: (10.51)\n",
      "More adventurously, if Fwere a family of time series models for x,\n",
      "algorithm (10.49) would still apply (now without any iid structure): x\u0003\n",
      "would be a time series sampled from model fO\u0016.\u0001/, andO\u0012\u0003Ds.x\u0003/the\n",
      "resampled statistic of interest. Bindependent realizations x\u0003bwould give\n",
      "O\u0012\u0003b,bD1;2;:::;B , andbsebootfrom (10.16).\n",
      " \n",
      "gfrCounts\n",
      "20 40 60 80 1000 510 15 20 25 30\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óèdf = 2df = 3,4,5,6df = 7\n",
      "Figure 10.5 Thegfr data of Figure 5.7 (histogram). Curves\n",
      "show the MLE Ô¨Åts from polynomial Poisson models, for degrees\n",
      "of freedom dfD2;3;:::;7 . The points on the curves show the\n",
      "Ô¨Åts computed at the centers x.j/of the bins, with the responses\n",
      "being the counts in the bins. The dashes at the base of the plot\n",
      "show the nine gfr values appearing in Table 10.2.\n",
      "As an example of parametric bootstrapping, Figure 10.5 expands the\n",
      "gfr investigation of Figure 5.7. In addition to the seventh-degree polyno-\n",
      "mial Ô¨Åt (5.62), we now show lower-degree polynomial Ô¨Åts for 2, 3, 4, 5,10.4 The Parametric Bootstrap 171\n",
      "and 6 degrees of freedom; df D2obviously gives a poor Ô¨Åt; df D3;4;5;6\n",
      "give nearly identical curves; df D7gives only a slightly better Ô¨Åt to the\n",
      "raw data.\n",
      "The plotted curves were obtained from the Poisson regression method\n",
      "used in Section 8.3, which we refer to as ‚ÄúLindsey‚Äôs method‚Äù.\n",
      "\u000fThex-axis was partitioned into KD32bins, with endpoints 13;16;19 ,\n",
      ":::;109 , and centerpoints, say,\n",
      "x./D.x.1/;x.2/;:::;x.K//; (10.52)\n",
      "x.1/D14:5,x.2/D17:5, etc.\n",
      "\u000fCount vectoryD.y1;y2;:::;yK/was computed\n",
      "ykD#fxiin binkg (10.53)\n",
      "(soygives the heights of the bars in Figure 10.5).\n",
      "\u000fAn independent Poisson model was assumed for the counts,\n",
      "ykind\u0018Poi.\u0016k/ forkD1;2;:::;K: (10.54)\n",
      "\u000fThe parametric model of degree ‚Äúdf‚Äù assumed that the \u0016kvalues were\n",
      "described by an exponential polynomial of degree df in the x.k/values,\n",
      "log.\u0016k/DdfX\n",
      "jD0Àájxj\n",
      ".k/: (10.55)\n",
      "\u000fThe MLEOÀáD.OÀá0;OÀá1;:::;OÀádf/in model (10.54)‚Äì(10.55) was found.5\n",
      "\u000fThe plotted curves in Figure 10.5 trace the MLE values O\u0016k,\n",
      "log.O\u0016k/DdfX\n",
      "jD0OÀájxj\n",
      ".k/: (10.56)\n",
      "How accurate are the curves? Parametric bootstraps were used to assess\n",
      "their standard errors. That is, Poisson resamples were generated according\n",
      "to\n",
      "y\u0003\n",
      "kind\u0018Poi.O\u0016k/ forkD1;2;:::;K; (10.57)\n",
      "and bootstrap MLE values O\u0016\u0003\n",
      "kcalculated as above, but now based on count\n",
      "vectory\u0003rather thany. All of this was done BD200times, yielding\n",
      "bootstrap standard errors (10.16).\n",
      "The results appear in Table 10.2, showing bsebootfor dfD2;3;:::;7\n",
      "5A single R command, glm(y\u0018poly(x,df),family=poisson) accomplishes\n",
      "this.172 The Jackknife and the Bootstrap\n",
      "Table 10.2 Bootstrap estimates of standard error for the gfr density.\n",
      "Poisson regression models (10.54) ‚Äì(10.55) ,dfD2;3;:::;7 , as in\n",
      "Figure 10.5; each BD200bootstrap replications; nonparametric\n",
      "standard errors based on binomial bin counts.\n",
      "Degrees of freedom Nonparametric\n",
      "standard errorgfr 2 3 4 5 6 7\n",
      "20.5 .28 .07 .13 .13 .12 .05 .00\n",
      "29.5 .65 .57 .57 .66 .74 1.11 1.72\n",
      "38.5 1.05 1.39 1.33 1.52 1.72 1.73 2.77\n",
      "47.5 1.47 1.91 2.12 1.93 2.15 2.39 4.25\n",
      "56.5 1.57 1.60 1.79 1.93 1.87 2.28 4.35\n",
      "65.5 1.15 1.10 1.07 1.31 1.34 1.27 1.72\n",
      "74.5 .76 .61 .62 .68 .81 .71 1.72\n",
      "83.5 .40 .30 .40 .38 .49 .68 1.72\n",
      "92.5 .13 .20 .29 .29 .34 .46 .00\n",
      "degrees of freedom evaluated at nine values of gfr. Variability generally\n",
      "increases with increasing df, as expected. Choosing a ‚Äúbest‚Äù model is a\n",
      "compromise between standard error and possible deÔ¨Ånitional bias as sug-\n",
      "gested by Figure 10.5, with perhaps df D3or 4, the winner.\n",
      "If we kept increasing the degrees of freedom, eventually (at df D32)\n",
      "we would exactly match the bar heights ykin the histogram. At this point\n",
      "the parametric bootstrap would merge into the nonparametric bootstrap.\n",
      "‚ÄúNonparametric‚Äù is another name for ‚Äúvery highly parameterized.‚Äù The\n",
      "huge sample sizes associated with modern applications have encouraged\n",
      "nonparametric methods, on the sometimes mistaken ground that estimation\n",
      "efÔ¨Åciency is no longer of concern. It is costly here, as the ‚Äúnonparametric‚Äù\n",
      "column of Table 10.2 shows.6\n",
      "Figure 10.6 returns to the student score eigenratio calculations of Fig-\n",
      "ure 10.2. The solid histogram shows 2000 parametric bootstrap replica-\n",
      "tions (10.49), with fO\u0016the Ô¨Åve-dimensional bivariate normal distribution\n",
      "N5.Nx;O‚Ä†/. HereNxandO‚Ä†are the usual MLE estimates for the expectation\n",
      "vector and covariance matrix based on the 22 Ô¨Åve-component student score\n",
      "vectors. It is narrower than the corresponding nonparametric bootstrap his-\n",
      "togram, with bsebootD0:070 compared with the nonparametric estimate\n",
      "6These are the binomial standard errors ≈íyk.n\u0000yk/=n¬ç1=2,nD211. The\n",
      "nonparametric results look much more competitive when estimating cdf‚Äôs rather than\n",
      "densities.10.4 The Parametric Bootstrap 173\n",
      " \n",
      "eigenratio*Frequency\n",
      "0.4 0.5 0.6 0.7 0.8 0.9020 40 60 80100 120\n",
      "Bootstrap Standard Errors\n",
      "Nonparametric .075\n",
      "Parametric .070\n",
      "Figure 10.6 Eigenratio example, student score data. Solid\n",
      "histogramBD2000 parametric bootstrap replications O\u0012\u0003from\n",
      "the Ô¨Åve-dimensional normal MLE; line histogram the 2000\n",
      "nonparametric replications of Figure 10.2. MLE O\u0012D:693 is\n",
      "vertical red line.\n",
      "0.075. (Note the different histogram bin limits from Figure 10.2, changing\n",
      "the details of the nonparametric histogram.)\n",
      "Parametric families act as regularizers , smoothing out the raw data and\n",
      "de-emphasizing outliers. In fact the student score data is not a good can-\n",
      "didate for normal modeling, having at least one notable outlier,7casting\n",
      "doubt on the smaller estimate of standard error.\n",
      "The classical statistician could only imagine a mathematical device that\n",
      "given any statisticO\u0012Ds.x/would produce a formula for its standard er-\n",
      "ror, as formula (1.2) does for Nx. The electronic computer issuch a device.\n",
      "As harnessed by the bootstrap, it automatically produces a numerical esti-\n",
      "mate of standard error (though not a formula), with no further cleverness\n",
      "required. Chapter 11 discusses a more ambitious substitution of computer\n",
      "power for mathematical analysis: the bootstrap computation of conÔ¨Ådence\n",
      "intervals.\n",
      "7As revealed by examining scatterplots of the Ô¨Åve variates taken two at a time. Fast and\n",
      "painless plotting is another advantage for twenty-Ô¨Årst-century data analysts.174 The Jackknife and the Bootstrap\n",
      "10.5 InÔ¨Çuence Functions and Robust Estimation\n",
      "The sample mean played a dominant role in classical statistics for reasons\n",
      "heavily weighted toward mathematical tractibility. Beginning in the 1960s,\n",
      "an important counter-movement, robust estimation , aimed to improve upon\n",
      "the statistical properties of the mean. A central element of that theory, the\n",
      "inÔ¨Çuence function , is closely related to the jackknife and inÔ¨Ånitesimal jack-\n",
      "knife estimates of standard error.\n",
      "We will only consider the case where X, the sample space, is an interval\n",
      "of the real line. The unknown probability distribution Fyielding the iid\n",
      "samplexD.x1;x2;:::;xn/in (10.2) is now the cdf of a density function\n",
      "f.x/ onX. A parameter of interest, i.e., a function of F, is to be estimated\n",
      "by the plug-in principle, O\u0012DT.OF/, where, as in Section 10.2, OFis the\n",
      "empirical probability distribution putting probability 1=n on each sample\n",
      "pointxi. For the mean,\n",
      "\u0012DT.F/DZ\n",
      "Xxf.x/dx andO\u0012DT\u0010\n",
      "OF\u0011\n",
      "D1\n",
      "nnX\n",
      "iD1xi:(10.58)\n",
      "(In Riemann‚ÄìStieltjes notation, \u0012DR\n",
      "xdF.x/ andO\u0012DR\n",
      "xdOF.x/ .)\n",
      "The inÔ¨Çuence function of T.F/ , evaluated at point xinX, is deÔ¨Åned to\n",
      "be\n",
      "IF.x/Dlim\n",
      "\u000f!0T..1\u0000\u000f/FC\u000fƒ±x/\u0000T.F/\n",
      "\u000f; (10.59)\n",
      "whereƒ±xis the ‚Äúone-point probability distribution‚Äù putting probability 1\n",
      "onx. In words, IF .x/measures the differential effect of modifying Fby\n",
      "putting additional probability on x. For the mean \u0012DR\n",
      "xf.x/dx we cal-\n",
      "culate that\n",
      "IF.x/Dx\u0000\u0012: (10.60)\n",
      "A fundamental theorem¬ésays thatO\u0012DT.OF/is approximately ¬é5\n",
      "O\u0012:D\u0012C1\n",
      "nnX\n",
      "iD1IF.xi/; (10.61)\n",
      "with the approximation becoming exact as ngoes to inÔ¨Ånity. This implies\n",
      "thatO\u0012\u0000\u0012is, approximately, the mean of the niid variates IF .xi/, and that\n",
      "the variance ofO\u0012is approximately\n",
      "varn\n",
      "O\u0012o:D1\n",
      "nvarfIF.x/g; (10.62)10.5 InÔ¨Çuence Functions and Robust Estimation 175\n",
      "varfIF.x/gbeing the variance of IF .x/for any one draw of xfromF. For\n",
      "the sample mean, using (10.60) in (10.62) gives the familiar equality\n",
      "varfNxgD1\n",
      "nvarfxg: (10.63)\n",
      "The sample mean suffers from an unbounded inÔ¨Çuence function (10.60),\n",
      "which grows ever larger as xmoves farther from \u0012. This makesNxunstable\n",
      "against heavy-tailed densities such as the Cauchy (4.39). Robust estimation\n",
      "theory seeks estimators O\u0012of bounded inÔ¨Çuence, that do well against heavy-\n",
      "tailed densities without giving up too much efÔ¨Åciency against light-tailed\n",
      "densities such as the normal. Of particular interest have been the trimmed\n",
      "mean and its close cousin the winsorized mean.\n",
      "Letx.Àõ/denote the 100 Àõth percentile of distribution F, satisfyingF.x.Àõ//\n",
      "DÀõor equivalently\n",
      "ÀõDZx.Àõ/\n",
      "\u00001f.x/dx: (10.64)\n",
      "TheÀõth trimmed mean of F,\u0012trim.Àõ/, is deÔ¨Åned as\n",
      "\u0012trim.Àõ/D1\n",
      "1\u00002ÀõZx.1\u0000Àõ/\n",
      "x.Àõ/xf.x/dx; (10.65)\n",
      "the mean of the central 1\u00002Àõportion ofF, trimming off the lower and\n",
      "upperÀõportions. This is not the same as the Àõth winsorized mean \u0012wins.Àõ/,\n",
      "\u0012wins.Àõ/DZ\n",
      "XW.x/f.x/dx; (10.66)\n",
      "where\n",
      "W.x/D8\n",
      "ÀÜ<\n",
      "ÀÜ:x.Àõ/ifx\u0014x.Àõ/\n",
      "x ifx.Àõ/\u0014x\u0014x.1\u0000Àõ/\n",
      "x.1\u0000Àõ/ifx\u0015x.1\u0000Àõ/I(10.67)\n",
      "\u0012trim.Àõ/removes the outer portions of F, while\u0012wins.Àõ/moves them into\n",
      "x.Àõ/orx.1\u0000Àõ/. In practice, empirical versions O\u0012trim.Àõ/andO\u0012wins.Àõ/are used,\n",
      "substituting the empirical density Of, with probability 1=n at eachxi, for\n",
      "f.\n",
      "There turns out to be an interesting relationship between the two: the\n",
      "inÔ¨Çuence function of \u0012trim.Àõ/is a function of \u0012wins.Àõ/,\n",
      "IFÀõ.x/DW.x/\u0000\u0012wins.Àõ/\n",
      "1\u00002Àõ: (10.68)\n",
      "This is pictured in Figure 10.7, where we have plotted empirical inÔ¨Çuence176 The Jackknife and the Bootstrap\n",
      "0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6‚àí0.5 0.0 0.5 1.0\n",
      "ALLInfluence Functionmean\n",
      "trimmed mean Œ± = 0. 2\n",
      "trimmed mean Œ± = 0. 4\n",
      "Figure 10.7 Empirical inÔ¨Çuence functions for the 47 leukemia\n",
      "ALL scores of Figure 1.4. The two dashed curves are IF Àõ.x/for\n",
      "the trimmed means (10.68), for ÀõD0:2andÀõD0:4. The solid\n",
      "curve is IF.x/for the sample mean Nx(10.60).\n",
      "functions (plugging in OFforFin deÔ¨Ånition (10.59)) relating to the 47\n",
      "leukemia ALL scores of Figure 1.4: IF 0:2.x/and IF0:4.x/are plotted, along\n",
      "with IF0.x/(10.60), that is, for the mean.\n",
      "Table 10.3 Trimmed means and their bootstrap standard deviations for\n",
      "the 47 leukemia ALL scores of Figure 1.4; BD1000 bootstrap\n",
      "replications for each trim value. The last column gives empirical inÔ¨Çuence\n",
      "function estimates of the standard error, which are also the inÔ¨Ånitesimal\n",
      "jackknife estimates (10.41) . These fail for the median.\n",
      "Trimmed Bootstrap\n",
      "Trim mean sd (IFse)\n",
      "Mean .0 .752 .040 (.040)\n",
      ".1 .729 .038 (.034)\n",
      ".2 .720 .035 (.034)\n",
      ".3 .725 .044 (.044)\n",
      ".4 .734 .047 (.054)\n",
      "Median .5 .733 .05310.6 Notes and Details 177\n",
      "The upper panel of Figure 1.4 shows a moderately heavy right tail for\n",
      "theALL distribution. Would it be more efÔ¨Åcient to estimate the center of\n",
      "the distribution with a trimmed mean rather than Nx? The bootstrap pro-\n",
      "vides an answer: bseboot(10.16) was calculated for NxandO\u0012trim.Àõ/,ÀõD\n",
      "0:1;0:2;0:3;0:4 , and 0.5, the last being the sample median. It appears that\n",
      "O\u0012trim.0:2/ is moderately better than Nx. This brings up an important question\n",
      "discussed in Chapter 20: if we use something like Table 10.3 to select an\n",
      "estimator, how does the selection process affect the accuracy of the result-\n",
      "ing estimate?\n",
      "We might also use the square root of formula (10.62) to estimate the\n",
      "standard errors of the various estimators, plugging in the empirical inÔ¨Çu-\n",
      "ence function for IF .x/. This turns out to be the same as using the inÔ¨Ånites-\n",
      "imal jackknife (10.41). These appear in the last column of Table 10.3. Pre-\n",
      "dictably, this approach fails for the sample median, whose inÔ¨Çuence func-\n",
      "tion is a square wave, sharply discontinuous at the median \u0012,\n",
      "IF.x/DÀô1ƒ±\n",
      ".2f.\u0012//: (10.69)\n",
      "Robust estimation offers a nice illustration of statistical progress in the\n",
      "computer age. Trimmed means go far back into the classical era. InÔ¨Çuence\n",
      "functions are an insightful inferential tool for understanding the tradeoffs in\n",
      "trimmed mean estimation. And Ô¨Ånally the bootstrap allows easy assessment\n",
      "of the accuracy of robust estimation, including some more elaborate ones\n",
      "not discussed here.\n",
      "10.6 Notes and Details\n",
      "Quenouille (1956) introduced what is now called the jackknife estimate\n",
      "of bias. Tukey (1958) realized that Quenouille-type calculations could be\n",
      "repurposed for nonparametric standard-error estimation, inventing formula\n",
      "(10.6) and naming it ‚Äúthe jackknife,‚Äù as a rough and ready tool. Miller‚Äôs im-\n",
      "portant 1964 paper, ‚ÄúA trustworthy jackknife,‚Äù asked when formula (10.6)\n",
      "could be trusted. (Not for the median.)\n",
      "The bootstrap (Efron, 1979) began as an attempt to better understand\n",
      "the jackknife‚Äôs successes and failures. Its name celebrates Baron Mun-\n",
      "chausen‚Äôs success in pulling himself up by his own bootstraps from the\n",
      "bottom of a lake. Burgeoning computer power soon overcame the boot-\n",
      "strap‚Äôs main drawback, prodigous amounts of calculation, propelling it into\n",
      "general use. Meanwhile, 1000Ctheoretical papers were published asking\n",
      "when the bootstrap itself could be trusted. (Most but not all of the time in\n",
      "common practice.)178 The Jackknife and the Bootstrap\n",
      "A main reference for the chapter is Efron‚Äôs 1982 monograph The Jack-\n",
      "knife, the Bootstrap and Other Resampling Plans . Its Chapter 6 shows the\n",
      "equality of three nonparametric standard error estimates: Jaeckel‚Äôs (1972)\n",
      "inÔ¨Ånitesimal jackknife (10.41); the empirical inÔ¨Çuence function estimate,\n",
      "based on (10.62); and what is known as the nonparametric delta method.\n",
      "Bootstrap Packages\n",
      "Various bootstrap packages in Rare available on the CRAN contributed-\n",
      "packages web site, bootstrap being an ambitious one. Algorithm 10.1\n",
      "shows a simple Rprogram for nonparametric bootstrapping. Aside from\n",
      "bookkeeping, it‚Äôs only a few lines long.\n",
      "Algorithm 10.1 RPROGRAM FOR THE NONPARAMETRIC BOOTSTRAP .\n",
      "Boot <- function (x, B, func, ...){\n",
      "# x is data vector or matrix (with each row a case)\n",
      "# B is number of bootstrap replications\n",
      "# func is R function that inputs a data vector or\n",
      "# matrix and returns a numeric number or vector\n",
      "# ... other arguments for func\n",
      "x <- as.matrix(x)\n",
      "n <- nrow(x)\n",
      "f0=func(x,...) # get size of output\n",
      "fmat <- matrix(0,length(f0),B)\n",
      "for (b in 1:B) {\n",
      "i=sample(1:n, n, replace = TRUE)\n",
      "fmat[,b] <- func(x[i, ],...)\n",
      "}\n",
      "drop(fmat)\n",
      "}\n",
      "¬é1[p. 158] The jackknife standard error. The 1982 monograph also contains\n",
      "Efron and Stein‚Äôs (1981) result on the bias of the jackknife variance esti-\n",
      "mate, the square of formula (10.6): modulo certain sample size considera-\n",
      "tions, the expectation of the jackknife variance estimate is biased upward\n",
      "for the true variance.\n",
      "For the sample mean Nx, the jackknife yields exactly the usual variance\n",
      "estimate (1.2),P\n",
      "i.xi\u0000Nx/2=.n.n\u00001//, while the ideal bootstrap estimate\n",
      "(B!1 ) gives\n",
      "nX\n",
      "iD1.xi\u0000Nx/2=n2: (10.70)10.6 Notes and Details 179\n",
      "As with the jackknife, we could append a fudge factor to get perfect agree-\n",
      "ment with (1.2), but there is no real gain in doing so.\n",
      "¬é2[p. 161] Bootstrap sample sizes. LetbseBindicate the bootstrap standard er-\n",
      "ror estimate (10.16) based on Breplications, and bse1the ‚Äúideal bootstrap,‚Äù\n",
      "B!1 . In any actual application, there are diminishing returns from in-\n",
      "creasingBpast a certain point, because bse1is itself a statistic whose value\n",
      "varies with the observed sample x(as in (10.70)), leaving an irreducible re-\n",
      "mainder of randomness in any standard error estimate. Section 6.4 of Efron\n",
      "and Tibshirani (1993) shows that BD200will almost always be plenty\n",
      "(for standard errors, but not for bootstrap conÔ¨Ådence intervals, Chapter 11).\n",
      "Smaller numbers, 25 or even less, can still be quite useful in complicated\n",
      "situations where resampling is expensive. An early complaint, ‚ÄúBootstrap\n",
      "estimates are random,‚Äù is less often heard in an era of frequent and massive\n",
      "simulations.\n",
      "¬é3[p. 161] The Bayesian bootstrap. Rubin (1981) suggested the Bayesian\n",
      "bootstrap (10.44). Section 10.6 of Efron (1982) used (10.45)‚Äì(10.46) as\n",
      "an objective Bayes justiÔ¨Åcation for what we will call the percentile-method\n",
      "bootstrap conÔ¨Ådence intervals in Chapter 12.\n",
      "¬é4[p. 161] Jackknife-after-bootstrap. For the eigenratio example displayed in\n",
      "Figure 10.2,BD2000 nonparametric bootstrap replications gave bsebootD\n",
      "0:075 . How accurate is this value? Bootstrapping the bootstrap seems like\n",
      "too much work, perhaps 200 times 2000 resamples. It turns out, though,\n",
      "that we can use the jackknife to estimate the variability of bsebootbased on\n",
      "just the original 2000 replications.\n",
      "Now the deleted sample estimate in (10.6) is bseboot.i/. The key idea is\n",
      "to consider those bootstrap samples x\u0003(10.13), among the original 2000,\n",
      "thatdo not include the point xi. About 37% of the original Bsamples will\n",
      "be in this subset. Section 19.4 of Efron and Tibshirani (1993) shows that\n",
      "applying deÔ¨Ånition (10.16) to this subset gives bseboot.i/. For the estimate of\n",
      "Figure 10.2, the jackknife-after-bootstrap calculations gave bsejackD0:022\n",
      "forbsebootD0:075 . In other words, 0.075 isn‚Äôt very accurate, which is\n",
      "to be expected for the standard error of a complicated statistic estimated\n",
      "from onlynD22observations. An inÔ¨Ånitesimal jackknife version of this\n",
      "technique will play a major role in Chapter 20.\n",
      "¬é5[p. 174] A fundamental theorem. Tukey can justly be considered the found-\n",
      "ing father of robust statistics, his 1960 paper being especially inÔ¨Çuential.\n",
      "Huber‚Äôs celebrated 1964 paper brought the subject into the realm of high-\n",
      "concept mathematical statistics. Robust Statistics: The Approach Based on\n",
      "InÔ¨Çuence Functions , the 1986 book by Hampel et al. , conveys the breadth\n",
      "of a subject only lightly scratched in our Section 10.5. Hampel (1974)180 The Jackknife and the Bootstrap\n",
      "introduced the inÔ¨Çuence function as a statistical tool. Boos and SerÔ¨Çing\n",
      "(1980) veriÔ¨Åed expression (10.62). Qualitative notions of robustness, more\n",
      "than speciÔ¨Åc theoretical results, have had a continuing inÔ¨Çuence on modern\n",
      "data analysis.11\n",
      "Bootstrap ConÔ¨Ådence Intervals\n",
      "The jackknife and the bootstrap represent a different use of modern com-\n",
      "puter power: rather than extending classical methodology‚Äîfrom ordinary\n",
      "least squares to generalized linear models, for example‚Äîthey extend the\n",
      "reach of classical inference.\n",
      "Chapter 10 focused on standard errors. Here we will take up a more am-\n",
      "bitious inferential goal, the bootstrap automation of conÔ¨Ådence intervals.\n",
      "The familiar standard intervals\n",
      "O\u0012Àô1:96bse; (11.1)\n",
      "for approximate 95% coverage, are immensely useful in practice but often\n",
      "not very accurate. If we observe O\u0012D10from a Poisson model O\u0012\u0018Poi.\u0012/,\n",
      "the standard 95% interval .3:8;16:2/ (usingbseDO\u00121=2) is a mediocre ap-\n",
      "proximation to the exact interval1\n",
      ".5:1;17:8/: (11.2)\n",
      "Standard intervals (11.1) are symmetric around O\u0012, this being their main\n",
      "weakness. Poisson distributions grow more variable as \u0012increases, which\n",
      "is why interval (11.2) extends farther to the right of O\u0012D10than to the\n",
      "left. Correctly capturing such effects in an automatic way is the goal of\n",
      "bootstrap conÔ¨Ådence interval theory.\n",
      "11.1 Neyman‚Äôs Construction for One-Parameter\n",
      "Problems\n",
      "The student score data of Table 3.1 comprised nD22pairs,\n",
      "xiD.mi;vi/; iD1;2;:::;22; (11.3)\n",
      "1Using the Neyman construction of Section 11.1, as explained there; see also Table 11.2\n",
      "in Section 11.4.\n",
      "181182 Bootstrap ConÔ¨Ådence Intervals\n",
      "wheremiandviwere student i‚Äôs scores on the ‚Äúmechanics‚Äù and ‚Äúvectors‚Äù\n",
      "tests. The sample correlation coefÔ¨Åcient O\u0012betweenmiandviwas com-\n",
      "puted to be\n",
      "O\u0012D0:498: (11.4)\n",
      "Question : What can we infer about the true correlation \u0012betweenmandv?\n",
      "Figure 3.2 displayed three possible Bayesian answers. ConÔ¨Ådence intervals\n",
      "provide the frequentist solution, by far the most popular in applied practice.\n",
      "‚àí0.4 ‚àí0.2 0.0 0.2 0.4 0.6 0.8 1.00 1 2 3 4\n",
      "CorrelationDensity\n",
      "0.093 0.751 0.498‚óè0.025 0.025\n",
      "Figure 11.1 The solid curve is the normal correlation coefÔ¨Åcient\n",
      "densityfO\u0012.r/(3.11) forO\u0012D0:498 , the MLE estimate for the\n",
      "student score data; O\u0012.lo/D0:093 andO\u0012.up/D0:751 are the\n",
      "endpoints of the 95% conÔ¨Ådence interval for \u0012, with\n",
      "corresponding densities shown by dashed curves. These yield tail\n",
      "areas 0.025 atO\u0012(11.6).\n",
      "Suppose, Ô¨Årst, that we assume a bivariate normal model (5.12) for the\n",
      "pairs.mi;vi/. In that case the probability density f\u0012.O\u0012/for sample corre-\n",
      "lationO\u0012given true correlation \u0012has known form (3.11). The solid curve in\n",
      "Figure 11.1 graphs ffor\u0012D0:498 , that is, for\u0012set equal to the observed\n",
      "valueO\u0012. In more careful notation, the curve graphs fO\u0012.r/as a function of\n",
      "the dummy variable2rtaking values in ≈í\u00001;1¬ç.\n",
      "2This is an example of a parametric bootstrap distribution (10.49), here with O\u0016beingO\u0012.11.1 Neyman‚Äôs construction 183\n",
      "Two other curves f\u0012.r/appear in Figure 11.1: for \u0012equaling\n",
      "O\u0012.lo/D0:093 andO\u0012.up/D0:751: (11.5)\n",
      "These were numerically calculated as the solutions to\n",
      "Z1\n",
      "O\u0012fO\u0012.lo/.r/drD0:025 andZO\u0012\n",
      "\u00001fO\u0012.up/.r/drD0:025: (11.6)\n",
      "In words,O\u0012.lo/is the smallest value of \u0012putting probability at least 0.025\n",
      "aboveO\u0012D0:498 , whileO\u0012.up/is the largest value with probability at least\n",
      "0.025 belowO\u0012;\n",
      "\u00122h\n",
      "O\u0012.lo/;O\u0012.up/i\n",
      "(11.7)\n",
      "is a 95% conÔ¨Ådence interval for the true correlation, statement (11.7) hold-\n",
      "ing true with probability 0.95, for every possible value of \u0012.\n",
      "We have just described Neyman‚Äôs construction of conÔ¨Ådence intervals\n",
      "for one-parameter problems f\u0012.O\u0012/. (Later we will consider the more difÔ¨Å-\n",
      "cult situation where there are ‚Äúnuisance parameters‚Äù in addition to the pa-\n",
      "rameter of interest \u0012.) One of the jewels of classical frequentist inference,\n",
      "it depends on a pivotal argument ‚Äî‚Äúingenious device‚Äù number 5 of Sec-\n",
      "tion 2.1‚Äîto show that it produces genuine conÔ¨Ådence intervals, i.e., ones\n",
      "that contain the true parameter value \u0012at the claimed probability level,\n",
      "0.95 in Figure 11.1. The argument appears in the chapter endnotes.¬é¬é1\n",
      "For the Poisson calculation (11.2) it was necessary to deÔ¨Åne exactly what\n",
      "‚Äúthe smallest value of \u0012putting probability at least 0.025 above O\u0012‚Äù meant.\n",
      "This was done assuming that, for any \u0012, half of the probability f\u0012.O\u0012/at\n",
      "O\u0012D10counted as ‚Äúabove,‚Äù and similarly for calculating the upper limit.\n",
      "Transformation Invariance\n",
      "ConÔ¨Ådence intervals enjoy the important and useful property of transfor-\n",
      "mation invariance. In the Poisson example (11.2), suppose our interest\n",
      "shifts from parameter \u0012to parameter \u001eDlog\u0012. The 95% exact inter-\n",
      "val (11.2) for \u0012then transforms to the exact 95% interval for \u001esimply by\n",
      "taking logs of the endpoints,\n",
      ".log.5:1/; log.17:8//D.1:63;2:88/: (11.8)\n",
      "To state things generally, suppose we observe O\u0012from a family of densi-\n",
      "tiesf\u0012.O\u0012/and construct a conÔ¨Ådence interval C.O\u0012/for\u0012of coverage level184 Bootstrap ConÔ¨Ådence Intervals\n",
      "Àõ(ÀõD0:95 in our examples). Now let parameter \u001ebe a monotonic in-\n",
      "creasing function of \u0012, say\n",
      "\u001eDm.\u0012/ (11.9)\n",
      "(m.\u0012/Dlog\u0012in (11.8)), and likewise O\u001eDm.O\u0012/for the point estimate.\n",
      "ThenC.O\u0012/maps point by point into C\u001e.O\u001e/, a level-ÀõconÔ¨Ådence interval\n",
      "for\u001e,\n",
      "C\u001e.O\u001e/Dn\n",
      "\u001eDm.\u0012/ for\u00122C\u0010\n",
      "O\u0012\u0011o\n",
      ": (11.10)\n",
      "This just says that the event f\u00122C.O\u0012/gis the same as the event f\u001e2\n",
      "C\u001e.O\u001e/g, so if the former always occurs with probability Àõthen so must the\n",
      "latter.\n",
      "‚àí0.5 0.0 0.5 1.0 1.50.0 0.5 1.0 1.5\n",
      "Fisher‚Ä≤s œÜ transformDensity\n",
      "0.093 0.975 0.546‚óè0.025 0.025\n",
      "Figure 11.2 The situation in Figure 11.1 after transformation to\n",
      "\u001eDm.\u0012/ according to (11.11). The curves are nearly N.\u001e;\u001b2/\n",
      "with standard deviation \u001bD1=p\n",
      "19D0:229 .\n",
      "Transformation invariance has an historical resonance with the normal\n",
      "correlation coefÔ¨Åcient. Fisher‚Äôs derivation of f\u0012.O\u0012/(3.11) in 1915 was a\n",
      "mathematical triumph, but a difÔ¨Åcult one to exploit in an era of mechanical\n",
      "computation. Most ingeniously, Fisher suggested instead working with the\n",
      "transformed parameter \u001eDm.\u0012/ where\n",
      "\u001eDm.\u0012/D1\n",
      "2log\u00121C\u0012\n",
      "1\u0000\u0012\u0013\n",
      "; (11.11)11.2 The Percentile Method 185\n",
      "and likewise with statistic O\u001eDm.O\u0012/. Then, to a surprisingly good approx-\n",
      "imation,\n",
      "O\u001eP \u0018N\u0012\n",
      "\u001e;1\n",
      "n\u00003\u0013\n",
      ": (11.12)\n",
      "See Figure 11.2, which shows Neyman‚Äôs construction on the \u001escale.\n",
      "In other words, we are back in Fisher‚Äôs favored situation (4.31), the sim-\n",
      "ple normal translation problem, where\n",
      "C\u001e\u0010\n",
      "O\u001e\u0011\n",
      "DO\u001eÀô1:961p\n",
      "n\u00003(11.13)\n",
      "is the ‚Äúobviously correct‚Äù 95% conÔ¨Ådence interval3for\u001e, closely approx-\n",
      "imating Neyman‚Äôs construction. The endpoints of (11.13) are then trans-\n",
      "formed back to the \u0012scale according to the inverse transformation\n",
      "\u0012De2\u001e\u00001\n",
      "e2\u001eC1; (11.14)\n",
      "giving (almost) the interval C.O\u0012/seen in Figure 11.1, but without the in-\n",
      "volved computations.\n",
      "Bayesian conÔ¨Ådence statements are inherently transformation invariant.\n",
      "The fact that the Neyman intervals are also invariant, unlike the standard\n",
      "intervals (11.1), has made them more palatable to Bayesian statisticians.\n",
      "Transformation invariance will play a major role in justifying the bootstrap\n",
      "conÔ¨Ådence intervals introduced next.\n",
      "11.2 The Percentile Method\n",
      "Our goal is to automate the calculation of conÔ¨Ådence intervals: given the\n",
      "bootstrap distribution of a statistical estimator O\u0012, we want to automatically\n",
      "produce an appropriate conÔ¨Ådence interval for the unseen parameter \u0012. To\n",
      "this end, a series of four increasingly accurate bootstrap conÔ¨Ådence interval\n",
      "algorithms will be described.\n",
      "The Ô¨Årst and simplest method is to use the standard interval (11.1),\n",
      "O\u0012Àô1:96bse for 95% coverage, with bse taken to be the bootstrap standard\n",
      "errorbseboot(10.16). The limitations of this approach become obvious in\n",
      "Figure 11.3, where the histogram shows BD2000 nonparametric boot-\n",
      "strap replicationsO\u0012\u0003of the sample correlation coefÔ¨Åcient for the student\n",
      "3This is an anachronism. Fisher hated the term ‚ÄúconÔ¨Ådence interval‚Äù after it was later\n",
      "coined by Neyman for his comprehensive theory. He thought of (11.13) as an example\n",
      "of the logic of inductive inference .186 Bootstrap ConÔ¨Ådence Intervals\n",
      " \n",
      "Bootstrap CorrelationsFrequency\n",
      "‚àí0.4 ‚àí0.2 0.0 0.2 0.4 0.6 0.8 1.00 20 40 60 80 100\n",
      "0.498 0.118 0.758‚óè\n",
      "Figure 11.3 Histogram of BD2000 nonparametric bootstrap\n",
      "replicationsO\u0012\u0003for the student score sample correlation; the solid\n",
      "curve is the ideal parametric bootstrap distribution fO\u0012.r/as in\n",
      "Figure 11.1. Observed correlation O\u0012D0:498 . Small triangles\n",
      "show histogram‚Äôs 0.025 and 0.975 quantiles.\n",
      "score data, obtained as in Section 10.2. The standard intervals are justiÔ¨Åed\n",
      "by taking literally the asymptotic normality of O\u0012,\n",
      "O\u0012P \u0018N.\u0012;\u001b2/; (11.15)\n",
      "\u001bthe true standard error.\n",
      "Relation (11.15) will generally hold for large enough sample size n, but\n",
      "we can see that for the student score data asymptotic normality has not\n",
      "yet set in, with the histogram being notably long-tailed to the left. We can‚Äôt\n",
      "expect good performance from the standard method in this case. (The para-\n",
      "metric bootstrap distribution is just as nonnormal, as shown by the smooth\n",
      "curve.)\n",
      "The percentile method uses the shape of the bootstrap distribution to\n",
      "improve upon the standard intervals (11.1). Having generated Bbootstrap\n",
      "replicationsO\u0012\u00031;O\u0012\u00032;:::;O\u0012\u0003B, either nonparametrically as in Section 10.2\n",
      "or parametrically as in Section 10.4, we use the obvious percentiles of their\n",
      "distribution to deÔ¨Åne the percentile conÔ¨Ådence limits. The histogram in\n",
      "Figure 11.3 has its 0.025 and 0.975 percentiles equal to 0.118 and 0.758,11.2 The Percentile Method 187\n",
      "and these are the endpoints of the central 95% nonparametric percentile\n",
      "interval.\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0\n",
      "Œ∏^‚àóŒ±\n",
      "‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè\n",
      ".118 .758.025.975 G^\n",
      "Figure 11.4 A 95% central conÔ¨Ådence interval via the percentile\n",
      "method, based on the 2000 nonparametric replications O\u0012\u0003of\n",
      "Figure 11.3.\n",
      "We can state things more precisely in terms of the bootstrap cdfOG.t/ ,\n",
      "the proportion of bootstrap samples less than t,\n",
      "OG.t/D#n\n",
      "O\u0012\u0003b\u0014to.\n",
      "B: (11.16)\n",
      "TheÀõth percentile pointO\u0012\u0003.Àõ/of the bootstrap distribution is given by the\n",
      "inverse function of OG,\n",
      "O\u0012\u0003.Àõ/DOG\u00001.Àõ/I (11.17)\n",
      "O\u0012\u0003.Àõ/is the value putting proportion Àõof the bootstrap sample to its left.\n",
      "The level-Àõupper endpoint of the percentile interval, say O\u0012%ile≈íÀõ¬ç, is by\n",
      "deÔ¨Ånition\n",
      "O\u0012%ile≈íÀõ¬çDO\u0012\u0003.Àõ/DOG\u00001.Àõ/: (11.18)\n",
      "In this notation, the 95% central percentile interval is\n",
      "\u0010\n",
      "O\u0012%ile≈í:025¬ç;O\u0012%ile≈í:975¬ç\u0011\n",
      ": (11.19)188 Bootstrap ConÔ¨Ådence Intervals\n",
      "The construction is illustrated in Figure 11.4.\n",
      "The percentile intervals are transformation invariant. Let \u001eDm.\u0012/ as\n",
      "in (11.9), and likewise O\u001eDm.O\u0012/(m.\u0001/monotonically increasing), with\n",
      "bootstrap replications O\u001e\u0003bDm.O\u0012\u0003b/forbD1;2;:::;B . The bootstrap\n",
      "percentiles transform in the same way,\n",
      "O\u001e\u0003.Àõ/Dm\u0010\n",
      "O\u0012\u0003.Àõ/\u0011\n",
      "; (11.20)\n",
      "so that, as in (11.18),\n",
      "O\u001e%ile≈íÀõ¬çDm\u0010\n",
      "O\u0012%ile≈íÀõ¬ç\u0011\n",
      "; (11.21)\n",
      "verifying transformation invariance.\n",
      "In what sense does the percentile method improve upon the standard\n",
      "intervals? One answer involves transformation invariance. Suppose there\n",
      "exists a monotone transformation \u001eDm.\u0012/ andO\u001eDm.O\u0012/such that\n",
      "O\u001e\u0018N.\u001e;\u001b2/ (11.22)\n",
      "for every\u0012, with\u001b2constant. Fisher‚Äôs transformation (11.11)‚Äì(11.12) al-\n",
      "most accomplishes this for the normal correlation coefÔ¨Åcient.\n",
      "It would then be true that parametric bootstrap replications would also\n",
      "follow (11.22),\n",
      "O\u001e\u0003\u0018N\u0010\n",
      "O\u001e;\u001b2\u0011\n",
      ": (11.23)\n",
      "That is, the bootstrap cdf OG\u001ewould be normal with mean O\u001eand variance\n",
      "\u001b2. TheÀõth percentile ofOG\u001ewould equal\n",
      "O\u001e%ile≈íÀõ¬çDO\u001e\u0003.Àõ/DO\u001eCz.Àõ/\u001b; (11.24)\n",
      "wherez.Àõ/denotes theÀõth percentile of a standard normal distribution,\n",
      "z.Àõ/DÀÜ\u00001.Àõ/ (11.25)\n",
      "(z.:975/D1:96,z.:025/D\u00001:96, etc.).\n",
      "In other words, the percentile method would provide Fisher‚Äôs ‚Äúobviously\n",
      "correct‚Äù intervals for \u001e,\n",
      "O\u001eÀô1:96\u001b (11.26)\n",
      "for 95% coverage for example. But, because of transformation invariance,\n",
      "the percentile intervals for our original parameter \u0012would also be exactly\n",
      "correct.\n",
      "Some comments concerning the percentile method are pertinent.11.2 The Percentile Method 189\n",
      "\u000fThe method does not require actually knowing the transformation to nor-\n",
      "malityO\u001eDm.O\u0012/, it only assumes its existence.\n",
      "\u000fIf a transformation to form (11.22) exists, then the percentile intervals\n",
      "are not only accurate, but also correct in the Fisherian sense of giving\n",
      "the logically appropriate inference.¬é ¬é2\n",
      "\u000fThe justifying assumption for the standard intervals (11.15), O\u0012P \u0018N.\u0012,\n",
      "\u001b2/, becomes more accurate as the sample size nincreases (usually\n",
      "with\u001bdecreasing as 1=pn), but the convergence can be slow in cases\n",
      "like that of the normal correlation coefÔ¨Åcient. The broader assumption\n",
      "(11.22), thatm.O\u0012/P \u0018N.m.\u0012/;\u001b2/forsome transformation m.\u0001/, speeds\n",
      "up convergence, irrespective of whether or not it holds exactly. Sec-\n",
      "tion 11.4 makes this point explicit, in terms of asymptotic rates of con-\n",
      "vergence.\n",
      "\u000fThe standard method works Ô¨Åne once it is applied on an appropriate\n",
      "scale, as in Figure 11.2. The trouble is that the method is nottransforma-\n",
      "tion invariant, leaving the statistician the job of Ô¨Ånding the correct scale.\n",
      "The percentile method can be thought of as a transformation-invariant\n",
      "version of the standard intervals, an ‚Äúautomatic Fisher‚Äù that substitutes\n",
      "massive computations for mathematical ingenuity.\n",
      "\u000fThe method requires bootstrap sample sizes¬éon the order of BD2000 .¬é3\n",
      "\u000fThe percentile method is not the last word in bootstrap conÔ¨Ådence in-\n",
      "tervals. Two improvements, the ‚ÄúBC‚Äù and ‚ÄúBCa‚Äù methods, will be dis-\n",
      "cussed in the next section. Table 11.1 compares the various intervals as\n",
      "applied to the student score correlation, O\u0012D0:498 .\n",
      "Table 11.1 Bootstrap conÔ¨Ådence limits for student score correlation,\n",
      "O\u0012D0:498 ,nD22. Parametric exact limits from Neyman‚Äôs construction\n",
      "as in Figure 11.1. The BC and BCa methods are discussed in the next two\n",
      "sections;.z0;a/, two constants required for BCa, are .\u00000:055;0:005/\n",
      "parametric, and .0:000;0:006/ nonparametric.\n",
      "Parametric Nonparametric\n",
      ".025 .975 .025 .975\n",
      "1. Standard .17 .83 .18 .82\n",
      "2. Percentile .11 .77 .13 .76\n",
      "3. BC .08 .75 .13 .76\n",
      "4. BCa .08 .75 .12 .76\n",
      "Exact .09 .75\n",
      "The label ‚Äúcomputer-intensive inference‚Äù seems especially apt as ap-190 Bootstrap ConÔ¨Ådence Intervals\n",
      "plied to bootstrap conÔ¨Ådence intervals. Neyman and Fisher‚Äôs constructions\n",
      "are expanded from a few special theoretically tractable cases to almost any\n",
      "situation where the statistician has a repeatable algorithm. Automation, the\n",
      "replacement of mathematical formulas with wide-ranging computer algo-\n",
      "rithms, will be a major theme of succeeding chapters.\n",
      "11.3 Bias-Corrected ConÔ¨Ådence Intervals\n",
      "The ideal form (11.23) for the percentile method, O\u001e\u0003\u0018N.O\u001e;\u001b2/, says\n",
      "that the transformation O\u001eDm.O\u0012/yields an unbiased estimator of con-\n",
      "stant variance. The improved methods of this section and the next take into\n",
      "account the possibility of bias and changing variance. We begin with bias.\n",
      "IfO\u001e\u0018N.\u001e;\u001b2/for all\u001eDm.\u0012/ , as hypothesized in (11.22), then\n",
      "O\u001e\u0003\u0018N.O\u001e;\u001b2/and\n",
      "Pr\u0003n\n",
      "O\u001e\u0003\u0014O\u001eo\n",
      "D0:50 (11.27)\n",
      "(Pr\u0003indicating bootstrap probability), in which case the monotonicity of\n",
      "m.\u0001/gives\n",
      "Pr\u0003n\n",
      "O\u0012\u0003\u0014O\u0012o\n",
      "D0:50: (11.28)\n",
      "That is,O\u0012\u0003ismedian unbiased4forO\u0012, and likewiseO\u0012for\u0012.\n",
      "We can check that. For a parametric family of densities f\u0012.O\u0012/, (11.28)\n",
      "implies\n",
      "ZO\u0012\n",
      "\u00001fO\u0012\u0010\n",
      "O\u0012\u0003\u0011\n",
      "dO\u0012\u0003D0:50: (11.29)\n",
      "For the normal correlation coefÔ¨Åcient density (3.11), nD22, numerical\n",
      "integration gives\n",
      "Z:498\n",
      "\u00001f:498\u0010\n",
      "O\u0012\u0003\u0011\n",
      "dO\u0012\u0003D0:478; (11.30)\n",
      "which is not far removed from 0.50, but far enough to have a small impact\n",
      "on proper inference. It suggests that O\u0012\u0003is biased upward relative toO\u0012‚Äî\n",
      "that‚Äôs why lessthan half of the bootstrap probability lies below O\u0012‚Äîand\n",
      "by implication that O\u0012is upwardly biased for estimating \u0012. Accordingly,\n",
      "conÔ¨Ådence intervals should be adjusted a little bit downward. The bias-\n",
      "corrected percentile method (BC for short) is a data-based algorithm for\n",
      "making such adjustments.\n",
      "4Median unbiasedness, unlike the usual mean unbiasedness deÔ¨Ånition, has the advantage\n",
      "of being transformation invariant.11.3 Bias-Corrected ConÔ¨Ådence Intervals 191\n",
      "Having simulated Bbootstrap replications O\u0012\u00031;O\u0012\u00032;:::;O\u0012\u0003B, paramet-\n",
      "ric or nonparametric, let p0be the proportion of replications less than O\u0012,\n",
      "p0D#n\n",
      "O\u0012\u0003b\u0014O\u0012o.\n",
      "B (11.31)\n",
      "(an estimate of (11.29)), and deÔ¨Åne the bias-correction value\n",
      "z0DÀÜ\u00001.p0/; (11.32)\n",
      "whereÀÜ\u00001is the inverse function of the standard normal cdf. The BC\n",
      "level-ÀõconÔ¨Ådence interval endpoint is deÔ¨Åned to be\n",
      "O\u0012BC≈íÀõ¬çDOG\u00001h\n",
      "ÀÜ\u0010\n",
      "2z0Cz.Àõ/\u0011i\n",
      "; (11.33)\n",
      "whereOGis the bootstrap cdf (11.16) and z.Àõ/DÀÜ\u00001.Àõ/(11.25).\n",
      "Ifp0D0:50, the median unbiased situation, then z0D0and\n",
      "O\u0012BC≈íÀõ¬çDOG\u00001h\n",
      "ÀÜ\u0010\n",
      "z.Àõ/\u0011i\n",
      "DOG\u00001.Àõ/DO\u0012%ile≈íÀõ¬ç; (11.34)\n",
      "the percentile limit (11.18). Otherwise, a bias correction is made. Taking\n",
      "p0D0:478 for the normal correlation example (the value we would get\n",
      "from an inÔ¨Ånite number of parametric bootstrap replications) gives bias\n",
      "correction value\u00000:055 . Notice that the BC limits are indeed shifted down-\n",
      "ward from the parametric percentile limits in Table 11.1. Nonparametric\n",
      "bootstrapping gave p0about 0.50 in this case, making the BC limits nearly\n",
      "the same as the percentile limits.\n",
      "A more general transformation argument motivates the BC deÔ¨Ånition\n",
      "(11.33). Suppose there exists a monotone transformation \u001eDm.\u0012/ and\n",
      "O\u001eDm.O\u0012/such that for any \u0012\n",
      "O\u001e\u0018N.\u001e\u0000z0\u001b;\u001b2/; (11.35)\n",
      "withz0and\u001bÔ¨Åxed constants. Then the BC endpoints are accurate, i.e.,\n",
      "have the claimed coverage probabilities, and are also ‚Äúobviously correct‚Äù\n",
      "in the Fisherian sense. See the chapter endnotes¬éfor proof and discussion. ¬é4\n",
      "As before, the statistican does not need to know the transformation m.\u0001/\n",
      "that leads toO\u001e\u0018N.\u001e\u0000z0\u001b;\u001b2/, only that it exists. It is a broader target\n",
      "thanO\u001e\u0018N.\u001e;\u001b2/(11.22), making the BC method better justiÔ¨Åed than\n",
      "the percentile method, irrespective of whether or not such a transformation\n",
      "exists. There is no extra computational burden: the bootstrap replications\n",
      "fO\u0012\u0003b; bD1;2;:::;Bg, parametric or nonparametric, provide OG(11.16)\n",
      "andz0(11.31)‚Äì(11.32), giving O\u0012BC≈íÀõ¬çfrom (11.33).192 Bootstrap ConÔ¨Ådence Intervals\n",
      "11.4 Second-Order Accuracy\n",
      "Coverage errors of the standard conÔ¨Ådence intervals typically decrease at\n",
      "orderO.1=pn/in the sample size n: having calculatedO\u0012stan≈íÀõ¬çDO\u0012Cz.Àõ/O\u001b\n",
      "for an iid sample xD.x1;x2;:::;xn/, we can expect the actual coverage\n",
      "probability to be\n",
      "Pr\u0012n\n",
      "\u0012\u0014O\u0012stan≈íÀõ¬ço:DÀõCc1ƒ±pn; (11.36)\n",
      "wherec1depends on the problem at hand; (11.36) deÔ¨Ånes ‚ÄúÔ¨Årst-order accu-\n",
      "racy.‚Äù It can connote painfully slow convergence to the nominal coverage\n",
      "levelÀõ, requiring sample size 4nto cut the error in half.\n",
      "Asecond-order accurate method, sayO\u00122nd≈íÀõ¬ç, makes errors of order only\n",
      "O.1=n/ ,\n",
      "Pr\u0012n\n",
      "\u0012\u0014O\u00122nd≈íÀõ¬ço:DÀõCc2=n: (11.37)\n",
      "The improvement is more than theoretical. In practical problems like that\n",
      "of Table 11.1, second-order accurate methods‚ÄîBCa, deÔ¨Åned in the follow-\n",
      "ing, is one such‚Äîoften provide nearly the claimed coverage probabilities,\n",
      "even in small-size samples.\n",
      "Neither the percentile method nor the BC method is second-order ac-\n",
      "curate (although, as in Table 11.1, they tend to be more accurate than the\n",
      "standard intervals). The difÔ¨Åculty for O\u0012BC≈íÀõ¬çlies in the ideal form (11.35),\n",
      "O\u001e\u0018N.\u001e\u0000z0\u001b;\u001b2/, where it is assumed O\u001eDm.O\u0012/hasconstant standard\n",
      "error\u001b. Instead, we now postulate the existence of a monotone transforma-\n",
      "tion\u001eDm.\u0012/ andO\u001eDm.O\u0012/less restrictive than (11.35),\n",
      "O\u001e\u0018N.\u001e\u0000z0\u001b\u001e;\u001b2\n",
      "\u001e/; \u001b\u001eD1Ca\u001e: (11.38)\n",
      "Here the ‚Äúacceleration‚Äù¬éais a small constant describing how the standard ¬é5\n",
      "deviation ofO\u001evaries with\u001e. IfaD0we are back in situation (11.34)5,\n",
      "but if not, an amendment to the BC formula (11.33) is required.\n",
      "The BCa method (‚Äúbias-corrected and accelerated‚Äù) takes its level- Àõ\n",
      "conÔ¨Ådence limit to be\n",
      "O\u0012BCa≈íÀõ¬çDOG\u00001\u0014\n",
      "ÀÜ\u0012\n",
      "z0Cz0Cz.Àõ/\n",
      "1\u0000a.z0Cz.Àõ//\u0013\u0015\n",
      ": (11.39)\n",
      "A still more elaborate transformation argument shows that, if there exists\n",
      "a monotone transformation \u001eDm.\u0012/ and constants z0andayielding\n",
      "5This assumes\u001b0D1on the right side of (11.38), which can always be achieved by\n",
      "further transforming \u001eto\u001e=\u001b .11.4 Second-Order Accuracy 193\n",
      "(11.38), then the BCa limits have their claimed coverage probabilities and,\n",
      "moreover, are correct in the Fisherian sense.\n",
      "BCa makes three corrections to the standard intervals (11.1): for non-\n",
      "normality ofO\u0012(through using the bootstrap percentiles rather than just the\n",
      "bootstrap standard error); for bias (through the bias correction value z0);\n",
      "and for nonconstant standard error (through a). Notice that if aD0then\n",
      "BCa (11.39) reduces to BC (11.33). If z0D0then BC reduces to the\n",
      "percentile method (11.18); and if OG, the bootstrap histogram, is normal,\n",
      "then (11.18) reduces to the standard interval (11.1). All three of the correc-\n",
      "tions, for nonnormality, bias, and acceleration, can have substantial effects\n",
      "in practice and are necessary to achieve second-order accuracy. A great\n",
      "deal of theoretical effort was devoted to verifying the second-order accu-\n",
      "racy and BCa intervals under reasonably general assumptions.6\n",
      "Table 11.2 Nominal 95% central conÔ¨Ådence intervals for Poisson\n",
      "parameter\u0012having observedO\u0012D10; actual tail areas above and below\n",
      "O\u0012D10deÔ¨Åned as in Figure 11.1 (atom of probability split at 10). For\n",
      "instance, lower standard limit 3.80 actually puts probability 0.004 above\n",
      "10, rather than nominal value 0.025. Bias correction value z0(11.32) and\n",
      "accelerationa(11.38) both equal 0.050.\n",
      "Nominal limits Tail areas\n",
      ".025 .975 Above Below\n",
      "1. Standard 3.80 16.20 .004 .055\n",
      "2. %ile 4.18 16.73 .007 .042\n",
      "3. BC 4.41 17.10 .010 .036\n",
      "4. BCa 5.02 17.96 .023 .023\n",
      "Exact 5.08 17.82 .025 .025\n",
      "The advantages of increased accuracy are not limited to large sample\n",
      "sizes. Table 11.2 returns to our original example of observing O\u0012D10\n",
      "from Poisson model O\u0012\u0018Poi.\u0012/. According to Neyman‚Äôs construction,\n",
      "the 0.95 exact limits give tail areas 0.025 in both the above and below\n",
      "directions, as in Figure 11.1, and this is nearly matched by the BCa limits.\n",
      "However the standard limits are much too conservative at the left end and\n",
      "anti-conservative at the right.\n",
      "6The mathematical side of statistics has also been affected by electronic computation,\n",
      "where it is called upon to establish the properties of general-purpose computer\n",
      "algorithms such as the bootstrap. Asymptotic analysis in particular has been aggressively\n",
      "developed, the veriÔ¨Åcation of second-order accuracy being a nice success story.194 Bootstrap ConÔ¨Ådence Intervals\n",
      "Table 11.3 95% nominal conÔ¨Ådence intervals for the parametric and\n",
      "nonparametric eigenratio examples of Figures 10.2 and 10.6.\n",
      "Parametric Nonparametric\n",
      ".025 .975 .025 .975\n",
      "1. Standard .556 .829 .545 .840\n",
      "2. %ile .542 .815 .517 .818\n",
      "3. BC .523 .828 .507 .813\n",
      "4. BCa .555 .820 .523 .828\n",
      ".z0D\u0000:029; aD:058/ .z 0D\u0000:049; aD:051/\n",
      "Bootstrap conÔ¨Ådence limits continue to provide better inferences in the\n",
      "vast majority of situations too complicated for exact analysis. One such\n",
      "situation is examined in Table 11.3. It relates to the eigenratio example\n",
      "illustrated in Figures 10.2‚Äì10.6. In this case the nonnormality and bias cor-\n",
      "rections stretch the bootstrap intervals to the left, but the acceleration effect\n",
      "pulls right, partially canceling out the net change from the standard inter-\n",
      "vals.\n",
      "The percentile and BC methods are completely automatic, and can be\n",
      "applied whenever a sufÔ¨Åciently large number of bootstrap replications are\n",
      "available. The same cannot be said of BCa. A drawback of the BCa method\n",
      "is that the acceleration ais not a function of the bootstrap distribution and\n",
      "must be computed separately. Often this is straightforward:\n",
      "\u000fFor one-parameter exponential families such as the Poisson, aequalsz0.\n",
      "\u000fIn one-sample nonparametric problems, acan be estimated from the\n",
      "jackknife resamples O\u0012.i/(10.5),\n",
      "OaD1\n",
      "6Pn\n",
      "iD1\u0010\n",
      "O\u0012.i/\u0000O\u0012.\u0001/\u00113\n",
      "\u0014Pn\n",
      "iD1\u0010\n",
      "O\u0012.i/\u0000O\u0012.\u0001/\u00112\u00151:5: (11.40)\n",
      "\u000fTheabc method computesain multiparameter exponential families (5.54),\n",
      "as does the resampling-based Ralgorithm accel .\n",
      "ConÔ¨Ådence intervals require the number of bootstrap replications Bto\n",
      "be on the order of 2000, rather than the 200 or fewer needed for standard\n",
      "errors; the corrections made to the standard intervals are more delicate than\n",
      "standard errors and require greater accuracy.\n",
      "There is one more cautionary note to sound concerning nuisance param-\n",
      "eters: biases can easily get out of hand when the parameter vector \u0016is11.5 Bootstrap- tIntervals 195\n",
      "high-dimensional. Suppose we observe\n",
      "xiind\u0018N.\u0016i;1/ foriD1;2;:::;n; (11.41)\n",
      "and wish to set a conÔ¨Ådence interval for \u0012DPn\n",
      "1\u00162\n",
      "i. The MLEO\u0012DPn\n",
      "1x2\n",
      "i\n",
      "will be sharply biased upward if nis at all large. To be speciÔ¨Åc, if nD10\n",
      "andO\u0012D20, we compute¬é ¬é6\n",
      "z0DÀÜ\u00001.0:156/D\u00001:01: (11.42)\n",
      "This makes7O\u0012BC≈í:025¬ç (11.33) equal a ludicrously small bootstrap per-\n",
      "centile,\n",
      "OG\u00001.0:000034/; (11.43)\n",
      "a warning sign against the BC or BCa intervals, which work most depend-\n",
      "ably forjz0jandjajsmall, say\u00140:2.\n",
      "A more general warning would be against blind trust in maximum likeli-\n",
      "hood estimates in high dimensions. Computing z0is a wise precaution even\n",
      "if it is not used for BC or BCa purposes, in case it alerts one to dangerous\n",
      "biases.\n",
      "ConÔ¨Ådence intervals for classical applications were most often based on\n",
      "the standard method (11.1) (with bse estimated by the delta method) except\n",
      "in a few especially simple situations such as the Poisson. Second-order ac-\n",
      "curate intervals are very much a computer-age development, with both the\n",
      "algorithms and the inferential theory presupposing high-speed electronic\n",
      "computation.\n",
      "11.5 Bootstrap- tIntervals\n",
      "The initial breakthrough on exact conÔ¨Ådence intervals came in the form of\n",
      "Student‚Äôstdistribution in 1908. Suppose we independently observe data\n",
      "from two possibly different normal distributions, xD.x1;x2;:::;xnx/\n",
      "andyD.y1;y2;:::;yny/,\n",
      "xiiid\u0018N.\u0016x;\u001b2/andyiiid\u0018N.\u0016y;\u001b2/; (11.44)\n",
      "and wish to form a 0.95 central conÔ¨Ådence interval for\n",
      "\u0012D\u0016y\u0000\u0016x: (11.45)\n",
      "The obvious estimate is\n",
      "O\u0012DNy\u0000Nx; (11.46)\n",
      "7AlsoO\u0012BCa≈í:025¬ç ,ais zero in this model.196 Bootstrap ConÔ¨Ådence Intervals\n",
      "but its distribution depends on the nuisance parameter \u001b2.\n",
      "Student‚Äôs masterstroke was to base inference about \u0012on the pivotal\n",
      "quantity\n",
      "tDO\u0012\u0000\u0012\n",
      "bse(11.47)\n",
      "wherebse2is an unbiased estimate of \u001b2,\n",
      "bse2D\u00121\n",
      "nxC1\n",
      "ny\u0013Pnx\n",
      "1.xi\u0000Nx/2CPny\n",
      "1.yi\u0000Ny/2\n",
      "nxCny\u00002I (11.48)\n",
      "tthen has the ‚ÄúStudent‚Äôs tdistribution‚Äù with df DnxCny\u00002degrees of\n",
      "freedom if\u0016xD\u0016y, no matter what \u001b2may be.\n",
      "Lettingt.Àõ/\n",
      "dfrepresent the 100 Àõth percentile of a tdfdistribution yields\n",
      "O\u0012t≈íÀõ¬çDO\u0012\u0000bse\u0001t.1\u0000Àõ/\n",
      "df (11.49)\n",
      "as the upper level- Àõinterval of a Student‚Äôs tconÔ¨Ådence limit. Applied to\n",
      "the difference between the AML andALL scores in Figure 1.4, the central\n",
      "0.95 Student‚Äôs tinterval for\u0012DEfAMLg\u0000EfALLgwas calculated to be\n",
      "\u0010\n",
      "O\u0012t≈í:025¬ç;O\u0012t≈í:975¬ç\u0011\n",
      "D.:062;:314/: (11.50)\n",
      "HerenxD47,nyD25, and dfD70.\n",
      "Student‚Äôs theory depends on the normality assumptions of (11.44). The\n",
      "bootstrap-tapproach is to accept (or pretend) that tin (11.47) is pivotal,\n",
      "but to estimate its distribution via bootstrap resampling. Nonparametric\n",
      "bootstrap samples are drawn separately from xandy,\n",
      "x\u0003D.x\u0003\n",
      "1;x\u0003\n",
      "2;:::;x\u0003\n",
      "nx/andy\u0003D.y\u0003\n",
      "1;y\u0003\n",
      "2;:::;y\u0003\n",
      "ny/; (11.51)\n",
      "from which we calculate O\u0012\u0003andbse\u0003, (11.46) and (11.48), giving\n",
      "t\u0003DO\u0012\u0003\u0000O\u0012\n",
      "bse\u0003; (11.52)\n",
      "withO\u0012playing the role of \u0012, as appropriate in the bootstrap world. Repli-\n",
      "cationsft\u0003b;bD1;2;:::;Bgprovide estimated percentiles t\u0003.Àõ/and cor-\n",
      "responding conÔ¨Ådence limits\n",
      "O\u0012\u0003\n",
      "t≈íÀõ¬çDO\u0012\u0000bse\u0001t\u0003.1\u0000Àõ/: (11.53)\n",
      "For the AML‚ÄìALL example, the t\u0003distribution differed only slightly\n",
      "from at70distribution; the resulting 0.95 interval was .0:072;0:323/ , nearly11.5 Bootstrap- tIntervals 197\n",
      "the same as (11.50), lending credence to the original normality assump-\n",
      "tions.\n",
      " \n",
      "t* valuesFrequency\n",
      "0 50 100 150 200\n",
      "t distribution\n",
      "df = 21\n",
      "‚àí3 ‚àí2 ‚àí1 0 1 2 3 4‚àí1.64 2.59\n",
      "Figure 11.5 BD2000 nonparametric replications of bootstrap- t\n",
      "statistic for the student score correlation; small triangles show\n",
      "0.025 and 0.975 percentile points. The histogram is sharply\n",
      "skewed to the right; the solid curve is Student‚Äôs tdensity for 21\n",
      "degrees of freedom.\n",
      "Returning to the student score correlation example of Table 11.1, we can\n",
      "apply bootstrap- tmethods by still taking tD.O\u0012\u0000\u0012/=bse to be notionally\n",
      "pivotal, but now with \u0012the true correlation, O\u0012the sample correlation, and\n",
      "bse the approximate standard error .1\u0000O\u00122/=p\n",
      "19. Figure 11.5 shows the\n",
      "histogram of BD2000 nonparametric bootstrap replications t\u0003D.O\u0012\u0003\u0000\n",
      "O\u0012/=bse\u0003. These gave bootstrap percentiles\n",
      "\u0010\n",
      "t\u0003.:025/;t\u0003.:975/\u0011\n",
      "D.\u00001:64;2:59/ (11.54)\n",
      "(which might be compared with .\u00002:08;2:08/ for a standard t21distribu-\n",
      "tion), and 0.95 interval .0:051;0:781/ from (11.53), somewhat out of place\n",
      "compared with the other entries in the right panel of Table 11.1.\n",
      "Bootstrap-tintervals are nottransformation invariant. This means they\n",
      "can perform poorly or well depending on the scale of application. If per-\n",
      "formed on Fisher‚Äôs scale (11.11) they agree well with exact intervals for198 Bootstrap ConÔ¨Ådence Intervals\n",
      "the correlation coefÔ¨Åcient. A practical difÔ¨Åculty is the requirement of a for-\n",
      "mula forbse.\n",
      "Nevertheless, the idea of estimating the actual distribution of a proposed\n",
      "pivotal quantity has great appeal to the modern statistical spirit. Calculat-\n",
      "ing the percentiles of the original Student tdistribution was a multi-year\n",
      "project in the early twentieth century. Now we can afford to calculate our\n",
      "own special ‚Äú ttable‚Äù for each new application. Spending such computa-\n",
      "tional wealth wisely, while not losing one‚Äôs inferential footing, is the cen-\n",
      "tral task and goal of twenty-Ô¨Årst-century statisticians.\n",
      "11.6 Objective Bayes Intervals and the ConÔ¨Ådence\n",
      "Distribution\n",
      "Interval estimates are ubiquitous. They play a major role in the scientiÔ¨Åc\n",
      "discourse of a hundred disciplines, from physics, astronomy, and biology\n",
      "to medicine and the social sciences. Neyman-style frequentist conÔ¨Ådence\n",
      "intervals dominate the literature, but there have been inÔ¨Çuential Bayesian\n",
      "and Fisherian developments as well, as discussed next.\n",
      "Given a one-parameter family of densities f\u0012.O\u0012/and a prior density\n",
      "g.\u0012/ , Bayes‚Äô rule (3.5) produces the posterior density of \u0012,\n",
      "g.\u0012jO\u0012/Dg.\u0012/f\u0012.O\u0012/=f.O\u0012/; (11.55)\n",
      "wheref.O\u0012/is the marginal densityR\n",
      "f\u0012.O\u0012/g.\u0012/d\u0012 . The Bayes 0.95 cred-\n",
      "ible interval C.\u0012jO\u0012/spans the central 0.95 region of g.\u0012jO\u0012/, say\n",
      "C.\u0012jO\u0012/D.a.O\u0012/;b.O\u0012//; (11.56)\n",
      "with\n",
      "Zb.O\u0012/\n",
      "a.O\u0012/g.\u0012jO\u0012/d\u0012D0:95; (11.57)\n",
      "and with posterior probability 0.025 in each tail region.\n",
      "ConÔ¨Ådence intervals, of course, require no prior information, making\n",
      "them eminently useful in day-to-day applied practice. The Bayesian equiv-\n",
      "alents are credible intervals based on uninformative priors, Section 3.2.\n",
      "‚ÄúMatching priors,‚Äù those whose credible intervals nearly match Neyman\n",
      "conÔ¨Ådence intervals, have been of particular interest. Jeffreys‚Äô prior (3.17),\n",
      "g.\u0012/DI1=2\n",
      "\u0012;\n",
      "I\u0012DZ\u0014@\n",
      "@\u0012logf\u0012.O\u0012/\u00152\n",
      "f\u0012.O\u0012/dO\u0012;(11.58)11.6 Objective Bayes intervals 199\n",
      "provides a generally accurate matching prior for one-parameter problems.\n",
      "Figure 3.2 illustrates this for the student score correlation, where the credi-\n",
      "ble interval.0:093;0:750/ is a near-exact match to the Neyman 0.95 inter-\n",
      "val of Figure 11.1.\n",
      "DifÔ¨Åculties begin with multiparameter families f\u0016.x/(5.1): we wish to\n",
      "construct an interval estimate for a one-dimensional function \u0012Dt.\u0016/ of\n",
      "thep-dimensional parameter vector \u0016, and must somehow remove the ef-\n",
      "fects of thep\u00001‚Äúnuisance parameters.‚Äù In a few rare situations, including\n",
      "the normal theory correlation coefÔ¨Åcient, this can be done exactly. Pivotal\n",
      "methods do the job for Student‚Äôs tconstruction. Bootstrap conÔ¨Ådence inter-\n",
      "vals greatly extend the reach of such methods, at a cost of greatly increased\n",
      "computation.\n",
      "Bayesians get rid of nuisance parameters by integrating them out of the\n",
      "posterior density g.\u0016jx/Dg.\u0016/f\u0016.x/=f.x/(3.6) (xnow representing\n",
      "all the data, ‚Äúx‚Äù equaling.x;y/for the Student tsetup (11.44)). That is,\n",
      "we calculate8the marginal density of \u0012Dt.\u0016/ givenx, and call ith.\u0012jx/.\n",
      "A credible interval for \u0012,C.\u0012jx/, is then constructed as in (11.56)‚Äì(11.57),\n",
      "withh.\u0012jx/playing the role of g.\u0012jO\u0012/. This leaves us the knotty problem\n",
      "of choosing an uninformative multidimensional prior g.\u0016/ . We will return\n",
      "to the question after Ô¨Årst discussing Ô¨Åducial methods, a uniquely Fisherian\n",
      "device.\n",
      "Fiducial constructions begin with what seems like an obviously incorrect\n",
      "interpretation of pivotality. We rewrite the Student tpivotaltD.O\u0012\u0000\u0012/=bse\n",
      "(11.47) as\n",
      "\u0012DO\u0012\u0000bse\u0001t; (11.59)\n",
      "wherethas a Student‚Äôs tdistribution with df degrees of freedom, t\u0018\n",
      "tdf. Having observed the data .x;y/(11.44), Ô¨Åducial theory assigns \u0012the\n",
      "distribution implied by (11.59), as if O\u0012andbse were Ô¨Åxed at their calculated\n",
      "values whiletwas distributed as tdf. ThenO\u0012t≈íÀõ¬ç(11.49), the Student tlevel-\n",
      "ÀõconÔ¨Ådence limit, is the 100 Àõth percentile of \u0012‚Äôs Ô¨Åducial distribution.\n",
      "We seem to have achieved a Bayesian posterior conclusion without any\n",
      "prior assumptions.9The historical development here is confused by Fisher‚Äôs\n",
      "refusal to accept Neyman‚Äôs conÔ¨Ådence interval theory, as well as his dispar-\n",
      "agement of Bayesian ideas. As events worked out, all of Fisher‚Äôs immense\n",
      "prestige was not enough to save Ô¨Åducial theory from the scrapheap of failed\n",
      "statistical methods.\n",
      "8Often a difÔ¨Åcult calculation, as discussed in Chapter 13.\n",
      "9‚ÄúEnjoying the Bayesian omelette without breaking the Bayesian eggs,‚Äù in L. J. Savage‚Äôs\n",
      "words.200 Bootstrap ConÔ¨Ådence Intervals\n",
      "And yet, in Arthur Koestler‚Äôs words, ‚ÄúThe history of ideas is Ô¨Ålled with\n",
      "barren truths and fertile errors.‚Äù Fisher‚Äôs underlying rationale went some-\n",
      "thing like this:O\u0012andbse exhaust the information about \u0012available from the\n",
      "data, after which there remains an irreducible component of randomness\n",
      "described by t. This is an idea of substantial inferential appeal, and one\n",
      "that can be rephrased in more general terms discussed next that bear on the\n",
      "question of uninformative priors.\n",
      "By deÔ¨Ånition, an upper conÔ¨Ådence limit O\u0012x≈íÀõ¬çsatisÔ¨Åes\n",
      "Prn\n",
      "\u0012\u0014O\u0012x≈íÀõ¬ço\n",
      "DÀõ (11.60)\n",
      "(where now we have indicated the observed data xin the notation), and so\n",
      "Prn\n",
      "O\u0012x≈íÀõ¬ç\u0014\u0012\u0014O\u0012x≈íÀõC\u000f¬ço\n",
      "D\u000f: (11.61)\n",
      "We can considerO\u0012x≈íÀõ¬ças a one-to-one function between Àõin.0;1/ and\u0012a\n",
      "point in its parameter space ‚Äö(assuming thatO\u0012x≈íÀõ¬çis smoothly increasing\n",
      "inÀõ). Letting\u000fgo to zero in (11.61) determines the conÔ¨Ådence density of\n",
      "\u0012, sayQgx.\u0012/,\n",
      "Qgx.\u0012/DdÀõ=d\u0012; (11.62)\n",
      "the local derivative of probability at location \u0012for the unknown parameter,\n",
      "the derivative being taken at \u0012DO\u0012x≈íÀõ¬ç.\n",
      "IntegratingQgx.\u0012/recoversÀõas a function of \u0012. Let\u00121DO\u0012x≈íÀõ1¬çand\n",
      "\u00122DO\u0012x≈íÀõ2¬çfor any two values Àõ1<Àõ2in.0;1/ . Then\n",
      "Z\u00122\n",
      "\u00121Qgx.\u0012/d\u0012DZ\u00122\n",
      "\u00121dÀõ\n",
      "d\u0012d\u0012DÀõ2\u0000Àõ1\n",
      "DPrf\u00121\u0014\u0012\u0014\u00122g;(11.63)\n",
      "as in (11.60). There is nothing controversial about (11.63) as long as we\n",
      "remember that the random quantity in Pr f\u00121\u0014\u0012\u0014\u00122gis not\u0012but rather\n",
      "the interval.\u00121;\u00122/, which varies as a function of x. Forgetting this leads to\n",
      "the textbook error of attributing Bayesian properties to frequentist results:\n",
      "‚ÄúThere is 0.95 probability that \u0012is in its 0.95 conÔ¨Ådence interval,‚Äù etc.\n",
      "This is exactly what the Ô¨Åducial argument does.10Whether or not one\n",
      "accepts (11.63), there is an immediate connection with matching priors .\n",
      "10Fiducial and conÔ¨Ådence densities agree, as can be seen in the Student tsituation (11.59),\n",
      "at least in the somewhat limited catalog of cases Fisher thought appropriate for Ô¨Åducial\n",
      "calculations.11.6 Objective Bayes intervals 201\n",
      "Suppose prior g.\u0016/ gives a perfect match to the conÔ¨Ådence interval system\n",
      "O\u0012x≈íÀõ¬ç. Then, by deÔ¨Ånition, its posterior density h.\u0012jx/must satisfy\n",
      "ZO\u0012x≈íÀõ¬ç\n",
      "\u00001h.\u0012jx/d\u0012DÀõDZO\u0012x≈íÀõ¬ç\n",
      "\u00001Qgx.\u0012/d\u0012 (11.64)\n",
      "for0 < Àõ < 1 . But this implies h.\u0012jx/equalsQgx.\u0012/for all\u0012. That is,\n",
      "the conÔ¨Ådence density Qgx.\u0012/is the posterior density of \u0012givenxfor any\n",
      "matching prior .\n",
      "0 5 10 15 20 250.000.020.040.060.080.100.120.14\n",
      "Œ∏Confidence Density\n",
      "5.08 17.82‚óè\n",
      "10\n",
      "Figure 11.6 ConÔ¨Ådence density (11.62) for Poisson parameter \u0012\n",
      "having observedO\u0012D10. There is area 0.95 under the curve\n",
      "between 5.08 and 17.82, as in Table 11.2, and areas 0.025 in each\n",
      "tail.\n",
      "Figure 11.6 graphs the conÔ¨Ådence density for O\u0012\u0018Poi.\u0012/having ob-\n",
      "servedO\u0012D10. This was obtained by numerically differentiating Àõas a\n",
      "function of\u0012(11.62),\n",
      "ÀõDPrf10\u0014Poi.\u0012/g; (11.65)\n",
      "‚Äú\u0014‚Äù including splitting the atom of probability at 10. According to Ta-\n",
      "ble 11.2,Qg10.\u0012/has area 0.95 between 5.08 and 17.82, and area 0.025 in\n",
      "each tail. Whatever its provenance, the graph delivers a striking picture of\n",
      "the uncertainty in the unknown value of \u0012.202 Bootstrap ConÔ¨Ådence Intervals\n",
      "Bootstrap conÔ¨Ådence intervals provide easily computable conÔ¨Ådence den-\n",
      "sities. LetOG.\u0012/ be the bootstrap cdf and Og.\u0012/ its density function (ob-\n",
      "tained by differentiating a smoothed version of OG.\u0012/ whenOGis based on\n",
      "Bbootstrap replications). The percentile conÔ¨Ådence limits O\u0012≈íÀõ¬çDOG\u00001.Àõ/\n",
      "(11.17) haveÀõDOG.\u0012/ , giving\n",
      "Qgx.\u0012/DOg.\u0012/: (11.66)\n",
      "(It is helpful to picture this in Figure 11.4.) For the percentile method, the\n",
      "bootstrap density isthe conÔ¨Ådence density.\n",
      "For the BCa intervals (11.39), the conÔ¨Ådence density is obtained by\n",
      "reweightingOg.\u0012/ ,\n",
      "Qgx.\u0012/Dcw.\u0012/Og.\u0012/; (11.67)\n",
      "where¬é¬é7\n",
      "w.\u0012/D'≈íz\u0012=.1Caz\u0012/\u0000z0¬ç\n",
      ".1Caz\u0012/2'.z\u0012Cz0/;withz\u0012DÀÜ\u00001OG.\u0012/\u0000z0:(11.68)\n",
      "Here'is the standard normal density, ÀÜits cdf, andcthe constant that\n",
      "makesQgx.\u0012/integrate to 1. In the usual case where the bootstrap cdf is es-\n",
      "timated from replications O\u0012\u0003b,bD1;2;:::;B (either parametric or non-\n",
      "parametric), the BCa conÔ¨Ådence density is a reweighted version of Og.\u0012/ .\n",
      "DeÔ¨Åne\n",
      "WbDw\u0010\n",
      "O\u0012\u0003b\u0011,BX\n",
      "iD1w\u0010\n",
      "O\u0012\u0003i\u0011\n",
      ": (11.69)\n",
      "Then the BCa conÔ¨Ådence density is the discrete density putting weight Wb\n",
      "onO\u0012\u0003b.\n",
      "Figure 11.7 returns to the student score data, nD22students, Ô¨Åve scores\n",
      "each, modeled normally as in Figure 10.6,\n",
      "xiiid\u0018N5.\u0015;‚Ä†/ foriD1;2;:::;22: (11.70)\n",
      "This is apD20-dimensional parametric family: 5 expectations, 5 vari-\n",
      "ances, 10 covariances. The parameter of interest was taken to be\n",
      "\u0012Dmaximum eigenvalue of ‚Ä†: (11.71)\n",
      "It had MLEO\u0012D683, this being the maximum eigenvalue of the MLE\n",
      "sample covariance matrix O‚Ä†(dividing each sum of squares by 22 rather\n",
      "than 21).\n",
      "BD8000 parametric bootstrap replications11O\u0012\u0003bgave percentile and\n",
      "11BD2000 would have been enough for most purposes, but BD8000 gave a sharper\n",
      "picture of the different curves.11.6 Objective Bayes intervals 203\n",
      "500 1000 15000 200 400 600 800\n",
      "Œ∏: maximum eigenvaluePosterior DensityBCa\n",
      "JeffreysPercentile\n",
      "683‚óè\n",
      "Figure 11.7 ConÔ¨Ådence densities for the maximum eigenvalue\n",
      "parameter (11.71), using a multivariate normal model (11.70) for\n",
      "the student score data. The dashed red curve is the percentile\n",
      "method, solid black the BCa (with .z0;a/D.0:178;0:093/ ). The\n",
      "dotted blue curve is the Bayes posterior density for \u0012, using\n",
      "Jeffreys‚Äô prior (11.72).\n",
      "BCa conÔ¨Ådence densities as shown. In this case the weights Wb(11.69)\n",
      "increased withO\u0012\u0003b, pushing the BCa density to the right. Also shown is the\n",
      "Bayes posterior density¬éfor\u0012starting from Jeffreys‚Äô multiparameter prior ¬é8\n",
      "density\n",
      "gJeff.\u0016/DjI\u0016j1=2; (11.72)\n",
      "whereI\u0016is the Fisher information matrix (5.26). It isn‚Äôt truly uninforma-\n",
      "tive here, moving its credible limits upward from the second-order accurate\n",
      "BCa conÔ¨Ådence limits. Formula (11.72) is discussed further in Chapter 13.\n",
      "Bayesian data analysis has the attractive property that, after examin-\n",
      "ing the data, we can express our remaining uncertainty in the language\n",
      "of probability. Fiducial and conÔ¨Ådence densities provide something similar\n",
      "for conÔ¨Ådence intervals, at least partially freeing the frequentist from the\n",
      "interpretive limitations of Neyman‚Äôs intervals.204 Bootstrap ConÔ¨Ådence Intervals\n",
      "11.7 Notes and Details\n",
      "Fisher‚Äôs theory of Ô¨Åducial inference (1930) preceded Neyman‚Äôs approach,\n",
      "formalized in (1937), which was presented as an attempt to put interval es-\n",
      "timation on a Ô¨Årm probabilistic basis, as opposed to the mysteries of Ô¨Ådu-\n",
      "cialism. The result was an elegant theory of exact and optimal intervals,\n",
      "phrased in hard-edged frequentistic terms. Readers familiar with the the-\n",
      "ory will know that Neyman‚Äôs construction‚Äîa favorite name in the physics\n",
      "literature‚Äîas pictured in Figure 11.1, requires some conditions on the fam-\n",
      "ily of densities f\u0012.O\u0012/to yield optimal intervals, a sufÔ¨Åcient condition being\n",
      "monotone likelihood ratios.\n",
      "Bootstrap conÔ¨Ådence intervals, Efron (1979, 1987), are neither exact nor\n",
      "optimal, but aim instead for wide applicability combined with near-exact\n",
      "accuracy. Second-order acuracy of BCa intervals was established by Hall\n",
      "(1988). BCa is emphatically a child of the computer age, routinely requir-\n",
      "ingBD2000 or more bootstrap replications per use. Shortcut methods are\n",
      "available. The ‚Äúabc method‚Äù (DiCiccio and Efron, 1992) needs only 1% as\n",
      "much computation, at the expense of requiring smoothness properties for\n",
      "\u0012Dt.\u0016/, and a less automatic coding of the exponential family setting for\n",
      "individual situations. In other words, it is less convenient.\n",
      "¬é1[p. 183] Neyman‚Äôs construction. For any given value of \u0012, let.\u0012.:025/;\u0012.:975//\n",
      "denote the central 95% interval of density f\u0012.O\u0012/, satisfying\n",
      "Z\u0012.:025/\n",
      "\u00001f\u0012\u0010\n",
      "O\u0012\u0011\n",
      "dO\u0012D0:025 andZ\u0012.:975/\n",
      "\u00001f\u0012\u0010\n",
      "O\u0012\u0011\n",
      "dO\u0012D0:975I\n",
      "(11.73)\n",
      "and letI\u0012.O\u0012/be the indicator function for O\u00122.\u0012.:025/;\u0012.:975//,\n",
      "I\u0012\u0010\n",
      "O\u0012\u0011\n",
      "D(\n",
      "1if\u0012.:025/<O\u0012 <\u0012.:975/\n",
      "0otherwise.(11.74)\n",
      "By deÔ¨Ånition, I\u0012.O\u0012/has a two-point probability distribution,\n",
      "I\u0012\u0010\n",
      "O\u0012\u0011\n",
      "D(\n",
      "1probability0:95\n",
      "0probability0:05:(11.75)\n",
      "This makesI\u0012.O\u0012/a pivotal statistic, one whose distribution does not de-\n",
      "pend upon\u0012.\n",
      "Neyman‚Äôs construction takes the conÔ¨Ådence interval C.O\u0012/corresponding\n",
      "to observed valueO\u0012to be\n",
      "C\u0010\n",
      "O\u0012\u0011\n",
      "Dn\n",
      "\u0012WI\u0012\u0010\n",
      "O\u0012\u0011\n",
      "D1o\n",
      ": (11.76)11.7 Notes and Details 205\n",
      "ThenC.O\u0012/has the desired coverage property\n",
      "Pr\u0012n\n",
      "\u00122C\u0010\n",
      "O\u0012\u0011o\n",
      "DPr\u0012n\n",
      "I\u0012\u0010\n",
      "O\u0012\u0011\n",
      "D1o\n",
      "D0:95 (11.77)\n",
      "for any choice of the true parameter \u0012. (For the normal theory correlation\n",
      "density off\u0012.O\u0012/,O\u0012.:025/ andO\u0012.:975/ are increasing functions of \u0012. This\n",
      "makes our previous construction (11.6) agree with (11.76).) The construc-\n",
      "tion applies quite generally, as long as we are able to deÔ¨Åne acceptance\n",
      "regions of the sample space having the desired target probability content\n",
      "for every choice of \u0012. This can be challenging in multiparameter families.\n",
      "¬é2[p. 189] Fisherian correctness. Fisher, arguing against the Neyman para-\n",
      "digm, pointed out that conÔ¨Ådence intervals could be accurate without being\n",
      "correct: having observed xiiid\u0018N.\u0012;1/ foriD1;2;:::;20 , the standard\n",
      "0.95 interval based on just the Ô¨Årst 10 observations would provide exact\n",
      "0.95 coverage while giving obviously incorrect inferences for \u0012. If we can\n",
      "reduce the situation to form (11.22), the percentile method intervals satisfy\n",
      "Fisher‚Äôs ‚Äúlogic of inductive inference‚Äù for correctness, as at (4.31).\n",
      "¬é3[p. 189] Bootstrap sample sizes. Why we need bootstrap sample sizes on\n",
      "the order ofBD2000 for conÔ¨Ådence interval construction can be seen\n",
      "in the estimation of the bias correction value z0(11.32). The delta-method\n",
      "standard error of z0DÀÜ\u00001.p0/is calculated to be\n",
      "1\n",
      "'.z0/\u0014p0.1\u0000p0/\n",
      "B\u00151=2\n",
      "; (11.78)\n",
      "with'.z/ the standard normal density. With p0:D0:5andz0:D0this is\n",
      "about1:25=B1=2, equaling 0.028 at BD2000 , a none-too-small error for\n",
      "use in the BC formula (11.33) or the BCa formula (11.39).\n",
      "¬é4[p. 191] BCa accuracy and correctness. The BCa conÔ¨Ådence limit O\u0012BCa≈íÀõ¬ç\n",
      "(11.39) is transformation invariant. DeÔ¨Åne\n",
      "z≈íÀõ¬çDz0Cz0Cz.Àõ/\n",
      "1\u0000a.z0Cz.Àõ//; (11.79)\n",
      "soO\u0012BCa≈íÀõ¬çDOG\u00001fÀÜ≈íz≈íÀõ¬ç¬çg. For a monotone increasing transformation\n",
      "\u001eDm.\u0012/ ,O\u001eDm.O\u0012/, andO\u001e\u0003Dm.r/ , the bootstrap cdf OHofO\u001e\u0003sat-\n",
      "isÔ¨ÅesOH\u00001.Àõ/Dm≈íOG\u00001.Àõ/¬ç sinceO\u001e\u0003.Àõ/Dm.O\u0012\u0003.Àõ//for the bootstrap\n",
      "percentiles. Therefore\n",
      "O\u001eBCa≈íÀõ¬çDOH\u00001fÀÜ.z≈íÀõ¬ç/gDm\u0010\n",
      "OG\u00001fÀÜ.z≈íÀõ¬ç/g\u0011\n",
      "Dm\u0010\n",
      "O\u0012BCa≈íÀõ¬ç\u0011\n",
      ";\n",
      "(11.80)\n",
      "verifying transformation invariance. (Notice that z0DÀÜ\u00001≈íOG.O\u0012/¬çequals206 Bootstrap ConÔ¨Ådence Intervals\n",
      "ÀÜ\u00001≈íOH.O\u001e/¬çand is also transformation invariant, as is a, as discussed pre-\n",
      "viously.)\n",
      "Exact conÔ¨Ådence intervals are transformation invariant, adding consider-\n",
      "ably to their inferential appeal. For approximate intervals, transformation\n",
      "invariance means that if we can demonstrate good behavior on any one\n",
      "scale then it remains good on all scales. The model (11.38) to the \u001escale\n",
      "can be re-expressed as\n",
      "n\n",
      "1CaO\u001eo\n",
      "Df1Ca\u001egf1Ca.Z\u0000z0/g; (11.81)\n",
      "whereZis a standard normal variate, Z\u0018N.0;1/ .\n",
      "Taking logarithms,\n",
      "CU; (11.82)\n",
      "Dlogf1Ca\u001eg, andUis the random variable\n",
      "logf1Ca.Z\u0000z0/g; (11.82) represents the simplest kind of translation\n",
      "rigidly shifts the distribution of U.\n",
      ",he obvious conÔ¨Ådence limit for \n",
      "\u0000U.1\u0000Àõ/; (11.83)\n",
      "whereU.1\u0000Àõ/is the100.1\u0000Àõ/th percentile of U, is then accurate, and\n",
      "also ‚Äúcorrect,‚Äù according to Fisher‚Äôs (admittedly vague) logic of inductive\n",
      "inference. It is an algebraic exercise, given in Section 3 of Efron (1987),\n",
      "and recoverO\u0012BCa≈íÀõ¬ç(11.39).ons \u0012!\u001e!\n",
      "SettingaD0shows the accuracy and correctness of O\u0012BC≈íÀõ¬ç(11.33).\n",
      "¬é5[p. 192] The acceleration a.Thisaappears in (11.38) as d\u001b\u001e=d\u001e, the\n",
      "rate of change ofO\u001e‚Äôs standard deviation as a function of its expectation.\n",
      "In one-parameter exponential families it turns out that this is one-third of\n",
      "d\u001b\u0012=d\u0012; that is, the transformation to normality \u001eDm.\u0012/ also decreases\n",
      "the instability of the standard deviation, though not to zero.\n",
      "The variance of the score function Plx.\u0012/determines the standard de-\n",
      "viation of the MLE O\u0012(4.17)‚Äì(4.18). In one-parameter exponential fami-\n",
      "lies, one-sixth the skewness ofPlx.\u0012/givesa. The skewness connection can\n",
      "be seen at work in estimate (11.40). In multivariate exponential families\n",
      "(5.50), the skewness must be evaluated in the ‚Äúleast favorable‚Äù direction,\n",
      "discussed further in Chapter 13. The R algorithm accel (book web site)\n",
      "usesBparametric bootstrap replications .OÀá\u0003b;O\u0012\u0003b/to estimatea. The per-\n",
      "centile and BC intervals require only the replications O\u0012\u0003b, while BCa also\n",
      "requires knowledge of the underlying exponential family. See Sections 4,\n",
      "6, and 7 of Efron (1987).11.7 Notes and Details 207\n",
      "¬é6[p. 195] Equation (11.42) .Model (11.41) makes O\u0012DPx2\n",
      "ianoncen-\n",
      "tral chi-square variable with noncentrality parameter \u0012DP\u00162\n",
      "iandn\n",
      "degrees of freedom, written as O\u0012\u0018\u001f2\n",
      "\u0012;n. WithO\u0012D20andnD10, the\n",
      "parametric bootstrap distribution is r\u0018\u001f2\n",
      "20;10. Numerical evaluation gives\n",
      "Prf\u001f2\n",
      "20;10\u001420gD0:156 , leading to (11.42).\n",
      "Efron (1985) concerns conÔ¨Ådence intervals for parameters \u0012Dt.\u0016/in\n",
      "model (11.41), where third-order accurate conÔ¨Ådence intervals can be cal-\n",
      "culated. The acceleration aequals zero for such problems, making the BC\n",
      "intervals second-order accurate. In practice, the BC intervals usually per-\n",
      "form well, and are a reasonable choice if the accleration ais unavailable.\n",
      "¬é7[p. 202] BCa conÔ¨Ådence density (11.68) .DeÔ¨Åne\n",
      "z\u0012DÀÜ\u00001h\n",
      "OG.\u0012/i\n",
      "\u0000z0Dz0Cz.Àõ/\n",
      "1\u0000a\u0000\n",
      "z0Cz.Àõ/\u0001; (11.84)\n",
      "so that\n",
      "z.Àõ/Dz\u0012\n",
      "1Caz\u0012\u0000z0andÀõDÀÜ\u0012z\u0012\n",
      "1Caz\u0012\u0000z0\u0013\n",
      ": (11.85)\n",
      "Here we are thinking of Àõand\u0012as functionally related by \u0012DO\u0012BCa≈íÀõ¬ç.\n",
      "Differentiation yields\n",
      "dÀõ\n",
      "dz\u0012D'\u0010\n",
      "z\u0012\n",
      "1Caz\u0012\u0000z0\u0011\n",
      ".1Caz\u0012/2;\n",
      "dz\u0012\n",
      "d\u0012D'\u0010\n",
      "z\u0012\n",
      "1Caz\u0012\u0000z0\u0011\n",
      ".1Caz\u0012/2'.z\u0012Cz0/Og.\u0012/;(11.86)\n",
      "which together give dÀõ=d\u0012 , verifying (11.68).\n",
      "The name ‚ÄúconÔ¨Ådence density‚Äù seems to appear Ô¨Årst in Efron (1993),\n",
      "though the idea is familiar in the Ô¨Åducial literature. An ambitious frequen-\n",
      "tist theory of conÔ¨Ådence distributions is developed in Xie and Singh (2013).\n",
      "¬é8[p. 203] Jeffreys‚Äô prior. Formula (11.72) is discussed further in Chapter 13,\n",
      "in the more general context of uninformative prior distributions. The the-\n",
      "ory of matching priors was initiated by Welch and Peers (1963), another\n",
      "important reference being Tibshirani (1989).12\n",
      "Cross-Validation and CpEstimates of\n",
      "Prediction Error\n",
      "Prediction has become a major branch of twenty-Ô¨Årst-century commerce.\n",
      "Questions of prediction arise naturally: how credit-worthy is a loan appli-\n",
      "cant? Is a new email message spam ? How healthy is the kidney of a poten-\n",
      "tial donor? Two problems present themselves: how to construct an effective\n",
      "prediction rule, and how to estimate the accuracy of its predictions. In the\n",
      "language of Chapter 1, the Ô¨Årst problem is more algorithmic, the second\n",
      "more inferential. Chapters 16‚Äì19, on machine learning , concern predic-\n",
      "tion rule construction. Here we will focus on the second question: having\n",
      "chosen a particular rule, how do we estimate its predictive accuracy?\n",
      "Two quite distinct approaches to prediction error assessment developed\n",
      "in the 1970s. The Ô¨Årst, depending on the classical technique of cross-\n",
      "validation, was fully general and nonparametric. A narrower (but more\n",
      "efÔ¨Åcient) model-based approach was the second, emerging in the form of\n",
      "Mallows‚ÄôCpestimate and the Akaike information criterion (AIC). Both\n",
      "theories will be discussed here, beginning with cross-validation, after a\n",
      "brief overview of prediction rules.\n",
      "12.1 Prediction Rules\n",
      "Prediction problems typically begin with a training setdconsisting of N\n",
      "pairs.xi;yi/,\n",
      "dDf.xi;yi/; iD1;2;:::;Ng; (12.1)\n",
      "wherexiis a vector of ppredictors andyia real-valued response . On the\n",
      "basis of the training set, a prediction rule rd.x/is constructed such that a\n",
      "predictionOyis produced for any point xin the predictor‚Äôs sample space X,\n",
      "OyDrd.x/ forx2X: (12.2)\n",
      "20812.1 Prediction Rules 209\n",
      "The inferential task is to assess the accuracy of the rule‚Äôs predictions. (In\n",
      "practice there are usually several competing rules under consideration and\n",
      "the main question is determining which is best.)\n",
      "In the spam data of Section 8.1, xicomprisedpD57keyword counts,\n",
      "whileyi(8.18) indicated whether or not message iwasspam . The rule\n",
      "rd.x/in Table 8.3 was an MLE logistic regression Ô¨Åt. Given a new mes-\n",
      "sage‚Äôs count vector, say x0,rd.x0/provided an estimated probability O\u00190of\n",
      "it being spam , which could be converted into a prediction Oy0according to\n",
      "Oy0D(\n",
      "1ifO\u00190\u00150:5\n",
      "0ifO\u00190<0:5:(12.3)\n",
      "The diabetes data of Table 7.2, Section 7.3, involved the pD10pre-\n",
      "dictorsxD(age,sex, . . . ,glu), obtained at baseline, and a response y\n",
      "measuring disease progression one year later. Given a new patient‚Äôs base-\n",
      "line measurements x0, we would like to predict his or her progression y0.\n",
      "Table 7.3 suggests two possible prediction rules, ordinary least squares and\n",
      "ridge regression using ridge parameter \u0015D0:1, either of which will pro-\n",
      "duce a predictionOy0. In this case we might assess prediction error in terms\n",
      "of squared error, .y0\u0000Oy0/2.\n",
      "In both of these examples, rd.x/was a regression estimator suggested\n",
      "by a probability model. One of the charms of prediction is that the rule\n",
      "rd.x/need not be based on an explicit model. Regression trees, as pictured\n",
      "in Figure 8.7, are widely used1prediction algorithms that do not require\n",
      "model speciÔ¨Åcations. Prediction, perhaps because of its model-free nature,\n",
      "is an area where algorithmic developments have run far ahead of their in-\n",
      "ferential justiÔ¨Åcation.\n",
      "Quantifying the prediction error of a rule rd.x/requires speciÔ¨Åcation of\n",
      "the discrepancy D.y;Oy/between a prediction Oyand the actual response y.\n",
      "The two most common choices are squared error\n",
      "D.y;Oy/D.y\u0000Oy/2; (12.4)\n",
      "andclassiÔ¨Åcation error\n",
      "D.y;Oy/D(\n",
      "1ify¬§Oy\n",
      "0ifyDOy;(12.5)\n",
      "when, as with the spam data, the response yis dichotomous. (Prediction\n",
      "of a dichotomous response is often called ‚ÄúclassiÔ¨Åcation.‚Äù)\n",
      "1Random forests , one of the most popular machine learning prediction algorithms, is an\n",
      "elaboration of regression trees. See Chapter 17.210 Cross-Validation and CpEstimates\n",
      "For the purpose of error estimation, we suppose that the pairs .xi;yi/in\n",
      "the training set dof (12.1) have been obtained by random sampling from\n",
      "some probability distribution Fon.pC1/-dimensional space RpC1,\n",
      ".xi;yi/iid\u0018F foriD1;2;:::;N: (12.6)\n",
      "Thetrue error rate Errdof rulerd.x/is the expected discrepancy of Oy0D\n",
      "rd.x0/fromy0given a new pair .x0;y0/drawn fromFindependently of\n",
      "d,\n",
      "ErrdDEFfD.y0;Oy0/gI (12.7)\n",
      "d(andrd.\u0001/) is held Ô¨Åxed in expectation (12.7), only .x0;y0/varying.\n",
      "Figure 12.1 concerns the supernova data , an example we will return to in\n",
      "the next section.¬éAbsolute magnitudes yihave been measured for ND39 ¬é1\n",
      "relatively nearby Type Ia supernovas, with the data scaled such that\n",
      "yiind\u0018N.\u0016i;1/; iD1;2;:::;39; (12.8)\n",
      "is a reasonable model. For each supernova, a vector xiofpD10spectral\n",
      "energies has been observed,\n",
      "xiD.xi1;xi2;:::;xi10/; iD1;2;:::;39: (12.9)\n",
      "Table 12.1 shows .xi;yi/foriD1;2;:::;5 . (The frequency measure-\n",
      "ments have been standardized to have mean 0 and variance 1, while yhas\n",
      "been adjusted to have mean 0.)\n",
      "On the basis of the training set dDf.xi;yi/; iD1;2;:::;39g, we\n",
      "wish to construct a rule rd.x/that, given the frequency vector x0for a\n",
      "newly observed Type Ia supernova, accurately predicts2its absolute mag-\n",
      "nitudey0. To this end, a lasso estimateQÀá.\u0015/ was Ô¨Åt, with yin (7.42) the\n",
      "vector.y1;y2;:::;y39/andxthe39\u000210matrix having ith rowxi;\u0015\n",
      "was selected to minimize a Cpestimate of prediction error, Section 12.3,\n",
      "yielding prediction rule\n",
      "Oy0Dx0\n",
      "0QÀá.\u0015/: (12.10)\n",
      "(So in this case constructing rd.x/itself involves error rate estimation.)\n",
      "2Type Ia supernovas were used as ‚Äústandard candles‚Äù in the discovery of dark energy and\n",
      "the cosmological expansion of the Universe, on the grounds that they have constant\n",
      "absolute magnitude. This isn‚Äôt exactly true. Our training set is unusual in that the 39\n",
      "supernovas are close enough to Earth to have yascertained directly. This allows the\n",
      "construction of a prediction rule based on the frequency vector x, which isobservable\n",
      "for distant supernovas, leading to improved calibration of the cosmological expansion.12.1 Prediction Rules 211\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚àí4 ‚àí2 0 2 4 6‚àí4 ‚àí2 0 2 4 6\n",
      "Predicted magnitude y^iAbsolute magnitude yi\n",
      "apparent mean squared\n",
      "error = 0.72\n",
      "Figure 12.1 The supernova data; observed absolute magnitudes\n",
      "yi(on log scale) plotted versus predictions Oyiobtained from lasso\n",
      "rule (12.10), for ND39nearby Type Ia supernovas. Predictions\n",
      "based on 10 spectral power measurements, 7 of which had\n",
      "nonzero coefÔ¨Åcients in QÀá.\u0015/ .\n",
      "The plotted points in Figure 12.1 are .Oyi;yi/foriD1;2;:::;ND39.\n",
      "These gave apparent error\n",
      "errD1\n",
      "NNX\n",
      "iD1.yi\u0000Oyi/2D0:720: (12.11)\n",
      "Comparing this withP.yi\u0000Ny/2=ND3:91 yields an impressive-looking\n",
      "‚ÄúR squared‚Äù value\n",
      "R2D1\u00000:720=3:91D0:816: (12.12)\n",
      "Things aren‚Äôt really that good (see (12.23)). Cross-validation and Cpmeth-\n",
      "ods allow us to correct apparent errors for the fact that rd.x/was chosen\n",
      "to make the predictions OyiÔ¨Åt the datayi.\n",
      "Prediction and estimation are close cousins but they are not twins. As\n",
      "discussed earlier, prediction is less model-dependent, which partly accounts\n",
      "for the distinctions made in Section 8.4. The prediction criterion Err (12.7)212 Cross-Validation and CpEstimates\n",
      "Table 12.1 Supernova data; 10 frequency measurements and response\n",
      "variable ‚Äúabsolute magnitude‚Äù for the Ô¨Årst 5 of ND39Type Ia\n",
      "supernovas. In terms of notation (12.1) , frequency measurements are x\n",
      "and magnitude y.\n",
      "SN1 SN2 SN3 SN4 SN5\n",
      "x1\u0000.84\u00001.89 .26\u0000.08 .41\n",
      "x2\u0000.93\u0000.46\u0000.80 1.02\u0000.81\n",
      "x3 .32 2.41 1.14 \u0000.21\u0000.13\n",
      "x4 .18 .77\u0000.86\u00001.12 1.31\n",
      "x5\u0000.68\u0000.94 .68\u0000.86\u0000.65\n",
      "x6\u00001.27\u00001.53\u0000.35 .72 .30\n",
      "x7 .34 .09\u00001.04 .62\u0000.82\n",
      "x8\u0000.43 .26\u00001.10 .56\u00001.53\n",
      "x9\u0000.02 .18\u00001.32 .62\u00001.49\n",
      "x10\u0000.3\u0000.54\u00001.70\u0000.49\u00001.09\n",
      "mag\u0000:54 2:12\u0000:22:95\u00003:75\n",
      "is an expectation over the .x;y/ space. This emphasizes good overall per-\n",
      "formance, without much concern for behavior at individual points xinX.\n",
      "Shrinkage usually improves prediction. Consider a Bayesian model like\n",
      "that of Section 7.1,\n",
      "\u0016i\u0018N.0;A/ andxij\u0016i\u0018N.\u0016i;1/ foriD1;2;:::;N:\n",
      "(12.13)\n",
      "The Bayes shrinkage estimator, which is ideal for estimation,\n",
      "O\u0016iDBxi; BDA=.AC1/; (12.14)\n",
      "is also ideal for prediction. Suppose that in addition to the observations\n",
      "xithere are independent unobserved replicates, one for each of the N xi\n",
      "values,\n",
      "yi\u0018N.\u0016i;1/ foriD1;2;:::;N; (12.15)\n",
      "that we wish to predict. The Bayes predictor\n",
      "OyiDBxi (12.16)\n",
      "has overall Bayes prediction error\n",
      "E(\n",
      "1\n",
      "NNX\n",
      "iD1.yi\u0000Oyi/2)\n",
      "DBC1; (12.17)12.2 Cross-Validation 213\n",
      "which cannot be improved upon. The MLE rule OyiDxihas Bayes predic-\n",
      "tion error 2, which is always worse than (12.17).\n",
      "As far as prediction is concerned it pays to overshrink, as illustrated in\n",
      "Figure 7.1 for the James‚ÄìStein version of situation (12.13). This is Ô¨Åne for\n",
      "prediction, but less Ô¨Åne for estimation if we are concerned about extreme\n",
      "cases; see Table 7.4. Prediction rules sacriÔ¨Åce the extremes for the sake of\n",
      "the middle, a particularly effective tactic in dichotomous situations (12.5),\n",
      "where the cost of individual errors is bounded. The most successful ma-\n",
      "chine learning prediction algorithms, discussed in Chapters 16‚Äì19, carry\n",
      "out a version of local Bayesian shrinkage in selected regions of X.\n",
      "12.2 Cross-Validation\n",
      "Having constructed a prediction rule rd.x/on the basis of training set d,\n",
      "we wish to know its prediction error Err DEFfD.y0;Oy0/g(12.7) for a\n",
      "new case obtained independently of d. A Ô¨Årst guess is the apparent error\n",
      "errD1\n",
      "NNX\n",
      "iD1D.yi;Oyi/; (12.18)\n",
      "the average discrepancy in the training set between yiand its prediction\n",
      "OyiDrd.xi/; err usually underestimates Err since rd.x/has been adjusted3\n",
      "to Ô¨Åt the observed responses yi.\n",
      "The ideal remedy, discussed in Section 12.4, would be to have an inde-\n",
      "pendent validation set (ortestset)dvalofNvaladditional cases,\n",
      "dvalDÀö\n",
      ".x0j;y0j/; jD1;2;:::;N val\t\n",
      ": (12.19)\n",
      "This would provide an unbiased estimate of Err,\n",
      "cErr valD1\n",
      "NvalNvalX\n",
      "jD1D.y0j;Oy0j/;Oy0jDrd.x0j/: (12.20)\n",
      "Cross-validation attempts to mimic cErr valwithout the need for a valida-\n",
      "tion set. DeÔ¨Åne d.i/to be the reduced training set in which pair .xi;yi/\n",
      "has been omitted, and let rd.i/.\u0001/indicate the rule constructed on the basis\n",
      "3Linear regression using ordinary least squares Ô¨Åtting provides a classical illustration:\n",
      "errDP\n",
      "i.yi\u0000Oyi/2=Nmust be increased toP\n",
      "i.yi\u0000Oyi/2=.N\u0000p/, wherepis\n",
      "the degrees of freedom, to obtain an unbiased estimate of the noise variance \u001b2.214 Cross-Validation and CpEstimates\n",
      "ofd.i/. The cross-validation estimate of prediction error is\n",
      "cErr cvD1\n",
      "NNX\n",
      "iD1D.yi;Oy.i//;Oy.i/Drd.i/.xi/: (12.21)\n",
      "Now.xi;yi/isnotinvolved in the construction of the prediction rule for\n",
      "yi.\n",
      "cErr cv(12.21) is the ‚Äúleave one out‚Äù version of cross-validation. A more\n",
      "common tactic is to leave out several pairs at a time: dis randomly parti-\n",
      "tioned intoJgroups of size about N=J each;d.j/, the training set with\n",
      "groupjomitted, provides rule rd.j/.x/, which is used to provide predic-\n",
      "tions for theyiin groupj. ThencErr cvis evaluated as in (12.21). Besides\n",
      "reducing the number of rule constructions necessary, from NtoJ, group-\n",
      "ing induces larger changes among the Jtraining sets, improving the predic-\n",
      "tive performance on rules rd.x/that include discontinuities. (The argument\n",
      "here is similar to that for the jackknife, Section 10.1.)\n",
      "Cross-validation was applied to the supernova data pictured in Figure 12.1.\n",
      "The 39 cases were split, randomly, into JD13groups of three cases each.\n",
      "This gave\n",
      "cErr cvD1:17; (12.22)\n",
      "(12.21), 62% larger than err D0:72 (12.11). The R2calculation (12.12)\n",
      "now yields the smaller value\n",
      "R2D1\u00001:17=3:91D0:701: (12.23)\n",
      "We can apply cross-validation to the spam data of Section 8.1, having\n",
      "ND4061 cases,pD57predictors, and dichotomous response y. For\n",
      "this example, each of the 57 predictors was itself dichotomized to be either\n",
      "0 or 1 depending on whether the original value xijequaled zero or not.\n",
      "A logistic regression, Section 8.1, regressing yion the 57 dichotomized\n",
      "predictors, gave apparent classiÔ¨Åcation error (12.5)\n",
      "errD0:064; (12.24)\n",
      "i.e., 295 wrong predictions among the 4061 cases. Cross-validation, with\n",
      "JD10groups of size 460 or 461 each, increased this to\n",
      "cErr cvD0:069; (12.25)\n",
      "an increase of 8%.\n",
      "Glmnet is an automatic model building program that, among other\n",
      "things, constructs a lasso sequence of logistic regression models, adding12.2 Cross-Validation 215\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè‚óè ‚óè‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0 10 20 30 40 500.00 0.05 0.10 0.15 0.20 0.25\n",
      "Degrees of freedomMisclassification error rate‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè‚óè ‚óè‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óèlogistic\n",
      "regression\n",
      "Figure 12.2 Spam data. Apparent error rate err (blue) and\n",
      "cross-validated estimate (red) for a sequence of prediction rules\n",
      "generated by glmnet . The degrees of freedom are the number of\n",
      "nonzero regression coefÔ¨Åcients: df D57corresponds to ordinary\n",
      "logistic regression, which gave apparent err 0.064, cross-validated\n",
      "rate 0.069. The minimum cross-validated error rate is 0.067.\n",
      "variables one at a time in their order of apparent predictive power; see\n",
      "Chapter 16. The blue curve in Figure 12.2 tracks the apparent error err\n",
      "(12.18) as a function of the number of predictors employed. Aside from nu-\n",
      "merical artifacts, err is monotonically decreasing, declining to err D0:064\n",
      "for the full model that employs all 57 predictors, i.e., for the usual logistic\n",
      "regression model, as in (12.24).\n",
      "Glmnet produced prediction error estimates cErr cvfor each of the suc-\n",
      "cessive models, shown by the red curve. These are a little noisy themselves,\n",
      "but settle down between 4% and 8% above the corresponding err estimates.\n",
      "The minimum value\n",
      "cErr cvD0:067 (12.26)\n",
      "occurred for the model using 47 predictors.\n",
      "The difference between (12.26) and (12.25) is too small to take seriously\n",
      "given the noise in the cErr cvestimates. There is a more subtle objection:\n",
      "the choice of ‚Äúbest‚Äù prediction rule based on comparative cErr cvestimates\n",
      "is not itself cross-validated. Each case .xi;yi/is involved in choosing its216 Cross-Validation and CpEstimates\n",
      "own best prediction, so cErr cvat the apparently optimum choice cannot be\n",
      "taken entirely at face value.\n",
      "Nevertheless, perhaps the principal use of cross-validation lies in choos-\n",
      "ing among competing prediction rules. Whether or not this is fully justiÔ¨Åed,\n",
      "it is often the only game in town. That being said, minimum predictive er-\n",
      "ror, no matter how effectuated, is a notably weaker selection principle than\n",
      "minimum variance of estimation.\n",
      "As an example, consider an iid normal sample\n",
      "xiiid\u0018N.\u0016;1/; iD1;2;:::;25; (12.27)\n",
      "having meanNxand medianMx. Both are unbiased for estimating \u0016, butNxis\n",
      "much more efÔ¨Åcient,\n",
      "var.Mx/=var.Nx/:D1:57: (12.28)\n",
      "Suppose we wish to predict a future observation x0independently selected\n",
      "from the same N.\u0016;1/ distribution. In this case there is very little advan-\n",
      "tage toNx,\n",
      "EÀö\n",
      ".x0\u0000Mx/2\tƒ±\n",
      "EÀö\n",
      ".x0\u0000Nx/2\t\n",
      "D1:02: (12.29)\n",
      "The noise in x0\u0018N.\u0016;1/ dominates its prediction error. Perhaps the\n",
      "proliferation of prediction algorithms to be seen in Part III reÔ¨Çects how\n",
      "weakly changes in strategy affect prediction error.\n",
      "Table 12.2 Ratio of predictive errors Ef.Nx0\u0000Mx/2g=Ef.Nx0\u0000Nx/2gforNx0\n",
      "the mean of an independent sample of size N0fromN.\u0016;1/ ;NxandMxare\n",
      "the mean and median from xi\u0018N.\u0016;1/ foriD1;2;:::;25 .\n",
      "N0 1 10 100 1000 1\n",
      "Ratio 1.02 1.16 1.46 1.56 1.57\n",
      "In this last example, suppose that our task was to predict the average\n",
      "Nx0ofN0further draws from the N.\u0016;1/ distribution. Table 12.2 shows\n",
      "the ratio of predictive errors as a function of N0. The superiority of the\n",
      "mean compared to the median reveals itself as N0gets larger. In this super-\n",
      "simpliÔ¨Åed example, the difference between prediction and estimation lies\n",
      "in predicting the average of oneversus an inÔ¨Ånite number of future obser-\n",
      "vations.\n",
      "DoescErr cvactually estimate Err das deÔ¨Åned in (12.7)? It seems like the\n",
      "answer must be yes, but there is some doubt expressed in the literature, for12.2 Cross-Validation 217\n",
      "reasons demonstrated in the following simulation: we take the true distri-\n",
      "butionFin (12.6) to be the discrete distribution OFthat puts weight 1=39\n",
      "on each of the 39 .xi;yi/pairs of the supernova data.4A random sample\n",
      "with replacement of size 39 from OFgives simulated data set d\u0003and pre-\n",
      "diction rulerd\u0003.\u0001/based on the lasso/ Cprecipe used originally. The same\n",
      "cross-validation procedure as before, applied to d\u0003, givescErr\u0003\n",
      "cv. Because\n",
      "this is a simulation, we can also compute the actual mean-squared error\n",
      "rate of rulerd\u0003.\u0001/applied to the true distribution OF,\n",
      "Err\u0003D1\n",
      "3939X\n",
      "iD1D.yi;rd\u0003.xi//: (12.30)\n",
      "*\n",
      "***\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "****\n",
      "***\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "** **\n",
      "*\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "***\n",
      "**\n",
      "******\n",
      "***\n",
      "***\n",
      "*\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "*** *\n",
      "***\n",
      "*\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*** ******\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "****\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "*******\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "***\n",
      "***\n",
      "***\n",
      "**\n",
      "***\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "****\n",
      "*** *\n",
      "**\n",
      "***\n",
      "****\n",
      "****\n",
      "*\n",
      "* ***\n",
      "*\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "**\n",
      "****\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "* **\n",
      "*\n",
      "*****\n",
      "***\n",
      "****\n",
      "*\n",
      "***\n",
      "**\n",
      "*\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "* **\n",
      "******\n",
      "**\n",
      "*\n",
      "**\n",
      "*\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "****\n",
      "**\n",
      "**\n",
      "********\n",
      "*\n",
      "*\n",
      "**\n",
      "*****\n",
      "**\n",
      "***\n",
      "*\n",
      "******\n",
      "*\n",
      "**\n",
      "***\n",
      "**\n",
      "*\n",
      "*******\n",
      "***\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "*\n",
      "**\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "* **\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "***\n",
      "*\n",
      "***\n",
      "****\n",
      "**\n",
      "**\n",
      "*****\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "1.0 1.5 2.0 2.50.5 1.0 1.5 2.0 2.5 3.0\n",
      "Err*Errcv‚àó\n",
      "‚óè\n",
      "Figure 12.3 Simulation experiment comparing true error Err\n",
      "with cross-validation estimate cErr\u0003\n",
      "cv; 500 simulations based on the\n",
      "supernova data. cErr\u0003\n",
      "cvand Err are negatively correlated.\n",
      "Figure 12.3 plots .Err\u0003;cErr\u0003\n",
      "cv/for 500 simulations, using squared er-\n",
      "ror discrepancy D.y;Oy/D.y\u0000Oy/2. Summary statistics are given in Ta-\n",
      "ble 12.3.cErr\u0003\n",
      "cvhas performed well overall, averaging 1.07, quite near the\n",
      "true Err 1.02, both estimates being 80% greater than the average appar-\n",
      "ent error 0.57. However, the Ô¨Ågure shows something unsettling: there is a\n",
      "4Simulation based on OFis the same as nonparametric bootstrap analysis, Chapter 10.218 Cross-Validation and CpEstimates\n",
      "Table 12.3 True error Err\u0003, cross-validated error cErr\u0003\n",
      "cv, and apparent\n",
      "error err\u0003; 500 simulations based on supernova data. Correlation \u00000:175\n",
      "between Err\u0003andcErr\u0003\n",
      "cv.\n",
      "Err\u0003cErr\u0003\n",
      "cv err\u0003\n",
      "Mean 1.02 1.07 .57\n",
      "St dev .27 .34 .16\n",
      "negative correlation between cErr\u0003\n",
      "cvand Err\u0003. Large values of cErr\u0003\n",
      "cvgo with\n",
      "smaller values of the true prediction error, and vice versa.\n",
      "Our original deÔ¨Ånition of Err,\n",
      "ErrdDEFfD.y0;rd.x0//g; (12.31)\n",
      "tookrd.\u0001/Ô¨Åxed as constructed from d, only.x0;y0/\u0018Frandom. In other\n",
      "words, Errdwas the expected prediction error for the speciÔ¨Åc rule rd.\u0001/, as\n",
      "is Err\u0003forrd\u0003.\u0001/. IfcErr\u0003\n",
      "cvis tracking Err\u0003we would expect to see a positive\n",
      "correlation in Figure 12.3.\n",
      "As it is, all we can say is that cErr\u0003\n",
      "cvis estimating the expected predictive\n",
      "error, wheredas well as.x0;y0/is random in deÔ¨Ånition (12.31). This\n",
      "makes cross-validation a strongly frequentist device: cErr cvis estimating the\n",
      "average prediction error of the algorithm producing rd.\u0001/, not ofrd.\u0001/itself.\n",
      "12.3 Covariance Penalties\n",
      "Cross-validation does its work nonparametrically and without the need for\n",
      "probabilistic modeling. Covariance penalty procedures require probability\n",
      "models, but within their ambit they provide less noisy estimates of predic-\n",
      "tion error. Some of the most prominent covariance penalty techniques will\n",
      "be examined here, including Mallows‚ÄôCp,Akaike‚Äôs information criterion\n",
      "(AIC), and Stein‚Äôs unbiased risk estimate (SURE).\n",
      "The covariance penalty approach treats prediction error estimation in\n",
      "a regression framework: the predictor vectors xiin the training set dD\n",
      "f.xi;yi/;iD1;2;:::;Ng(12.1) are considered Ô¨Åxed at their observed\n",
      "values, not random as in (12.6). An unknown vector \u0016of expectations\n",
      "\u0016iDEfyighas yielded the observed vector of responses yaccording to\n",
      "some given probability model, which to begin with we assume to have the12.3 Covariance Penalties 219\n",
      "simple form\n",
      "y\u0018.\u0016;\u001b2I/I (12.32)\n",
      "that is, theyiare uncorrelated, with yihaving unknown mean \u0016iand vari-\n",
      "ance\u001b2. We take\u001b2as known, though in practice it must usually be esti-\n",
      "mated.\n",
      "A regression rule r.\u0001/has been used to produce an estimate of vector \u0016,\n",
      "O\u0016Dr.y/: (12.33)\n",
      "(Onlyyis included in the notation since the predictors xiare considered\n",
      "Ô¨Åxed and known.) For instance we might take\n",
      "O\u0016Dr.y/DX.X0X/\u00001X0y; (12.34)\n",
      "whereXis theN\u0002pmatrix having xias theith row, as suggested by the\n",
      "linear regression model \u0016DXÀá.\n",
      "In covariance penalty calculations, the estimator O\u0016also functions as a\n",
      "predictor. We wonder how accurate O\u0016Dr.y/will be in predicting a new\n",
      "vector of observations y0from model (12.32),\n",
      "y0\u0018.\u0016;\u001b2I/; independent of y: (12.35)\n",
      "To begin with, prediction error will be assessed in terms of squared dis-\n",
      "crepancy,\n",
      "ErriDE0Àö\n",
      ".y0i\u0000O\u0016i/2\t\n",
      "(12.36)\n",
      "for component i, whereE0indicates expectation with y0irandom butO\u0016i\n",
      "held Ô¨Åxed. Overall prediction error is the average5\n",
      "Err\u0001D1\n",
      "NNX\n",
      "iD1Erri: (12.37)\n",
      "Theapparent error for component iis\n",
      "erriD.yi\u0000O\u0016i/2: (12.38)\n",
      "A simple but powerful lemma underlies the theory of covariance penalties.\n",
      "Lemma LetEindicate expectation over both yin(12.32) andy0in\n",
      "(12.35) . Then\n",
      "EfErrigDEferrigC2cov.O\u0016i;yi/; (12.39)\n",
      "5Err\u0001is sometimes called ‚Äúinsample error,‚Äù as opposed to ‚Äúoutsample error‚Äù Err (12.7),\n",
      "though in practice the two tend to behave similarly.220 Cross-Validation and CpEstimates\n",
      "where the last term is the covariance between the ith components ofO\u0016and\n",
      "y,\n",
      "cov.O\u0016i;yi/DEf.O\u0016i\u0000\u0016i/.yi\u0000\u0016i/g: (12.40)\n",
      "(Note: (12.40) does not require EfO\u0016igD\u0016i.)\n",
      "Proof Letting\u000fiDyi\u0000\u0016iandƒ±iD.O\u0016i\u0000\u0016i/, the elementary equality\n",
      ".\u000fi\u0000ƒ±i/2D\u000f2\n",
      "i\u00002\u000fiƒ±iCƒ±2\n",
      "ibecomes\n",
      ".yi\u0000O\u0016i/2D.yi\u0000\u0016i/2\u00002.O\u0016i\u0000\u0016i/.yi\u0000\u0016i/C.O\u0016i\u0000\u0016i/2;(12.41)\n",
      "and likewise\n",
      ".y0i\u0000O\u0016i/2D.y0i\u0000\u0016i/2\u00002.O\u0016i\u0000\u0016i/.y0i\u0000\u0016i/C.O\u0016i\u0000\u0016i/2:(12.42)\n",
      "Taking expectations, (12.41) gives\n",
      "EferrigD\u001b2\u00002cov.O\u0016i;yi/CE.O\u0016i\u0000\u0016i/2; (12.43)\n",
      "while (12.42) gives\n",
      "EfErrigD\u001b2CE.O\u0016i\u0000\u0016i/2; (12.44)\n",
      "the middle term on the right side of (12.42) equaling zero because of the\n",
      "independence of y0iandO\u0016i. Taking the difference between (12.44) and\n",
      "(12.43) veriÔ¨Åes the lemma. ‚ñ†\n",
      "Note: The lemma remains valid if \u001b2varies withi.\n",
      "The lemma says that, on average, the apparent error err iunderstimates\n",
      "the true prediction error Err iby the covariance penalty 2cov.O\u0016i;yi/. (This\n",
      "makes intuitive sense since cov .\u0016i;yi/measures the amount by which yi\n",
      "inÔ¨Çuences its own prediction O\u0016i.) Covariance penalty estimates of predic-\n",
      "tion error take the form\n",
      "cErriDerriC2dcov.O\u0016i;yi/; (12.45)\n",
      "wheredcov.O\u0016i;yi/approximates cov .\u0016i;yi/; overall prediction error (12.37)\n",
      "is estimated by\n",
      "cErr\u0001DerrC2\n",
      "NNX\n",
      "iD1dcov.O\u0016i;yi/; (12.46)\n",
      "where errDPerri=Nas before.\n",
      "The form ofdcov.O\u0016i;yi/in (12.45) depends on the context assumed for\n",
      "the prediction problem.12.3 Covariance Penalties 221\n",
      "(1) Suppose thatO\u0016Dr.y/in (12.32)‚Äì(12.33) is linear ,\n",
      "O\u0016DcCMy; (12.47)\n",
      "wherecis a knownN-vector andMa knownN\u0002Nmatrix. Then the\n",
      "covariance matrix between O\u0016andyis\n",
      "cov.O\u0016;y/D\u001b2M; (12.48)\n",
      "giving cov.O\u0016i;yi/D\u001b2Mii,Miitheith diagonal element of M,\n",
      "cErriDerriC2\u001b2Mii; (12.49)\n",
      "and, since errDP\n",
      "i.yi\u0000O\u0016i/2=N,\n",
      "cErr\u0001D1\n",
      "NNX\n",
      "iD1.yi\u0000O\u0016i/2C2\u001b2\n",
      "Ntr.M/: (12.50)\n",
      "Formula (12.50) is Mallows‚ÄôCpestimate of prediction error. For OLS\n",
      "estimation (12.34), MDX.X0X/\u00001X0has tr.M/Dp, the number of\n",
      "predictors, so\n",
      "cErr\u0001D1\n",
      "NNX\n",
      "iD1.yi\u0000O\u0016i/2C2\n",
      "N\u001b2p: (12.51)\n",
      "For the supernova data (12.8)‚Äì(12.9), the OLS predictor O\u0016DX.X0X/\u00001\n",
      "X0yyielded errDP.yi\u0000O\u0016i/2=39D0:719 . The covariance penalty, with\n",
      "ND39,\u001b2D1, and6pD10, was 0.513, giving Cpestimate of predic-\n",
      "tion error\n",
      "cErr\u0001D0:719C0:513D1:23: (12.52)\n",
      "For OLS regression, the degrees of freedom p, the rank of matrix Xin\n",
      "(12.34), determines the covariance penalty .2=N/\u001b2pin (12.51). Compar-\n",
      "ing this with (12.46) leads to a general deÔ¨Ånition of degrees of freedom df\n",
      "for a regression rule O\u0016Dr.y/,\n",
      "dfD.1=\u001b2/NX\n",
      "iD1dcov.O\u0016i;yi/: (12.53)\n",
      "This deÔ¨Ånition provides common ground for comparing different types of\n",
      "regression rules. Rules with larger df are more Ô¨Çexible and tend toward\n",
      "better apparent Ô¨Åts to the data, but require bigger covariance penalties for\n",
      "fair comparison.\n",
      "6We are not counting the intercept as an 11th predictor since yand all thexiwere\n",
      "standardized to have mean 0, all our models assuming zero intercept.222 Cross-Validation and CpEstimates\n",
      "(2) For lasso estimation (7.42) and (12.10), it can be shown that for-\n",
      "mula (12.51), with pequaling the number of nonzero regression coefÔ¨Å-\n",
      "cients, holds to a good approximation.¬éThe lasso rule used in Figure 12.1 ¬é2\n",
      "for the supernova data had pD7; err was 0.720 for this rule, almost the\n",
      "same as for the OLS rule above, but the Cppenalty is less, 2\u00017=39D0:359 ,\n",
      "giving\n",
      "cErr\u0001D0:720C0:359D1:08; (12.54)\n",
      "compared with 1.23 for OLS. This estimate does not account for the data-\n",
      "based selection of the choice pD7, see item (4)below.\n",
      "(3) If we are willing to add multivariate normality to model (12.32),\n",
      "y\u0018Np.\u0016;\u001b2I/; (12.55)\n",
      "we can drop the assumption of linearity (12.47). In this case it can be\n",
      "shown that, for any differentiable estimator O\u0016Dr.y/, the covariance in\n",
      "formula (12.39) is given by¬é ¬é3\n",
      "cov.O\u0016i;yi/D\u001b2Ef@O\u0016i=@yig; (12.56)\n",
      "\u001b2times the partial derivative of O\u0016iwith respect to yi. (Another measure of\n",
      "yi‚Äôs inÔ¨Çuence on its own prediction.) The SURE formula (Stein‚Äôs unbiased\n",
      "risk estimator) is\n",
      "cErriDerriC2\u001b2@O\u0016i\n",
      "@yi; (12.57)\n",
      "with corresponding estimate for overall prediction error\n",
      "cErr\u0001DerrC2\u001b2\n",
      "NNX\n",
      "iD1@O\u0016i\n",
      "@yi: (12.58)\n",
      "SURE was applied to the rule O\u0016Dlowess(x,y,1/3) for the kid-\n",
      "ney Ô¨Åtness data of Figure 1.2. The open circles in Figure 12.4 plot the\n",
      "component-wise degrees of freedom estimates7\n",
      "@O\u0016i\n",
      "@yi; iD1;2;:::;ND157; (12.59)\n",
      "(obtained by numerical differentiation) versus age i. Their sum\n",
      "NX\n",
      "iD1@O\u0016i\n",
      "@yiD6:67 (12.60)\n",
      "7Notice that the factor \u001b2in (12.56) cancels out in (12.53).12.3 Covariance Penalties 223\n",
      "estimates the total degrees of freedom, as in (12.53), implying that\n",
      "lowess(x,y,1/3) is about as Ô¨Çexible as a sixth-degree polynomial Ô¨Åt,\n",
      "with df = 7.\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè ‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè ‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè ‚óè ‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "20 30 40 50 60 70 80 900.0 0.1 0.2 0.3 0.4\n",
      "agedf estimateDegrees of Freedom\n",
      "SURE = 6.67\n",
      "Bootstrap = 6.81\n",
      "Figure 12.4 Analysis of the lowess(x,y,1/3) Ô¨Åt to kidney\n",
      "data of Figure 1.2. Open circles are SURE coordinate-wise df\n",
      "estimates@O\u0016i=@yi, plotted versus age i, giving total degrees of\n",
      "freedom 6.67. The solid curve tracks bootstrap coordinate-wise\n",
      "estimates (12.65), with their sum giving total df D6:81.\n",
      "(4) Theparametric bootstrap8of Section 10.4 can be used to estimate\n",
      "the covariances cov .O\u0016i;yi/in the lemma (12.39). The data vector yis as-\n",
      "sumed to be generated from a member f\u0016.y/of a given parametric family\n",
      "FDÀö\n",
      "f\u0016.y/;\u00162\t\n",
      "; (12.61)\n",
      "yieldingO\u0016Dr.y/,\n",
      "f\u0016!y!O\u0016Dr.y/: (12.62)\n",
      "Parametric bootstrap replications of yandO\u0016are obtained by analogy with\n",
      "8There is also a nonparametric bootstrap competitor to cross-validation, the ‚Äú.632\n",
      "estimate;‚Äù see the chapter endnote ¬é4.224 Cross-Validation and CpEstimates\n",
      "(12.62),9\n",
      "fO\u0016!y\u0003!O\u0016\u0003Dr.y\u0003/: (12.63)\n",
      "A large number Bof replications then yield bootstrap estimates\n",
      "dcov.O\u0016i;yi/D1\n",
      "BBX\n",
      "bD1.O\u0016\u0003b\n",
      "i\u0000O\u0016\u0003\u0001\n",
      "i/.y\u0003b\n",
      "i\u0000y\u0003\u0001\n",
      "i/; (12.64)\n",
      "the dot notation indicating averages over the Breplications.\n",
      "BD1000 parametric bootstrap replications .O\u0016\u0003;y\u0003/were obtained\n",
      "from the normal model (12.55), taking O\u0016in (12.63) to be the estimate from\n",
      "lowess(x,y,1/3) as in Figure 1.2. A standard linear regression, of y\n",
      "as a 12th-degree polynomial function of age, gave O\u001b2D3:28. Covariances\n",
      "were computed as in (12.64), yielding coordinate-wise degrees of freedom\n",
      "estimates (12.53),\n",
      "dfiDdcov.O\u0016i;yi/=O\u001b2: (12.65)\n",
      "The solid curve in Figure 12.4 plots df ias a function of age i. These are\n",
      "seen to be similar to but less noisy than the SURE estimates. They totaled\n",
      "6.81, nearly the same as (12.60). The overall covariance penalty term in\n",
      "(12.46) equaled 0.284, increasing cErr\u0001by about 9% over err D3:15.\n",
      "The advantage of parametric bootstrap estimates (12.64) of covariance\n",
      "penalties is their applicability to anyprediction ruleO\u0016Dr.y/no matter\n",
      "how exotic. Applied to the lasso estimates for the supernova data, BD\n",
      "1000 replications yielded total df D6:85 for the rule that always used\n",
      "pD7predictors, compared with the theoretical approximation df D7.\n",
      "Another 1000 replications, now letting O\u0016\u0003Dr.y\u0003/choose the apparently\n",
      "bestp\u0003each time, increased the df estimate to 7.48, so the adaptive choice\n",
      "ofpcost about 0.6 extra degrees of freedom. These calculations exem-\n",
      "plify modern computer-intensive inference, carrying through error estima-\n",
      "tion for complicated adaptive prediction rules on a totally automatic basis.\n",
      "(5) Covariance penalties can apply to measures of prediction error other\n",
      "than squared error D.yi;O\u0016i/D.yi\u0000O\u0016i/2. We will discuss two examples\n",
      "of a general theory. First consider classiÔ¨Åcation , whereyiequals 0 or 1 and\n",
      "9It isn‚Äôt necessary for the O\u0016in (12.63) to equalO\u0016Dr.y/. The calculation (12.64) was\n",
      "rerun takingO\u0016in (12.63) from lowess(x,y,1/6) (but withr.y/still from\n",
      "lowess(x,y,1/3) ) with almost identical results. In general, one might take O\u0016in\n",
      "(12.63) to be from a more Ô¨Çexible, less biased, estimator than r.y/.12.3 Covariance Penalties 225\n",
      "similarly the predictor O\u0016i, with dichotomous error\n",
      "D.yi;O\u0016i/D(\n",
      "1ifyi¬§O\u0016i\n",
      "0ifyiDO\u0016i;(12.66)\n",
      "as in (12.5).10In this situation, the apparent error is the observed proportion\n",
      "of prediction mistakes in the training set (12.1),\n",
      "errD#fyi¬§O\u0016ig=N: (12.67)\n",
      "Now the true prediction error for case iis\n",
      "ErriDPr0fy0i¬§O\u0016ig; (12.68)\n",
      "the conditional probability given O\u0016ithat an independent replicate y0iofyi\n",
      "will be incorrectly predicted. The lemma holds as stated in (12.39), leading\n",
      "to the prediction error estimate\n",
      "cErr\u0001D#fyi¬§O\u0016ig\n",
      "NC2\n",
      "NNX\n",
      "iD1cov.O\u0016i;yi/: (12.69)\n",
      "Some algebra yields\n",
      "cov.O\u0016i;yi/D\u0016i.1\u0000\u0016i/.PrfO\u0016iD1jyiD1g\u0000PrfO\u00161D1jyiD0g/;\n",
      "(12.70)\n",
      "with\u0016iDPrfyiD1g, showing again the covariance penalty measuring\n",
      "the self-inÔ¨Çuence of yion its own prediction.\n",
      "As a second example, suppose that the observations yiare obtained\n",
      "from different members of a one-parameter exponential family f\u0016.y/D\n",
      ".\u0015/gf0.y/(8.32),\n",
      "yi\u0018f\u0016i.yi/ foriD1;2;:::;N; (12.71)\n",
      "and that error is measured by the deviance (8.31),\n",
      "D.y;O\u0016/D2Z\n",
      "Yfy.Y/log\u0012fy.Y/\n",
      "fO\u0016.Y/\u0013\n",
      "dY: (12.72)\n",
      "According to (8.33), the apparent errorPD.yi;O\u0016i/is then\n",
      "errD2\n",
      "NNX\n",
      "iD1log\u0012fyi.yi/\n",
      "fO\u0016i.yi/\u0013\n",
      "D2\n",
      "NÀö\n",
      "log\u0000\n",
      "fy.y/\u0001\n",
      "\u0000log\u0000\n",
      "fO\u0016.y/\u0001\t\n",
      ":\n",
      "(12.73)\n",
      "10More generally,O\u0019iis some predictor of Pr fyiD1g, andO\u0016iis the indicator function\n",
      "I.O\u0019i\u00150:5/ .226 Cross-Validation and CpEstimates\n",
      "In this case the general theory gives overall covariance penalty\n",
      "penaltyD2\n",
      "NNX\n",
      "iD1cov\u0010\n",
      "O\u0015i;yi\u0011\n",
      "; (12.74)\n",
      "whereO\u0015iis the natural parameter in family (8.32) corresponding to O\u0016\u0015\n",
      "(e.g.,O\u0015iDlogO\u0016ifor Poisson observations). Moreover, if O\u0016is obtained\n",
      "as the MLE of \u0016in a generalized linear model with pdegrees of freedom\n",
      "(8.22),\n",
      "penalty:D2p\n",
      "N(12.75)\n",
      "to a good approximation. The corresponding version of cErr\u0001(12.46) can\n",
      "then be written as\n",
      "cErr\u0001:D\u00002\n",
      "NÀö\n",
      "log\u0000\n",
      "fO\u0016.y/\u0001\n",
      "\u0000p\t\n",
      "Cconstant; (12.76)\n",
      "the constant.2=N/ log.fy.y//not depending onO\u0016.\n",
      "The term in brackets is the Akaike information criterion (AIC): if the\n",
      "statistician is comparing possible prediction rules r.j/.y/for a given data\n",
      "sety, the AIC says to select the rule maximizing the penalized maximum\n",
      "likelihood\n",
      "log\u0000\n",
      "fO\u0016.j/.y/\u0001\n",
      "\u0000p.j/; (12.77)\n",
      "whereO\u0016.j/is rulej‚Äôs MLE andp.j/its degrees of freedom. Comparison\n",
      "with (12.76) shows that for GLMs, the AIC amounts to selecting the rule\n",
      "with the smallest value of cErr.j/\n",
      "\u0001.\n",
      "\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001\n",
      "Cross-validation does not require a probability model, but if such a model\n",
      "is available then the error estimate cErr cvcan be improved by bootstrap\n",
      "smoothing .11With the predictor vectors xiconsidered Ô¨Åxed as observed, a\n",
      "parametric model generates the data set dDf.xi;yi/;iD1;:::;Ngas\n",
      "in (12.62), from which we calculate the prediction rule rd.\u0001/and the error\n",
      "estimatecErr cv(12.21),\n",
      "f\u0016!d!rd.\u0001/!cErr cv: (12.78)\n",
      "Substituting the estimated density fO\u0016forf\u0016, as in (12.63), provides\n",
      "11Perhaps better known as ‚Äúbagging;‚Äù see Chapter 17.12.4 Training, Validation, and Ephemeral Predictors 227\n",
      "parametric bootstrap replicates of cErr cv,\n",
      "fO\u0016!d\u0003!rd\u0003.\u0001/!cErr\u0003\n",
      "cv: (12.79)\n",
      "Some large number Bof replications can then be averaged to give the\n",
      "smoothed estimate\n",
      "ErrD1\n",
      "BBX\n",
      "bD1cErr\u0003b\n",
      "cv: (12.80)\n",
      "Err averages out the considerable noise in cErr cv, often signiÔ¨Åcantly reduc-\n",
      "ing its variability.12\n",
      "A surprising result, referenced in the endnotes, shows that Err approxi-\n",
      "mates the covariance penalty estimate Err \u0001. Speaking broadly, Err \u0001is what‚Äôs\n",
      "left after excess randomness is squeezed out of cErr cv(an example of ‚ÄúRao‚Äì\n",
      "Blackwellization,‚Äù to use classical terminology). Improvements can be quite\n",
      "substantial.¬éCovariance penalty estimates, when believable parametric ¬é4\n",
      "models are available, should be preferred to cross-validation.\n",
      "12.4 Training, Validation, and Ephemeral Predictors\n",
      "Good Practice suggests splitting the full set of observed predictor‚Äìresponse\n",
      "pairs.x;y/ into a training set dof sizeN(12.1), and a validation set\n",
      "dval, of sizeNval(12.19). The validation set is put into a vault while the\n",
      "training set is used to develop an effective prediction rule rd.x/. Finally,\n",
      "dvalis removed from the vault and used to calculate cErr val(12.20), an honest\n",
      "estimate of the predictive error rate of rd.\n",
      "This isa good idea, and seems foolproof, at least if one has enough data\n",
      "to afford setting aside a substantial portion for a validation set during the\n",
      "training process. Nevertheless, there remains some peril of underestimating\n",
      "the true error rate, arising from ephemeral predictors, those whose predic-\n",
      "tive powers fade away over time. A contrived, but not completely fanciful,\n",
      "example illustrates the danger.\n",
      "The example takes the form of an imaginary microarray study involving\n",
      "360 subjects, 180 patients and 180 healthy controls, coded\n",
      "yiD(\n",
      "1patient\n",
      "0control;iD1;2;:::;360: (12.81)\n",
      "12A related tactic pertaining to grouped cross-validation is to repeat calculation (12.21) for\n",
      "several different randomly selected splits into Jgroups, and then average the resulting\n",
      "cErrcvestimates.228 Cross-Validation and CpEstimates\n",
      "Each subject is assessed on a microarray measuring the genetic activity of\n",
      "pD100genes, these being the predictors\n",
      "xiD.xi1;xi2;xi3;:::;xi100/0: (12.82)\n",
      "One subject per day is assessed, alternating patients and controls.\n",
      "0 50 100 150 200 250 300 3500 20 40 60 80 100 \n",
      "************************************************ ****************************** ******* *********** ******************************************************** ****************************** ************************************************************ ************************ ************************************************************************* ****************************** ********************************************** ****************************** ****************************** ****************************** ****************************** ************************************** ****************************** ****************************** ****************************** ******************** **************************************************************** ******************************************* ****************************** ************************************************************ *************************************************************** ****************************************** ******************************************************************************************************************************************************************************** ********************************************************************* ************************************************************ **************************************** ****************************** ****************************** ****************************** ************* ****************************** ****************************** ****************************** ***************************************************** ************************************************* ************************************************ ****************************** ****************************** ****************************** ****************************** ****************************** ******************************************************************************** ***************************************************** ****************************** ****************************** ****************************** ************************************************************ ************************************************ ************************************************************ ******************** *********************************** *********************************************** ****************************** ****************************** ************* ************************************************************ *************************** ****************************** **************************************** ****************************** ****************************** ****************************** ********************************************************************************************************** ****************************** ****************************** ****************************** ****************************** ****************************** ************************************************************ ****************************** ****************************** *************** ****************************** ****************************************************** ************************************************* ************************************************************ ****************************** ****************************** ****************************** ****************************** ****************************** ***************************************** ****************************** ****************************** ************************************************************ ****************************** ****************************** ****************************** ************************************************************ ************************************** ************************************************************ ************************************************************ ****************************** ****************************** *********************************************** ************************************************************ ****************************** ****************************** ****************************** ****************************** ****************************** **** ************************************************************ ****************************** ****************************** ******************************** ***** ************************************************** ****************************** ******************************* ********************************************************************** ****************************** ************************************************************ ****************************** ****************************** ********************************************* ********************************************* ****************************** ****** ************************************************************************************************************************ ****************************** ****************************** ****************************** ****************************** ****************************** *************************** ****************************** ****************************** ****************** ****************************** ****************************** ****************************** ****************************** ****************************************************************************************** ****************************** ****************************** ****************************** *********************** ***************************** ****************************** ****************************** ******************************************************************************************\n",
      "subjectsgenes\n",
      "Figure 12.5 Orange bars indicate transient episodes, (12.84) and\n",
      "the reverse, for imaginary medical study (12.81)‚Äì(12.82).\n",
      "The measurements xijare independent of each other and of the yi‚Äôs,\n",
      "xijind\u0018N.\u0016ij;1/ foriD1;2;:::;360 andjD1;2;:::;100:\n",
      "(12.83)\n",
      "Most of the\u0016ijequal zero, but each gene‚Äôs measurements can experience\n",
      "‚Äútransient episodes‚Äù of two possible types: in type 1,\n",
      "\u0016ijD(\n",
      "2 ifyiD1\n",
      "\u00002ifyiD0;(12.84)\n",
      "while type 2 reverses signs. The episodes are about 30 days long, randomly\n",
      "and independently located between days 1 and 360, with an average of two\n",
      "episodes per gene. The orange bars in Figure 12.5 indicate the episodes.\n",
      "For the purpose of future diagnoses we wish to construct a prediction\n",
      "ruleOyDrd.x/. To this end we randomly divide the 360 subjects into a12.4 Ephemeral Predictors 229\n",
      "training setdof sizeND300and a validation set dvalof sizeNvalD\n",
      "60. The popular ‚Äúmachine learning‚Äù prediction program Random Forests ,\n",
      "Chapter 17, is applied. Random Forests forms rd.x/by averaging the pre-\n",
      "dictions of a large number of randomly subsampled regression trees (Sec-\n",
      "tion 8.4).\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0 100 200 300 4000.00.10.20.30.40.5Training set random 300 days, test set the remainder\n",
      "# treesPrediction error‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè15%\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0 100 200 300 4000.00.10.20.30.40.5Training days 1‚àí300, test days 301‚àí360\n",
      "# treesPrediction error‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè27%\n",
      "15%\n",
      "Figure 12.6 Test error (blue) and cross-validated training error\n",
      "(black), for Random Forest prediction rules using the imaginary\n",
      "medical study (12.81)‚Äì(12.82). Top panel : training set randomly\n",
      "selected 300 days, test set the remaining 60 days. Bottom panel :\n",
      "training set the Ô¨Årst 300 days, test set the last 60 days.\n",
      "The top panel of Figure 12.6 shows the results, with blue points indi-\n",
      "cating test-set error and black the (cross-validated) training-set error. Both\n",
      "converge to 15% as the number of Random Forest trees grows large. This\n",
      "seems to conÔ¨Årm an 85% success rate for prediction rule rd.x/.\n",
      "One change has been made for the bottom panel: now the training set\n",
      "is the data for days 1 through 300, and the test set days 301 through 360.230 Cross-Validation and CpEstimates\n",
      "The cross-validated training-set prediction error still converges to 15%, but\n",
      "cErr valis now 27%, nearly double.\n",
      "The reason isn‚Äôt hard to see. Any predictive power must come from the\n",
      "transient episodes, which lose efÔ¨Åcacy outside of their limited span. In the\n",
      "Ô¨Årst example the test days are located among the training days, and inherit\n",
      "their predictive accuracy from them. This mostly fails in the second setup,\n",
      "where the test days are farther removed from the training days. (Only the\n",
      "orange bars crossing the 300-day line can help lower cErr valin this situa-\n",
      "tion.)\n",
      "An obvious, but often ignored, dictum is that cErr valis more believable if\n",
      "the test set is further separated from the training set. ‚ÄúFurther‚Äù has a clear\n",
      "meaning in studies with a time or location factor, but not necessarily in\n",
      "general. ForJ-fold cross-validation, separation is improved by removing\n",
      "contiguous blocks of N=J cases for each group, rather than by random\n",
      "selection, but the amount of separation is still limited, making cErr cvless\n",
      "believable than a suitably constructed cErr val.\n",
      "The distinction between transient, ephemeral predictors and dependable\n",
      "ones is sometimes phrased as the difference between correlation and cau-\n",
      "sation. For prediction purposes, if not for scientiÔ¨Åc exegesis, we may be\n",
      "happy to settle for correlations as long as they are persistent enough for\n",
      "our purposes. We return to this question in Chapter 15 in the discussion of\n",
      "large-scale hypothesis testing.\n",
      "A notorious cautionary tale of fading correlations concerns Google Flu\n",
      "Trends ,¬éa machine-learning algorithm for predicting inÔ¨Çuenza outbreaks. ¬é5\n",
      "Introduced in 2008, the algorithm, based on counts of internet search terms,\n",
      "outperformed traditional medical surveys in terms of speed and predictive\n",
      "accuracy. Four years later, however, the algorithm failed, badly overesti-\n",
      "mating what turned out to be a nonexistent Ô¨Çu epidemic. Perhaps one les-\n",
      "son here is that the Google algorithmists needed a validation set years‚Äînot\n",
      "weeks or months‚Äîremoved from the training data.\n",
      "Error rate estimation is mainly frequentist in nature, but the very large\n",
      "data sets available from the internet have encouraged a disregard for infer-\n",
      "ential justiÔ¨Åcation of any type. This can be dangerous. The heterogeneous\n",
      "nature of ‚Äúfound‚Äù data makes statistical principles of analysis more, not\n",
      "less, relevant.\n",
      "12.5 Notes and Details\n",
      "The evolution of prediction algorithms and their error estimates nicely il-\n",
      "lustrates the inÔ¨Çuence of electronic computation on statistical theory and12.5 Notes and Details 231\n",
      "practice. The classical recipe for cross-validation recommended splitting\n",
      "the full data set in two, doing variable selection, model choice, and data Ô¨Åt-\n",
      "ting on the Ô¨Årst half, and then testing the resulting procedure on the second\n",
      "half. Interest revived in 1974 with the independent publication of papers\n",
      "by Geisser and by Stone, featuring leave-one-out cross-validation of pre-\n",
      "dictive error rates.\n",
      "A question of bias versus variance arises here. A rule based on only N=2\n",
      "cases is less accurate than the actual rule based on all N. Leave-one-out\n",
      "cross-validation minimizes this type of bias, at the expense of increased\n",
      "variability of error rate estimates for ‚Äújumpy‚Äù rules of a discontinuous\n",
      "nature. Current best practice is described in Section 7.10 of Hastie et al.\n",
      "(2009), where J-fold cross-validation with Jperhaps 10 is recommended,\n",
      "possibly averaged over several random data splits.\n",
      "Nineteen seventy-three was another good year for error estimation, fea-\n",
      "turing Mallows‚Äô Cpestimator and Akaike‚Äôs information criterion. Efron\n",
      "(1986) extended Cpmethods to a general class of situations (see below),\n",
      "established the connection with AIC, and suggested bootstrapping methods\n",
      "for covariance penalties. The connection between cross-validation and co-\n",
      "variance penalties was examined in Efron (2004), where the Rao‚ÄìBlackwell-\n",
      "type relationship mentioned at the end of Section 12.3 was demonstrated.\n",
      "The SURE criterion appeared in Charles Stein‚Äôs 1981 paper. Ye (1998)\n",
      "suggested the general degrees of freedom deÔ¨Ånition (12.53).\n",
      "¬é1[p. 210] Standard candles and dark energy. Adam Riess, Saul Perlmutter,\n",
      "and Brian Schmidt won the 2011 Nobel Prize in physics for discovering\n",
      "increasing rates of expansion of the Universe, attributed to an Einsteinian\n",
      "concept of dark energy . They measured cosmic distances using Type Ia\n",
      "supernovas as ‚Äústandard candles.‚Äù The type of analysis suggested by Fig-\n",
      "ure 12.1 is intended to improve the cosmological distance scale.\n",
      "¬é2[p. 222] Data-based choice of a lasso estimate. The regularization param-\n",
      "eter\u0015for a lasso estimator (7.42) controls the number of nonzero coefÔ¨Å-\n",
      "cients ofQÀá.\u0015/ , with larger\u0015yielding fewer nonzeros. Efron et al. (2004)\n",
      "and Zou et al. (2007) showed that a good approximation for the degrees of\n",
      "freedom df (12.53) of a lasso estimate is the number of its nonzero coefÔ¨Å-\n",
      "cients. Substituting this for pin (12.51) provides a quick version of cErr\u0001.\n",
      "This was minimized at df D7for the supernova example in Figure 12.1\n",
      "(12.54).\n",
      "¬é3[p. 222] Stein‚Äôs unbiased risk estimate. The covariance formula (12.56) is\n",
      "obtained directly from integration by parts. The computation is clear from232 Cross-Validation and CpEstimates\n",
      "the one-dimensional version of (12.55), ND1:\n",
      "cov.O\u0016;y/DZ1\n",
      "\u00001\u00141p\n",
      "2\u0019\u001b2e\u00001\n",
      "2.y\u0000\u0016/2\n",
      "\u001b2.y\u0000\u0016/\u0015\n",
      "O\u0016.y/dy\n",
      "D\u001b2Z1\n",
      "\u00001\u00141p\n",
      "2\u0019\u001b2e\u00001\n",
      "2.y\u0000\u0016/2\n",
      "\u001b2\u0015@O\u0016.y/\n",
      "@ydy\n",
      "D\u001b2E\u001a@O\u0016.y/\n",
      "@y\u001b\n",
      ":(12.85)\n",
      "Broad regularity conditions for SURE are given in Stein (1981).\n",
      "¬é4[p. 227] The .632 rule. Bootstrap competitors to cross-validation are dis-\n",
      "cussed in Efron (1983) and Efron and Tibshirani (1997). The most success-\n",
      "ful of these, the ‚Äú.632 rule‚Äù is generally less variable than leave-one-out\n",
      "cross-validation. We suppose that nonparametric bootstrap data sets d\u0003b,\n",
      "bD1;2;:::;B , have been formed, each by sampling with replacement\n",
      "Ntimes from the original Nmembers ofd(12.1). Data set d\u0003bproduces\n",
      "rule\n",
      "r\u0003b.x/Drd\u0003b.x/; (12.86)\n",
      "giving predictions\n",
      "y\u0003b\n",
      "iDr\u0003b.xi/: (12.87)\n",
      "LetIb\n",
      "iD1if pair.xi;yi/isnotind\u0003b, and 0 if it is. (About e\u00001D\n",
      "0:368 of theN\u0001B Ib\n",
      "iwill equal 1, the remaining 0.632 equaling 0.) The\n",
      "‚Äúout of bootstrap‚Äù estimate of prediction error is\n",
      "cErr outDNX\n",
      "iD1BX\n",
      "jD1Ib\n",
      "iD\u0010\n",
      "yi;Oy\u0003b\n",
      "i\u0011,NX\n",
      "iD1BX\n",
      "jD1Ib\n",
      "i; (12.88)\n",
      "the average discrepancy in the omitted cases.\n",
      "cErr outis similar to a grouped cross-validation estimate that omits about\n",
      "37% of the cases each time. The .632 rule compensates for the upward bias\n",
      "incErr outby incorporating the downwardly biased apparent error (12.18),\n",
      "cErr:632D0:632cErr outC0:368 err: (12.89)\n",
      "cErr outhas resurfaced in the popular Random Forests prediction algorithm,\n",
      "Chapter 17, where a closely related procedure gives the ‚Äúout of bag‚Äù esti-\n",
      "mate of Err.\n",
      "¬é5[p. 230] Google Flu Trends. Harford‚Äôs 2014 article, ‚ÄúBig data: A big mis-\n",
      "take?,‚Äù concerns the enormous ‚Äúfound‚Äù data sets available in the internet\n",
      "age, and the dangers of forgetting the principles of statistical inference in\n",
      "their analysis. Google Flu Trends is his primary cautionary example.13\n",
      "Objective Bayes Inference and Markov Chain\n",
      "Monte Carlo\n",
      "From its very beginnings, Bayesian inference exerted a powerful inÔ¨Çuence\n",
      "on statistical thinking. The notion of a single coherent methodology em-\n",
      "ploying only the rules of probability to go from assumption to conclusion\n",
      "was and is immensely attractive. For 200 years, however, two impediments\n",
      "stood between Bayesian theory‚Äôs philosophical attraction and its practical\n",
      "application.\n",
      "1 In the absence of relevant past experience, the choice of a prior distribu-\n",
      "tion introduces an unwanted subjective element into scientiÔ¨Åc inference.\n",
      "2 Bayes‚Äô rule (3.5) looks simple enough, but carrying out the numerical\n",
      "calculation of a posterior distribution often involves intricate higher-\n",
      "dimensional integrals.\n",
      "The two impediments Ô¨Åt neatly into the dichotomy of Chapter 1, the Ô¨Årst\n",
      "being inferential and the second algorithmic.1\n",
      "A renewed cycle of Bayesian enthusiasm took hold in the 1960s, at Ô¨Årst\n",
      "concerned mainly with coherent inference. Building on work by Bruno de\n",
      "Finetti and L. J. Savage, a principled theory of subjective probability was\n",
      "constructed: the Bayesian statistician, by the careful elicitation of prior\n",
      "knowledge, utility, and belief, arrives at the correct subjective prior dis-\n",
      "tribution for the problem at hand. Subjective Bayesianism is particularly\n",
      "appropriate for individual decision making, say for the business executive\n",
      "trying to choose the best investment in the face of uncertain information.\n",
      "It is less appropriate for scientiÔ¨Åc inference, where the sometimes skep-\n",
      "tical world of science puts a premium on objectivity. An answer came from\n",
      "the school of objective Bayes inference . Following the approach of Laplace\n",
      "and Jeffreys, as discussed in Section 3.2, their goal was to fashion objec-\n",
      "tive, or ‚Äúuninformative,‚Äù prior distributions that in some sense were unbi-\n",
      "ased in their effects upon the data analysis.\n",
      "1The exponential family material in this chapter provides technical support, but is not\n",
      "required in detail for a general understanding of the main ideas.\n",
      "233234 Objective Bayes Inference and MCMC\n",
      "In what came as a surprise to the Bayes community, the objective school\n",
      "has been the most successful in bringing Bayesian ideas to bear on scien-\n",
      "tiÔ¨Åc data analysis. Of the 24 articles in the December 2014 issue of the\n",
      "Annals of Applied Statistics , 8 employed Bayesian analysis, predominantly\n",
      "based on objective priors.\n",
      "This is where electronic computation enters the story. Commencing in\n",
      "the 1980s, dramatic steps forward were made in the numerical calculation\n",
      "of high-dimensional Bayes posterior distributions. Markov chain Monte\n",
      "Carlo (MCMC) is the generic name for modern posterior computation al-\n",
      "gorithms. These proved particularly well suited for certain forms of objec-\n",
      "tive Bayes prior distributions.\n",
      "Taken together, objective priors and MCMC computations provide an\n",
      "attractive package for the statistician faced with a complicated data analy-\n",
      "sis situation. Statistical inference becomes almost automatic, at least com-\n",
      "pared with the rigors of frequentist analysis. This chapter discusses both\n",
      "parts of the package, the choice of prior and the subsequent computational\n",
      "methods. Criticisms arise, both from the frequentist viewpoint and that of\n",
      "informative Bayesian analysis, which are brought up here and also in Chap-\n",
      "ter 21.\n",
      "13.1 Objective Prior Distributions\n",
      "AÔ¨Çat, or uniform, distribution over the space of possible parameter values\n",
      "seems like the obvious choice for an uninformative prior distribution, and\n",
      "has been so ever since Laplace‚Äôs advocacy in the late eighteenth century.\n",
      "For a Ô¨Ånite parameter space , say\n",
      "Df\u0016.1/;\u0016.2/;:::;\u0016.K/g; (13.1)\n",
      "‚ÄúÔ¨Çat‚Äù has the obvious meaning\n",
      "gÔ¨Çat.\u0016/D1\n",
      "Kfor all\u00162: (13.2)\n",
      "IfKis inÔ¨Ånite, or if is continuous, we can still take\n",
      "gÔ¨Çat.\u0016/Dconstant: (13.3)\n",
      "Bayes‚Äô rule (3.5) gives the same posterior distribution for any choice of the\n",
      "constant,\n",
      "gÔ¨Çat.\u0016jx/DgÔ¨Çat.\u0016/f\u0016.x/=f.x/; with\n",
      "f.x/DZ\n",
      "f\u0016.x/gÔ¨Çat.\u0016/d\u0016:(13.4)13.1 Objective Prior Distributions 235\n",
      "Notice thatgÔ¨Çat.\u0016/cancels out of gÔ¨Çat.\u0016jx/. The fact that gÔ¨Çat.\u0016/is ‚Äúim-\n",
      "proper,‚Äù that is, it integrates to inÔ¨Ånity, doesn‚Äôt affect the formal use of\n",
      "Bayes‚Äô rule in (13.4) as long as f.x/ is Ô¨Ånite.\n",
      "Notice also that gÔ¨Çat.\u0016jx/amounts to taking the posterior density of\n",
      "\u0016to be proportional to the likelihood function Lx.\u0016/Df\u0016.x/(withx\n",
      "Ô¨Åxed and\u0016varying over ). This brings us close to Fisherian inference,\n",
      "with its emphasis on the direct interpretation of likelihoods, but Fisher was\n",
      "adamant in his insistance that likelihood was not probability.\n",
      "0 5 10 15 20 25 300.000.020.040.060.080.100.12\n",
      "Œ∏Densitiesconfidence\n",
      "densityposterior density,\n",
      "flat prior\n",
      "10Jeffreys\n",
      "Figure 13.1 The solid curve is Ô¨Çat-prior posterior density (13.4)\n",
      "having observed xD10from Poisson model x\u0018Poi.\u0016/; it is\n",
      "shifted about 0.5 units right from the conÔ¨Ådence density (dashed)\n",
      "of Figure 11.6. Jeffreys‚Äô prior gives a posterior density (dotted)\n",
      "nearly the same as the conÔ¨Ådence density.\n",
      "The solid curve in Figure 13.1 shows gÔ¨Çat.\u0016jx/for the Poisson situation\n",
      "of Table 11.2,\n",
      "x\u0018Poi.\u0016/; (13.5)\n",
      "withxD10observed;gÔ¨Çat.\u0016jx/is shifted almost exactly 0.5 units right\n",
      "of the conÔ¨Ådence density from Figure 11.6. (‚Äú \u0012‚Äù is\u0016itself in this case.)2\n",
      "Fisher‚Äôs withering criticism of Ô¨Çat-prior Bayes inference focused on its\n",
      "2The reader may wish to review Chapter 11, particularly Section 11.6, for these\n",
      "constructions.236 Objective Bayes Inference and MCMC\n",
      "lack of transformation invariance. If we were interested in \u0012Dlog.\u0016/\n",
      "rather than\u0016,gÔ¨Çat.\u0012jx/would not be the transformation to the log scale of\n",
      "gÔ¨Çat.\u0016jx/. Jeffreys‚Äô prior, (3.17) or (11.72), which does transform correctly,\n",
      "is\n",
      "gJeff.\u0016/D1ƒ±p\u0016 (13.6)\n",
      "forx\u0018Poi.\u0016/;gJeff.\u0016jxD10/is then a close match to the conÔ¨Ådence\n",
      "density in Figure 13.1.\n",
      "Coverage Matching Priors\n",
      "A variety of improvements and variations on Jeffreys‚Äô prior have been\n",
      "suggested for use as general-purpose uninformative prior distributions, as\n",
      "brieÔ¨Çy discussed in the chapter endnotes.¬éAll share the drawback seen in ¬é1\n",
      "Figure 11.7: the posterior distribution g.\u0016jx/can have unintended effects\n",
      "on the resulting inferences for a real-valued parameter of interest \u0012Dt.\u0016/.\n",
      "This is unavoidable; it is mathematically impossible for any single prior to\n",
      "be uninformative for every choice of \u0012Dt.\u0016/.\n",
      "The label ‚Äúuninformative‚Äù for a prior sometimes means ‚Äúgives Bayes\n",
      "posterior intervals that closely match conÔ¨Ådence intervals.‚Äù Perhaps sur-\n",
      "prisingly, this deÔ¨Ånition has considerable resonance in the Bayes commu-\n",
      "nity. Such priors can be constructed for any given scalar parameter of in-\n",
      "terest\u0012Dt.\u0016/, for instance the maximum eigenvalue parameter of Fig-\n",
      "ure 11.7. In brief, the construction proceeds as follows.¬é ¬é2\n",
      "\u000fThep-dimensional parameter vector \u0016is transformed to a form that\n",
      "makes\u0012the Ô¨Årst coordinate, say\n",
      "\u0016!.\u0012;\u0017/; (13.7)\n",
      "where\u0017is a.p\u00001/-dimensioned nuisance parameter.\n",
      "\u000fThe transformation is chosen so that the Fisher information matrix (11.72)\n",
      "for.\u0012;\u0017/ has the ‚Äúdiagonal‚Äù form\n",
      "\u0012I\u0012\u00120\n",
      "00I\u0017\u0017\u0013\n",
      ": (13.8)\n",
      "(This is always possible.)\n",
      "\u000fFinally, the prior for .\u0012;\u0017/ is taken proportional to\n",
      "g.\u0012;\u0017/DI1=2\n",
      "\u0012\u0012h.\u0017/; (13.9)\n",
      "whereh.\u0017/ is an arbitrary .p\u00001/-dimensional density. In other words,13.2 Conjugate Prior Distributions 237\n",
      "g.\u0012;\u0017/ combines the one-dimensional Jeffreys‚Äô prior (3.17) for \u0012with\n",
      "an arbitrary independent prior for the orthogonal nuisance parameter\n",
      "vector\u0017.\n",
      "The main thing to notice about (13.9) is that g.\u0012;\u0017/ represents different\n",
      "priors on the original parameter vector \u0016for different functions \u0012Dt.\u0016/.\n",
      "No single prior g.\u0016/ can be uninformative for all choices of the parameter\n",
      "of interest\u0012.\n",
      "Calculatingg.\u0012;\u0017/ can be difÔ¨Åcult. One alternative is to go directly to\n",
      "the BCa conÔ¨Ådence density (11.68)‚Äì(11.69), which can be interpreted as\n",
      "the posterior distribution from an uninformative prior (because its integrals\n",
      "agree closely with conÔ¨Ådence interval endpoints).\n",
      "Coverage matching priors are not much used in practice, and in fact none\n",
      "of the eight Annals of Applied Statistics objective Bayes papers mentioned\n",
      "earlier were of type (13.9). A form of ‚Äúalmost uninformative‚Äù priors, the\n",
      "conjugates, is more popular, mainly because of the simpler computation of\n",
      "their posterior distributions.\n",
      "13.2 Conjugate Prior Distributions\n",
      "A mathematically convenient class of prior distributions, the conjugate pri-\n",
      "ors, applies to samples from an exponential family,3Section 5.5,\n",
      "f\u0016.x/DeÀõx\u0000 .Àõ/f0.x/: (13.10)\n",
      "Here we have indexed the family with the expectation parameter\n",
      "\u0016DEffxg; (13.11)\n",
      "rather than the canonical parameter Àõ. On the right-hand side of (13.10), Àõ\n",
      "can be thought of as a one-to-one function of \u0016(the so-called ‚Äúlink func-\n",
      "tion‚Äù), e.g.,ÀõDlog.\u0016/for the Poisson family. The observed data is a\n",
      "random sample xD.x1;x2;:::;xn/fromf\u0016,\n",
      "x1;x2;:::;xniid\u0018f\u0016; (13.12)\n",
      "having density function\n",
      "f\u0016.x/Den≈íÀõNx\u0000 .Àõ/¬çf0.x/; (13.13)\n",
      "the averageNxDPxi=nbeing sufÔ¨Åcient.\n",
      "3We will concentrate on one-parameter families, though the theory extends to the\n",
      "multiparameter case. Figure 13.2 relates to a two-parameter situation.238 Objective Bayes Inference and MCMC\n",
      "The family of conjugate priors for \u0016,gn0;x0.\u0016/, allows the statistician\n",
      "to choose two parameters, n0andx0,\n",
      "gn0;x0.\u0016/Dcen0≈íx0Àõ\u0000 .Àõ/¬çƒ±\n",
      "V.\u0016/; (13.14)\n",
      "V.\u0016/ the variance of an xfromf\u0016,\n",
      "V.\u0016/DvarffxgI (13.15)\n",
      "cis the constant that makes gn0;x0.\u0016/integrate to 1 with respect to Lebesgue\n",
      "measure on the interval of possible \u0016values. The interpretation is that x0\n",
      "represents the average of n0hypothetical prior observations from f\u0016.\n",
      "The utility of conjugate priors is seen in the following theorem.\n",
      "Theorem 13.1¬éDeÔ¨Åne ¬é3\n",
      "nCDn0CnandNxCDn0\n",
      "nCx0Cn\n",
      "nCNx: (13.16)\n",
      "Then the posterior density of \u0016givenxD.x1;x2;:::;xn/is\n",
      "g.\u0016jx/DgnC;NxC.\u0016/I (13.17)\n",
      "moreover, the posterior expectation of \u0016givenxis\n",
      "Ef\u0016jxgDn0\n",
      "nCx0Cn\n",
      "nCNx: (13.18)\n",
      "The intuitive interpretation is quite satisfying: we begin with a hypo-\n",
      "thetical prior sample of size n0, sufÔ¨Åcient statistic x0; observex, a sam-\n",
      "ple of sizen; and update our prior distribution gn0;x0.\u0016/to a distribution\n",
      "gnC;NxC.\u0016/of the same form. Moreover, Ef\u0016jxgequals the average of a\n",
      "hypothetical sample with n0copies ofx0,\n",
      ".x0;x0;:::;x0;x1;x2;:::;xn/: (13.19)\n",
      "As an example, suppose xiiid\u0018Poi.\u0016/, that is we have ni.i.d. observa-\n",
      "tions from a Poisson distribution, Table 5.1. Formula (13.14) gives conju-\n",
      "gate prior¬é ¬é4\n",
      "gn0;x0.\u0016/Dc\u0016n0x0\u00001e\u0000n0\u0016; (13.20)\n",
      "cnot depending on \u0016. So in the notation of Table 5.1, gn0;x0.\u0016/is a gamma\n",
      "distribution, Gam .n0x0;1=n0/. The posterior distribution is\n",
      "g.\u0016jx/DgnC;NxC.\u0016/\u0018Gam.nCNxC;1=nC/\n",
      "\u00181\n",
      "nCGnCNxC;(13.21)13.2 Conjugate Prior Distributions 239\n",
      "whereG\u0017indicates a standard gamma distribution,¬é ¬é5\n",
      "G\u0017DGam.\u0017;1/: (13.22)\n",
      "Table 13.1 Conjugate priors (13.14) ‚Äì(13.16) for four familiar\n",
      "one-parameter exponential families, using notation in Table 5.1; the last\n",
      "column shows the posterior distribution of \u0016givennobservations xi,\n",
      "starting from prior gn0;x0.\u0016/. In line 4,G\u0017is the standard gamma\n",
      "distribution Gam.\u0017;1/ , with\u0016the same as gamma parameter \u001bin\n",
      "Table 5.1. The chapter endnotes give the density of the inverse gamma\n",
      "distribution1=G\u0017, and corresponding results for chi-squared variates.\n",
      "Namexidistribution gn0;x0.\u0016/ g.\u0016 jx/\n",
      "1. Normal N.\u0016;\u001b2\n",
      "1/ N.x0;\u001b2\n",
      "1=n0/ N.NxC;\u001b2\n",
      "1=nC/\n",
      "(\u001b2\n",
      "1known)\n",
      "2. Poisson Poi .\u0016/ Gam.n0x0;1=n0/ Gam.nCNxC;1=nC/\n",
      "3. Binomial Bi .1;\u0016/ Be.n0x0;n0.1\u0000x0//Be.nCNxC;nC.1\u0000NxC//\n",
      "4. Gamma \u0016G\u0017=\u0017 n 0x0\u0017=Gn0\u0017C1nCNxC\u0017=GnC\u0017C1\n",
      "(\u0017known)\n",
      "Table 13.1 describes the conjugate prior and posterior distributions for\n",
      "four familiar one-parameter families. The binomial case, where \u0016is the\n",
      "‚Äúsuccess probability‚Äù \u0019in Table 5.1, is particularly evocative: indepen-\n",
      "dent coin Ô¨Çips x1;x2;:::;xngive, say,sDP\n",
      "ixiDnNxsuccesses. Prior\n",
      "gn0;x0.\u0019/amounts to assuming proportion x0Ds0=n0prior successes in\n",
      "n0Ô¨Çips. Formula (13.18) becomes\n",
      "Ef\u0019jxgDs0Cs\n",
      "n0Cn(13.23)\n",
      "for the posterior expectation of \u0019. The choice .n0;x0/D.2;1=2/ for in-\n",
      "stance gives Bayesian estimate .sC1/=.nC2/for\u0019, pulling the MLE s=n\n",
      "a little bit toward 1/2.\n",
      "The size ofn0, the number of hypothetical prior observations, deter-\n",
      "mines how informative or uninformative the prior gn0;x0.\u0016/is. Recent\n",
      "objective Bayes literature has favored choosing n0small,n0D1being\n",
      "popular. The hope here is to employ a proper prior (one that has a Ô¨Ånite\n",
      "integral), while still not injecting much unwarranted information into the\n",
      "analysis. The choice of x0is also by convention. One possibility is to set240 Objective Bayes Inference and MCMC\n",
      "x0D Nx, in which case the posterior expectation Ef\u0016jxg(13.18) equals\n",
      "the MLENx. Another possibility is choosing x0equal to a ‚Äúnull‚Äù value, for\n",
      "instancex0D0for effect size estimation in (3.28).\n",
      "Table 13.2 Vasoconstriction data; volume of air inspired in 39 cases, 19\n",
      "without vasoconstriction ( yD0) and 20 with vasoconstriction ( yD1).\n",
      "yD0 yD1\n",
      "60 98 85 115\n",
      "74 98 88 120\n",
      "78 104 88 126\n",
      "78 104 90 126\n",
      "78 113 90 128\n",
      "88 118 93 136\n",
      "90 120 104 143\n",
      "95 123 108 151\n",
      "95 137 110 154\n",
      "98 111 157\n",
      "As a miniature example of objective Bayes inference, we consider the\n",
      "vasoconstriction data¬éof Table 13.2: nD39measurements of lung volume ¬é6\n",
      "have been obtained, 19 without vasoconstriction .yD0/and 20 with.yD\n",
      "1/. Here we will think of the yias binomial variates,\n",
      "yiind\u0018Bi.1;\u0019i/; iD1;2;:::;39; (13.24)\n",
      "following logistic regression model (8.5),\n",
      "log\u0012\u0019i\n",
      "1\u0000\u0019i\u0013\n",
      "DÀõ0CÀõ1xi; (13.25)\n",
      "with thexias Ô¨Åxed covariates (the values in Table 13.2).\n",
      "LettingXiD.1;xi/0, (13.24)‚Äì(13.25) results in a two-parameter expo-\n",
      "nential family (8.24),\n",
      "fÀõ.y/Denh\n",
      "Àõ0OÀá\u0000 .Àõ/i\n",
      "f0.y/; (13.26)\n",
      "having\n",
      "OÀáD1\n",
      "n nX\n",
      "iD1yi;nX\n",
      "iD1xiyi!0\n",
      "and .Àõ/D1\n",
      "nnX\n",
      "iD1log.1CeÀõ0Xi/:\n",
      "The MLEOÀõhas approximate 2\u00022covariance matrixOVas given in (8.30).13.2 Conjugate Prior Distributions 241\n",
      "In Figure 13.2, the posterior distributions are graphed in terms of\n",
      "DOV\u00001=2.Àõ\u0000OÀõ/ (13.27)\n",
      "rather thanÀõor\u0016, making the contours of equal density roughly circular\n",
      "and centered at zero.\n",
      "A. Flat Prior\n",
      "Œ≥1Œ≥2 0.1  0.3 \n",
      " 0.5  0.75  0.9  0.975 \n",
      " 0.99  0.997 \n",
      " 0.999  0.999 \n",
      "‚àí4 ‚àí2 0 2 4‚àí4 ‚àí2 0 2 4\n",
      "‚óèB. Jeffreys' Prior\n",
      "Œ≥1Œ≥2 0.1 \n",
      " 0.3  0.5 \n",
      " 0.75  0.9  0.975 \n",
      " 0.99  0.997 \n",
      " 0.999 \n",
      "‚àí4 ‚àí2 0 2 4‚àí4 ‚àí2 0 2 4\n",
      " 0.1  0.5  0.9  0.99 \n",
      " 0.999  0.999 \n",
      "‚óè\n",
      "C. Conjugate Prior\n",
      "Œ≥1Œ≥2\n",
      " 0.1  0.3 \n",
      " 0.5  0.75  0.9  0.975 \n",
      " 0.99  0.997 \n",
      " 0.999 \n",
      "‚àí4 ‚àí2 0 2 4‚àí4 ‚àí2 0 2 4\n",
      " 0.1  0.5  0.9  0.99 \n",
      " 0.999  0.999 \n",
      "‚óèD. Bootstrap Distribution\n",
      "Œ≥1Œ≥2\n",
      " 0.1  0.3 \n",
      " 0.5  0.75  0.9  0.975  0.99 \n",
      " 0.997  0.997 \n",
      " 0.999 \n",
      "‚àí4 ‚àí2 0 2 4‚àí4 ‚àí2 0 2 4\n",
      "‚óè 0.1  0.5  0.9  0.99 \n",
      " 0.999  0.999 \n",
      "Figure 13.2 Vasoconstriction data; contours of equal posterior\n",
      "(13.27) from four uninformative priors, as described\n",
      "in the text. Numbers indicate probability content within contours;\n",
      "light dashed contours from Panel A, Ô¨Çat prior.\n",
      "Panel A of Figure 13.2 illustrates the Ô¨Çat prior posterior density of \n",
      "given the data yin model (13.24)‚Äì(13.25). The heavy lines are contours\n",
      "of equal density, with the one labeled ‚Äú0.9‚Äù containing 90% of the pos-\n",
      "terior probability, etc. Panel B shows the corresponding posterior density242 Objective Bayes Inference and MCMC\n",
      "contours obtained from Jeffreys‚Äô multiparameter prior (11.72), in this case\n",
      "gJeff.Àõ/DjVÀõj\u00001=2; (13.28)\n",
      "VÀõthe covariance matrix of OÀõ, as calculated from (8.30). For comparison\n",
      "purposes the light dashed curves show some of the Ô¨Çat prior contours from\n",
      "panel A. The effect of gJeff.Àõ/is to reduce the Ô¨Çat prior bulge toward the\n",
      "upper left corner.\n",
      "Panel C relates to the conjugate prior4g1;0.Àõ/. Besides reducing the Ô¨Çat\n",
      "prior bulge,g1;0.Àõ/pulls the contours slightly downward.\n",
      "Panel D shows the parametric bootstrap distribution: model (13.24)‚Äì\n",
      "(13.25), withOÀõreplacingÀõ, gave resamples y\u0003and MLE replications OÀõ\u0003.\n",
      "\u0003DOV\u00001=2.OÀõ\u0003\u0000OÀõ/considerably accentuate the bulge\n",
      "toward the left.\n",
      "‚àí4 ‚àí2 0 2 40.00 0.01 0.02 0.03 0.04\n",
      "Œ≥1Density\n",
      "********************************************\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "***********************BCaraw bootstrapJeffreys\n",
      "in Ô¨Årst coordinate of densities for \n",
      "(13.27), for the vasoconstriction data. Dashed red curve: raw\n",
      "(unweighted) distribution of BD8000 parametric replications\n",
      "from model (13.24)‚Äì(13.25); solid black curve: BCa density\n",
      "(11.68) (z0D0:123 ,aD0:053 ); dotted blue curve: posterior\n",
      "density using Jeffreys multiparameter prior (11.72).\n",
      "4The role ofNxin (13.13) is taken by OÀáin (13.26), sog1;0hasOÀáD0,n0D1. This\n",
      "makesg1;0.Àõ/Dexpf\u0000 .Àõ/g. The factorV.\u0016/ in (13.14) is absent in the conjugate\n",
      "prior forÀõ(as opposed to \u0016).13.3 Model Selection and the Bayesian Information Criterion 243\n",
      "This doesn‚Äôt necessarily imply that a bootstrap analysis would give much\n",
      "different answers than the three (quite similar) objective Bayes results. For\n",
      "any particular real-valued parameter of interest \u0012, the raw bootstrap distri-\n",
      "bution (equal weight on each replication) would be reweighted according to\n",
      "the BCa formula (11.68) in order to produce accurate conÔ¨Ådence intervals.\n",
      "Figure 13.3 compares the raw bootstrap distribution, the BCa conÔ¨Ådence\n",
      "density, and the posterior density obtained from Jeffreys‚Äô prior, for \u0012equal\n",
      "in (13.27). The BCa density is shifted to the\n",
      "right of Jeffreys‚Äô.\n",
      "Critique of Objective Bayes Inference\n",
      "Despite its simplicity, or perhaps because of it, objective Bayes procedures\n",
      "are vulnerable to criticism from both ends of the statistical spectrum. From\n",
      "the subjectivist point of view, objective Bayes is only partially Bayesian: it\n",
      "employs Bayes‚Äô theorem but without doing the hard work of determining a\n",
      "convincing prior distribution. This introduces frequentist elements into its\n",
      "practice‚Äîclearly so in the case of Jeffreys‚Äô prior‚Äîalong with frequentist\n",
      "incoherencies.\n",
      "For the frequentist, objective Bayes analysis can seem dangerously un-\n",
      "tethered from the usual standards of accuracy, having only tenuous large-\n",
      "sample claims to legitimacy. This is more than a theoretical objection. The\n",
      "practical advantages claimed for Bayesian methods depend crucially on the\n",
      "Ô¨Åne structure of the prior. Can we safely ignore stopping rules or selective\n",
      "inference (e.g., choosing the largest of many estimated parameters for spe-\n",
      "cial attention) for a prior not based on some form of genuine experience?\n",
      "In an era of large, complicated, and difÔ¨Åcult data-analytic problems, ob-\n",
      "jective Bayes methods are answering a felt need for relatively straightfor-\n",
      "ward paths to solution. Granting their usefulness, it is still reasonable to\n",
      "hope for better justiÔ¨Åcation,5or at least for more careful comparisons with\n",
      "competing methods as in Figure 13.3.\n",
      "13.3 Model Selection and the Bayesian Information Criterion\n",
      "Data-based model selection has become a major theme of modern statisti-\n",
      "cal inference. In the problem‚Äôs simplest form, the statistician observes data\n",
      "xand wishes to choose between a smaller model M0and a larger model\n",
      "5Chapter 20 discusses the frequentist assessment of Bayes and objective Bayes estimates.244 Objective Bayes Inference and MCMC\n",
      "M1. The classic textbook example takes xD.x1;x2;:::;xn/0as an inde-\n",
      "pendent normal sample,\n",
      "xiiid\u0018N.\u0016;1/ foriD1;2;:::;n; (13.29)\n",
      "withM0the null hypothesis \u0016D0andM1the general two-sided alter-\n",
      "native,\n",
      "M0W\u0016D0; M1W\u0016¬§0: (13.30)\n",
      "(We can include \u0016D0inM1with no effect on what follows.) From a\n",
      "frequentist viewpoint, choosing between M0andM1in (13.29)‚Äì(13.30)\n",
      "amounts to running a hypothesis test of H0W\u0016D0, perhaps augmented\n",
      "with a conÔ¨Ådence interval for \u0016.\n",
      "Bayesian model selection aims for more: an evaluation of the posterior\n",
      "probabilities of M0andM1givenx. A full Bayesian speciÔ¨Åcation re-\n",
      "quires prior probabilities for the two models,\n",
      "\u00190DPrfM0gand\u00191D1\u0000\u00190DPrfM1g; (13.31)\n",
      "and conditional prior densities for \u0016within each model,\n",
      "g0.\u0016/Dg.\u0016jM0/andg1.\u0016/Dg.\u0016jM1/: (13.32)\n",
      "Letf\u0016.x/be the density of xgiven\u0016. Each model induces a marginal\n",
      "density forx, say\n",
      "f0.x/DZ\n",
      "M0f\u0016.x/g0.\u0016/d\u0016 andf1.x/DZ\n",
      "M1f\u0016.x/g1.\u0016/d\u0016:\n",
      "(13.33)\n",
      "Bayes‚Äô theorem, in its ratio form (3.8), then gives posterior probabilities\n",
      "\u00190.x/DPrfM0jxgand\u00191.x/DPrfM1jxg (13.34)\n",
      "satisfying\n",
      "\u00191.x/\n",
      "\u00190.x/D\u00191\n",
      "\u00190B.x/; (13.35)\n",
      "whereB.x/is the Bayes factor\n",
      "B.x/Df1.x/\n",
      "f0.x/; (13.36)\n",
      "leading to the elegant statement that the posterior odds ratio is the prior\n",
      "odds ratio times the Bayes factor.\n",
      "All of this is of more theoretical than applied use. Prior speciÔ¨Åcations\n",
      "(13.31)‚Äì(13.32) are usually unavailable in practical settings (which is why13.3 Model Selection and the BIC 245\n",
      "standard hypothesis testing is so popular). The objective Bayes school has\n",
      "concentrated on estimating the Bayes factor B.x/, with the understanding\n",
      "that the prior odds ratio \u00191=\u00190in (13.35) would be roughly evaluated de-\n",
      "pending on the speciÔ¨Åc circumstances‚Äîperhaps set to the Laplace choice\n",
      "\u00191=\u00190D1.\n",
      "Table 13.3 Jeffreys‚Äô scale of evidence for the interpretation of Bayes\n",
      "factors.\n",
      "Bayes factor Evidence for M1\n",
      "<1 negative\n",
      "1‚Äì3 barely worthwhile\n",
      "3‚Äì20 positive\n",
      "20‚Äì150 strong\n",
      ">150 very strong\n",
      "Jeffreys suggested a scale of evidence for interpreting Bayes factors, re-\n",
      "produced in Table 13.3;¬éB.x/D10for instance constitutes positive but¬é7\n",
      "notstrong evidence in favor of the bigger model. Jeffreys‚Äô scale is a Bayes-\n",
      "ian version of Fisher‚Äôs interpretive scale for the outcome of a hypothetic\n",
      "test, with coverage value (one minus the signiÔ¨Åcance level) 0.95 famously\n",
      "constituting ‚ÄúsigniÔ¨Åcant‚Äù evidence against the null hypothesis. Table 13.4\n",
      "shows Fisher‚Äôs scale, as commonly interpreted in the biomedical and social\n",
      "sciences.\n",
      "Table 13.4 Fisher‚Äôs scale of evidence against null hypothesis M0and in\n",
      "favor of M1, as a function of coverage level (1 minus the p-value).\n",
      "Coverage ( p-value) Evidence for M1\n",
      ".80 (.20) null\n",
      ".90 (.10) borderline\n",
      ".95 (.05) moderate\n",
      ".975 (.025) substantial\n",
      ".99 (.01) strong\n",
      ".995 (.005) very strong\n",
      ".999 (.001) overwhelming\n",
      "Even if we accept the reduction of model selection to assessing the\n",
      "Bayes factor B.x/in (13.35), and even if we accept Jeffreys‚Äô scale of in-\n",
      "terpretation, this still leaves a crucial question: how to compute B.x/in246 Objective Bayes Inference and MCMC\n",
      "practice, without requiring informative choices of the priors g0andg1in\n",
      "(13.32).\n",
      "A popular objective Bayes answer is provided by the Bayesian informa-\n",
      "tion criterion¬é(BIC). For a given model Mwe deÔ¨Åne ¬é8\n",
      "BIC.M/DlogÀö\n",
      "fO\u0016.x/\t\n",
      "\u0000p\n",
      "2log.n/; (13.37)\n",
      "whereO\u0016is the MLE,pthe degrees of freedom (number of free parameters)\n",
      "inM, andnthe sample size. Then the BIC approximation to Bayes factor\n",
      "B.x/(13.36) is\n",
      "logBBIC.x/DBIC.M1/\u0000BIC.M0/\n",
      "DlogÀö\n",
      "fO\u00161.x/=fO\u00160.x/\t\n",
      "\u0000p1\u0000p0\n",
      "2log.n/;(13.38)\n",
      "the subscripts indexing the MLEs and degrees of freedom in M1andM0.\n",
      "This can be restated in somewhat more familiar terms. Letting W.x/be\n",
      "Wilks‚Äô likelihood ratio statistic ,\n",
      "W.x/D2logÀö\n",
      "fO\u00161.x/=fO\u00160.x/\t\n",
      "; (13.39)\n",
      "we have\n",
      "logBBIC.x/D1\n",
      "2fW.x/\u0000dlog.n/g; (13.40)\n",
      "withdDp1\u0000p0:W.x/approximately follows a \u001f2\n",
      "ddistribution under\n",
      "model M0,E0fW.x/g:Dd, implyingBBIC.x/will tend to be less than\n",
      "one, favoring M0if it is true, ever more strongly as nincreases.\n",
      "We can apply BIC selection to the vasoconstriction data of Table 13.2,\n",
      "taking M1to be model (13.24)‚Äì(13.25), and M0to be the submodel\n",
      "havingÀõ1D0. In this case dD1in (13.40). Direct calculation gives\n",
      "WD7:07 and\n",
      "BBICD5:49; (13.41)\n",
      "positive but not strong evidence against M0according to Jeffreys‚Äô scale.\n",
      "By comparison, the usual frequentist z-value for testing Àõ1D0is 2.36,\n",
      "coverage level 0.982, between substantial andstrong evidence against M0\n",
      "on Fisher‚Äôs scale.\n",
      "The BIC was named in reference to Akaike‚Äôs information criterion (AIC),\n",
      "AIC.M/DlogÀö\n",
      "fO\u0016.x/\t\n",
      "\u0000p; (13.42)\n",
      "which suggests, as in (12.73), basing model selection on the sign of\n",
      "AIC.M1/\u0000AIC.M0/D1\n",
      "2fW.x/\u00002dg: (13.43)13.3 Model Selection and the BIC 247\n",
      "The BIC penalty dlog.n/in (13.40) grows more severe than the AIC\n",
      "penalty2dasngets larger, increasingly favoring selection of M0rather\n",
      "thanM1. The distinction is rooted in Bayesian notions of coherent behav-\n",
      "ior, as discussed in what follows.\n",
      "Where does the BIC penalty term dlog.n/in (13.40) come from? A Ô¨Årst\n",
      "answer uses the simple normal model xi\u0018N.\u0016;1/ , (13.29)‚Äì(13.30). M0\n",
      "has priorg0.\u0016/Dg.\u0016jM0/equal a delta function at zero. Suppose we\n",
      "takeg1.\u0016/Dg.\u0016jM1/in (13.32) to be the Gaussian conjugate prior\n",
      "g1.\u0016/\u0018N.M;A/: (13.44)\n",
      "The discussion following (13.23) in Section 13.2 suggests setting MD0\n",
      "andAD1, corresponding to prior information equivalent to one of the n\n",
      "actual observations. In this case we can calculate the actual Bayes factor\n",
      "B.x/,\n",
      "logB.x/D1\n",
      "2\u001an\n",
      "nC1W.x/\u0000log.nC1/\u001b\n",
      "; (13.45)\n",
      "nearly equaling log BBIC.x/(dD1), for largen. JustiÔ¨Åcations of the BIC\n",
      "formula as an approximate Bayes factor follow generalizations of this kind\n",
      "of argument, as discussed in the chapter endnotes.\n",
      "The difference between BIC and frequentist hypothesis testing grows\n",
      "more drastic for large n. Suppose M0is a regression model and M1isM0\n",
      "augmented with one additional covariate (so dD1). Letzbe a standard\n",
      "z-value for testing the hypothesis that M1is no improvement over M0,\n",
      "zP \u0018N.0;1/ underM0: (13.46)\n",
      "Table 13.5 shows BBIC.x/as a function of zandn. AtnD15Fisher‚Äôs\n",
      "and Jeffreys‚Äô scales give roughly similar assessments of the evidence against\n",
      "M0(though Jeffreys‚Äô nomenclature is more conservative). At the other end\n",
      "of the table, at nD10;000 , the inferences are contradictory: zD3:29,\n",
      "withp-value 0.001 and coverage level 0.999, is overwhelming evidence\n",
      "forM1on Fisher‚Äôs scale, but barely worthwhile for Jeffreys‚Äô. Bayesian\n",
      "coherency, the axiom that inferences should be consistent over related sit-\n",
      "uations, lies behind the contradiction.\n",
      "SupposenD1in the simple normal model (13.29)‚Äì(13.30). That is, we\n",
      "observe only the single variable\n",
      "x\u0018N.\u0016;1/; (13.47)\n",
      "and wish to decide between M0W\u0016D0andM1W\u0016¬§0. Letg.1/\n",
      "1.\u0016/\n",
      "denote our M1prior density (13.32) for this situation.248 Objective Bayes Inference and MCMC\n",
      "Table 13.5 BIC Bayes factors corresponding to z-values for testing one\n",
      "additional covariate; coverage value (1 minus the signiÔ¨Åcance level) of a\n",
      "two-sided hypothesis test as interpreted by Fisher‚Äôs scale of evidence,\n",
      "right. Jeffreys‚Äô scale of evidence, Table 13.3, is in rough agreement with\n",
      "Fisher fornD15, but favors the null much more strongly for larger\n",
      "sample sizes.\n",
      "n\n",
      "Coverz-value15 50 250 1000 2500 5000 10000 Fisher\n",
      ".80 1.28 .59 .32 .14 .07 .05 .03 .02 null\n",
      ".90 1.64 1.00 .55 .24 .12 .08 .05 .04 borderline\n",
      ".95 1.96 1.76 .97 .43 .22 .14 .10 .07 moderate\n",
      ".975 2.24 3.18 1.74 .78 .39 .25 .17 .12 substantial\n",
      ".99 2.58 7.12 3.90 1.74 .87 .55 .39 .28 strong\n",
      ".995 2.81 13.27 7.27 3.25 1.63 1.03 .73 .51 very strong\n",
      ".999 3.29 57.96 31.75 14.20 7.10 4.49 3.17 2.24 overwhelming\n",
      "The casen>1 in (13.29) is logically identical to (13.47). Letting x.n/Dpn.Pxi=n/and\u0016.n/Dpn\u0016gives\n",
      "x.n/\u0018N\u0010\n",
      "\u0016.n/;1\u0011\n",
      "; (13.48)\n",
      "with (13.30) becoming M0W\u0016.n/D0andM1W\u0016.n/¬§0. Coherency\n",
      "requires that\u0016.n/in (13.48) have the same M1prior as\u0016in (13.47). Since\n",
      "\u0016D\u0016.n/=pn, this implies that g.n/\n",
      "1.\u0016/, theM1prior for sample size n,\n",
      "satisÔ¨Åes\n",
      "g.n/\n",
      "1.\u0016/Dg.1/\n",
      "1\u0000\n",
      "\u0016ƒ±pn\u0001ƒ±pn; (13.49)\n",
      "this being ‚Äúsample size coherency.‚Äù\n",
      "The effect of (13.49) is to spread the M1prior density g.n/\n",
      "1.\u0016/farther\n",
      "away from the null value \u0016D0at ratepn, while the M0priorg.n/\n",
      "0.\u0016/\n",
      "stays Ô¨Åxed. For any Ô¨Åxed value of the sufÔ¨Åcient statistic x.n/(x.n/being\n",
      "‚Äúz‚Äù in Table 13.5), this results in the Bayes factor B.x.n//decreasing at\n",
      "rate1=pn; the frequentist/Bayesian contradiction seen in Table 13.5 goes\n",
      "beyond the speciÔ¨Åcs of the BIC algorithm.\n",
      "\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001\n",
      "Ageneral information criterion takes the form\n",
      "GIC.M/DlogfO\u0016.x/\u0000pcn; (13.50)13.3 Model Selection and the BIC 249\n",
      "wherecnis any sequence of positive numbers; cnDlog.n/=2 for BIC\n",
      "(13.37) andcnD1for AIC (13.42). The difference\n",
      "¬Å\u0011GIC.M1/\u0000GIC.M0/D1\n",
      "2.W.x/\u00002cnd/; (13.51)\n",
      "dDp1\u0000p0, will be positive if W.x/>2cnd. FordD1, as in Table 13.5,\n",
      "¬Åwill favor M1ifW.x/\u00152cn, with approximate probability, if M0is\n",
      "actually true,\n",
      "Prf\u001f2\n",
      "1\u00152cng: (13.52)\n",
      "This equals 0.157 for the AIC choice cnD1; for BIC,nD10;000 , it\n",
      "equals 0.0024. The choice\n",
      "cnD1:92 (13.53)\n",
      "makes Prf¬Å > 0jM0g:D0:05, agreeing with the usual frequentist 0.05\n",
      "rejection level.\n",
      "The BIC is consistent : Prf¬Å>0ggoes to zero as n!1 ifM0is true.\n",
      "This isn‚Äôt true of (13.53) for instance, where we will have Pr f¬Å > 0g:D\n",
      "0:05 no matter how large nmay be, but consistency is seldom compelling\n",
      "as a practical argument.\n",
      "ConÔ¨Ådence intervals help compensate for possible frequentist overÔ¨Åt-\n",
      "ting. WithzD3:29 andnD10;000 , the 95% conÔ¨Ådence interval for \u0016\n",
      "in model M1(13.30) is.0:013;0:053/ . Whether or not such a small effect\n",
      "is interesting depends on the scientiÔ¨Åc context. The fact that BIC says ‚Äúnot\n",
      "interesting‚Äù speaks to its inherent small-model bias.\n",
      "The prostate cancer study data of Section 3.3 provides a more challeng-\n",
      "ing model selection problem. Figure 3.4 shows the histogram of ND6033\n",
      "observationsxi, each measuring the effects of one gene. The histogram has\n",
      "49 bins, each of width 0.2, with centers cjranging from\u00004:4to 5.2;yj,\n",
      "the height of the histogram at cj, is the number of xiin binj,\n",
      "yjD#fxi2binjg forjD1;2;:::;49: (13.54)\n",
      "We assume that the yjfollow a Poisson regression model as in Sec-\n",
      "tion 8.3,\n",
      "yjind\u0018Poi.\u0017j/; jD1;2;:::;49; (13.55)\n",
      "and wish to Ô¨Åt a log polynomial GLM model to the \u0017j. The model selection\n",
      "question is ‚ÄúWhat degree polynomial?‚Äù Degree 2 corresponds to normal\n",
      "densities, but the long tails seen in Figure 3.4 suggest otherwise.\n",
      "Models of degree 2 through 8 are assessed in Figure 13.4. Four model\n",
      "selection measures are compared: AIC (13.42); BIC (13.37) with nD49,250 Objective Bayes Inference and MCMC\n",
      "2 3 4 5 6 7 8‚àí120 ‚àí100 ‚àí80 ‚àí60 ‚àí40\n",
      "Model polynomial degreeAIC and BIC BIC 6033BIC 49 and GICAIC\n",
      "Figure 13.4 Log polynomial models of degree 2 through 8\n",
      "applied to the prostate study histogram of Figure 3.4. Model\n",
      "selection criteria: AIC (13.42); BIC (13.37) with nD49, number\n",
      "of bins, or 6033, number of genes; GIC (13.50) using classic\n",
      "Fisher hypothesis choice cnD1:92. All four selected the\n",
      "fourth-degree model as best.\n",
      "the number of yjvalues (bins), and also nD6033 , the number of genes;\n",
      "and GIC (13.50), with cnD1:92 (13.53), the choice based on classic Fish-\n",
      "erian hypothesis testing. (This is almost the same as BIC nD49, since\n",
      "log.49/=2D1:95.) A fourth-degree polynomial model was the winner\n",
      "under all four criteria.\n",
      "The ‚Äúuntethered‚Äù criticism made against objective Bayes methods in\n",
      "general is particularly applicable to BIC. The concept of ‚Äúsample size‚Äù\n",
      "is not well deÔ¨Åned, as the prostate study example shows. Sample size co-\n",
      "herency (13.49), the rationale for BIC‚Äôs strong bias toward smaller models,\n",
      "is less convincing in the absence of priors based on genuine experience (es-\n",
      "pecially if there is no prospect of the sample size changing). Whatever its\n",
      "vulnerabilities, BIC model selection has nevertheless become a mainstay\n",
      "of objective Bayes model selection, not least because of its freedom from\n",
      "the choice of Bayesian priors.13.4 Gibbs Sampling and MCMC 251\n",
      "13.4 Gibbs Sampling and MCMC\n",
      "Miraculously blessed with visions of the future, a Bayesian statistician of\n",
      "the 1970s would certainly be pleased with the prevalence of Bayes method-\n",
      "ology in twenty-Ô¨Årst-century applications. But his pleasure might be tinged\n",
      "with surprise that the applications were mostly of the objective, ‚Äúuninfor-\n",
      "mative‚Äù type, rather than taken from the elegant de Finetti‚ÄìSavage school\n",
      "of subjective inference.\n",
      "The increase in Bayesian applications, and the change in emphasis from\n",
      "subjective to objective, had more to do with computation than philoso-\n",
      "phy. Better computers and algorithms facilitated the calculation of formerly\n",
      "intractable Bayes posterior distributions. Technology determines practice,\n",
      "and the powerful new algorithms encouraged Bayesian analyses of large\n",
      "and complicated models where subjective priors (or those based on actual\n",
      "past experience) were hard to come by. Add in the fact that the algorithms\n",
      "worked most easily with simple ‚Äúconvenience‚Äù priors like the conjugates\n",
      "of Section 13.2, and the stage was set for an objective Bayes renaissance.\n",
      "At Ô¨Årst glance it‚Äôs hard to see why Bayesian computations should be\n",
      "daunting. From parameter vector \u0012, datax, density function f\u0012.x/, and\n",
      "prior density g.\u0012/, Bayes‚Äô rule (3.5)‚Äì(3.6) directly produces the posterior\n",
      "density\n",
      "g.\u0012jx/Dg.\u0012/f\u0012.x/=f.x/; (13.56)\n",
      "wheref.x/is the marginal density\n",
      "f.x/DZ\n",
      "g.\u0012/f\u0012.x/d\u0012: (13.57)\n",
      "The posterior probability of any set Ain the parameter space is then\n",
      "PfAjxgDZ\n",
      "Ag.\u0012/f\u0012.x/d\u0012\u001eZ\n",
      "g.\u0012/f\u0012.x/d\u0012: (13.58)\n",
      "This is easy to write down but usually difÔ¨Åcult to evaluate if \u0012is multidi-\n",
      "mensional.\n",
      "Modern Bayes methods attack the problem through the application of\n",
      "computer power. Even if we can‚Äôt integrate g.\u0012jx/, perhaps we can sample\n",
      "from it. If so, a sufÔ¨Åciently large sample, say\n",
      "\u0012.1/;\u0012.2/;:::;\u0012.B/\u0018g.\u0012jx/ (13.59)\n",
      "would provide estimates\n",
      "OPfAjxgD#n\n",
      "\u0012.j/2Ao.\n",
      "B; (13.60)252 Objective Bayes Inference and MCMC\n",
      "and similarly for posterior moments, correlations, etc. We would in this\n",
      "way be employing the same general tactic as the bootstrap, applied now\n",
      "for Bayesian rather than frequentist purposes‚Äîtoward the same goal as the\n",
      "bootstrap, of freeing practical applications from the constraints of mathe-\n",
      "matical tractability.\n",
      "The two most popular computational methods,6Gibbs sampling and\n",
      "Markov chain Monte Carlo (MCMC), are based on Markov chain algo-\n",
      "rithms; that is, the posterior samples \u0012.b/are produced in sequence, each\n",
      "one depending only on \u0012.b\u00001/and not on its more distant predecessors. We\n",
      "begin with Gibbs sampling.\n",
      "The central idea of Gibbs sampling is to reduce the generation of mul-\n",
      "tidimensional vectors \u0012D.\u00121;\u00122;:::;\u0012K/to a series of univariate calcu-\n",
      "lations. Let\u0012.k/denote\u0012with component kremoved, and g.k/the condi-\n",
      "tional density of \u0012kgiven\u0012.k/and the datax,\n",
      "\u0012kj\u0012.k/;x\u0018g.k/\u0010\n",
      "\u0012kj\u0012.k/;x\u0011\n",
      ": (13.61)\n",
      "The algorithm begins at some arbitrary initial value \u0012.0/. Having computed\n",
      "\u0012.1/,\u0012.2/,:::,\u0012.b\u00001/, the components of \u0012.b/are generated according to\n",
      "conditional distributions (13.61),\n",
      "\u0012.b/\n",
      "k\u0018g.k/\u0010\n",
      "\u0012kÀáÀá\u0012.b\u00001/\n",
      ".k/;x\u0011\n",
      "forkD1;2;:::;K: (13.62)\n",
      "As an example, we take xto be thenD20observations for yD1in\n",
      "the vasoconstriction data of Table 13.2, and assume that these are a normal\n",
      "sample,\n",
      "xiiid\u0018N.\u0016;\u001c/; iD1;2;:::;nD20: (13.63)\n",
      "The sufÔ¨Åcient statistics for estimating the bivariate parameter \u0012D.\u0016;\u001c/\n",
      "are the sample mean and variance\n",
      "NxDnX\n",
      "1xi=n andTDnX\n",
      "1.xi\u0000Nx/2=.n\u00001/; (13.64)\n",
      "having independent normal and gamma distributions,\n",
      "Nx\u0018N.\u0016;\u001c=n/ andT\u0018\u001cG\u0017=\u0017; (13.65)\n",
      "with\u0017Dn\u00001\n",
      "2, the latter being Gam .\u0017;\u001c=\u0017/ in the notation of Table 5.1.\n",
      "6The two methods are often referred to collectively as MCMC because of mathematical\n",
      "connections, with ‚ÄúMetropolis-Hasting algorithm‚Äù referring to the second type of\n",
      "procedure.13.4 Gibbs Sampling and MCMC 253\n",
      "For our Bayes prior distribution we take the conjugates\n",
      "\u001c\u0018k1\u001c1=Gk1C1and\u0016j\u001c\u0018N.\u00160;\u001c=n0/: (13.66)\n",
      "In terms of Table 13.1, .x0;n0\u0017/D.\u001c1;k1/for the gamma, while .x0;\u001b2\n",
      "1/D\n",
      ".\u00160;\u001c/for the normal. (A simple speciÔ¨Åcation would take \u0016\u0018N.\u00160;\u001c1=n0/.)\n",
      "Multiplying the normal and gamma functional forms in Table 5.1 yields\n",
      "density function\n",
      "f\u0016;\u001c.Nx;T/Dc\u001c\u0000.\u0017C1\n",
      "2/exp\u001a\n",
      "\u00001\n",
      "\u001ch\n",
      "\u0017TCn\n",
      "2.Nx\u0000\u0016/2i\u001b\n",
      "(13.67)\n",
      "and prior density\n",
      "g.\u0016;\u001c/Dc\u001c\u0000.k1C2:5/exp\u001a\n",
      "\u00001\n",
      "\u001ch\n",
      "k1\u001c1Cn0\n",
      "2.\u0016\u0000\u00160/2i\u001b\n",
      "; (13.68)\n",
      "cindicating positive constants that do not affect the posterior computations.\n",
      "The posterior density cg.\u0016;\u001c/f \u0016;\u001c.Nx;T/ is then calculated to be\n",
      "g.\u0016;\u001cjNx;T/Dc\u001c\u0000.\u0017Ck1C3/expf\u0000Q=\u001cg;\n",
      "whereQD.k1\u001c1CT/CnC\n",
      "2.\u0016\u0000N\u0016C/2Cn0n\n",
      "2nC.\u00160\u0000Nx/2:(13.69)\n",
      "HerenCDn0CnandN\u0016CD.n0\u00160CnNx/=nC.\n",
      "In order to make use of Gibbs sampling we need to know the full con-\n",
      "ditional distributionsg.\u0016j\u001c;Nx;T/ andg.\u001cj\u0016;Nx;T/ , as in (13.62). (In this\n",
      "case,kD2,\u00121D\u0016, and\u00122D\u001c.) This is where the conjugate expressions\n",
      "in Table 13.1 come into play. Inspection of density (13.69) shows that\n",
      "\u0016j\u001c;Nx;T\u0018N\u0012\n",
      "N\u0016C;\u001c\n",
      "nC\u0013\n",
      "and\u001cj\u0016;Nx;T\u0018Q\n",
      "G\u0017Ck1C2:(13.70)\n",
      "BD10;000 Gibbs samples \u0012.b/D.\u0016.b/;\u001c.b//were generated starting\n",
      "from\u0012.0/D.Nx;T/D.116;554/ . The prior speciÔ¨Åcations were chosen to\n",
      "be (presumably) uninformative or mildly informative,\n",
      "n0D1; \u00160DNx; k1D1or9:5; and\u001c1DT: (13.71)\n",
      "(In which caseN\u0016CD NxandQD.\u0017Ck1/TCnC.\u0016\u0000Nx/2. From\n",
      "\u0017D.n\u00001/=2 , we see thatk1corresponds to about 2k1hypothetical prior\n",
      "observations.) The resulting posterior distributions for \u001care shown by the\n",
      "histograms in Figure 13.5.\n",
      "As a point of frequentist comparison, BD10;000 parametric bootstrap\n",
      "replications (which involve no prior assumptions),\n",
      "O\u001c\u0003\u0018O\u001cG\u0017=\u0017;O\u001cDT; (13.72)254 Objective Bayes Inference and MCMC\n",
      " \n",
      "œÑFrequency\n",
      "200 400 600 800 1000 12000200400600800100012001400\n",
      "Tk1=1k1=9.5\n",
      "parametric\n",
      "bootstrap\n",
      "Figure 13.5 Posterior distributions for variance parameter \u001c,\n",
      "model (13.63)‚Äì(13.65), volume of air inspired for\n",
      "vasoconstriction group yD1from Table 13.2. Solid teal\n",
      "histogram:BD10;000 Gibbs samples with k1D1; black line\n",
      "histogram:BD10;000 samples with k1D9:5; red line\n",
      "histogram: 10,000 parametric bootstrap samples (13.72) suggests\n",
      "even thek1D1prior has substantial posterior effect.\n",
      "are seen to be noticeably more dispersed than even the k1D1Bayes\n",
      "posterior distribution, the likely choice for an objective Bayes analysis.\n",
      "Bayes techniques, even objective ones, have regularization effects that may\n",
      "or may not be appropriate.\n",
      "A similar, independent Gibbs sample of size 10,000 was obtained for the\n",
      "19yD0vasoconstriction measurements in Table 13.2, with speciÔ¨Åcations\n",
      "as in (13.71), kD1. Let\n",
      "ƒ±.b/D\u0016.b/\n",
      "1\u0000\u0016.b/\n",
      "0\u0010\n",
      "\u001c.b/\n",
      "1C\u001c.b/\n",
      "0\u00111=2; (13.73)\n",
      "where.\u0016.b/\n",
      "1;\u001c.b/\n",
      "1/and.\u0016.b/\n",
      "0;\u001c.b/\n",
      "0/denote thebth Gibbs samples from the\n",
      "yD1andyD0runs.\n",
      "Figure 13.6 shows the posterior distribution of ƒ±. Twenty-eight of the13.4 Gibbs Sampling and MCMC 255\n",
      " \n",
      "Œ¥Frequency\n",
      "0.0 0.5 1.0 1.50100200300400500600\n",
      "Pr(Œ¥<0)=0.0028\n",
      "Figure 13.6 BD10,000 Gibbs samples for ‚ÄúBayes t-statistic‚Äù\n",
      "(13.73) comparing yD1withyD0values for vasoconstriction\n",
      "data.\n",
      "BD10,000 values ƒ±.b/were less than 0, giving a ‚ÄúBayesian t-test‚Äù estimate\n",
      "Pfƒ±<0jNx1;Nx0;T1;T0gD0:0028: (13.74)\n",
      "(The usualt-test yielded one-sided p-value 0.0047 against the null hy-\n",
      "pothesis\u00160D\u00161.) An appealing feature of Gibbs sampling is that having\n",
      "obtained\u0012.1/;\u0012.2/;:::;\u0012.B/(13.59) the posterior distribution of any pa-\n",
      ".b/Dt.\u0012.b//.ined directly from the Bvalues\n",
      "Gibbs sampling requires the ability to sample from the full conditional\n",
      "distributions (13.61). A more general Markov chain Monte Carlo method,\n",
      "commonly referred to as MCMC, makes clearer the basic idea. Suppose the\n",
      "space of possible \u0012values is Ô¨Ånite, say f\u0012.1/;\u0012.2/;::: ,\u0012.M/g, and we\n",
      "wish to simulate samples from a posterior distribution putting probability\n",
      "p.i/ on\u0012.i/,\n",
      "pD.p.1/;p.2/;:::;p.M/ /: (13.75)\n",
      "The MCMC algorithm begins with the choice of a ‚Äúcandidate‚Äù proba-\n",
      "bility distribution q.i;j/ for moving from \u0012.i/to\u0012.j/; in theoryq.i;j/\n",
      "can be almost anything, for instance q.i;j/D1=.M\u00001/forj¬§i. The\n",
      "simulated samples \u0012.b/are obtained by a random walk: if \u0012.b/equals\u0012.i/,256 Objective Bayes Inference and MCMC\n",
      "then\u0012.bC1/equals\u0012.j/with probability7\n",
      "Q.i;j/Dq.i;j/\u0001min\u001ap.j/q.j;i/\n",
      "p.i/q.i;j/;1\u001b\n",
      "(13.76)\n",
      "forj¬§i, while with probability\n",
      "Q.i;i/D1\u0000X\n",
      "j¬§iQ.i;j/ (13.77)\n",
      "\u0012.bC1/D\u0012.b/D\u0012.i/. Markov chain theory then says that, under quite\n",
      "general conditions, the empirical distribution of the random walk values\n",
      "\u0012.b/will approach the desired distribution pasbgets large.\n",
      "A heuristic argument for why this happens begins by supposing that \u0012.1/\n",
      "was in fact generated by sampling from the target distribution p, Prf\u0012.1/D\n",
      "igDp.i/, and then\u0012.2/was obtained according to transition probabilities\n",
      "(13.76)‚Äì(13.77). A little algebra shows that (13.76) implies\n",
      "p.i/Q.i;j/Dp.j/Q.j;i/; (13.78)\n",
      "the so-called balance equations . This results in\n",
      "Prn\n",
      "\u0012.2/Dio\n",
      "Dp.i/Q.i;i/CX\n",
      "j¬§ip.j/Q.j;i/\n",
      "Dp.i/MX\n",
      "jD1Q.i;j/Dp.i/:(13.79)\n",
      "In other words, if \u0012.1/has distribution pthen so will\u0012.2/, and like-\n",
      "wise\u0012.3/;\u0012.4/;:::;pis the equilibrium distribution of the Markov chain\n",
      "random walk deÔ¨Åned by transition probabilities Q. Under reasonable con-\n",
      "ditions,¬é\u0012.b/must asymptotically attain distribution pno matter how \u0012.1/ ¬é9\n",
      "is initially selected.\n",
      "13.5 Example: Modeling Population Admixture\n",
      "MCMC has had a big impact in statistical genetics, where Bayesian mod-\n",
      "eling is popular and useful for representing the complex evolutionary pro-\n",
      "cesses. Here we illustrate its use in demography and modeling admixture ‚Äî\n",
      "estimating the contributions from ancestral populations in an individual\n",
      "7In Bayes applications, p.i/Dg.\u0012.i/jx/Dg.\u0012.i//f\u0012.i/.x/=f.x/(13.56).\n",
      "However,f.x/is not needed since it cancels out of (13.76), a considerable advantage\n",
      "in complicated situations when f.x/is often unavailable, and a prime reason for the\n",
      "popularity of MCMC.13.5 Example: Modeling Population Admixture 257\n",
      "genome. For example, we might consider human ancestry, and for each\n",
      "individual wish to estimate the proportion of their genome coming from\n",
      "European ,African , and Asian origins. The procedure we describe\n",
      "here is unsupervised‚Äîa type of soft clustering‚Äîbut we will see it can be\n",
      "very informative with regard to such questions. We have a sample of nin-\n",
      "dividuals, and we assume each arose from possible admixture among J\n",
      "parent populations, each with their own characteristic vector of allele fre-\n",
      "quencies. For us JD3, and letQi2S3denote a probability vector for in-\n",
      "dividualirepresenting the proportions of their heritage coming from pop-\n",
      "ulationsj2f1;2;3g(see Section 5.4). We have genomic measurements\n",
      "for each individual, in our case SNPs (single-nucleotide polymorphisms)\n",
      "at each ofMwell-spaced loci, and hence can assume they are in linkage\n",
      "equilibrium. At each SNP we have a measurement that identiÔ¨Åes the two\n",
      "alleles (one per chromosome), where each can be either the wild-type Aor\n",
      "the mutationa. That is, we have the genotype Gimat SNPmfor individual\n",
      "i: a three-level factor with levels fAA;Aa;aagwhich we code as 0;1;2 .\n",
      "Table 13.6 shows some examples.\n",
      "Table 13.6 A subset of the genotype data on 197 individuals, each with\n",
      "genotype measurements at 100 SNPs. In this case the ethnicity is\n",
      "known for each individual, one of Japanese ,African ,European ,\n",
      "orAfrican American . For example, individual NA12239 has\n",
      "genotype Aa for SNP1, NA19247 hasAA, and NA20126 hasaa.\n",
      "Subject SNP 1SNP2SNP3\u0001\u0001\u0001 SNP97 SNP98 SNP99 SNP100\n",
      "NA10852 1 1 0 \u0001\u0001\u0001 1 1 0 0\n",
      "NA12239 1 1 0 \u0001\u0001\u0001 1 1 0 0\n",
      "NA19072 0 0 0 \u0001\u0001\u0001 0 0 0 0\n",
      "NA19247 0 0 2 \u0001\u0001\u0001 0 0 0 2\n",
      "NA20126 2 0 0 \u0001\u0001\u0001 2 0 0 0\n",
      "NA18868 0 0 1 \u0001\u0001\u0001 0 0 0 1\n",
      "NA19257 0 0 0 \u0001\u0001\u0001 0 0 0 0\n",
      "NA19079 0 1 0 \u0001\u0001\u0001 0 1 0 0\n",
      "NA19067 0 0 0 \u0001\u0001\u0001 0 0 0 0\n",
      "NA19904 0 0 1 \u0001\u0001\u0001 0 0 0 1\n",
      "LetPjbe the (unknown) M-vector of minor allele frequencies (propor-\n",
      "tions actually) in population j. We have available a sample of nindivid-\n",
      "uals, and for each sample we have their genomic information measured at\n",
      "each of theMloci. Some of the individuals might appear to have pure an-\n",
      "cestral origins, but many do not. Our goal is to estimate Qi; iD1;:::;n;\n",
      "andPj; j2f1;2;3g.258 Objective Bayes Inference and MCMC\n",
      "For this purpose it is useful to pose a generative model. We Ô¨Årst create a\n",
      "pair of variables XimD.X.1/\n",
      "im;X.2/\n",
      "im/corresponding to each Gim, to which\n",
      "we allocate the two alleles (in arbitrary order). For example, if GimD1\n",
      "(corresponding to Aa), then we might set X.1/\n",
      "imD0andX.2/\n",
      "imD1(or\n",
      "vice versa). If GimD0they are both 0, and ifGimD2, they are both 1.\n",
      "LetZim2f1;2;3g2represent the ancestral origin for individual iof each\n",
      "of these allele copies Ximat locusm, again a two-vector with elements\n",
      "ZimD.Z.1/\n",
      "im; Z.2/\n",
      "im/. Then our generative model goes as follows.\n",
      "1Z.c/\n",
      "im\u0018Mult.1;Qi/, independently at each m, for each copy cD1;2.\n",
      "That is, we select the ancestral origin of each chromosome at locus m\n",
      "according to the individual‚Äôs mixture proportions Qi.\n",
      "2X.c/\n",
      "im\u0018Bi.1; Pjm/ifZ.c/\n",
      "imDj, for each copy cD1;2. What this\n",
      "means is that, for each of the two ancestral picks at locus m(one for\n",
      "each arm of the chromosome), we draw a binomial with the appropriate\n",
      "allele frequency.\n",
      "To complete the Bayesian speciÔ¨Åcation, we need to supply priors for the\n",
      "Qiand also forPjm. Although one can get fancy here, we resort to the\n",
      "recommended Ô¨Çat priors, which are\n",
      "\u000fQi\u0018D.\u0015;\u0015;\u0015/ , a Ô¨Çat three-component Dirichlet, independently for\n",
      "each subjecti¬éand ¬é10\n",
      "/ independently for each population j, and each locus m\n",
      "(the beta distribution; see ¬é10in the end notes).\n",
      "D1. In practice, these could values \u0015D\n",
      "get updated as well, but for the purposes of this demonstration we leave\n",
      "them Ô¨Åxed at these values.\n",
      "LetXbe then\u0002M\u00022array of observed alleles for all nsamples.\n",
      "We wish to estimate the posterior distribution Pr .P;QjX/, referring col-\n",
      "lectively to all the elements of PandQ.\n",
      "For this purpose we use Gibbs sampling, which amounts to the following\n",
      "sequence.\n",
      "0 InitializeZ.0/; P.0/;Q.0/.\n",
      "1 SampleZ.b/from the conditional distribution Pr .ZjX;P.b\u00001/;Q.b\u00001//.\n",
      "2 SampleP.b/;Q.b/from the conditional distribution Pr .P;QjX;Z.b//.\n",
      "Gibbs is effective when one can sample efÔ¨Åciently from these conditional\n",
      "distributions, which is the case here.13.5 Example: Modeling Population Admixture 259\n",
      "In step 2, we can sample PandQseparately. It can be seen that for each\n",
      ".j;m/ we should sample Pjmfrom\n",
      "PjmjX;Z\u0018D.\u0015Cn.0/\n",
      "jm; \u0015Cn.1/\n",
      "jm/; (13.80)\n",
      "whereZDZ.b/and\n",
      "n.0/\n",
      "jmD#f.i;c/WX.c/\n",
      "imD0andZ.c/\n",
      "imDjg;\n",
      "n.1/\n",
      "jmD#f.i;c/WX.c/\n",
      "imD1andZ.c/\n",
      "imDjg:(13.81)\n",
      "This follows from the conjugacy of the two-component Dirichlet (beta)\n",
      "with the binomial distribution, Table 13.1.\n",
      "UpdatingQiinvolves simulating from\n",
      "Cmi3/; (13.82)\n",
      "wheremijis the number of allele copies in individual ithat originated\n",
      "(according to ZDZ.b/) in population j:\n",
      "mijD#f.c;m/WZ.c/\n",
      "imDjg: (13.83)\n",
      "‚óè‚óè\n",
      "‚óè ‚óè‚óè\n",
      "‚óè ‚óè ‚óè‚óè ‚óè‚óè‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè ‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè‚óè ‚óè‚óè\n",
      "‚óè ‚óè ‚óè‚óè ‚óè‚óè‚óè\n",
      "‚óè‚óè ‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè ‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè ‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè ‚óè ‚óè‚óè\n",
      "‚óè\n",
      "‚óè ‚óè ‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè ‚óè ‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè ‚óè ‚óè‚óè ‚óè‚óè\n",
      "‚óè ‚óè ‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè\n",
      "‚óè ‚óè‚óè\n",
      "‚óè‚óè ‚óè‚óè\n",
      "‚óè‚óè ‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè ‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óèEuropean\n",
      "Japanese\n",
      "African\n",
      "African American\n",
      "Figure 13.7 Barycentric coordinate plot for the estimated\n",
      "posterior means of the Qibased on MCMC sampling.\n",
      "Step 1 can be performed by simulating Z.c/\n",
      "imindependently, for each i;m;260 Objective Bayes Inference and MCMC\n",
      "andcfrom\n",
      "Pr.Z.c/\n",
      "imDjjX;P;Q/DQijPr.X.c/\n",
      "imjP;Z.c/\n",
      "imDj/\n",
      "P3\n",
      "`D1Qi`Pr.X.c/\n",
      "imjP;Z.c/\n",
      "imD`/:(13.84)\n",
      "The probabilities on the right refer back to our generative distribution de-\n",
      "scribed earlier.\n",
      "Figure 13.7 shows a triangle plot that summarizes the result of running\n",
      "the MCMC algorithm on our 197 subjects. We used a burn in of 1000\n",
      "complete iterations, and then a further 2000 to estimate the distribution\n",
      "of the parameters of interest, in this case the Qi. Each dot in the Ô¨Ågure\n",
      "represents a three-component probability vector, and is the posterior mean\n",
      "of the sampled Qifor each subject. The points are colored according to\n",
      "the known ethnicity. Although this algorithm is unsupervised, we see that\n",
      "the ethnic groups cluster nicely in the corners of the simplex, and allow\n",
      "us to identify these clusters. The African American group is spread\n",
      "between the African andEuropean clusters (with a little movement\n",
      "toward the Japanese ).\n",
      "\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001\n",
      "Markov chain methods are versatile tools that have proved their value in\n",
      "Bayesian applications. There are some drawbacks.\n",
      "\u000fThe algorithms are not universal in the sense of maximum likelihood,\n",
      "requiring some individual ingenuity with each application.\n",
      "\u000fAs a result, applications, especially of Gibbs sampling, have favored\n",
      "a small set of convenient priors, mainly Jeffreys and conjugates, that\n",
      "simplify the calculations. This can cast doubt on the relevance of the\n",
      "resulting Bayes inferences.\n",
      "\u000fSuccessive realizations \u0012.b/are highly correlated with each other, mak-\n",
      "ing the convergence of estimates such as N\u0012DP\u0012.b/=Bslow.\n",
      "\u000fThe correlation makes it difÔ¨Åcult to assign a standard error to N\u0012. Actual\n",
      "applications ignore an initial B0of the\u0012.b/values (as a ‚Äúburn-in‚Äù period)\n",
      "and go on to large enough Bsuch that estimates like N\u0012appear to settle\n",
      "down. However, neither the choice of B0nor that ofBmay be clear.\n",
      "Objective Bayes offers a paradigm of our book‚Äôs theme, the effect of\n",
      "electronic computation on statistical inference: ingenious new algorithms\n",
      "facilitated Bayesian applications over a wide class of applied problems and,\n",
      "in doing so, inÔ¨Çuenced the dominant philosophy of the whole area.13.6 Notes and Details 261\n",
      "13.6 Notes and Details\n",
      "The books by Savage (1954) and de Finetti (1972), summarizing his ear-\n",
      "lier work, served as foundational texts for the subjective Bayesian school\n",
      "of inference. Highly inÔ¨Çuential, they championed a framework for Bayes-\n",
      "ian applications based on coherent behavior and the careful elucidation of\n",
      "personal probabilities. A current leading text on Bayesian methods, Carlin\n",
      "and Louis (2000), does not reference either Savage or de Finetti. Now Jef-\n",
      "freys (1961), again following earlier works, claims foundational status. The\n",
      "change of direction has not gone without protest from the subjectivists‚Äî\n",
      "see Adrian Smith‚Äôs discussion of O‚ÄôHagan (1995)‚Äîbut is nonetheless al-\n",
      "most a complete rout.\n",
      "Metropolis et al. (1953), as part of nuclear weapons research, devel-\n",
      "oped the Ô¨Årst MCMC algorithm. A vigorous line of work on Markov chain\n",
      "methods for solving difÔ¨Åcult probability problems has continued to Ô¨Çour-\n",
      "ish under such names as particle Ô¨Åltering and sequential Monte Carlo; see\n",
      "Gerber and Chopin (2015) and its enthusiastic discussion.\n",
      "Modeling population admixture (Pritchard et al. , 2000) is one of sev-\n",
      "eral applications of hierarchical Bayesian models and MCMC in genetics.\n",
      "Other applications include haplotype estimation and motif Ô¨Ånding, as well\n",
      "as estimation of phylogenetic trees. The examples in this section were de-\n",
      "veloped with the kind help of Hua Tang and David Golan, both from the\n",
      "Stanford Genetics department. Hua suggested the example and provided\n",
      "helpful guidance; David provided the data, and ran the MCMC algorithm\n",
      "using the STRUCTURE program in the Pritchard lab.\n",
      "¬é1[p. 236] Uninformative priors. A large catalog of possible uninformative\n",
      "priors has been proposed, thoroughly surveyed by Kass and Wasserman\n",
      "(1996). One approach is to use the likelihood from a small part of the data,\n",
      "say just one or two data points out of n, as the prior, as with the ‚Äúintrin-\n",
      "sic priors‚Äù of Berger and Pericchi (1996), or O‚ÄôHagan‚Äôs (1995) ‚Äúfractional\n",
      "Bayes factors.‚Äù Another approach is to minimize some mathematical mea-\n",
      "sure of prior information, as with Bernardo‚Äôs (1979) ‚Äúreference priors‚Äù or\n",
      "Jaynes‚Äô (1968) ‚Äúmaximum entropy‚Äù criterion. Kass and Wasserman list a\n",
      "dozen more possibilities.\n",
      "¬é2[p. 236] Coverage matching priors. Welch and Peers (1963) showed that,\n",
      "for a multiparameter family f\u0016.x/and real-valued parameter of interest\n",
      "\u0012Dt.\u0016/, there exist priors g.\u0016/ such that the Bayes credible interval of\n",
      "coverageÀõhas frequentist coverage ÀõCO.1=n/ , withnthe sample size. In\n",
      "other words, the credible intervals are ‚Äúsecond-order accurate‚Äù conÔ¨Ådence\n",
      "intervals. Tibshirani (1989), building on Stein‚Äôs (1985) work, produced the262 Objective Bayes Inference and MCMC\n",
      "nice formulation (13.9). Stein‚Äôs paper developed the least-favorable fam-\n",
      "ily, the one-parameter subfamily of f\u0016.x/that does not inappropriately in-\n",
      "crease the amount of Fisher information for estimating \u0012. Cox and Reid‚Äôs\n",
      "(1987) orthogonal parameters form (13.8) is formally equivalent to the\n",
      "least favorable family construction.\n",
      "Least favorable family versions of reference priors and intrinsic priors\n",
      "have been proposed to avoid the difÔ¨Åculty with general-purpose uninfor-\n",
      "mative priors seen in Figure 11.7. They do so, but at the price of requiring\n",
      "a different prior for each choice of \u0012Dt.\u0016/‚Äîwhich begins to sound more\n",
      "frequentistic than Bayesian.\n",
      "¬é3[p. 238] Conjugate families theorem. Theorem 13.1, (13.16)‚Äì(13.18), is\n",
      "rigorously derived in Diaconis and Ylvisaker (1979). Families other than\n",
      "(13.14) have conjugate-like properties, but not the neat posterior expecta-\n",
      "tion result (13.18).\n",
      "¬é4[p. 238] Poisson formula (13.20) .This follows immediately from (13.14),\n",
      "usingÀõDlog.\u0016/, .Àõ/D\u0016, andV.\u0016/D\u0016for the Poisson.\n",
      "¬é5[p. 239] Inverse gamma and chi-square distributions. AG\u0017variate (13.22)\n",
      "has density\u0016\u0017\u00001e\u0000\u0016=¬Ä.\u0017/ . An inverse gamma variate1=G\u0017has density\n",
      "\u0016\u0000.\u0017C1/e\u00001=\u0016=¬Ä.\u0017/ , so\n",
      "gn0;x0.\u0016/Dc\u0016\u0000.n0x0C2/e\u0000n0x0\u0017=\u0016(13.85)\n",
      "is the gamma conjugate density in Table 13.1. The gamma results can be\n",
      "restated in terms of chi-squared variates:\n",
      "xi\u0018\u0016\u001f2\n",
      "m\n",
      "mD\u0016Gm=2\n",
      "m=2(13.86)\n",
      "has conjugate prior\n",
      "gn0;x0.\u0016/\u0018n0x0m=\u001f2\n",
      "n0mC2; (13.87)\n",
      "aninverse chi-squared distribution.\n",
      "¬é6[p. 240] Vasoconstriction data. Efron and Gous (2001) use this data to illus-\n",
      "trate a theory connecting Bayes factors with Fisherian hypothesis testing.\n",
      "It is part of a larger data set appearing in Finney (1947), also discussed in\n",
      "Kass and Raftery (1995).\n",
      "¬é7[p. 245] Jeffreys‚Äô and Fisher‚Äôs scales of evidence. Jeffreys‚Äô scale as it ap-\n",
      "pears in Table 13.3 is taken from the slightly amended form in Kass and\n",
      "Raftery (1995). Efron and Gous (2001) compare it with Fisher‚Äôs scale for\n",
      "the contradictory results of Table 13.5. Fisher and Jeffreys worked in dif-\n",
      "ferent scientiÔ¨Åc contexts‚Äîsmall-sample agricultural experiments versus13.6 Notes and Details 263\n",
      "hard-science geostatistics‚Äîwhich might explain Jeffreys‚Äô more stringent\n",
      "conception of what constitutes signiÔ¨Åcant evidence.\n",
      "¬é8[p. 246] The Bayesian information criterion. The BIC was proposed by\n",
      "Schwarz (1978). Kass and Wasserman (1996) provide an extended discus-\n",
      "sion of the BIC and model selection. ‚ÄúProofs‚Äù of (13.37) ultimately depend\n",
      "on sample size coherency (13.49), as in Efron and Gous (2001). Quotation\n",
      "marks are used here to indicate the basically qualitative nature of BIC: if\n",
      "we think of the data points as being collected in pairs then nbecomesn=2\n",
      "in (13.38), etc., so it doesn‚Äôt pay to put too Ô¨Åne a point on the criterion.\n",
      "¬é9[p. 256] MCMC convergence. Suppose we begin the MCMC random walk\n",
      "(13.76)‚Äì(13.77) by choosing \u0012.1/according to some arbitrary starting dis-\n",
      "tributionp.1/. Letp.b/be the distribution of \u0012.b/, obtained after bsteps of\n",
      "the random walk. Markov chain theory says that, under certain broad con-\n",
      "ditions onQ.i;j/ ,p.b/will converge to the target distribution p(13.75).\n",
      "Moreover, the convergence is geometric in the L1normPjp.b/\n",
      "k\u0000pkj, suc-\n",
      "cessive discrepancies eventually decreasing by a multiplicative factor. A\n",
      "proof appears in Tanner and Wong (1987). Unfortunately, the factor won‚Äôt\n",
      "be known in most applications, and the actual convergence may be quite\n",
      "slow.\n",
      "¬é10[p. 258] Dirichlet distribution. The Dirichlet is a multivariate generaliza-\n",
      "tion of the beta distribution (Section 5.1), typically used to represent prior\n",
      "distributions for the multinomial distribution. For xD.x1;x2;:::;xk/0,\n",
      "withxj2.0;1/ ,P\n",
      "jxjD1, theD.\u0017/ density is deÔ¨Åned as\n",
      "f\u0017.x/D1\n",
      "B.\u0017/kY\n",
      "jD1x\u0017j\u00001\n",
      "j; (13.88)\n",
      "whereB.\u0017/DQ\n",
      "j¬Ä.\u0017j/=¬Ä.P\n",
      "j\u0017j/.14\n",
      "Statistical Inference and Methodology in the\n",
      "Postwar Era\n",
      "The fundamentals of statistical inference‚Äîfrequentist, Bayesian, Fisherian\n",
      "‚Äîwere set in place by the end of the Ô¨Årst half of the twentieth century, as\n",
      "discussed in Part I of this book. The postwar era witnessed a massive ex-\n",
      "pansion of statistical methodology, responding to the data-driven demands\n",
      "of modern scientiÔ¨Åc technology. We are now at the end of Part II, ‚ÄúEarly\n",
      "Computer-Age Methods,‚Äù having surveyed the march of new statistical\n",
      "algorithms and their inferential justiÔ¨Åcation from the 1950s through the\n",
      "1990s.\n",
      "This was a time of opportunity for the discipline of statistics, when the\n",
      "speed of computation increased by a factor of a thousand, and then another\n",
      "thousand. As we said before, a land bridge had opened to a new continent,\n",
      "but not everyone was eager to cross. We saw a mixed picture: the computer\n",
      "played a minor or negligible role in the development of some inÔ¨Çuential\n",
      "topics such as empirical Bayes, but was fundamental to others such as the\n",
      "bootstrap.\n",
      "Fifteen major topics were examined in Chapters 6 through 13. What fol-\n",
      "lows is a short scorecard of their inferential afÔ¨Ånities, Bayesian, frequentist,\n",
      "or Fisherian, as well as an assessment of the computer‚Äôs role in their devel-\n",
      "opment. None of this is very precise, but the overall picture, illustrated in\n",
      "Figure 14.1, is evocative.\n",
      "Empirical Bayes\n",
      "Robbins‚Äô original development of formula (6.5) was frequentistic, but most\n",
      "statistical researchers were frequentists in the postwar era so that could be\n",
      "expected. The obvious Bayesian component of empirical Bayes arguments\n",
      "is balanced by their frequentist emphasis on (nearly) unbiased estimation\n",
      "of Bayesian estimators, as well as the restriction to using only current data\n",
      "for inference. Electronic computation played hardly any role in the theory‚Äôs\n",
      "development (as indicated by blue coloring in the Ô¨Ågure). Of course mod-\n",
      "264Postwar Inference and Methodology 265\n",
      " \n",
      "‚óè ‚óè‚óè\n",
      "Bayesian FrequentistFisherian\n",
      "‚óèKaplan‚àíMeier‚óèlog‚àírank\n",
      "‚óèglm‚óèproportional hazards\n",
      "(partial likelihood)\n",
      "‚óèbootstrap\n",
      "‚óèempirical\n",
      "Bayes‚óèobjective\n",
      "Bayes\n",
      "(mcmc)\n",
      "‚óèjackknife‚óèCV‚óèJames‚àíStein‚óèregression trees\n",
      "‚óèridge\n",
      "regression‚óèBIC‚óèmissing data\n",
      "(EM)\n",
      "‚óè AIC‚àíCp\n",
      "Figure 14.1 Bayesian, frequentist, and Fisherian inÔ¨Çuences, as\n",
      "described in the text, on 15 major topics, 1950s through 1990s.\n",
      "Colors indicate the importance of electronic computation in their\n",
      "development: red, crucial; violet, very important; green,\n",
      "important; light blue, less important; blue, negligible.\n",
      "ern empirical Bayes applications are heavily computational, but that is the\n",
      "case for most methods now.\n",
      "James‚ÄìStein and Ridge Regression\n",
      "The frequentist roots of James‚ÄìStein estimation are more deÔ¨Ånitive, es-\n",
      "pecially given the force of the James‚ÄìStein theorem (7.16). Nevertheless,\n",
      "the empirical Bayes interpretation (7.13) lends James‚ÄìStein some Bayes-\n",
      "ian credibility. Electronic computation played no role in its development.\n",
      "This was less true for ridge regression, colored light blue in the Ô¨Ågure,\n",
      "where the matrix calculation (7.36) would have been daunting in the pre-\n",
      "electronic age. The Bayesian justiÔ¨Åcation (7.37)‚Äì(7.39) of ridge regression266 Postwar Inference and Methodology\n",
      "carries more weight than for James‚ÄìStein, given the absence of a strong\n",
      "frequentist theorem.\n",
      "Generalized Linear Models\n",
      "GLM development began with a pronounced Fisherian emphasis on like-\n",
      "lihood1modeling, but settled down to more or less standard frequentist\n",
      "regression theory. A key operational feature, low-dimensional sufÔ¨Åcient\n",
      "statistics, limited its computational demands, but GLM theory could not\n",
      "have developed before the age of electronic computers (as indicated by\n",
      "green coloring).\n",
      "Regression Trees\n",
      "Model building by means of regression trees is a computationally intensive\n",
      "enterprise, indicated by its red color in Figure 14.1. Its justiÔ¨Åcation has\n",
      "been mainly in terms of asymptotic frequentist properties.\n",
      "Survival Analysis\n",
      "The Kaplan‚ÄìMeier estimate, log-rank test, and proportional hazards model\n",
      "move from the frequentist pole of the diagram toward the Fisherian pole\n",
      "as the conditioning arguments in Sections 9.2 through 9.4 become more\n",
      "elaborate. The role of computation in their development increases in the\n",
      "same order. Kaplan‚ÄìMeier estimates can be done by hand (and were),\n",
      "while it is impossible to contemplate proportional hazards analysis with-\n",
      "out the computer. Partial likelihood, the enabling argument for the theory,\n",
      "is a quintessential Fisherian device.\n",
      "Missing Data and the EM Algorithm\n",
      "The imputation of missing data has a Bayesian Ô¨Çavor of indirect evidence ,\n",
      "but the ‚Äúfake data‚Äù principle (9.44)‚Äì(9.46) has Fisherian roots. Fast compu-\n",
      "tation was important to the method‚Äôs development, particularly so for the\n",
      "EM algorithm.\n",
      "Jackknife and Bootstrap\n",
      "The purpose of the jackknife was to calculate frequentist standard errors\n",
      "and biases. Electronic computation was of only minor importance in its\n",
      "1More explicitly, quasilikelihoods , an extension to a wider class of exponential family\n",
      "models.Postwar Inference and Methodology 267\n",
      "development. By contrast, the bootstrap is the archetype for computer-\n",
      "intensive statistical inference. It combines frequentism with Fisherian de-\n",
      "vices: plug-in estimation of accuracy estimates, as in (10.18)‚Äì(10.19), and\n",
      "correctness arguments for bootstrap conÔ¨Ådence intervals, (11.79)‚Äì(11.83).\n",
      "Cross-Validation\n",
      "The renaissance of interest in cross-validation required fast computation,\n",
      "especially for assessing modern computer-intensive prediction algorithms.\n",
      "As pointed out in the text following Figure (12.3), cross-validation is a\n",
      "strongly frequentist procedure.\n",
      "BIC, AIC, and Cp\n",
      "These three algorithms were designed to avoid computation, BIC for Bayes-\n",
      "ian model selection, Section (13.3), AIC and Cpfor unbiased estimation\n",
      "of frequentist prediction error, (12.76) and (12.50).\n",
      "Objective Bayes and MCMC\n",
      "In addition to their Bayesian provenance, objective Bayes methods have\n",
      "some connection with Ô¨Åducial ideas and the bootstrap, as discussed in Sec-\n",
      "tion 11.5. (An argument can be made that they are at least as frequentist as\n",
      "they are Bayesian‚Äîsee the notes below‚Äîthough that has not been acted\n",
      "upon in coloring the Ô¨Ågure.) Gibbs sampling and MCMC, the enabling al-\n",
      "gorithms, epitomize modern computer-intensive inference.\n",
      "Notes\n",
      "Figure 14.1 is an updated version of Figure 8 in Efron (1998), ‚ÄúR. A. Fisher\n",
      "in the 21st Century.‚Äù There the difÔ¨Åculty of properly placing objective\n",
      "Bayes is confessed, with Erich Lehmann arguing for a more frequentist\n",
      "(decision-theoretic) location: ‚ÄúIn fact, the concept of uninformative prior\n",
      "is philosophically close to Wald‚Äôs least favorable distribution, and the two\n",
      "often coincide.‚Äù\n",
      "Figure 14.1 shows a healthy mixture of philosophical and computational\n",
      "tactics at work, with all three edges (but not the center) of the triangle in\n",
      "play. All new points will be red (computer-intensive) as we move into the\n",
      "twenty-Ô¨Årst century in Part III. Our triangle will have to struggle to accom-\n",
      "modate some major developments based on machine learning, a philosoph-\n",
      "ically atheistic approach to statistical inference.Part III\n",
      "Twenty-First-Century Topics15\n",
      "Large-Scale Hypothesis Testing and\n",
      "False-Discovery Rates\n",
      "By the Ô¨Ånal decade of the twentieth century, electronic computation fully\n",
      "dominated statistical practice. Almost all applications, classical or other-\n",
      "wise, were now performed on a suite of computer platforms: SAS, SPSS,\n",
      "Minitab, Matlab, S (later R), and others.\n",
      "The trend accelerates when we enter the twenty-Ô¨Årst century, as statis-\n",
      "tical methodology struggles, most often successfully, to keep up with the\n",
      "vastly expanding pace of scientiÔ¨Åc data production. This has been a two-\n",
      "way game of pursuit, with statistical algorithms chasing ever larger data\n",
      "sets, while inferential analysis labors to rationalize the algorithms.\n",
      "Part III of our book concerns topics in twenty-Ô¨Årst-century1statistics.\n",
      "The word ‚Äútopics‚Äù is intended to signal selections made from a wide cat-\n",
      "alog of possibilities. Part II was able to review a large portion (though\n",
      "certainly not all) of the important developments during the postwar period.\n",
      "Now, deprived of the advantage of hindsight, our survey will be more illus-\n",
      "trative than deÔ¨Ånitive.\n",
      "For many statisticians, microarrays provided an introduction to large-\n",
      "scale data analysis. These were revolutionary biomedical devices that en-\n",
      "abled the assessment of individual activity for thousands of genes at once‚Äî\n",
      "and, in doing so, raised the need to carry out thousands of simultaneous\n",
      "hypothesis tests, done with the prospect of Ô¨Ånding only a few interesting\n",
      "genes among a haystack of null cases. This chapter concerns large-scale\n",
      "hypothesis testing and the false-discovery rate , the breakthrough in statis-\n",
      "tical inference it elicited.\n",
      "1Actually what historians might call ‚Äúthe long twenty-Ô¨Årst century‚Äù since we will begin\n",
      "in 1995.\n",
      "271272 Large-scale Hypothesis Testing and FDRs\n",
      "15.1 Large-Scale Testing\n",
      "Theprostate cancer data, Figure 3.4, came from a microarray study of\n",
      "nD102men, 52 prostate cancer patients and 50 normal controls. Each\n",
      "man‚Äôs gene expression levels were measured on a panel of ND6033\n",
      "genes, yielding a 6033\u0002102matrix of measurements xij,\n",
      "xijDactivity ofith gene forjth man: (15.1)\n",
      "For each gene, a two-sample tstatistic (2.17) tiwas computed com-\n",
      "paring genei‚Äôs expression levels for the 52 patients with those for the 50\n",
      "controls. Under the null hypothesis H0ithat the patients‚Äô and the controls‚Äô\n",
      "responses come from the same normal distribution of gene iexpression\n",
      "levels,tiwill follow a standard Student tdistribution with 100 degrees of\n",
      "freedom,t100. The transformation\n",
      "ziDÀÜ\u00001.F100.ti//; (15.2)\n",
      "whereF100is the cdf of a t100distribution and ÀÜ\u00001the inverse function of\n",
      "a standard normal cdf, makes zistandard normal under the null hypothesis:\n",
      "H0iWzi\u0018N.0;1/: (15.3)\n",
      "Of course the investigators were hoping to spot some non-null genes,\n",
      "ones for which the patients and controls respond differently. It can be\n",
      "shown that a reasonable model for both null and non-null genes is2¬é ¬é1\n",
      "zi\u0018N.\u0016i;1/; (15.4)\n",
      "\u0016ibeing the effect size for genei. Null genes have \u0016iD0, while the\n",
      "investigators hoped to Ô¨Ånd genes with large positive or negative \u0016ieffects.\n",
      "Figure 15.1 shows the histogram of the 6033 zivalues. The red curve is\n",
      "the scaled N.0;1/ density that would apply if in fact allof the genes were\n",
      "null, that is if all of the \u0016iequaled zero.3We can see that the curve is a\n",
      "little too high near the center and too low in the tails. Good! Even though\n",
      "most of the genes appear null, the discrepancies from the curve suggest that\n",
      "there are some non-null cases, the kind the investigators hoped to Ô¨Ånd.\n",
      "Large-scale testing refers exactly to this situation: having observed a\n",
      "large number Nof test statistics, how should we decide which if any of the\n",
      "null hypotheses to reject? Classical testing theory involved only a single\n",
      "case,ND1. A theory of multiple testing arose in the 1960s, ‚Äúmultiple‚Äù\n",
      "2This is model (3.28), with zinow replacing the notation xi.\n",
      "3It isce\u0000z2=2=p\n",
      "2\u0019withcchosen to make the area under the curve equal the area of\n",
      "the histogram.15.1 Large-Scale Testing 273\n",
      "‚àí4 ‚àí2 0 2 40 100 200 300 400 500\n",
      "z‚àívaluesCounts\n",
      "Figure 15.1 Histogram of ND6033z -values, one for each gene\n",
      "in the prostate cancer study. If all genes were null (15.3) the\n",
      "histogram would track the red curve. For which genes can we\n",
      "reject the null hypothesis?\n",
      "meaningNbetween 2 and perhaps 20. The microarray era produced data\n",
      "sets withNin the hundreds, thousands, and now even millions. This sounds\n",
      "like piling difÔ¨Åculty upon difÔ¨Åculty, but in fact there are some inferential\n",
      "advantages to the large- Nframework, as we will see.\n",
      "The most troubling fact about large-scale testing is how easy it is to be\n",
      "fooled. Running 100 separate hypothesis tests at signiÔ¨Åcance level 0.05\n",
      "will produce about Ô¨Åve ‚ÄúsigniÔ¨Åcant‚Äù results even if each case is actually\n",
      "null. The classical Bonferroni bound avoids this fallacy by strengthening\n",
      "the threshold of evidence required to declare an individual case signiÔ¨Åcant\n",
      "(i.e., non-null). For an overall signiÔ¨Åcance level Àõ, perhapsÀõD0:05, with\n",
      "Nsimultaneous tests, the Bonferroni bound rejects the ith null hypothesis\n",
      "H0ionly if it attains individual signiÔ¨Åcance level Àõ=N . ForÀõD0:05,\n",
      "ND6033 , andH0iWzi\u0018N.0;1/ , the one-sided Bonferroni threshold\n",
      "for signiÔ¨Åcance is\u0000ÀÜ\u00001.0:05=N/D4:31 (compared with 1.645 for ND\n",
      "1). Only four of the prostate study genes surpass this threshold.\n",
      "Classic hypothesis testing is usually phrased in terms of signiÔ¨Åcance lev-\n",
      "elsandp-values . If test statistic zhas cdfF0.z/under the null hypothesis274 Large-scale Hypothesis Testing and FDRs\n",
      "then4\n",
      "pD1\u0000F0.z/ (15.5)\n",
      "is the right-sided p-value, larger zgiving smaller p-value. ‚ÄúSigniÔ¨Åcance\n",
      "level‚Äù refers to a prechosen threshold value, e.g., ÀõD0:05. The null\n",
      "hypothesis is ‚Äúrejected at level Àõ‚Äù if we observe p\u0014Àõ. Table 13.4 on\n",
      "page 245 (where ‚Äúcoverage level‚Äù means one minus the signiÔ¨Åcance level)\n",
      "shows Fisher‚Äôs scale for interpreting p-values.\n",
      "A level-Àõtest for a single null hypothesis H0satisÔ¨Åes, by deÔ¨Ånition,\n",
      "ÀõDPrfreject trueH0g: (15.6)\n",
      "For a collection of Nnull hypotheses H0i, the family-wise error rate is the\n",
      "probability of making even one false rejection,\n",
      "FWERDPrfreject any true H0ig: (15.7)\n",
      "Bonferroni‚Äôs procedure controls FWER at level Àõ: letI0be the indices of\n",
      "thetrueH0i, having sayN0members. Then\n",
      "FWERDPr8\n",
      "<\n",
      ":[\n",
      "I0\u0010\n",
      "pi\u0014Àõ\n",
      "N\u00119\n",
      "=\n",
      ";\u0014X\n",
      "I0Prn\n",
      "pi\u0014Àõ\n",
      "No\n",
      "DN0Àõ\n",
      "N\u0014Àõ;(15.8)\n",
      "the top line following from Boole‚Äôs inequality (which doesn‚Äôt require even\n",
      "independence among the pi).\n",
      "The Bonferroni bound is quite conservative: for ND6033 andÀõD\n",
      "0:05 we reject only those cases having pi\u00148:3\u000110\u00006. One can do only a\n",
      "little better under the FWER constraint. ‚ÄúHolm‚Äôs procedure,‚Äù¬éwhich offers ¬é2\n",
      "modest improvement over Bonferroni, goes as follows.\n",
      "\u000fOrder the observed p-values from smallest to largest,\n",
      "p.1/\u0014p.2/\u0014p.3/\u0014:::\u0014p.i/\u0014:::\u0014p.N/; (15.9)\n",
      "withH0.i/denoting the corresponding null hypotheses.\n",
      "\u000fLeti0be the smallest index isuch that\n",
      "p.i/>Àõ=.N\u0000iC1/: (15.10)\n",
      "\u000fReject all null hypotheses H0.i/fori <i0andaccept all withi\u0015i0.\n",
      "4The left-sidedp-value ispDF0.z/. We will avoid two-sided p-values in this\n",
      "discussion.15.2 False-Discovery Rates 275\n",
      "It can be shown that Holm‚Äôs procedure controls FWER at level Àõ, while\n",
      "being slightly more generous than Bonferroni in declaring rejections.\n",
      "15.2 False-Discovery Rates\n",
      "The FWER criterion aims to control the probability of making even one\n",
      "false rejection among Nsimultaneous hypothesis tests. Originally devel-\n",
      "oped for small-scale testing, say N\u001420, FWER usually proved too con-\n",
      "servative for scientists working with Nin the thousands. A quite different\n",
      "and more liberal criterion, false-discovery rate (FDR) control, has become\n",
      "standard.\n",
      " ¬†Null Actual Non-Null Null N0 - a Non-Null a b N1 - b N - R R N N1 N0 Decision \n",
      "Figure 15.2 A decision rule Dhas rejectedRout ofNnull\n",
      "hypotheses;aof these decisions were incorrect, i.e., they were\n",
      "‚Äúfalse discoveries,‚Äù while bof them were ‚Äútrue discoveries.‚Äù The\n",
      "false-discovery proportion Fdp equals a=R.\n",
      "Figure 15.2 diagrams the outcome of a hypothetical decision rule Dap-\n",
      "plied to the data for Nsimultaneous hypothesis-testing problems, N0null\n",
      "andN1DN\u0000N0non-null. An omniscient oracle has reported the rule‚Äôs\n",
      "results:Rnull hypotheses have been rejected; aof these were cases of\n",
      "false discovery , i.e., valid null hypotheses, for a ‚Äúfalse-discovery propor-\n",
      "tion‚Äù (Fdp) of\n",
      "Fdp.D/Da=R: (15.11)\n",
      "(We deÔ¨Åne FdpD0ifRD0.) Fdp is unobservable‚Äîwithout the oracle\n",
      "we cannot see a‚Äîbut under certain assumptions we can control its expec-\n",
      "tation.276 Large-scale Hypothesis Testing and FDRs\n",
      "DeÔ¨Åne\n",
      "FDR.D/DEfFdp.D/g: (15.12)\n",
      "A decision rule Dcontrols FDR at level q, withqa prechosen value be-\n",
      "tween 0 and 1, if\n",
      "FDR.D/\u0014q: (15.13)\n",
      "It might seem difÔ¨Åcult to Ô¨Ånd such a rule, but in fact a quite simple but in-\n",
      "genious recipe does the job. Ordering the observed p-values from smallest\n",
      "to largest as in (15.9), deÔ¨Åne imaxto be the largest index for which\n",
      "p.i/\u0014i\n",
      "Nq; (15.14)\n",
      "and let Dqbe the rule5that rejectsH0.i/fori\u0014imax, accepting otherwise.\n",
      "A proof of the following theorem is referenced in the chapter endnotes.¬é ¬é3\n",
      "Theorem (Benjamini‚ÄìHochberg FDR Control) If thep-values correspond-\n",
      "ing to valid null hypotheses are independent of each other, then\n",
      "FDR.Dq/D\u00190q\u0014q; where\u00190DN0=N: (15.15)\n",
      "In other words Dqcontrols FDR at level \u00190q. The null proportion \u00190is\n",
      "unknown (though estimable), so the usual claim is that Dqcontrols FDR at\n",
      "levelq. Not much is sacriÔ¨Åced: large-scale testing problems are most often\n",
      "Ô¨Åshing expeditions in which most of the cases are null, putting \u00190near 1,\n",
      "identiÔ¨Åcation of a few non-null cases being the goal. The choice qD0:1\n",
      "is typical practice.\n",
      "The popularity of FDR control hinges on the fact that it is more generous\n",
      "than FWER in declaring signiÔ¨Åcance.6Holm‚Äôs procedure (15.10) rejects\n",
      "null hypothesis H0.i/if\n",
      "p.i/\u0014Threshold(Holm‚Äôs) DÀõ\n",
      "N\u0000iC1; (15.16)\n",
      "whileDq(15.13) has threshold\n",
      "p.i/\u0014Threshold( Dq)Dq\n",
      "Ni: (15.17)\n",
      "5Sometimes denoted ‚ÄúBH q‚Äù after its inventors Benjamini and Hochberg; see the chapter\n",
      "endnotes.\n",
      "6The classic term ‚ÄúsigniÔ¨Åcant‚Äù for a non-null identiÔ¨Åcation doesn‚Äôt seem quite right for\n",
      "FDR control, especially given the Bayesian connections of Section 15.3, and we will\n",
      "sometimes use ‚Äúinteresting‚Äù instead.15.2 False-Discovery Rates 277\n",
      "In the usual range of interest, large Nand smalli, the ratio\n",
      "Threshold( Dq)\n",
      "Threshold(Holm‚Äôs)Dq\n",
      "Àõ\u0012\n",
      "1\u0000i\u00001\n",
      "N\u0013\n",
      "i (15.18)\n",
      "increases almost linearly with i.\n",
      "**************************************************\n",
      "0 10 20 30 40 500e+00 5e‚àí04 1e‚àí03\n",
      "index ip‚àívalue\n",
      "Holm'sFDR\n",
      "i = 7i = 28\n",
      "Figure 15.3 Orderedp-valuesp.i/D1\u0000ÀÜ.z.i//plotted versus\n",
      "ifor the 50 largest z-values from the prostate data in\n",
      "Figure 15.1. The FDR control boundary (algorithm Dq,qD0:1)\n",
      "rejectsH0.i/for the 28 smallest values p.i/, while Holm‚Äôs FWER\n",
      "procedure (ÀõD0:1) rejects for only the 7 smallest values. (The\n",
      "upward slope of Holm‚Äôs boundary (15.16) is too small to see\n",
      "here.)\n",
      "Figure 15.3 illustrates the comparison for the right tail of the prostate\n",
      "data of Figure 15.1, with piD1\u0000ÀÜ.zi/(15.3), (15.5), and ÀõDqD\n",
      "0:1. The FDR procedure rejects H0.i/for the 28 largest z-values (z.i/\u0015\n",
      "3:33), while FWER control rejects only the 7 most extreme z-values (z.i/\u0015\n",
      "4:14).\n",
      "Hypothesis testing has been a traditional stronghold of frequentist deci-\n",
      "sion theory, with ‚ÄúType 1‚Äù error control being strictly enforced, very often\n",
      "at the 0.05 level. It is surprising that a new control criterion, FDR, has\n",
      "taken hold in large-scale testing situations. A critic, noting FDR‚Äôs relaxed\n",
      "rejection standards in Figure 15.3, might raise some pointed questions.278 Large-scale Hypothesis Testing and FDRs\n",
      "1 Is controlling a rate(i.e., FDR) as meaningful as controlling a probabil-\n",
      "ity(of Type 1 error)?\n",
      "2 How should qbe chosen?\n",
      "3 The control theorem depends on independence among the p-values.\n",
      "Isn‚Äôt this unlikely in situations such as the prostate study?\n",
      "4 The FDR signiÔ¨Åcance for gene i0, say one with zi0D3, depends on the\n",
      "results of all the other genes: the more ‚Äúother‚Äù zivalues exceed 3, the\n",
      "more interesting gene i0becomes (since that increases i0‚Äôs indexiin\n",
      "the ordered list (15.9), making it more likely that pi0lies below the Dq\n",
      "threshold (15.14)). Does this make inferential sense?\n",
      "A Bayes/empirical Bayes restatement of the Dqalgorithm helps answer\n",
      "these questions, as discussed next.\n",
      "15.3 Empirical Bayes Large-Scale Testing\n",
      "In practice, single-case hypothesis testing has been a frequentist preserve.\n",
      "Its methods demand little from the scientist‚Äîonly the choice of a test\n",
      "statistic and the calculation of its null distribution‚Äîwhile usually deliver-\n",
      "ing a clear verdict. By contrast, Bayesian model selection, whatever its in-\n",
      "ferential virtues, raises the kinds of difÔ¨Åcult modeling questions discussed\n",
      "in Section 13.3.\n",
      "It then comes as a pleasant surprise that things are different for large-\n",
      "scale testing: Bayesian methods, at least in their empirical Bayes manifes-\n",
      "tation, no longer demand heroic modeling efforts, and can help untangle\n",
      "the interpretation of simultaneous test results. This is particularly true for\n",
      "the FDR control algorithm Dqof the previous section.\n",
      "A simple Bayesian framework for simultaneous testing is provided by\n",
      "thetwo-groups model : each of the Ncases (the genes for the prostate\n",
      "study) is either null with prior probability \u00190or non-null with probabil-\n",
      "ity\u00191D1\u0000\u00190; the resulting observation zthen has density either f0.z/\n",
      "orf1.z/,\n",
      "\u00190DPrfnullgf0.z/density if null ;\n",
      "\u00191DPrfnon-nullgf1.z/density if non-null :(15.19)\n",
      "For the prostate study, \u00190is nearly 1, and f0.z/is the standard normal den-\n",
      "sity\u001e.z/Dexp.\u0000z2=2/=p\n",
      "2\u0019(15.3), while the non-null density remains\n",
      "to be estimated.\n",
      "LetF0.z/andF1.z/be the cdf values corresponding to f0.z/andf1.z/,15.3 Empirical Bayes Large-Scale Testing 279\n",
      "with ‚Äúsurvival curves‚Äù\n",
      "S0.z/D1\u0000F0.z/ andS1.z/D1\u0000F1.z/; (15.20)\n",
      "S0.z0/being the probability that a null z-value exceeds z0, and similarly\n",
      "forS1.z/. Finally, deÔ¨Åne S.z/ to be the mixture survival curve\n",
      "S.z/D\u00190S0.z/C\u00191S1.z/: (15.21)\n",
      "Themixture density\n",
      "f.z/D\u00190f0.z/C\u00191f1.z/ (15.22)\n",
      "determinesS.z/ ,\n",
      "S.z0/DZ1\n",
      "z0f.z/dz: (15.23)\n",
      "Suppose now that observation zifor caseiis seen to exceed some thresh-\n",
      "old valuez0, perhapsz0D3. Bayes‚Äô rule gives\n",
      "Fdr.z0/\u0011Prfcaseiis nulljzi\u0015z0g\n",
      "D\u00190S0.z0/=S.z0/;(15.24)\n",
      "the correspondence with (3.5) on page 23 being \u00190Dg.\u0016/ ,S0.z0/D\n",
      "f\u0016.x/, andS.z0/Df.x/ . Fdr is the ‚ÄúBayes false-discovery rate,‚Äù as con-\n",
      "trasted with the frequentist quantity FDR (15.12).\n",
      "In typical applications, S0.z0/is assumed known7(equaling1\u0000ÀÜ.z0/\n",
      "in the prostate study), and \u00190is assumed to be near 1. The denominator\n",
      "S.z0/in (15.24) is unknown, but‚Äîand this is the crucial point‚Äîit has an\n",
      "obvious estimate in large-scale testing situations, namely\n",
      "OS.z0/DN.z0/=N; whereN.z0/D#fzi\u0015z0g: (15.25)\n",
      "(By the deÔ¨Ånition of the two-group model, each zihas marginal density\n",
      "f.z/ , makingOS.z0/the usual empirical estimate of S.z0/(15.23).) Plug-\n",
      "ging into (15.24) yields an empirical Bayes estimate of the Bayes false-\n",
      "discovery rate\n",
      "cFdr.z0/D\u00190S0.z0/ƒ±OS.z0/: (15.26)\n",
      "The connection with FDR control is almost immediate. First of all, from\n",
      "deÔ¨Ånitions (15.5) and (15.20) we have piDS0.zi/; also for the ith from\n",
      "the largestz-value we haveOS.z.i//Di=N (15.25). Putting these together,\n",
      "condition (15.14), p.i/\u0014.i=N/q , becomes\n",
      "S0.z.i//\u0014OS.z.i//\u0001q; (15.27)\n",
      "7But see Section 15.5.280 Large-scale Hypothesis Testing and FDRs\n",
      "orS0.z.i//=OS.z.i//\u0014q, which can be written as\n",
      "cFdr.z.i//\u0014\u00190q (15.28)\n",
      "(15.26). In other words, the Dqalgorithm, which rejects those null hy-\n",
      "potheses having8p.i/\u0014.i=N/q , is in fact rejecting those cases for which\n",
      "the empirical Bayes posterior probability of nullness is too small, as de-\n",
      "Ô¨Åned by (15.28). The Bayesian nature of FDR control offers a clear advan-\n",
      "tage to the investigating scientist, who gets a numerical assessment of the\n",
      "probability that he or she will be wasting time following up any one of the\n",
      "selected cases.\n",
      "We can now respond to the four questions at the end of the previous\n",
      "section:\n",
      "1 FDR control does relate to a probability‚Äîthe Bayes posterior probabil-\n",
      "ity of nullness.\n",
      "2 The choice of qforDqamounts to setting the maximum tolerable amount\n",
      "of Bayes risk of nullness9(usually after taking \u00190D1in (15.28)).\n",
      "3 Most often the zi, and hence the pi, will be correlated with each other.\n",
      "Even under correlation, however, OS.z0/in (15.25) is still unbiased for\n",
      "S.z0/, makingcFdr.z0/(15.26) nearly unbiased for Fdr .z0/(15.24). There\n",
      "isa price to be paid for correlation, which increases the variance of\n",
      "S0.z0/andcFdr.z0/.\n",
      "4 In the Bayes two-groups model (15.19), all of the non-null ziare i.i.d.\n",
      "observations from the non-null density f1.z/, with survival curve S1.z/.\n",
      "The number of null cases ziexceeding some threshold z0hasÔ¨Åxed ex-\n",
      "pectationN\u00190S0.z0/. Therefore an increase in the number of observed\n",
      "valuesziexceedingz0must come from a heavier right tail for f1.z/,\n",
      "implying a greater posterior probability of non-nullness Fdr .z0/(15.24).\n",
      "This point is made more clearly in the local false-discovery framework\n",
      "of the next section. It emphasizes the ‚Äúlearning from the experience of\n",
      "others‚Äù aspect of empirical Bayes inference, Section 7.4. The question\n",
      "of ‚ÄúWhich others?‚Äù is returned to in Section 15.6.\n",
      "Figure 15.4 illustrates the two-group model (15.19). The Ncases are\n",
      "8The algorithm, as stated just before the FDR control theorem (15.15), is actually a little\n",
      "more liberal in allowing rejections.\n",
      "9For a case of particular interest, the calculation can be reversed: if the case has ordered\n",
      "indexithen, according to (15.14), the value qDNpi=iputs it exactly on the boundary\n",
      "of rejection, making this its q-value. The 50th largest z-value for the prostate data has\n",
      "ziD2:99 ,piD0:00139 , andq-value 0.168, that being both the frequentist\n",
      "boundary for rejection and the empirical Bayes probability of nullness.15.3 Empirical Bayes Large-Scale Testing 281\n",
      "Figure 15.4 A diagram of the two-groups model (15.19). Here\n",
      "the statistician observes values zifrom a mixture density\n",
      "f.z/D\u00190f0.z/C\u00191f1.z/and decides to reject or accept the\n",
      "null hypothesis H0idepending on whether ziexceeds or is less\n",
      "than the threshold value z0.\n",
      "randomly dispatched to the two arms in proportions \u00190and\u00191, at which\n",
      "point they produce z-values according to either f0.z/orf1.z/. Suppose\n",
      "we are using a simple decision rule Dthat rejects the ith null hypothesis if\n",
      "ziexceeds some threshold z0, and accepts otherwise,\n",
      "DW(\n",
      "RejectH0i ifzi>z0\n",
      "AcceptH0iifzi\u0014z0:(15.29)\n",
      "The oracle of Figure 15.2 knows that N0.z0/Daof the null case z-\n",
      "values exceeded z0, and similarly N1.z0/Dbof the non-null cases, lead-\n",
      "ing to\n",
      "N.z0/DN0.z0/CN1.z0/DR (15.30)\n",
      "total rejections. The false-discovery proportion (15.11) is\n",
      "FdpDN0.z0/\n",
      "N.z0/(15.31)\n",
      "but this is unobservable since we see only N.z0/.\n",
      "The clever inferential strategy of false-discovery rate theory substitutes\n",
      "theexpectation ofN0.z0/,\n",
      "EfN0.z0/gDN\u00190S0.z0/; (15.32)282 Large-scale Hypothesis Testing and FDRs\n",
      "forN0.z0/in (15.31), giving\n",
      "dFdpDN\u00190S0.z0/\n",
      "N.z0/D\u00190S0.z0/\n",
      "OS.z0/DcFdr.z0/; (15.33)\n",
      "using (15.25) and (15.26). Starting from the two-groups model, cFdr.z0/is\n",
      "an obvious empirical (i.e., frequentist) estimate of the Bayesian probability\n",
      "Fdr.z0/, as well as of Fdp.\n",
      "If placed in the Bayes‚ÄìFisher‚Äìfrequentist triangle of Figure 14.1, false-\n",
      "discovery rates would begin life near the frequentist corner but then mi-\n",
      "grate at least part of the way toward the Bayes corner. There are remark-\n",
      "able parallels with the James‚ÄìStein estimator of Chapter 7. Both theories\n",
      "began with a striking frequentist theorem, which was then inferentially\n",
      "rationalized in empirical Bayes terms. Both rely on the use of indirect\n",
      "evidence‚Äîlearning from the experience of others. The difference is that\n",
      "James‚ÄìStein estimation always aroused controversy, while FDR control\n",
      "has been quickly welcomed into the pantheon of widely used methods.\n",
      "This could reÔ¨Çect a change in twenty-Ô¨Årst-century attitudes or, perhaps,\n",
      "only that the Dqrule better conceals its Bayesian aspects.\n",
      "15.4 Local False-Discovery Rates\n",
      "Tail-area statistics ( p-values) were synonymous with classic one-at-a-time\n",
      "hypothesis testing, and the Dqalgorithm carried over p-value interpreta-\n",
      "tion to large-scale testing theory. But tail-area calculations are neither nec-\n",
      "essary nor desirable from a Bayesian viewpoint, where, having observed\n",
      "test statisticziequal to some value z0, we should be more interested in the\n",
      "probability of nullness given ziDz0than givenzi\u0015z0.\n",
      "To this end we deÔ¨Åne the local false-discovery rate\n",
      "fdr.z0/DPrfcaseiis nulljziDz0g (15.34)\n",
      "as opposed to the tail-area false-discovery rate Fdr .z0/(15.24). The main\n",
      "point of what follows is that reasonably accurate empirical Bayes estimates\n",
      "of fdr are available in large-scale testing problems.\n",
      "As a Ô¨Årst try, suppose that Z0, a proposed region for rejecting null hy-\n",
      "potheses, is a small interval centered at z0,\n",
      "Z0D\u0014\n",
      "z0\u0000d\n",
      "2;z0Cd\n",
      "2\u0015\n",
      "; (15.35)\n",
      "withdperhaps 0.1. We can redraw Figure 15.4, now with N0.Z0/,N1.Z0/,15.4 Local False-Discovery Rates 283\n",
      "andN.Z0/the null, non-null, and total number of z-values in Z0. The local\n",
      "false-discovery proportion,\n",
      "fdp.z0/DN0.Z0/=N.Z0/ (15.36)\n",
      "is unobservable, but we can replace N0.Z0/withN\u00190f0.z0/d, its approx-\n",
      "imate expectation as in (15.31)‚Äì(15.33), yielding the estimate10\n",
      "cfdr.z0/DN\u00190f0.z0/d=N.Z0/: (15.37)\n",
      "Estimate (15.37) would be needlessly noisy in practice; z-value distri-\n",
      "butions tend to be smooth, allowing the use of regression estimates for\n",
      "fdr.z0/. Bayes‚Äô theorem gives\n",
      "fdr.z/D\u00190f0.z/=f.z/ (15.38)\n",
      "in the two-groups model (15.19) (with \u0016in (3.5) now the indicator of null\n",
      "or non-null states, and xnowz). Drawing a smooth curve Of.z/ through\n",
      "the histogram of the z-values yields the more efÔ¨Åcient estimate\n",
      "cfdr.z0/D\u00190f0.z0/=Of.z0/I (15.39)\n",
      "the null proportion \u00190can be estimated‚Äîsee Section 15.5‚Äîor set equal to\n",
      "1.\n",
      "Figure 15.5 shows cfdr.z/for the prostate study data of Figure 15.1,\n",
      "whereOf.z/ in (15.39) has been estimated as described below. The curve\n",
      "hovers near 1 for the 93% of the cases having jzij\u00142, sensibly suggesting\n",
      "that there is no involvement with prostate cancer for most genes. It declines\n",
      "quickly forjzij\u00153, reaching the conventionally ‚Äúinteresting‚Äù threshold\n",
      "cfdr.z/\u00140:2 (15.40)\n",
      "forzi\u00153:34 andzi\u0014\u00003:40. This was attained for 27 genes in the right\n",
      "tail and 25 in the left, these being reasonable candidates to Ô¨Çag for follow-\n",
      "up investigation.\n",
      "The curveOf.z/ used in (15.39) was obtained from a fourth-degree log\n",
      "polynomial Poisson regression Ô¨Åt to the histogram in Figure 15.1, as in\n",
      "Figure 10.5 (10.52)‚Äì(10.56). Log polynomials of degree 2 through 6 were\n",
      "Ô¨Åt by maximum likelihood, giving total residual deviances (8.35) shown in\n",
      "Table 15.1. An enormous improvement in Ô¨Åt is seen in going from degree\n",
      "3 to 4, but nothing signiÔ¨Åcant after that, with decreases less than the null\n",
      "value 2 suggested by (12.75).\n",
      "10Equation (15.37) makes argument (4) of the previous section clearer: having more\n",
      "‚Äúother‚Äùz-values fall into Z0increasesN.Z0/, decreasingcfdr.z0/and making it more\n",
      "likely thatziDz0represents a non-null case.284 Large-scale Hypothesis Testing and FDRs\n",
      "‚àí4 ‚àí2 0 2 40.0 0.2 0.4 0.6 0.8 1.0\n",
      "z‚àívaluefdr and Fdrlocal fdr\n",
      "‚àí3.40 3.34\n",
      "Figure 15.5 Local false-discovery rate estimate cfdr.z/(15.39)\n",
      "for prostate study of Figure 15.1; 27 genes on the right and 25 on\n",
      "the left, indicated by dashes, have cfdr.zi/\u00140:2; light dashed\n",
      "curves are the left and right tail-area estimates cFdr.z/(15.26).\n",
      "Table 15.1 Total residual deviances from log polynomial Poisson\n",
      "regressions of the prostate data, for polynomial degrees 2 through 6;\n",
      "degree 4 is preferred.\n",
      "Degree 2 3 4 5 6\n",
      "Deviance 138.6 137.0 65.1 64.1 63.7\n",
      "The points in Figure 15.6 represent the log bin counts from the histogram\n",
      "in Figure 15.1 (excluding zero counts), with the solid curve showing the\n",
      "4th-degree MLE polynomial Ô¨Åt. Also shown is the standard normal log\n",
      "density\n",
      "logf0.z/D\u00001\n",
      "2z2Cconstant: (15.41)\n",
      "It Ô¨Åts reasonably well for jzj<2, emphasizing the null status of the gene\n",
      "majority.\n",
      "The cutoffcfdr.z/\u00140:2for declaring a case interesting is not completely\n",
      "arbitrary. DeÔ¨Ånitions (15.38) and (15.22), and a little algebra, show that it15.4 Local False-Discovery Rates 285\n",
      "‚àí4 ‚àí2 0 2 4 60123456\n",
      "z‚àívaluelog density\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè4th degree log\n",
      "polynomial\n",
      "N(0,1)\n",
      "Figure 15.6 Points are log bin counts for Figure 15.1‚Äôs\n",
      "histogram. The solid black curve is a fourth-degree\n",
      "log-polynomial Ô¨Åt used to calculate cfdr.z/in Figure 15.5. The\n",
      "dashed red curve, the log null density (15.41), provides a\n",
      "reasonable Ô¨Åt forjzj\u00142.\n",
      "is equivalent to\n",
      "f1.z/\n",
      "f0.z/\u00154\u00190\n",
      "\u00191: (15.42)\n",
      "If we assume \u00190\u00150:90, as is reasonable in most large-scale testing situa-\n",
      "tions, this makes the Bayes factor f1.z/=f0.z/quite large,\n",
      "f1.z/\n",
      "f0.z/\u001536; (15.43)\n",
      "‚Äústrong evidence‚Äù against the null hypothesis in Jeffreys‚Äô scale, Table 13.3.\n",
      "There is a simple relation between the local and tail-area false-discovery\n",
      "rates:¬é ¬é4\n",
      "Fdr.z0/DEffdr.z/jz\u0015z0gI (15.44)\n",
      "so Fdr.z0/is the average value of fdr .z/forzgreater thanz0. In interesting\n",
      "situations, fdr .z/will be a decreasing function for large values of z, as on\n",
      "the right side of Figure 15.5, making Fdr .z0/ < fdr.z0/. This accounts286 Large-scale Hypothesis Testing and FDRs\n",
      "for the conventional signiÔ¨Åcance cutoff cFdr.z/\u00140:1being smaller than\n",
      "cfdr.z/\u00140:2(15.40).¬é ¬é5\n",
      "The Bayesian interpretation of local false-discovery rates carries with it\n",
      "the advantages of Bayesian coherency. We don‚Äôt have to change deÔ¨Ånitions\n",
      "as with left-sided and right-sided tail-area cFdr estimates, since cfdr.z/ap-\n",
      "plies without change to both tails.11Also, we don‚Äôt need a separate theory\n",
      "for ‚Äútrue-discovery rates,‚Äù since\n",
      "tdr.z0/\u00111\u0000fdr.z0/D\u00191f1.z0/=f.z0/ (15.45)\n",
      "is the conditional probability that case iisnon-null givenziDz0.\n",
      "15.5 Choice of the Null Distribution\n",
      "The null distribution, f0.z/in the two-groups model (15.19), plays a cru-\n",
      "cial role in large-scale testing, just as it does in the classic single-case the-\n",
      "ory. Something different however happens in large-scale problems: with\n",
      "thousands of z-values to examine at once, it can become clear that the con-\n",
      "ventional theoretical null is inappropriate for the situation at hand. Put more\n",
      "positively, large-scale applications may allow us to empirically determine\n",
      "a more realistic null distribution.\n",
      "Thepolice data of Figure 15.7 illustrates what can happen. Possi-\n",
      "ble racial bias in pedestrian stops was assessed for ND2749 New York\n",
      "City police ofÔ¨Åcers in 2006. Each ofÔ¨Åcer was assigned a score zi, large\n",
      "positive scores suggesting racial bias. The zivalues were summary scores\n",
      "from a complicated logistic regression model intended to compensate for\n",
      "differences in the time of day, location, and context of the stops. Logistic\n",
      "regression theory suggested the theoretical null distribution\n",
      "H0iWzi\u0018N.0;1/ (15.46)\n",
      "for the absence of racial bias.\n",
      "The trouble is that the center of the z-value histogram in Figure 15.7,\n",
      "which should track the N.0;1/ curve applying to the presumably large\n",
      "fraction of null-case ofÔ¨Åcers, is much too wide. (Unlike the situation for the\n",
      "prostate data in Figure 15.1.) An MLE Ô¨Åtting algorithm discussed below\n",
      "produced the empirical null\n",
      "H0iWzi\u0018N.0:10;1:402/ (15.47)\n",
      "11Going further,zin the two-groups model could be multidimensional. Then tail-area\n",
      "false-discovery rates would be unavailable, but (15.38) would still legitimately deÔ¨Åne\n",
      "fdr.z/.15.5 Choice of the Null Distribution 287\n",
      "z‚àívaluesFrequency\n",
      "‚àí6 ‚àí4 ‚àí2 0 2 4 60 50 100 150 200\n",
      "N(0,1)\n",
      "N(0.1,1.402)\n",
      "Figure 15.7 Police data; histogram of zscores forND2749\n",
      "New York City police ofÔ¨Åcers, with large zisuggesting racial\n",
      "bias. The center of the histogram is too wide compared with the\n",
      "theoretical null distribution zi\u0018N.0;1/ . An MLE Ô¨Åt to central\n",
      "data gave N.0:10;1:402/as empirical null.\n",
      "as appropriate here. This is reinforced by a QQ plot of the zivalues shown\n",
      "in Figure 15.8, where we see most of the cases falling nicely along a\n",
      "N.0:09;1:422/line, with just a few outliers at both extremes.\n",
      "There is a lot at stake here. Based on the empirical null (15.47) only\n",
      "four ofÔ¨Åcers reached the ‚Äúprobably racially biased‚Äù cutoff cfdr.zi/\u00140:2,\n",
      "the four circled points at the far right of Figure 15.8; the Ô¨Åfth point had\n",
      "cfdrD0:38 while all the others exceeded 0.80. The theoretical N.0;1/\n",
      "null was much more severe, assigning cfdr\u00140:2to the 125 ofÔ¨Åcers having\n",
      "zi\u00152:50. One can imagine the difference in newspaper headlines.\n",
      "From a classical point of view it seems heretical to question the theo-\n",
      "retical null distribution, especially since there is no substitute available in\n",
      "single-case testing. Once alerted by data sets like the police study, however,\n",
      "it is easy to list reasons for doubt:\n",
      "\u000fAsymptotics Taylor series approximations go into theoretical null calcu-\n",
      "lations such as (15.46), which can lead to inaccuracies, particularly in the\n",
      "crucial tails of the null distribution.\n",
      "\u000fCorrelations False-discovery rate methods are correct on the average ,288 Large-scale Hypothesis Testing and FDRs\n",
      "*************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************\n",
      "‚àí3 ‚àí2 ‚àí1 0 1 2 3‚àí10 ‚àí5 0 5\n",
      "Normal QuantilesSample Quantiles\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "intercept = 0.089\n",
      "slope = 1.424\n",
      "Figure 15.8 QQ plot of police data zscores; most scores closely\n",
      "follow the N.0:09;1:422/line with a few outliers at either end.\n",
      "The circled points are cases having local false-discovery estimate\n",
      "cfdr.zi/\u00140:2, based on the empirical null. Using the theoretical\n",
      "N.0;1/ null gives 216 cases with cfdr.zi/\u00140:2, 91 on the left and\n",
      "125 on the right.\n",
      "even with correlations among the Nz-values. However, severe correlation\n",
      "destabilizes the z-value histogram, which can become randomly wider or\n",
      "narrower than theoretically predicted, undermining theoretical null results\n",
      "for the data set at hand.¬é ¬é6\n",
      "\u000fUnobserved covariates The police study was observational : individual\n",
      "encounters were not assigned at random to the various ofÔ¨Åcers but simply\n",
      "observed as they happened. Observed covariates such as the time of day\n",
      "and the neighborhood were included in the logistic regression model, but\n",
      "one can never rule out the possibility of inÔ¨Çuential unobserved covariates.\n",
      "\u000fEffect size considerations The hypothesis-testing setup, where a large\n",
      "fraction of the cases are truly null, may not be appropriate. An effect\n",
      "sizemodel, with\u0016i\u0018g.\u0001/andzi\u0018N.\u0016i;1/, might apply, with the\n",
      "priorg.\u0016/ nothaving an atom at \u0016D0. The nonatomic choice g.\u0016/\u0018\n",
      "N.0:10;0:632/provides a good Ô¨Åt to the QQ plot in Figure 15.8.15.5 Choice of the Null Distribution 289\n",
      "Empirical Null Estimation\n",
      "Our point of view here is that the theoretical null (15.46), zi\u0018N.0;1/ , is\n",
      "not completely wrong but needs adjustment for the data set at hand. To this\n",
      "end we assume the two-groups model (15.19), with f0.z/normal but not\n",
      "necessarily N.0;1/ , say\n",
      "f0.z/\u0018N.ƒ±0;\u001b2\n",
      "0/: (15.48)\n",
      "In order to compute the local false-discovery rate fdr .z/D\u00190f0.z/=f.z/\n",
      "we want to estimate the three numerator parameters .ƒ±0;\u001b0;\u00190/, the mean\n",
      "and standard deviation of the null density and the proportion of null cases.\n",
      "(The denominator f.z/ is estimated as in Section 15.4.)\n",
      "Our key assumptions (besides (15.48)) are that \u00190is large, say \u00190\u0015\n",
      "0:90, and that most of the zinear 0 are null cases. The algorithm locfdr\n",
      "¬ébegins by selecting a set A0nearzD0in which it is assumed that allthe¬é7\n",
      "ziinA0are null; in terms of the two-groups model, the assumption can be\n",
      "stated as\n",
      "f1.z/D0forz2A0: (15.49)\n",
      "Modest violations of (15.49), which are to be expected, produce small bi-\n",
      "ases in the empirical null estimates. Maximum likelihood based on the\n",
      "number and values of the ziobserved in A0yield the empirical null es-\n",
      "timates¬é.Oƒ±0;O\u001b0;O\u00190/. ¬é8\n",
      "Applied to the police data, locfdr choseA0D≈í\u00001:8;2:0¬ç and pro-\n",
      "duced estimates\n",
      "\u0010\n",
      "Oƒ±0;O\u001b0;O\u00190\u0011\n",
      "D.0:10;1:40;0:989/: (15.50)\n",
      "Two small simulation studies described in Table 15.2 give some idea of the\n",
      "variabilities and biases inherent in the locfdr estimation process.\n",
      "The third method, somewhere between the theoretical and empirical null\n",
      "estimates but closer to the former, relies on permutations. The vector zof\n",
      "6033z-values for the prostate data of Figure 15.1 was obtained from a\n",
      "study of 102 men, 52 cancer patients and 50 controls. Randomly permuting\n",
      "the men‚Äôs data, that is randomly choosing 50 of the 102 to be ‚Äúcontrols‚Äù and\n",
      "the remaining 52 to be ‚Äúpatients,‚Äù and then carrying through steps (15.1)‚Äì\n",
      "(15.2) gives a vector z\u0003in which any actual cancer/control differences have\n",
      "been suppressed. A histogram of the z\u0003\n",
      "ivalues (perhaps combining sev-\n",
      "eral permutations) provides the ‚Äúpermutation null.‚Äù Here we are extending\n",
      "Fisher‚Äôs original permutation idea, Section 4.4, to large-scale testing.\n",
      "Ten permutations of the prostate study data produced an almost perfect290 Large-scale Hypothesis Testing and FDRs\n",
      "Table 15.2 Means and standard deviations of .Oƒ±0;O\u001b0;O\u00190/for two\n",
      "simulation studies of empirical null estimation using locfdr .ND5000\n",
      "cases each trial with .ƒ±0;\u001b0;\u00190/as shown; 250 trials; two-groups model\n",
      "(15.19) with non-null density f1.z/equal to N.3;1/ (left side) or\n",
      "N.4:2;1/ (right side).\n",
      "ƒ±0\u001b0\u00190ƒ±0\u001b0\u00190\n",
      "true 0 1.0 .95 .10 1.40 .95\n",
      "mean .015 1.017 .962 .114 1.418 .958\n",
      "st dev .019 .017 .005 .025 .029 .006\n",
      "N.0;1/ permutation null. (This is as expected from the classic theory of\n",
      "permutationt-tests.) Permutation methods reliably overcome objection 1\n",
      "to the theoretical null distribution, over-reliance on asymptotic approxima-\n",
      "tions, but cannot cure objections 2, 3, and 4.¬é ¬é9\n",
      "Whatever the cause of disparity, the operational difference between the\n",
      "theoretical and empirical null distribution is clear: with the latter, the sig-\n",
      "niÔ¨Åcance of an outlying case is judged relative to the dispersion of the\n",
      "majority, not by a theoretical yardstick as with the former. This was per-\n",
      "suasive for the police data, but the story isn‚Äôt one-sided. Estimating the null\n",
      "distribution adds substantially to the variability of cfdr orcFdr. For situations\n",
      "such as the prostate data, when the theoretical null looks nearly correct,12\n",
      "it is reasonable to stick with it.\n",
      "The very large data sets of twenty-Ô¨Årst-century applications encourage\n",
      "self-contained methodology that proceeds from just the data at hand using\n",
      "a minimum of theoretical constructs. False-discovery rate empirical Bayes\n",
      "analysis of large-scale testing problems, with data-based estimation of O\u00190,\n",
      "Of0, andOf, comes close to the ideal in this sense.\n",
      "15.6 Relevance\n",
      "False-discovery rates return us to the purview of indirect evidence , Sec-\n",
      "tions 6.4 and 7.4. Our interest in any one gene in the prostate cancer study\n",
      "depends on its own zscore of course, but also on the other genes‚Äô scores‚Äî\n",
      "‚Äúlearning from the experience of others,‚Äù in the language used before.\n",
      "The crucial question we have been avoiding is ‚ÄúWhich others?‚Äù Our tacit\n",
      "answer has been ‚ÄúAll the cases that arrive in the same data set,‚Äù all the genes\n",
      "12Thelocfdr algorithm gave .Oƒ±0;O\u001b0;O\u00190/D.0:00;1:06;0:984/ for the prostate data.15.6 Relevance 291\n",
      "in the prostate study, all the ofÔ¨Åcers in the police study. Why this can be a\n",
      "dangerous tactic is shown in our Ô¨Ånal example.\n",
      "ADTI (diffusion tensor imaging) study compared six dyslexic children\n",
      "with six normal controls. Each DTI scan recorded Ô¨Çuid Ô¨Çows at ND15,443\n",
      "‚Äúvoxels,‚Äù i.e., at 15,443 three-dimensional brain coordinates. A score zi\n",
      "comparing dyslexics with normal controls was calculated for each voxel i,\n",
      "calibrated such that the theoretical null distribution of ‚Äúno difference‚Äù was\n",
      "H0iWzi\u0018N.0;1/ (15.51)\n",
      "as at (15.3).\n",
      " \n",
      "z‚àíscoreFrequency\n",
      "‚àí4 ‚àí2 0 2 40 200 400 600 800 1000\n",
      "3.17\n",
      "Figure 15.9 Histogram of zscores for the DTI study, comparing\n",
      "dyslexic versus normal control children at 15,443 brain locations.\n",
      "A FDR analysis based on the empirical null distribution gave 149\n",
      "voxels withcfdr.zi/\u00140:20, those having zi\u00153:17 (indicated by\n",
      "red dashes).\n",
      "Figure 15.9 shows the histogram of all 15,443 zivalues, normal-looking\n",
      "near the center and with a heavy right tail; locfdr gave empirical null\n",
      "parameters\n",
      "\u0010\n",
      "Oƒ±0;O\u001b0;O\u00190\u0011\n",
      "D.\u00000:12;1:06;0:984/; (15.52)\n",
      "the 149 voxels with zi\u00153:17 havingcfdr values\u00140:20. Using the the-292 Large-scale Hypothesis Testing and FDRs\n",
      "oretical null (15.51) yielded only modestly different results, now the 177\n",
      "voxels withzi\u00153:07 havingcfdri\u00140:20.\n",
      "20 40 60 80‚àí2 0 2 4\n",
      "Distance xZ scores\n",
      "16%ile84%ile\n",
      "median****\n",
      "***\n",
      "*****\n",
      "******\n",
      "***\n",
      "**\n",
      "***\n",
      "****\n",
      "***\n",
      "****\n",
      "** *\n",
      "********\n",
      "***\n",
      "***\n",
      "***\n",
      "***\n",
      "*****\n",
      "****\n",
      "*******\n",
      "**\n",
      "**\n",
      "******\n",
      "***********\n",
      "***\n",
      "*****\n",
      "*****\n",
      "***\n",
      "****\n",
      "****\n",
      "****\n",
      "*****\n",
      "***\n",
      "****\n",
      "*\n",
      "***\n",
      "*\n",
      "*** ***\n",
      "*\n",
      "Figure 15.10 A plot of 15,443 ziscores from a DTI study\n",
      "(vertical axis) and voxel distances xifrom the back of the brain\n",
      "(horizontal axis). The starred points are the 149 voxels with\n",
      "cfdr.zi/\u00140:20, which occur mostly for xiin the interval ≈í50;70¬ç .\n",
      "In Figure 15.10 the voxel scores zi, graphed vertically, are plotted ver-\n",
      "susxi, the voxel‚Äôs distance from the back of the brain. Waves of differing\n",
      "response are apparent. Larger values occur in the interval 50\u0014x\u001470,\n",
      "where the entire z-value distribution‚Äîlow, medium, and high‚Äîis pushed\n",
      "up. Most of the 149 voxels having cfdri\u00140:20 occur at the top of this wave.\n",
      "Figure 15.10 raises the problem of fair comparison. Perhaps the 4,653\n",
      "voxels withxibetween 50 and 70 should be compared only with each other,\n",
      "and not with all 15,443 cases. Doing so gave\n",
      "\u0010\n",
      "Oƒ±0;O\u001b0;O\u00190\u0011\n",
      "D.0:23;1:18;0:970/; (15.53)\n",
      "only 66 voxels having cfdri\u00140:20, those withzi\u00153:57.\n",
      "All of this is a question of relevance : which other voxels iare relevant\n",
      "to the assessment of signiÔ¨Åcance for voxel i0? One might argue that this is\n",
      "a question for the scientist who gathers the data and not for the statistical\n",
      "analyst, but that is unlikely to be a fruitful avenue, at least not without15.6 Relevance 293\n",
      "a lot of back-and-forth collaboration. Standard Bayesian analysis solves\n",
      "the problem by dictate: the assertion of a prior is also an assertion of its\n",
      "relevance. Empirical Bayes situations expose the dangers lurking in such\n",
      "assertions.\n",
      "Relevance was touched upon in Section 7.4, where the limited transla-\n",
      "tion rule (7.47) was designed to protect extreme cases from being shrunk\n",
      "too far toward the bulk of ordinary ones. One could imagine having a ‚Äúrel-\n",
      "evance function‚Äù \u001a.xi;zi/that, given the covariate information xiand re-\n",
      "sponsezifor casei, somehow adjusts an ensemble false-discovery rate es-\n",
      "timate to correctly apply to the case of interest‚Äîbut such a theory barely\n",
      "exists.¬é ¬é10\n",
      "Summary\n",
      "Large-scale testing, particularly in its false-discovery rate implementation,\n",
      "is not at all the same thing as the classic Fisher‚ÄìNeyman‚ÄìPearson theory:\n",
      "\u000fFrequentist single-case hypothesis testing depends on the theoretical\n",
      "long-run behavior of samples from the theoretical null distribution. With\n",
      "data available from say ND5000 simultaneous tests, the statistician\n",
      "has his or her own ‚Äúlong run‚Äù in hand, diminishing the importance of\n",
      "theoretical modeling. In particular, the data may cast doubt on the the-\n",
      "oretical null, providing a more appropriate empirical null distribution in\n",
      "its place.\n",
      "\u000fClassic testing theory is purely frequentist, whereas false-discovery rates\n",
      "combine frequentist and Bayesian thinking.\n",
      "\u000fIn classic testing, the attained signiÔ¨Åcance level for case idepends only\n",
      "on its own score zi, whilecfdr.zi/orcFdr.zi/also depends on the ob-\n",
      "servedz-values for other cases.\n",
      "\u000fApplications of single-test theory usually hope for rejection of the null\n",
      "hypothesis, a familiar prescription being 0.80 power at size 0.05. The\n",
      "opposite is true for large-scale testing, where the usual goal is to ac-\n",
      "cept most of the null hypotheses, leaving just a few interesting cases for\n",
      "further study.\n",
      "\u000fSharp null hypotheses such as \u0016D0are less important in large-scale\n",
      "applications, where the statistician is happy to accept a hefty proportion\n",
      "of uninterestingly small, but nonzero, effect sizes \u0016i.\n",
      "\u000fFalse-discovery rate hypothesis testing involves a substantial amount of\n",
      "estimation, blurring the line beteen the two main branches of statistical\n",
      "inference.294 Large-scale Hypothesis Testing and FDRs\n",
      "15.7 Notes and Details\n",
      "The story of false-discovery rates illustrates how developments in scien-\n",
      "tiÔ¨Åc technology (microarrays in this case) can inÔ¨Çuence the progress of\n",
      "statistical inference. A substantial theory of simultaneous inference was\n",
      "developed between 1955 and 1995, mainly aimed at the frequentist control\n",
      "of family-wise error rates in situations involving a small number of hypoth-\n",
      "esis tests, maybe up to 20. Good references are Miller (1981) and Westfall\n",
      "and Young (1993).\n",
      "Benjamini and Hochberg‚Äôs seminal 1995 paper introduced false-discov-\n",
      "ery rates at just the right time to catch the wave of large-scale data sets, now\n",
      "involving thousands of simultaneous tests, generated by microarray appli-\n",
      "cations. Most of the material in this chapter is taken from Efron (2010),\n",
      "where the empirical Bayes nature of Fdr theory is emphasized. The po-\n",
      "lice data is discussed and analyzed at length in Ridgeway and MacDonald\n",
      "(2009).\n",
      "¬é1[p. 272] Model (15.4) .Section 7.4 of Efron (2010) discusses the following\n",
      "result for the non-null distribution of z-values: a transformation such as\n",
      "(15.2) that produces a z-value (i.e., a standard normal random variable z\u0018\n",
      "N.0;1/ ) under the null hypothesis gives, to a good approximation, z\u0018\n",
      "N.\u0016;\u001b2\n",
      "\u0016/under reasonable alternatives. For the speciÔ¨Åc situation in (15.2),\n",
      "Student‚Äôstwith 100 degrees of freedom, \u001b2\n",
      "\u0016:D1as in (15.4).\n",
      "¬é2[p. 274] Holm‚Äôs procedure. Methods of FWER control, including Holm‚Äôs\n",
      "procedure, are surveyed in Chapter 3 of Efron (2010). They display a large\n",
      "amount of mathematical ingenuity, and provided the background against\n",
      "which FDR theory developed.\n",
      "¬é3[p. 276] FDR control theorem. Benjamini and Hochberg‚Äôs striking control\n",
      "theorem (15.15) was rederived by Storey et al. (2004) using martingale\n",
      "theory. The basic idea of false discoveries, as displayed in Figure 15.2,\n",
      "goes back to Soric (1989).\n",
      "¬é4[p. 285] Formula (15.44) .Integrating fdr .z/D\u00190f0.z/=f.z/ gives\n",
      "Effdr.z/jz\u0015z0gDZ1\n",
      "z0\u00190f0.z/dz,Z1\n",
      "z0f.z/dz\n",
      "D\u00190S0.z0/=S.z0/DFdr.z0/:(15.54)\n",
      "¬é5[p. 286] Thresholds for Fdr and fdr. Suppose the survival curves S0.z/and\n",
      "S1.z/(15.20) satisfy the ‚ÄúLehmann alternative‚Äù relationship\n",
      "logS0.z/ (15.55)15.7 Notes and Details 295\n",
      "is a positive constant less than 1. (This is a\n",
      "reasonable condition for the non-null density f1.z/to produce larger pos-\n",
      "itive values of zthan does the null density f0.z/.) Differentiating (15.55)\n",
      "gives\n",
      "\u00190\n",
      "\u00191f0.z/\n",
      "f1.z/D1\n",
      "\u00190\n",
      "\u00191S0.z/\n",
      "S1.z/; (15.56)\n",
      "after some rearrangement. But fdr .z/D\u00190f0.z/=.\u00190f0.z/C\u00191f1.z// is\n",
      "algebraically equivalent to\n",
      "fdr.z/\n",
      "1\u0000fdr.z/D\u00190\n",
      "\u00191f0.z/\n",
      "f1.z/; (15.57)\n",
      "and similarly for Fdr .z/=.1\u0000Fdr.z//, yielding\n",
      "fdr.z/\n",
      "1\u0000fdr.z/D1\n",
      "Fdr.z/\n",
      "1\u0000Fdr.z/: (15.58)\n",
      "For largez, both fdr.z/and Fdr.z/go to zero, giving the asymptotic rela-\n",
      "tionship\n",
      ": (15.59)dr.z/=\n",
      "D1=2for instance, fdr .z/will be about twice Fdr .z/wherezis large.\n",
      "This motivates the suggested relative thresholds cfdr.zi/\u00140:20 compared\n",
      "withcFdr.zi/\u00140:10.\n",
      "¬é6[p. 288] Correlation effects. The Poisson regression method used to esti-\n",
      "mateOf.z/ in Figure 15.5 proceeds as if the components of the N-vector of\n",
      "zivalueszare independent. Approximation (10.54), that the kth bin count\n",
      "ykP \u0018Poi.\u0016k/, requires independence. If not, it can be shown that var .yk/\n",
      "increases above the Poisson value \u0016kas\n",
      "var.yk/:D\u0016kCÀõ2ck: (15.60)\n",
      "Hereckis a Ô¨Åxed constant depending on f.z/ , whileÀõ2is the mean square\n",
      "correlation between all pairs ziandzj,\n",
      "Àõ2D2\n",
      "4NX\n",
      "iD1X\n",
      "j¬§icov.zi;zj/23\n",
      "5,\n",
      "N.N\u00001/: (15.61)\n",
      "Estimates like cfdr.z/in Figure 15.5 remain nearly unbiased under correla-\n",
      "tion, but their sampling variability increases as a function of Àõ. Chapters 7\n",
      "and 8 of Efron (2010) discuss correlation effects in detail.\n",
      "Often,Àõcan be estimated. Let Xbe the6033\u000250matrix of gene ex-\n",
      "pression levels measured for the control subject in the prostate study. Rows296 Large-scale Hypothesis Testing and FDRs\n",
      "iandjprovide an unbiased estimate of cor .zi;zj/2. Modern computation\n",
      "is sufÔ¨Åciently fast to evaluate all N.N\u00001/=2 pairs (though that isn‚Äôt nec-\n",
      "essary, sampling is faster) from which estimate OÀõis obtained. It equaled\n",
      "0:016Àô0:001 for the control subjects, and 0:015Àô0:001 for the6033\u000252\n",
      "matrix of the cancer patients. Correlation is not much of a worry for the\n",
      "prostate study, but other microarray studies show much larger OÀõvalues.\n",
      "Sections 6.4 and 8.3 of Efron (2010) discuss how correlations can under-\n",
      "cut inferences based on the theoretical null even when it is correct for all\n",
      "the null cases.\n",
      "¬é7[p. 289] The program locfdr .Available from CRAN, this is an R pro-\n",
      "gram that provides fdr and Fdr estimates, using both the theoretical and\n",
      "empirical null distributions.\n",
      "¬é8[p. 289] ML estimation of the empirical null. LetA0be the ‚Äúzero set‚Äù\n",
      "(15.49),z0the set ofziobserved to be in A0,I0their indices, and N0the\n",
      "number ofziinA0. Also deÔ¨Åne\n",
      "\u001eƒ±0;\u001b0.z/De\u00001\n",
      "2\u0010z\u0000ƒ±0\n",
      "\u001b0\u00112\u001eq\n",
      "2\u0019\u001b2\n",
      "0;\n",
      "P.ƒ±0;\u001b0/DZ\n",
      "A0\u001eƒ±0;\u001b0.z/dz and\u0012D\u00190P.ƒ±0;\u001b0/:(15.62)\n",
      "(So\u0012DPrfzi2A0gaccording to (15.48)‚Äì(15.49).) Then z0has density\n",
      "and likelihood\n",
      "fƒ±0;\u001b0;\u00190.z0/D\" \n",
      "N\n",
      "N0!\n",
      "\u0012N0.1\u0000\u0012/N\u0000N0#\"Y\n",
      "I0\u001eƒ±0;\u001b0.zi/\n",
      "P.ƒ±0;\u001b0/#\n",
      ";(15.63)\n",
      "the Ô¨Årst factor being the binomial probability of seeing N0of theziin\n",
      "A0, and the second the conditional probability of those zifalling within\n",
      "A0. The second factor is numerically maximized to give .Oƒ±0;O\u001b0/, while\n",
      "O\u0012DN0=Nis obtained from the Ô¨Årst, and then O\u00190DO\u0012=P.Oƒ±0;O\u001b0/. This is\n",
      "a partial likelihood argument, as in Section 9.4; locfdr centers A0at the\n",
      "median of the N zivalues, with width about twice the interquartile range\n",
      "estimate of\u001b0.\n",
      "¬é9[p. 290] The permutation null. An impressive amount of theoretical effort\n",
      "concerned the ‚Äúpermutation t-test‚Äù: in a single-test two-sample situation,\n",
      "permuting the data and computing the tstatistic gives, after a great many\n",
      "repetitions, a histogram dependably close to that of the standard tdistri-\n",
      "bution; see Hoeffding (1952). This was Fisher‚Äôs justiÔ¨Åcation for using the\n",
      "standardt-test on nonnormal data.\n",
      "The argument cuts both ways. Permutation methods tend to recreate the15.7 Notes and Details 297\n",
      "theoretical null, even in situations like that of Figure 15.7 where it isn‚Äôt\n",
      "appropriate. The difÔ¨Åculties are discussed in Section 6.5 of Efron (2010).\n",
      "¬é10[p. 293] Relevance theory. Suppose that in the DTI example shown in Fig-\n",
      "ure 15.10 we want to consider only voxels with xD60as relevant to an\n",
      "observedziwithxiD60. Now there may not be enough relevant cases to\n",
      "adequately estimate fdr .zi/or Fdr.zi/. Section 10.1 of Efron (2010) shows\n",
      "how the complete-data estimates cfdr.zi/orcFdr.zi/can be efÔ¨Åciently mod-\n",
      "iÔ¨Åed to conform to this situation.16\n",
      "Sparse Modeling and the Lasso\n",
      "The amount of data we are faced with keeps growing. From around the\n",
      "late 1990s we started to see wide data sets, where the number of variables\n",
      "far exceeds the number of observations. This was largely due to our in-\n",
      "creasing ability to measure a large amount of information automatically. In\n",
      "genomics, for example, we can use a high-throughput experiment to auto-\n",
      "matically measure the expression of tens of thousands of genes in a sam-\n",
      "ple in a short amount of time. Similarly, sequencing equipment allows us\n",
      "to genotype millions of SNPs (single-nucleotide polymorphisms) cheaply\n",
      "and quickly. In document retrieval and modeling, we represent a document\n",
      "by the presence or count of each word in the dictionary. This easily leads to\n",
      "a feature vector with 20,000 components, one for each distinct vocabulary\n",
      "word, although most would be zero for a small document. If we move to\n",
      "bi-grams or higher, the feature space gets really large.\n",
      "In even more modest situations, we can be faced with hundreds of vari-\n",
      "ables. If these variables are to be predictors in a regression or logistic re-\n",
      "gression model, we probably do not want to use them all. It is likely that a\n",
      "subset will do the job well, and including all the redundant variables will\n",
      "degrade our Ô¨Åt. Hence we are often interested in identifying a good subset\n",
      "of variables. Note also that in these wide-data situations, even linear mod-\n",
      "els are over-parametrized, so some form of reduction or regularization is\n",
      "essential.\n",
      "In this chapter we will discuss some of the popular methods for model\n",
      "selection, starting with the time-tested and worthy forward-stepwise ap-\n",
      "proach. We then look at the lasso, a popular modern method that does se-\n",
      "lection and shrinkage via convex optimization. The LARs algorithm ties\n",
      "these two approaches together, and leads to methods that can deliver paths\n",
      "of solutions.\n",
      "Finally, we discuss some connections with other modern big- and wide-\n",
      "data approaches, and mention some extensions.\n",
      "29816.1 Forward Stepwise Regression 299\n",
      "16.1 Forward Stepwise Regression\n",
      "Stepwise procedures have been around for a very long time. They were\n",
      "originally devised in times when data sets were quite modest in size, in\n",
      "particular in terms of the number of variables. Originally thought of as the\n",
      "poor cousins of ‚Äúbest-subset‚Äù selection, they had the advantage of being\n",
      "much cheaper to compute (and in fact possible to compute for large p). We\n",
      "will review best-subset regression Ô¨Årst.\n",
      "Suppose we have a set of nobservations on a response yiand a vec-\n",
      "tor ofppredictorsx0\n",
      "iD.xi1;xi2;:::;xip/, and we plan to Ô¨Åt a linear\n",
      "regression model. The response could be quantitative, so we can think of\n",
      "Ô¨Åtting a linear model by least squares. It could also be binary, leading to a\n",
      "linear logistic regression model Ô¨Åt by maximum likelihood. Although we\n",
      "will focus on these two cases, the same ideas transfer exactly to other gen-\n",
      "eralized linear models, the Cox model, and so on. The idea is to build a\n",
      "model using a subset of the variables; in fact the smallest subset that ade-\n",
      "quately explains the variation in the response is what we are after, both for\n",
      "inference and for prediction purposes. Suppose our loss function for Ô¨Åtting\n",
      "the linear model is L(e.g. sum of squares, negative log-likelihood). The\n",
      "method of best-subset regression is simple to describe, and is given in Al-\n",
      "gorithm 16.1. Step 3 is easy to state, but requires a lot of computation. For\n",
      "Algorithm 16.1 BEST-SUBSET REGRESSION .\n",
      "1 Start withmD0and the null modelO\u00110.x/DOÀá0, estimated by the mean\n",
      "of theyi.\n",
      "2 At stepmD1, pick the single variable jthat Ô¨Åts the response best,\n",
      "in terms of the loss Levaluated on the training data, in a univariate\n",
      "regressionO\u00111.x/DOÀá0Cx0\n",
      "jOÀáj. SetA1Dfjg.\n",
      "3 For each subset size m2f2;3;:::;Mg(withM\u0014min.n\u00001;p/ )\n",
      "identify the best subset Amof sizemwhen Ô¨Åtting a linear model\n",
      "O\u0011m.x/DOÀá0Cx0\n",
      "AmOÀáAmwithmof thepvariables, in terms of the\n",
      "lossL.\n",
      "4 Use some external data or other means to select the ‚Äúbest‚Äù amongst these\n",
      "Mmodels.\n",
      "pmuch larger than about 40 it becomes prohibitively expensive to perform\n",
      "exactly‚Äîa so-called ‚ÄúN-P complete‚Äù problem because of its combinatorial\n",
      "complexity (there are 2psubsets). Note that the subsets need not be nested:300 Sparse Modeling and the Lasso\n",
      "the best subset of size mD3, say, need not include both or any of the\n",
      "variables in the best subset of size mD2.\n",
      "In step 4 there are a number of methods for selecting m. Originally the\n",
      "Cpcriterion of Chapter 12 was proposed for this purpose. Here we will\n",
      "favorK-fold cross-validation, since it is applicable to all the methods dis-\n",
      "cussed in this chapter.\n",
      "It is interesting to digress for a moment on how cross-validation works\n",
      "here. We are using it to select the subset size mon the basis of prediction\n",
      "performance (on future data). With KD10, we divide the ntraining obser-\n",
      "vations randomly into 10 equal size groups. Leaving out say group kD1,\n",
      "we perform steps 1‚Äì3 on the 9=10 ths, and for each of the chosen models,\n",
      "we summarize the prediction performance on the group-1 data. We do this\n",
      "KD10times, each time with group kleft out. We then average the 10 per-\n",
      "formance measures for each m, and select the value of mcorresponding to\n",
      "the best performance. Notice that for each m, the 10 modelsO\u0011m.x/might\n",
      "involve different subsets of variables! This is not a concern, since we are\n",
      "trying to Ô¨Ånd a good value of mfor the method. Having identiÔ¨Åed Om, we\n",
      "rerun step 3 on the entire training set, and deliver the chosen model O\u0011Om.x/.\n",
      "As hinted above, there are problems with best-subset regression. A pri-\n",
      "mary issue is that it works exactly only for relatively small p. For example,\n",
      "we cannot run it on the spam data with 57 variables (at least not in 2015 on\n",
      "a Macbook Pro!). We may also think that even if we could do the compu-\n",
      "tations, with such a large search space the variance of the procedure might\n",
      "be too high.\n",
      "As a result, more manageable stepwise procedures were invented. For-\n",
      "ward stepwise regression, Algorithm 16.2, is a simple modiÔ¨Åcation of best-\n",
      "subset, with the modiÔ¨Åcation occurring in step 3. Forward stepwise re-\n",
      "gression produces a nested sequence of models ;:::\u001aAm\u00001\u001aAm\u001a\n",
      "AmC1:::. It starts with the null model, here an intercept, and adds vari-\n",
      "ables one at a time. Even with large p, identifying the best variable to add\n",
      "at each step is manageable, and can be distributed if clusters of machines\n",
      "are available. Most importantly, it is feasible for large p. Figure 16.1 shows\n",
      "the coefÔ¨Åcient proÔ¨Åles for forward-stepwise linear regression on the spam\n",
      "training data. Here there are 57 input variables (relative prevalence of par-\n",
      "ticular words in the document), and an ‚ÄúofÔ¨Åcial‚Äù (train, test) split of (3065,\n",
      "1536) observations. The response is coded as +1 if the email was spam,\n",
      "else -1. The Ô¨Ågure caption gives the details. We saw the spam data earlier,\n",
      "in Table 8.3, Figure 8.7 and Figure 12.2.\n",
      "Fitting the entire forward-stepwise linear regression path as in the Ô¨Ågure\n",
      "(whenn>p ) has essentially the same cost as a single least squares Ô¨Åt on16.1 Forward Stepwise Regression 301\n",
      "Algorithm 16.2 FORWARD STEPWISE REGRESSION .\n",
      "1 Start withmD0and the null modelO\u00110.x/DOÀá0, estimated by the mean\n",
      "of theyi.\n",
      "2 At stepmD1, pick the single variable jthat Ô¨Åts the response best,\n",
      "in terms of the loss Levaluated on the training data, in a univariate\n",
      "regressionO\u00111.x/DOÀá0Cx0\n",
      "jOÀáj. SetA1Dfjg.\n",
      "3 For each subset size m2f2;3;:::;Mg(withM\u0014min.n\u00001;p/ )\n",
      "identify the variable kthat when augmented with Am\u00001to form Am,\n",
      "leads to the modelO\u0011m.x/DOÀá0Cx0\n",
      "AmOÀáAmthat performs best in terms\n",
      "of the lossL.\n",
      "4 Use some external data or other means to select the ‚Äúbest‚Äù amongst these\n",
      "Mmodels.\n",
      "all the variables. This is because the sequence of models can be updated\n",
      "each time a variable is added.¬éHowever, this is a consequence of the linear ¬é1\n",
      "model and squared-error loss.\n",
      "Suppose instead we run a forward stepwise logistic regression. Here up-\n",
      "dating does not work, and the entire Ô¨Åt has to be recomputed by maximum\n",
      "likelihood each time a variable is added. Identifying which variable to add\n",
      "in step 3 in principle requires Ô¨Åtting an .mC1/-variable model p\u0000m\n",
      "times, and seeing which one reduces the deviance the most. In practice, we\n",
      "can use score tests which are much cheaper to evaluate.¬éThese amount ¬é2\n",
      "to using the quadratic approximation to the log-likelihood from the Ô¨Ånal\n",
      "iteratively reweighted least-squares (IRLS) iteration for Ô¨Åtting the model\n",
      "withmterms. The score test for a variable not in the model is equivalent\n",
      "to testing for the inclusion of this variable in the weighted least-squares Ô¨Åt.\n",
      "Hence identifying the next variable is almost back to the previous cases,\n",
      "requiringp\u0000msimple regression updates.¬éFigure 16.2 shows the test ¬é3\n",
      "misclassiÔ¨Åcation error for forward-stepwise linear regression and logistic\n",
      "regression on the spam data, as a function of the number of steps. They\n",
      "both level off at around 25 steps, and have a similar shape. However, the\n",
      "logistic regression gives more accurate classiÔ¨Åcations.1\n",
      "Although forward-stepwise methods are possible for large p, they get\n",
      "tedious for very large p(in the thousands), especially if the data could sup-\n",
      "port a model with many variables. However, if the ideal active set is fairly\n",
      "1For this example we can halve the gap between the curves by optimizing the prediction\n",
      "threshold for linear regression.302 Sparse Modeling and the Lasso\n",
      "0.0 0.1 0.2 0.3 0.4 0.5 0.6‚àí0.2 0.00.20.40.60.8Forward‚àíStepwise Regression\n",
      "R2 on Training DataCoefficients\n",
      "Figure 16.1 Forward stepwise linear regression on the spam\n",
      "data. Each curve corresponds to a particular variable, and shows\n",
      "the progression of its coefÔ¨Åcient as the model grows. These are\n",
      "plotted against the training R2, and the vertical gray bars\n",
      "correspond to each step. Starting at the left at step 1, the Ô¨Årst\n",
      "selected variable explains R2D0:16; adding the second increases\n",
      "R2to0:25, etc. What we see is that early steps have a big impact\n",
      "on theR2, while later steps hardly have any at all. The vertical\n",
      "black line corresponds to step 25 (see Figure 16.2), and we see\n",
      "that after that the step-wise improvements in R2are negligible.\n",
      "small, even with many thousands of variables forward-stepwise selection\n",
      "is a viable option.\n",
      "Forward-stepwise selection delivers a sequence of models, as seen in\n",
      "the previous Ô¨Ågures. One would generally want to select a single model,\n",
      "and as discussed earlier, we often use cross-validation for this purpose.\n",
      "Figure 16.3 illustrates using stepwise linear regression on the spam data.\n",
      "Here the sequence of models are Ô¨Åt using squared-error loss on the bi-\n",
      "nary response variable. However, cross-validation scores each model for\n",
      "misclassiÔ¨Åcation error , the ultimate goal of this modeling exercise. This\n",
      "highlights one of the advantages of cross-validation in this context. A con-\n",
      "venient (differentiable and smooth) loss function is used to Ô¨Åt the sequence16.2 The Lasso 303\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0 10 20 30 40 50 600.0 0.1 0.2 0.3 0.4Spam Data\n",
      "StepTest Misclassification Error‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óèForward‚àíStepwise Linear Regression\n",
      "Forward‚àíStepwise Logistic Regression\n",
      "Figure 16.2 Forward-stepwise regression on the spam data.\n",
      "Shown is the misclassiÔ¨Åcation error on the test data, as a function\n",
      "of the number of steps. The brown dots correspond to linear\n",
      "regression, with the response coded as -1 and +1; a prediction\n",
      "greater than zero is classiÔ¨Åed as +1, one less than zero as -1. The\n",
      "blue dots correspond to logistic regression, which performs better.\n",
      "We see that both curves essentially reach their minima after 25\n",
      "steps.\n",
      "of models. However, we can use any performance measure to evaluate the\n",
      "sequence of models; here misclassiÔ¨Åcation error is used. In terms of the\n",
      "parameters of the linear model, misclassiÔ¨Åcation error would be a difÔ¨Åcult\n",
      "and discontinuous loss function to use for parameter estimation. All we\n",
      "need to use it for here is pick the best model size. There appears to be little\n",
      "beneÔ¨Åt in going beyond 25‚Äì30 terms.\n",
      "16.2 The Lasso\n",
      "The stepwise model-selection methods of the previous section are useful\n",
      "if we anticipate a model using a relatively small number of variables, even\n",
      "if the pool of available variables is very large. If we expect a moderate\n",
      "number of variables to play a role, these methods become cumbersome.\n",
      "Another black mark against forward-stepwise methods is that the sequence304 Sparse Modeling and the Lasso\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0 10 20 30 40 500.0 0.1 0.2 0.3 0.4Spam Data\n",
      "StepTest and CV Misclassification Error‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óèTest Error\n",
      "10‚àífold CV Error\n",
      "Figure 16.3 Ten-fold cross-validated misclassiÔ¨Åcation errors\n",
      "(green) for forward-stepwise regression on the spam data, as a\n",
      "function of the step number. Since each error is an average of 10\n",
      "numbers, we can compute a (crude) standard error; included in\n",
      "the plot are pointwise standard-error bands. The brown curve is\n",
      "the misclassiÔ¨Åcation error on the test data.\n",
      "of models is derived in a greedy fashion, without any claimed optimality.\n",
      "The methods we describe here are derived from a more principled proce-\n",
      "dure; indeed they solve a convex optimization , as deÔ¨Åned below.\n",
      "We will Ô¨Årst present the lasso for squared-error loss, and then the more\n",
      "general case later. Consider the constrained linear regression problem\n",
      "minimize\n",
      "Àá02R;Àá2Rp1\n",
      "nnX\n",
      "iD1.yi\u0000Àá0\u0000x0\n",
      "iÀá/2subject tokÀák1\u0014t; (16.1)\n",
      "wherekÀák1DPp\n",
      "jD1jÀájj, the`1norm of the coefÔ¨Åcient vector. Since both\n",
      "the loss and the constraint are convex in Àá, this is a convex optimization\n",
      "problem, and it is known as the lasso . The constraintkÀák1\u0014trestricts the\n",
      "coefÔ¨Åcients of the model by pulling them toward zero; this has the effect\n",
      "of reducing their variance, and prevents overÔ¨Åtting. Ridge regression is an\n",
      "earlier great uncle of the lasso, and solves a similar problem to (16.1), ex-\n",
      "cept the constraint is kÀák2\u0014t; ridge regression bounds the quadratic `216.2 The Lasso 305\n",
      "Œ≤^Œ≤^ 2. . Œ≤\n",
      "1Œ≤2\n",
      "Œ≤1Œ≤\n",
      "Figure 16.4 An example with Àá2R2to illustrate the difference\n",
      "between ridge regression and the lasso. In both plots, the red\n",
      "contours correspond to the squared-error loss function, with the\n",
      "unrestricted least-squares estimate OÀáin the center. The blue\n",
      "regions show the constraints, with the lasso on the left and ridge\n",
      "on the right. The solution to the constrained problem corresponds\n",
      "to the value of Àáwhere the expanding loss contours Ô¨Årst touch the\n",
      "constraint region. Due to the shape of the lasso constraint, this\n",
      "will often be at a corner (or an edge more generally), as here,\n",
      "which means in this case that the minimizing ÀáhasÀá1D0. For\n",
      "the ridge constraint, this is unlikely to happen.\n",
      "norm of the coefÔ¨Åcient vector. It also has the effect of pulling the coefÔ¨Å-\n",
      "cients toward zero, in an apparently very similar way. Ridge regression is\n",
      "discussed in Section 7.3.2Both the lasso and ridge regression are shrinkage\n",
      "methods, in the spirit of the James‚ÄìStein estimator of Chapter 7.\n",
      "A big difference, however, is that for the lasso, the solution typically has\n",
      "many of theÀájequal to zero, while for ridge they are all nonzero. Hence\n",
      "the lasso does variable selection and shrinkage, while ridge only shrinks.\n",
      "Figure 16.4 illustrates this for Àá2R2. In higher dimensions, the `1norm\n",
      "has sharp edges and corners, which correspond to coefÔ¨Åcient estimates zero\n",
      "inÀá.\n",
      "Since the constraint in the lasso treats all the coefÔ¨Åcients equally, it usu-\n",
      "ally makes sense for all the elements of xto be in the same units. If not, we\n",
      "2Here we use the ‚Äúbound‚Äù form of ridge regression, while in Section 7.3 we use the\n",
      "‚ÄúLagrange‚Äù form. They are equivalent, in that for every ‚ÄúLagrange‚Äù solution, there is a\n",
      "corresponding bound solution.306 Sparse Modeling and the Lasso\n",
      "typically standardize the predictors beforehand so that each has variance\n",
      "one.\n",
      "Two natural boundary values fortin (16.1) are tD0andtD1 .\n",
      "The former corresponds to the constant model (the Ô¨Åt is the mean of the\n",
      "yi,)3and the latter corresponds to the unrestricted least-squares Ô¨Åt. In fact,\n",
      "ifn > p , andOÀáis the least-squares estimate, then we can replace 1by\n",
      "kOÀák1, and any value of t\u0015kOÀák1is a non-binding constraint.¬éFigure 16.5 ¬é4\n",
      "0.0 0.1 0.2 0.3 0.4 0.5 0.6‚àí0.2 0.0 0.2 0.4Lasso Regression\n",
      "R2 on Training DataCoefficients0.000.060.20 1.05 2.45 3.47\n",
      "Figure 16.5 The lasso linear regression regularization path on the\n",
      "spam data. Each curve corresponds to a particular variable, and\n",
      "shows the progression of its coefÔ¨Åcient as the regularization\n",
      "boundtgrows. These curves are plotted against the training R2\n",
      "rather thant, to make the curves comparable with the\n",
      "forward-stepwise curves in Figure 16.1. Some values of tare\n",
      "indicated at the top. The vertical gray bars indicate changes in the\n",
      "active set of nonzero coefÔ¨Åcients, typically an inclusion. Here we\n",
      "see clearly the role of the `1penalty; astis relaxed, coefÔ¨Åcients\n",
      "become nonzero, but in a smoother fashion than in forward\n",
      "stepwise.\n",
      "shows the regularization path4for the lasso linear regression problem on\n",
      "3We typically do not restrict the intercept in the model.\n",
      "4Also known as the homotopy path.16.2 The Lasso 307\n",
      "thespam data; that is, the solution path for all values of t. This can be com-\n",
      "puted exactly, as we will see in Section 16.4, because the coefÔ¨Åcient pro-\n",
      "Ô¨Åles are piecewise linear in t. It is natural to compare this coefÔ¨Åcient proÔ¨Åle\n",
      "with the analogous one in Figure 16.1 for forward-stepwise regression. Be-\n",
      "cause of the control of kOÀá.t/k1, we don‚Äôt see the same range as in forward\n",
      "stepwise, and observe somewhat smoother behavior. Figure 16.6 contrasts\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0 10 20 30 40 500.0 0.1 0.2 0.3 0.4Spam Data\n",
      "StepTest Misclassification Error‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óèForward‚àíStepwise Linear Regression\n",
      "Forward‚àíStepwise Logistic Regression\n",
      "Lasso Linear Regression\n",
      "Lasso Logistic Regression\n",
      "Figure 16.6 Lasso versus forward-stepwise regression on the\n",
      "spam data. Shown is the misclassiÔ¨Åcation error on the test data,\n",
      "as a function of the number of variables in the model. Linear\n",
      "regression is coded brown, logistic regression blue; hollow dots\n",
      "forward stepwise, solid dots lasso. In this case it appears stepwise\n",
      "and lasso achieve the same performance, but lasso takes longer to\n",
      "get there, because of the shrinkage.\n",
      "the prediction performance on the spam data for lasso regularized models\n",
      "(linear regression and logistic regression) versus forward-stepwise models.\n",
      "The results are rather similar at the end of the path; here forward stepwise\n",
      "can achieve classiÔ¨Åcation performance similar to that of lasso regularized\n",
      "logistic regression with about half the terms. Lasso logistic regression (and\n",
      "indeed any likelihood-based linear model) is Ô¨Åt by penalized maximum308 Sparse Modeling and the Lasso\n",
      "likelihood:\n",
      "minimize\n",
      "Àá02R;Àá2Rp1\n",
      "nnX\n",
      "iD1L.yi;Àá0CÀá0xi/subject tokÀák1\u0014t: (16.2)\n",
      "HereLis the negative of the log-likelihood function for the response dis-\n",
      "tribution.\n",
      "16.3 Fitting Lasso Models\n",
      "The lasso objectives (16.1) or (16.2) are differentiable and convex in Àáand\n",
      "Àá0, and the constraint is convex in Àá. Hence solving these problems is a\n",
      "convex optimization problem, for which standard packages are available.\n",
      "It turns out these problems have special structure that can be exploited\n",
      "to yield efÔ¨Åcient algorithms for Ô¨Åtting the entire path of solutions as in\n",
      "Figures 16.1 and 16.5. We will start with problem (16.1), which we rewrite\n",
      "in the more convenient Lagrange form:\n",
      "minimize\n",
      "Àá2Rp1\n",
      "2nky\u0000XÀák2C\u0015kÀák1: (16.3)\n",
      "Here we have centered yand the columns of Xbeforehand, and hence\n",
      "the intercept has been omitted. The Lagrange and constraint versions are\n",
      "equivalent, in the sense that any solution OÀá.\u0015/ to (16.3) with \u0015\u00150corre-\n",
      "sponds to a solution to (16.1) with tDkOÀá.\u0015/k1. Here large values of \u0015will\n",
      "encourage solutions with small `1norm coefÔ¨Åcient vectors, and vice-versa;\n",
      "\u0015D0corresponds to the ordinary least squares Ô¨Åt.\n",
      "The solution to (16.3) satisÔ¨Åes the subgradient condition\n",
      "\u00001\n",
      "nhxj;y\u0000XOÀáiC\u0015sjD0; jD1;:::;p; (16.4)\n",
      "wheresj2sign.OÀáj/; jD1;:::;p . This notation means sjDsign.OÀáj/\n",
      "ifOÀáj¬§0, andsj2≈í\u00001;1¬ç ifOÀájD0.) We use the inner-product notation\n",
      "ha;biDa0bin (16.4), which leads to more evocative expressions. These\n",
      "subgradient conditions are the modern way of characterizing solutions to\n",
      "problems of this kind, and are equivalent to the Karush‚ÄìKuhn‚ÄìTucker op-\n",
      "timality conditions. From these conditions we can immediately learn some\n",
      "properties of a lasso solution.\n",
      "\u000f1\n",
      "njhxj;y\u0000XOÀáijD\u0015for all members of the active set ; i.e., each of the\n",
      "variables in the model (with nonzero coefÔ¨Åcient) has the same covari-\n",
      "ance with the residuals (in absolute value).16.4 Least-Angle Regression 309\n",
      "\u000f1\n",
      "njhxk;y\u0000XOÀáij\u0014\u0015for all variables not in the active set (i.e. with\n",
      "coefÔ¨Åcients zero).\n",
      "These conditions are interesting and have a big impact on computation.\n",
      "Suppose we have the solution OÀá.\u00151/at\u00151, and we decrease \u0015by a small\n",
      "amount to\u00152< \u00151. The coefÔ¨Åcients and hence the residuals change, in\n",
      "such a way that the covariances all remain tied at the smaller value \u00152. If\n",
      "in the process the active set has not changed, and nor have the signs of\n",
      "their coefÔ¨Åcients, then we get an important consequence: OÀá.\u0015/ islinear for\n",
      "\u00152≈í\u00152;\u00151¬ç. To see this, suppose Aindexes the active set, which is the\n",
      "same at\u00151and\u00152, and letsAbe the constant sign vector. Then we have\n",
      "X0\n",
      "A.y\u0000XOÀá.\u00151//DnsA\u00151;\n",
      "X0\n",
      "A.y\u0000XOÀá.\u00152//DnsA\u00152:\n",
      "By subtracting and solving we get\n",
      "OÀáA.\u00152/\u0000OÀáA.\u00151/Dn.\u00151\u0000\u00152/.X0\n",
      "AXA/\u00001sA; (16.5)\n",
      "and the remaining coefÔ¨Åcients (with indices not in A) are all zero. This\n",
      "shows that the full coefÔ¨Åcient vector OÀá.\u0015/ is linear for\u00152≈í\u00152;\u00151¬ç. In fact,\n",
      "the coefÔ¨Åcient proÔ¨Åles for the lasso are continuous and piecewise linear\n",
      "over the entire range of \u0015, with knots occurring whenever the active set\n",
      "changes, or the signs of the coefÔ¨Åcients change.\n",
      "Another consequence is that we can easily determine \u0015max, the smallest\n",
      "value for\u0015such that the solution OÀá.\u0015max/D0. From (16.4) this can be\n",
      "seen to be\u0015maxDmaxj1\n",
      "njhxj;yij.\n",
      "These two facts plus a few more details enable us to compute the exact\n",
      "solution path for the squared-error-loss lasso; that is the topic of the next\n",
      "section.\n",
      "16.4 Least-Angle Regression\n",
      "We have just seen that the lasso coefÔ¨Åcient proÔ¨Åle OÀá.\u0015/ is piecewise lin-\n",
      "ear in\u0015, and that the elements of the active set are tied in their absolute\n",
      "covariance with the residuals. With r.\u0015/Dy\u0000XOÀá.\u0015/ , the covariance be-\n",
      "tweenxjand the evolving residual is cj.\u0015/D1\n",
      "njhxj;r.\u0015/ij. Hence these\n",
      "also change in a piecewise linear fashion, with cj.\u0015/D\u0015forj2A, and\n",
      "cj.\u0015/\u0014\u0015forj62A. This inspires the Least-Angle Regression algorithm,\n",
      "given in Algorithm 16.3, which exploits this linearity to Ô¨Åt the entire lasso\n",
      "regularization path.310 Sparse Modeling and the Lasso\n",
      "Algorithm 16.3 LEAST -ANGLE REGRESSION .\n",
      "1 Standardize the predictors to have mean zero and unit `2norm. Start\n",
      "with the residual r0Dy\u0000Ny,Àá0D.Àá1;Àá2;:::;Àáp/D0.\n",
      "2 Find the predictor xjmost correlated with r0; i.e., with largest value for\n",
      "1\n",
      "njhxj;r0ij. Call this value \u00150, deÔ¨Åne the active set ADfjg, andXA,\n",
      "the matrix consisting of this single variable.\n",
      "3 ForkD1;2;:::;KDmin.n\u00001;p/ do:\n",
      "(a) DeÔ¨Åne the least-squares direction ƒ±D1\n",
      "n\u0015k\u00001.X0\n",
      "AXA/\u00001X0\n",
      "Ark\u00001, and\n",
      "deÔ¨Åne thep-vector¬Åsuch that¬ÅADƒ±, and the remaining elements\n",
      "are zero.\n",
      "(b) Move the coefÔ¨Åcients ÀáfromÀák\u00001in the direction ¬Åtoward their\n",
      "least-squares solution on XA:Àá.\u0015/DÀák\u00001C.\u0015k\u00001\u0000\u0015/¬Å for\n",
      "0 < \u0015\u0014\u0015k\u00001, keeping track of the evolving residuals r.\u0015/D\n",
      "y\u0000XÀá.\u0015/Drk\u00001\u0000.\u0015k\u00001\u0000\u0015/XAƒ±.\n",
      "(c) Keeping track of1\n",
      "njhx`;r.\u0015/ijfor`‚Ä¶A, identify the largest value of\n",
      "\u0015at which a variable ‚Äúcatches up‚Äù with the active set; if the variable\n",
      "has index`, that means1\n",
      "njhx`;r.\u0015/ij D\u0015. This deÔ¨Ånes the next\n",
      "‚Äúknot‚Äù\u0015k.\n",
      "(d) Set ADA[`,ÀákDÀá.\u0015k/DÀák\u00001C.\u0015k\u00001\u0000\u0015k/¬Å, andrkD\n",
      "y\u0000XÀák.\n",
      "4 Return the sequence f\u0015k; ÀákgK\n",
      "0.\n",
      "In step 3(a)ƒ±D.X0\n",
      "AXA/\u00001sAas in (16.5). We can think of the LAR al-\n",
      "gorithm as a democratic version of forward-stepwise regression. In forward-\n",
      "stepwise regression, we identify the variable that will improve the Ô¨Åt the\n",
      "most, and then move all the coefÔ¨Åcients toward the new least-squares Ô¨Åt.\n",
      "As described in endnotes ¬é1and¬é3, this is sometimes done by computing\n",
      "the inner products of each (unadjusted) variable with the residual, and pick-\n",
      "ing the largest in absolute value. In step 3 of Algorithm 16.3, we move the\n",
      "coefÔ¨Åcients for the variables in the active set Atoward their least-squares\n",
      "Ô¨Åt (keeping their inner products tied), but stop when a variable not in A\n",
      "catches up in inner product. At that point, it is invited into the club, and the\n",
      "process continues.\n",
      "Step 3(c) can be performed efÔ¨Åciently because of the linearity of the\n",
      "evolving inner products; for each variable not in A, we can determine ex-\n",
      "actly when (in \u0015time) it would catch up, and hence which catches up Ô¨Årst\n",
      "and when. Since the path is piecewise linear, and we know the slopes, this16.4 Least-Angle Regression 311\n",
      "0.0 0.1 0.2 0.3 0.4 0.5 0.60.0 0.1 0.2 0.3 0.4\n",
      "R2 on Training DataCovariance with Residuals\n",
      "Figure 16.7 Covariance evolution on the spam data. As\n",
      "variables tie for maximal covariance, they become part of the\n",
      "active set. These occasions are indicated by the vertical gray bars,\n",
      "again plotted against the training R2as in Figure 16.5.\n",
      "means we know the path exactly without further computation between \u0015k\u00001\n",
      "and the newly found \u0015k.\n",
      "The name ‚Äúleast-angle regression‚Äù derives from the fact that in step 3(b)\n",
      "the Ô¨Åtted vector evolves in the direction X¬ÅDXAƒ±, and its inner product\n",
      "with each active vector is given by X0\n",
      "AXAƒ±DsA. Since all the columns\n",
      "ofXhave unit norm, this means the angles between each active vector and\n",
      "the evolving Ô¨Åtted vector are equal and hence minimal.\n",
      "The main computational burden in Algorithm 16.3 is in step 3(a), com-\n",
      "puting the new direction, each time the active set is updated. However, this\n",
      "is easily performed using standard updating of a QR decomposition, and\n",
      "hence the computations for the entire path are of the same order as that of\n",
      "a single least-squares Ô¨Åt using all the variables.\n",
      "The vertical gray lines in Figure 16.5 show when the active set changes.\n",
      "We see the slopes change at each of these transitions. Compare with the\n",
      "corresponding Figure 16.1 for forward-stepwise regression.\n",
      "Figure 16.7 shows the the decreasing covariance during the steps of the312 Sparse Modeling and the Lasso\n",
      "LAR algorithm. As each variable joins the active set, the covariances be-\n",
      "come tied. At the end of the path, the covariances are all zero, because this\n",
      "is the unregularized ordinary least-squares solution.\n",
      "It turns out that the LAR algorithm is not quite the lasso path; variables\n",
      "candrop out of the active set as the path evolves. This happens when a coef-\n",
      "Ô¨Åcient curve passes through zero. The subgradient equations (16.4) imply\n",
      "that the sign of each active coefÔ¨Åcient matches the sign of the gradient.\n",
      "However, a simple addition to step 3(c) in Algorithm 16.3 takes care of the\n",
      "issue:\n",
      "3(c)+ lasso modiÔ¨Åcation : If a nonzero coefÔ¨Åcient crosses zero before the\n",
      "next variable enters, drop it from Aand recompute the joint least-squares\n",
      "direction¬Åusing the reduced set.\n",
      "Figure 16.5 was computed using the lars package in R, with the lasso\n",
      "option set to accommodate step 3(c)+; in this instance there was no need for\n",
      "dropping. Dropping tends to occur when some of the variables are highly\n",
      "correlated.\n",
      "Lasso and Degrees of Freedom\n",
      "We see in Figure 16.6 (left panel) that forward-stepwise regression is more\n",
      "aggressive than the lasso, in that it brings down the training MSE faster.\n",
      "We can use the covariance formula for df from Chapter 12 to quantify the\n",
      "amount of Ô¨Åtting at each step.\n",
      "In the right panel we show the results of a simulation for estimating the\n",
      "df of forward-stepwise regression and the lasso for the spam data. Recall\n",
      "the covariance formula\n",
      "dfD1\n",
      "\u001b2nX\n",
      "iD1cov.yi;Oyi/: (16.6)\n",
      "These covariances are of course with respect to the sampling distribution of\n",
      "theyi, which we do not have access to since these are real data. So instead\n",
      "we simulate from Ô¨Åtted values from the full least-squares Ô¨Åt, by adding\n",
      "Gaussian errors with the appropriate (estimated) standard deviation. (This\n",
      "is the parametric bootstrap calculation (12.64).)\n",
      "It turns out that each step of the LAR algorithm spends one df, as is\n",
      "evidenced by the brown curve in the right plot of Figure 16.8. Forward\n",
      "stepwise spends more df in the earlier stages, and can be erratic.\n",
      "Under some technical conditions on the Xmatrix (that guarantee that16.5 Fitting Generalized Lasso Models 313\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "010203040500.40.50.60.70.80.9Spam Training Data\n",
      "StepTraining MSE‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óèForward‚àíStepwise\n",
      "Lasso\n",
      "010203040500102030405060\n",
      "StepDf\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óèForward Stepwise\n",
      "Lasso\n",
      "Figure 16.8 Left: Training mean-squared error (MSE) on the\n",
      "spam data, for forward-stepwise regression and the lasso, as a\n",
      "function of the size of the active set. Forward stepwise is more\n",
      "aggressive than the lasso, in that it (over-)Ô¨Åts the training data\n",
      "more quickly. Right: Simulation showing the degrees of freedom\n",
      "or df of forward-stepwise regression versus lasso. The lasso uses\n",
      "one df per step, while forward stepwise is greedier and uses more,\n",
      "especially in the early steps. Since these df were computed using\n",
      "5000 random simulated data sets, we include standard-error bands\n",
      "on the estimates.\n",
      "LAR delivers the lasso path), one can show that the df is exactly one per\n",
      "step. More generally, for the lasso, if we deÔ¨Åne cdf.\u0015/DjA.\u0015/j(the size\n",
      "of the active set at \u0015), we have that E≈ícdf.\u0015/¬çDdf.\u0015/ . In other words, the\n",
      "size of the active set is an unbiased estimate of df.\n",
      "Ordinary least squares with a predetermined sequence of variables spends\n",
      "one df per variable. Intuitively forward stepwise spends more, because it\n",
      "pays a price (in some extra df) for searching.¬éAlthough the lasso does ¬é5\n",
      "search for the next variable, it does not Ô¨Åt the new model all the way, but\n",
      "just until the next variable enters. At this point, one new df has been spent.\n",
      "16.5 Fitting Generalized Lasso Models\n",
      "So far we have focused on the lasso for squared-error loss, and exploited\n",
      "the piecewise-linearity of its coefÔ¨Åcient proÔ¨Åle to efÔ¨Åciently compute the\n",
      "entire path. Unfortunately this is not the case for most other loss functions,314 Sparse Modeling and the Lasso\n",
      "so obtaining the coefÔ¨Åcient path is potentially more costly. As a case in\n",
      "point, we will use logistic regression as an example; in this case in (16.2) L\n",
      "represents the negative binomial log-likelihood. Writing the loss explicitly\n",
      "and using the Lagrange form for the penalty, we wish to solve\n",
      "minimize\n",
      "Àá02R;Àá2Rp\u0000\"\n",
      "1\n",
      "nnX\n",
      "iD1yilog\u0016iC.1\u0000yi/log.1\u0000\u0016i/#\n",
      "C\u0015kÀák1:(16.7)\n",
      "Here we assume the yi2f0;1gand\u0016iare the Ô¨Åtted probabilities\n",
      "\u0016iDeÀá0Cx0\n",
      "iÀá\n",
      "1CeÀá0Cx0\n",
      "iÀá: (16.8)\n",
      "Similar to (16.4), the solution satisÔ¨Åes the subgradient condition\n",
      "1\n",
      "nhxj;y\u0000\u0016i\u0000\u0015sjD0; jD1;:::;p; (16.9)\n",
      "wheresj2sign.Àáj/; jD1;:::;p , and\u00160D.\u00161;:::;\u0016n/.5How-\n",
      "ever, the nonlinearity of \u0016iinÀájresults in piecewise nonlinear coefÔ¨Åcient\n",
      "proÔ¨Åles. Instead we settle for a solution path on a sufÔ¨Åciently Ô¨Åne grid of\n",
      "values for\u0015. It is once again easy to see that the largest value of \u0015we need\n",
      "consider is\n",
      "\u0015maxDmax\n",
      "jjhxj;y\u0000Ny1ij; (16.10)\n",
      "since this is the smallest value of \u0015for whichOÀáD0, andOÀá0Dlogit.Ny/. A\n",
      "reasonable sequence is 100 values \u00151> \u00152> ::: > \u0015 100equally spaced\n",
      "on the log-scale from \u0015maxdown to\u000f\u0015max, where\u000fis some small fraction\n",
      "such as0:001 .\n",
      "An approach that has proven to be surprisingly efÔ¨Åcient is path-wise\n",
      "coordinate descent .\n",
      "\u000fFor each value \u0015k, solve the lasso problem for one Àájonly, holding all\n",
      "the others Ô¨Åxed. Cycle around until the estimates stabilize.\n",
      "\u000fBy starting at \u00151, where all the parameters are zero, we use warm starts\n",
      "in computing the solutions at the decreasing sequence of \u0015values. The\n",
      "warm starts provide excellent initializations for the sequence of solutions\n",
      "OÀá.\u0015k/.\n",
      "\u000fThe active set grows slowly as \u0015decreases. Computational hedges that\n",
      "guess the active set prove to be particularly efÔ¨Åcient. If the guess is good\n",
      "(and correct), one iterates coordinate descent using only those variables,\n",
      "5The equation for the intercept is1\n",
      "nPn\n",
      "iD1yiD1\n",
      "nPn\n",
      "iD1\u0016i.16.5 Fitting Generalized Lasso Models 315\n",
      "until convergence. One more sweep through all the variables conÔ¨Årms\n",
      "the hunch.\n",
      "TheRpackage glmnet employs a proximal-Newton strategy at each\n",
      "value\u0015k.\n",
      "1 Compute a weighted least squares (quadratic) approximation to the log-\n",
      "likelihoodLat the current estimate for the solution vector OÀá.\u0015k/; This\n",
      "produces a working response and observation weights, as in a regular\n",
      "GLM.\n",
      "2 Solve the weighted least-squares lasso at \u0015kby coordinate descent, using\n",
      "warm starts and active-set iterations.\n",
      "We now give some details, which illustrate why these particular strate-\n",
      "gies are effective. Consider the weighted least-squares problem\n",
      "minimize\n",
      "Àáj1\n",
      "2nnX\n",
      "iD1wi.zi\u0000Àá0\u0000x0\n",
      "iÀá/2C\u0015kÀák1; (16.11)\n",
      "with all but ÀájÔ¨Åxed at their current values. Writing QriDzi\u0000Àá0\u0000P\n",
      "`¬§jxi`Àá`, we can recast (16.11) as\n",
      "minimize\n",
      "Àáj1\n",
      "2nnX\n",
      "iD1wi.Qri\u0000xijÀáj/2C\u0015jÀájj; (16.12)\n",
      "a one-dimensional problem. The subgradient equation is\n",
      "1\n",
      "nnX\n",
      "iD1wixij.Qri\u0000xijÀáj/\u0000\u0015\u0001sign.Àáj/D0: (16.13)\n",
      "The simplest form of the solution occurs if each variable is standardized to\n",
      "have weighted mean zero and variance one, and the weights sum to one; in\n",
      "that case we have a two-step solution.\n",
      "1 Compute the weighted simple least-squares coefÔ¨Åcient\n",
      "QÀájDhxj;QriwDnX\n",
      "iD1wixijQri: (16.14)\n",
      "2Soft-thresholdQÀájto produceOÀáj:\n",
      "OÀájD(\n",
      "0 ifjQÀáj<\u0015I\n",
      "sign.QÀáj/.jQÀájj\u0000\u0015/ otherwise:(16.15)316 Sparse Modeling and the Lasso\n",
      "Without the standardization, the solution is almost as simple but less\n",
      "intuitive.\n",
      "Hence each coordinate-descent update essentially requires an inner prod-\n",
      "uct, followed by the soft thresholding operation. This is especially conve-\n",
      "nient forxijthat are stored in sparse-matrix format, since then the inner\n",
      "products need only visit the nonzero values. If the coefÔ¨Åcient is zero be-\n",
      "fore the step, and remains zero, one just moves on, otherwise the model is\n",
      "updated.\n",
      "Moving from the solution at \u0015k(for whichjhxj;riwjD\u0015kfor all the\n",
      "nonzero coefÔ¨Åcients OÀáj), down to the smaller \u0015kC1, one might expect all\n",
      "variables for which jhxj;riwj\u0015\u0015kC1would be natural candidates for the\n",
      "new active set. The strong rules lower the bar somewhat, and include any\n",
      "variables for which jhxj;riwj\u0015\u0015kC1\u0000.\u0015k\u0000\u0015kC1/; this tends to rarely\n",
      "make mistakes, and still leads to considerable computational savings.\n",
      "Apart from variations in the loss function, other penalties are of interest\n",
      "as well. In particular, the elastic net penalty bridges the gap between the\n",
      "lasso and ridge regression. That penalty is deÔ¨Åned as\n",
      "PÀõ.Àá/D1\n",
      "2.1\u0000Àõ/kÀák2\n",
      "2CÀõkÀák1; (16.16)\n",
      "where the factor 1=2 in the Ô¨Årst term is for mathematical convenience.\n",
      "When the predictors are excessively correlated, the lasso performs some-\n",
      "what poorly, since it has difÔ¨Åculty in choosing among the correlated cousins.\n",
      "Like ridge regression, the elastic net shrinks the coefÔ¨Åcients of correlated\n",
      "variables toward each other, and tends to select correlated variables in\n",
      "groups. In this case the coordinate-descent update is almost as simple as\n",
      "in (16.15)\n",
      "OÀájD(\n",
      "0 ifjQÀáj<Àõ\u0015I\n",
      "sign.QÀáj/.jQÀájj\u0000Àõ\u0015/\n",
      "1C.1\u0000Àõ/\u0015otherwise;(16.17)\n",
      "again assuming the observations have weighted variance equal to one. When\n",
      "ÀõD0, the update corresponds to a coordinate update for ridge regression.\n",
      "Figure 16.9 compares lasso with forward-stepwise logistic regression on\n",
      "thespam data, here using all binarized variables and their pairwise interac-\n",
      "tions. This amounts to 3061 variables in all, once degenerate variables have\n",
      "been excised. Forward stepwise takes a long time to run, since it enters one\n",
      "variable at a time, and after each one has been selected, a new GLM must\n",
      "be Ô¨Åt. The lasso path, as Ô¨Åt by glmnet , includes many new variables at\n",
      "each step (\u0015k), and is extremely fast (6 s for the entire path). For very large16.6 Post-Selection Inference for the Lasso 317\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.1 0.2 0.3 0.4Spam Data with Interactions\n",
      "Percentage NULL Deviance Explained on Training DataTest Misclassification Error‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óèForward‚àíStepwise Logistic Regression\n",
      "Lasso Logistic Regression\n",
      "Figure 16.9 Test misclassiÔ¨Åcation error for lasso versus\n",
      "forward-stepwise logistic regression on the spam data, where we\n",
      "consider pairwise interactions as well as main effects (3061\n",
      "predictors in all). Here the minimum error for lasso is 0:057\n",
      "versus0:064 for stepwise logistic regression, and 0:071 for the\n",
      "main-effects-only lasso logistic regression model. The stepwise\n",
      "models went up to 134 variables before encountering convergence\n",
      "issues, while the lasso had a largest active set of size 682.\n",
      "and wide modern data sets (millions of examples and millions of variables),\n",
      "the lasso path algorithm is feasible and attractive.\n",
      "16.6 Post-Selection Inference for the Lasso\n",
      "This chapter is mostly about building interpretable models for prediction,\n",
      "with little attention paid to inference; indeed, inference is generally difÔ¨Åcult\n",
      "for adaptively selected models.\n",
      "Suppose we have Ô¨Åt a lasso regression model with a particular value for\n",
      "\u0015, which ends up selecting a subset Aof sizejAjDkof thepavail-\n",
      "able variables. The question arises as to whether we can assign p-values\n",
      "to these selected variables, and produce conÔ¨Ådence intervals for their co-\n",
      "efÔ¨Åcients. A recent burst of research activity has made progress on these\n",
      "important problems. We give a very brief survey here, with references ap-318 Sparse Modeling and the Lasso\n",
      "pearing in the notes.¬éWe discuss post-selection inference more generally ¬é6\n",
      "in Chapter 20.\n",
      "One question that arises is whether we are interested in making infer-\n",
      "ences about the population regression parameters using the full set of p\n",
      "predictors, or whether interest is restricted to the population regression pa-\n",
      "rameters using only the subset A.\n",
      "For the Ô¨Årst case, it has been proposed that one can view the coefÔ¨Åcients\n",
      "of the selected model as an efÔ¨Åcient but biased estimate of the full popu-\n",
      "lation coefÔ¨Åcient vector. The idea is to then debias this estimate, allowing\n",
      "inference for the full vector of coefÔ¨Åcients. Of course, sharper inference\n",
      "will be available for the stronger variables that were selected in the Ô¨Årst\n",
      "place.\n",
      "‚àí2 ‚àí1 0 1 2\n",
      "PredictorCoefficient\n",
      "s5 s8 s9 s16 s25 s26 s28Naive interval\n",
      "Selection‚àíadjusted interval\n",
      "Figure 16.10 HIV data. Linear regression of drug resistance in\n",
      "HIV-positive patients on seven sites, indicators of mutations at\n",
      "particular genomic locations. These seven sites were selected\n",
      "from a total of 30 candidates, using the lasso. The naive 95%\n",
      "conÔ¨Ådence intervals (dark) use standard linear-regression\n",
      "inference, ignoring the selection event. The light intervals are\n",
      "95% conÔ¨Ådence intervals, using linear regression, but conditioned\n",
      "on the selection event.\n",
      "For the second case, the idea is to condition on the selection event(s)\n",
      "and hence the set Aitself, and then perform conditional inference on the16.7 Connections and Extensions 319\n",
      "unrestricted (i.e. not lasso-shrunk) regression coefÔ¨Åcients of the response\n",
      "on only the variables in A. For the case of a lasso with squared-error loss,\n",
      "it turns out that the set of response vectors y2RNthat would lead to a\n",
      "particular subset Aof variables in the active set form a convex polytope\n",
      "inRN(if we condition on the signs of the coefÔ¨Åcients as well; ignoring\n",
      "the signs leads to a Ô¨Ånite union of such polytopes). This, along with del-\n",
      "icate Gaussian conditioning arguments, leads to truncated Gaussian and\n",
      "t-distrubtions for parameters of interest.\n",
      "Figure 16.10 shows the results of using the lasso to select variables in\n",
      "an HIV study. The outcome Yis a measure of the resistence to an HIV-1\n",
      "treatment (nucleoside reverse transcriptase inhibitor), and the 30 predictors\n",
      "are indicators of whether mutations had occurred at particular genomic\n",
      "sites. Lasso regression with 10-fold cross-validation selected a value of\n",
      "\u0015D0:003 and the seven sites indicated in the Ô¨Ågure had nonzero coefÔ¨Å-\n",
      "cients. The dark bars in the Ô¨Ågure indicate standard 95% conÔ¨Ådence inter-\n",
      "vals for the coefÔ¨Åcients of the selected variables, using linear regression,\n",
      "and ignoring the fact that the lasso was used to select the variables. Three\n",
      "variables are signiÔ¨Åcant, and two more nearly so. The lighter bars are con-\n",
      "Ô¨Ådence intervals in a similar regression, but conditioned on the selection\n",
      "event.¬éWe see that they are generally wider, and only variable s25remains¬é7\n",
      "signiÔ¨Åcant.\n",
      "16.7 Connections and Extensions\n",
      "There are interesting connections between lasso models and other popular\n",
      "approaches to the prediction problem. We will brieÔ¨Çy cover two of these\n",
      "here, namely support-vector machines and boosting.\n",
      "Lasso Logistic Regression and the SVM\n",
      "We show in Section 19.3 that ridged logistic regression has a lot in com-\n",
      "mon with the linear support-vector machine. For separable data the limit\n",
      "as\u0015#0in ridged logistic regression coincides with the SVM. In addition\n",
      "their loss functions are somewhat similar. The same holds true for `1regu-\n",
      "larized logistic regression versus the `1SVM‚Äîtheir end-path limits are the\n",
      "same. In fact, due to the similarity of the loss functions, their solutions are\n",
      "not too different elsewhere along the path. However, the end-path behavior\n",
      "is a little more complex. They both converge to the `1maximizing margin\n",
      "separator‚Äîthat is, the margin is measured with respect to the `1distance\n",
      "of points to the decision boundary, or maximum absolute coordinate.¬é¬é8320 Sparse Modeling and the Lasso\n",
      "Lasso and Boosting\n",
      "In Chapter 17 we discuss boosting, a general method for building a com-\n",
      "plex prediction model using simple building components. In its simplest\n",
      "form (regression) boosting amounts to the following simple iteration:\n",
      "1 Inititialize bD0andF0.x/WD0.\n",
      "2 ForbD1;2;:::;B :\n",
      "(a) compute the residuals riDyi\u0000Fb\u00001.xi/; iD1;:::;n ;\n",
      "(b) Ô¨Åt a small regression tree to the observations .xi;ri/n\n",
      "1, which we can\n",
      "think of as estimating a function gb.x/; and\n",
      "(c) updateFb.x/DFb\u00001.x/C\u000f\u0001gb.x/.\n",
      "The ‚Äúsmallness‚Äù of the tree limits the interaction order of the model (e.g.\n",
      "a tree with only two splits involves at most two variables). The number\n",
      "of termsBand the shrinkage parameter \u000fare both tuning parameters that\n",
      "control the rate of learning (and hence overÔ¨Åtting), and need to be set, for\n",
      "example by cross-validation.\n",
      "In words this algorithm performs a search in the space of trees for the one\n",
      "most correlated with the residual, and then moves the Ô¨Åtted function Fb\n",
      "a small amount in that direction‚Äîa process known as forward-stagewise\n",
      "Ô¨Åtting . One can paraphrase this simple algorithm in the context of linear\n",
      "regression, where in step 2(b) the space of small trees is replaced by linear\n",
      "functions.\n",
      "1 Inititialize Àá0D0, and standardize all the variables xj; jD1;:::;p .\n",
      "2 ForbD1;2;:::;B :\n",
      "(a) compute the residuals rDy\u0000XÀáb;\n",
      "(b) Ô¨Ånd the predictor xjmost correlated with the residual vector r; and\n",
      "(c) updateÀábtoÀábC1, whereÀábC1\n",
      "jDÀáb\n",
      "jC\u000f\u0001sj(sjbeing the sign of\n",
      "the correlation), leaving all the other components alone.\n",
      "For small\u000fthe solution paths for this least-squares boosting and the lasso\n",
      "are very similar. It is natural to consider the limiting case or inÔ¨Ånitesimal\n",
      "forward stagewise Ô¨Åtting, which we will abbreviate iFS. One can imagine\n",
      "a scenario where a number of variables are vying to win the competition\n",
      "in step 2(b), and once they are tied their coefÔ¨Åcients move in concert as\n",
      "they each get incremented. This was in fact the inspiration for the LAR\n",
      "algorithm 16.3, where Arepresents the set of tied variables, and ƒ±is the\n",
      "relative number of turns they each have in getting their coefÔ¨Åcients up-\n",
      "dated. It turns out that iFS is often but not always exactly the lasso; it can\n",
      "instead be characterized as a type of monotone lasso.¬é ¬é916.8 Notes and Details 321\n",
      "Not only do these connections inspire new insights and algorithms for\n",
      "the lasso, they also offer insights into boosting. We can think of boosting\n",
      "as Ô¨Åtting a monotone lasso path in the high-dimensional space of variables\n",
      "deÔ¨Åned by all possible trees of a certain size.\n",
      "Extensions of the Lasso\n",
      "The idea of using `1regularization to induce sparsity has taken hold, and\n",
      "variations of these ideas have spread like wildÔ¨Åre in applied statistical mod-\n",
      "eling. Along with advances in convex optimization, hardly any branch of\n",
      "applied statistics has been left untouched. We don‚Äôt go into detail here, but\n",
      "refer the reader to the references in the endnotes. Instead we will end this\n",
      "section with a (non-exhaustive) list of such applications, which may entice\n",
      "the reader to venture into this domain.\n",
      "\u000fThe group lasso penaltyPK\n",
      "kD1k\u0012kk2applies to vectors \u0012kof parame-\n",
      "ters, and selects whole groups at a time. Armed with these penalties, one\n",
      "can derive lasso-like schemes for including multilevel factors in linear\n",
      "models, as well as hierarchical schemes for including low-order interac-\n",
      "tions.\n",
      "\u000fThe graphical lasso applies `1penalties in the problem of edge selection\n",
      "in dependence graphs.\n",
      "\u000fSparse principal components employ `1penalties to produce compo-\n",
      "nents with many loadings zero. The same ideas are applied to discrimi-\n",
      "nant analysis and canonical correlation analysis.\n",
      "\u000fThe nuclear norm of a matrix is the sum of its singular values‚Äîa lasso\n",
      "penalty on matrices. Nuclear-norm regularization is popular in matrix\n",
      "completion for estimating missing entries in a matrix.\n",
      "16.8 Notes and Details\n",
      "Classical regression theory aimed for an unbiased estimate of each predic-\n",
      "tor variable‚Äôs effect. Modern wide data sets, often with enormous numbers\n",
      "of predictors p, make that an untenable goal. The methods described here,\n",
      "by necessity, use shrinkage methods, biased estimation, and sparsity.\n",
      "The lasso was introduced by Tibshirani (1996), and has spawned a great\n",
      "deal of research. The recent monograph by Hastie et al. (2015) gives a\n",
      "compact summary of some of the areas where the lasso and sparsity have\n",
      "been applied. The regression version of boosting was given in Hastie et al.\n",
      "(2009, Chapter 16), and inspired the least-angle regression algorithm (Efron322 Sparse Modeling and the Lasso\n",
      "et al. , 2004)‚Äîa new and more democratic version of forward-stepwise re-\n",
      "gression, as well as a fast algorithm for Ô¨Åtting the lasso. These authors\n",
      "showed under some conditions that each step of the LAR algorithm corre-\n",
      "sponds to one df; Zou et al. (2007) show that, with a Ô¨Åxed \u0015, the size of\n",
      "the active set is unbiased for the df for the lasso. Hastie et al. (2009) also\n",
      "view boosting as Ô¨Åtting a lasso regularization path in the high-dimensional\n",
      "space of trees.\n",
      "Friedman et al. (2010) developed the pathwise coordinate-descent algo-\n",
      "rithm for generalized lasso problems, and provide the glmnet package\n",
      "forR(Friedman et al. , 2009). Strong rules for lasso screening are due to\n",
      "Tibshirani et al. (2012). Hastie et al. (2015, Chapter 3) show the similarity\n",
      "between the`1SVM and lasso logistic regression.\n",
      "We now give some particular technical details on topics covered in the\n",
      "chapter.\n",
      "¬é1[p. 301] Forward-stepwise computations. Building up the forward-stepwise\n",
      "model can be seen as a guided Gram‚ÄìSchmidt orthogonalization (QR de-\n",
      "composition). After step r, allp\u0000rvariables not in the model are orthog-\n",
      "onal to therin the model, and the latter are in QR form. Then the next\n",
      "variable to enter is the one most correlated with the residuals. This is the\n",
      "one that will reduce the residual sum-of-squares the most, and one requires\n",
      "p\u0000rn-vector inner products to identify it. The regression is then updated\n",
      "trivially to accommodate the chosen one, which is then regressed out of the\n",
      "p\u0000r\u00001remaining variables.\n",
      "¬é2[p. 301] Iteratively reweighted least squares (IRLS). Generalized linear\n",
      "models (Chapter 8) are Ô¨Åt by maximum-likelihood, and since the log-likeli-\n",
      "hood is differentiable and concave, typically a Newton algorithm is used.\n",
      "The Newton algorithm can be recast as an iteratively reweighted linear re-\n",
      "gression algorithm (McCullagh and Nelder, 1989). At each iteration one\n",
      "computes a working response variablezi, and a weight per observation wi\n",
      "(both of which depend on the current parameter vector OÀá). Then the New-\n",
      "ton update forOÀáis obtained by a weighted least-squares Ô¨Åt of the zion the\n",
      "xiwith weights wi(Hastie et al. , 2009, Section 4.4.1).\n",
      "¬é3[p. 301] Forward-stepwise logistic regression computations. Although the\n",
      "current model is in the form of a weighted least-squares Ô¨Åt, the p\u0000rvari-\n",
      "ables not in the model cannot be kept orthogonal to those in the model (the\n",
      "weights keep changing!). However, since our current model will have per-\n",
      "formed a weighted QR decomposition (say), this orthogonalization can be\n",
      "obtained without too much cost. We will need p\u0000rmultiplications of an\n",
      "r\u0002nmatrix with an nvector‚ÄîO..p\u0000r/\u0001r\u0001n/computations. An even\n",
      "simpler alternative for the selection is to use the size of the gradient of the16.8 Notes and Details 323\n",
      "log-likelihood, which simply requires an inner product jhy\u0000O\u0016r;xjijfor\n",
      "each omitted variable xj(assuming all the variables are standardized to\n",
      "unit variance).\n",
      "¬é4[p. 306] Best`1interpolant. Ifp > n , then another boundary solution\n",
      "becomes interesting for the lasso. For tsufÔ¨Åciently large, we will be able\n",
      "to achieve a perfect Ô¨Åt to the data, and hence a zero residual. There will\n",
      "be many such solutions, so it becomes interesting to Ô¨Ånd the perfect-Ô¨Åt so-\n",
      "lution with smallest value of t: the minimum- `1-norm perfect-Ô¨Åt solution.\n",
      "This requires solving a separate convex-optimization problem.\n",
      "¬é5[p. 313] More on df. When the search is easy in that a variable stands out as\n",
      "far superior, LAR takes a big step, and forward stepwise spends close to a\n",
      "unit df. On the other hand, when there is close competition, the LAR steps\n",
      "are small, and a unit df is spent for little progress, while forward stepwise\n",
      "can spend a fair bit more than a unit df (the price paid for searching). In\n",
      "fact, the df jcurve for forward stepwise can exceed pforj < p (Jansen\n",
      "et al. , 2015).\n",
      "¬é6[p. 318] Post-selection inference. There has been a lot of activity around\n",
      "post-selection inference for lasso and related methods, all of it since 2012.\n",
      "To a large extent this was inspired by the work of Berk et al. (2013), but\n",
      "more tailored to the particular selection process employed by the lasso. For\n",
      "the debiasing approach we look to the work of Zhang and Zhang (2014),\n",
      "van de Geer et al. (2014) and Javanmard and Montanari (2014). The condi-\n",
      "tional inference approach began with Lockhart et al. (2014), and then was\n",
      "developed further in a series of papers (Lee et al. , 2016; Taylor et al. , 2015;\n",
      "Fithian et al. , 2014), with many more in the pipeline.\n",
      "¬é7[p. 319] Selective inference software. The example in Figure 16.10 was pro-\n",
      "duced using the Rpackage selectiveInference (Tibshirani et al. ,\n",
      "2016). Thanks to Rob Tibshirani for providing this example.\n",
      "¬é8[p. 319] End-path behavior of ridge and lasso logistic regression for sep-\n",
      "arable data. The details here are somewhat technical, and rely on dual\n",
      "norms. Details are given in Hastie et al. (2015, Section 3.6.1).\n",
      "¬é9[p. 320] LAR and boosting. Least-squares boosting moves the ‚Äúwinning‚Äù\n",
      "coefÔ¨Åcient in the direction of the correlation of its variable with the resid-\n",
      "ual. The direction ƒ±computed in step 3(a) of the LAR algorithm may have\n",
      "some components whose signs do not agree with their correlations, espe-\n",
      "cially if the variables are very correlated. This can be Ô¨Åxed by a particular\n",
      "nonnegative least-squares Ô¨Åt to yield an exact path algorithm for iFS; de-\n",
      "tails can be found in Efron et al. (2004).17\n",
      "Random Forests and Boosting\n",
      "In the modern world we are often faced with enormous data sets, both\n",
      "in terms of the number of observations nand in terms of the number of\n",
      "variablesp. This is of course good news‚Äîwe have always said the more\n",
      "data we have, the better predictive models we can build. Well, we are there\n",
      "now‚Äîwe have tons of data, and must Ô¨Ågure out how to use it.\n",
      "Although we can scale up our software to Ô¨Åt the collection of linear and\n",
      "generalized linear models to these behemoths, they are often too modest\n",
      "and can fall way short in terms of predictive power. A need arose for some\n",
      "general purpose tools that could scale well to these bigger problems, and\n",
      "exploit the large amount of data by Ô¨Åtting a much richer class of functions,\n",
      "almost automatically. Random forests and boosting are two relatively re-\n",
      "cent innovations that Ô¨Åt the bill, and have become very popular as ‚Äúout-the-\n",
      "box‚Äù learning algorithms that enjoy good predictive performance. Random\n",
      "forests are somewhat more automatic than boosting, but can also suffer a\n",
      "small performance hit as a consequence.\n",
      "These two methods have something in common: they both represent the\n",
      "Ô¨Åtted model by a sum of regression trees. We discuss trees in some detail\n",
      "in Chapter 8. A single regression tree is typically a rather weak prediction\n",
      "model; it is rather amazing that an ensemble of trees leads to the state of\n",
      "the art in black-box predictors!\n",
      "We can broadly describe both these methods very simply.\n",
      "Random forest Grow many deep regression trees to randomized versions\n",
      "of the training data, and average them. Here ‚Äúrandomized‚Äù is a wide-\n",
      "ranging term, and includes bootstrap sampling and/or subsampling of\n",
      "the observations, as well as subsampling of the variables.\n",
      "Boosting Repeatedly grow shallow trees to the residuals, and hence build\n",
      "up an additive model consisting of a sum of trees.\n",
      "The basic mechanism in random forests is variance reduction by averag-\n",
      "ing. Each deep tree has a high variance, and the averaging brings the vari-\n",
      "32417.1 Random Forests 325\n",
      "ance down. In boosting the basic mechanism is bias reduction, although\n",
      "different Ô¨Çavors include some variance reduction as well. Both methods\n",
      "inherit all the good attributes of trees, most notable of which is variable\n",
      "selection.\n",
      "17.1 Random Forests\n",
      "Suppose we have the usual setup for a regression problem, with a training\n",
      "set consisting of an n\u0002pdata matrixXand ann-vector of responses y. A\n",
      "tree (Section 8.4) Ô¨Åts a piecewise constant surface Or.x/ over the domain X\n",
      "by recursive partitioning. The model is built in a greedy fashion, each time\n",
      "creating two daughter nodes from a terminal node by deÔ¨Åning a binary split\n",
      "using one of the available variables. The model can hence be represented\n",
      "by a binary tree. Part of the art in using regression trees is to know how\n",
      "deep to grow the tree, or alternatively how much to prune it back. Typi-\n",
      "cally that is achieved using left-out data or cross-validation. Figure 17.1\n",
      "shows a tree Ô¨Åt to the spam training data. The splitting variables and split\n",
      "points are indicated. Each node is labeled as spam orham (not spam;\n",
      "see footnote 7 on page 115). The numbers beneath each node show mis-\n",
      "classiÔ¨Åed/total. The overall misclassiÔ¨Åcation error on the test data is 9:3%,\n",
      "which compares poorly with the performance of the lasso (Figure 16.9:\n",
      "7:1% for linear lasso, 5:7% for lasso with interactions). The surface Or.x/\n",
      "here is clearly complex, and by its nature represents a rather high-order\n",
      "interaction (the deepest branch is eight levels, and involves splits on eight\n",
      "different variables). Despite the promise to deliver interpretable models,\n",
      "this bushy tree is not easy to interpret. Nevertheless, trees have some desir-\n",
      "able properties. The following lists some of the good and bad properties of\n",
      "trees.\n",
      "LTrees automatically select variables; only variables used in deÔ¨Åning splits\n",
      "areinthe model.\n",
      "LTree-growing algorithms scale well to large n; growing a tree is a divide-\n",
      "and-conquer operation.\n",
      "LTrees handle mixed features (quantitative/qualitative) seamlessly, and\n",
      "can deal with missing data.\n",
      "LSmall trees are easy to interpret.\n",
      "MLarge trees are not easy to interpret.\n",
      "MTrees do not generally have good prediction performance.\n",
      "Trees are inherently high-variance function estimators, and the bushier\n",
      "they are, the higher the variance. The early splits dictate the architecture of326 Random Forests and Boosting\n",
      "600/1536\n",
      "280/1177\n",
      "180/1065\n",
      " 80/861\n",
      " 80/652\n",
      " 77/423\n",
      " 20/238\n",
      " 19/236   1/2 57/185\n",
      " 48/113\n",
      " 37/101   1/12  9/72  3/229  0/209100/204\n",
      " 36/123\n",
      " 16/94\n",
      " 14/89   3/5  9/29 16/81  9/112\n",
      "  6/109   0/3 48/359\n",
      " 26/337\n",
      " 19/110\n",
      " 18/109   0/1  7/227  0/22\n",
      "spam\n",
      "spamspamspamspam\n",
      "spamspam\n",
      "spam\n",
      "spam\n",
      "spam\n",
      "spamspamham\n",
      "hamham\n",
      "hamhamhamham\n",
      "ham\n",
      "ham\n",
      "ham\n",
      "hamhamham\n",
      "hamhamhamhamhamhamhamham\n",
      "ch$<0.0555\n",
      "remove<0.06\n",
      "ch!<0.191\n",
      "george<0.005\n",
      "hp<0.03\n",
      "CAPMAX<10.5\n",
      "receive<0.125 edu<0.045\n",
      "our<1.2CAPAVE<2.7505\n",
      "free<0.065\n",
      "business<0.145george<0.15hp<0.405\n",
      "CAPAVE<2.907\n",
      "1999<0.58ch$>0.0555\n",
      "remove>0.06\n",
      "ch!>0.191\n",
      "george>0.005\n",
      "hp>0.03\n",
      "CAPMAX>10.5\n",
      "receive>0.125 edu>0.045\n",
      "our>1.2CAPAVE>2.7505\n",
      "free>0.065\n",
      "business>0.145george>0.15hp>0.405\n",
      "CAPAVE>2.907\n",
      "1999>0.58\n",
      "Figure 17.1 Regression tree Ô¨Åt to the binary spam data, a bigger\n",
      "version of Figure 8.7. The initial trained tree was far bushier than\n",
      "the one displayed; it was then optimally pruned using 10-fold\n",
      "cross-validation.\n",
      "the tree. On the other hand, deep bushy trees localize the training data (us-\n",
      "ing the variables that matter) to a relatively small region around the target\n",
      "point. This suggests low bias. The idea of random forests (and its predeces-\n",
      "sor bagging) is to grow many very bushy trees, and get rid of the variance\n",
      "by averaging. In order to beneÔ¨Åt from averaging, the individual trees should\n",
      "not be too correlated. This is achieved by injecting some randomness into\n",
      "the tree-growing process. Random forests achieve this in two ways.17.1 Random Forests 327\n",
      "1 Bootstrap: each tree is grown to a bootstrap resampled training data set,\n",
      "which makes them different and somewhat decorrelates them.\n",
      "2 Split-variable randomization: each time a split is to be performed, the\n",
      "search for the split variable is limited to a random subset of mof thep\n",
      "variables. Typical values of marepporp=3.\n",
      "WhenmDp, the randomization amounts to using only step 1, and was\n",
      "an earlier ancestor of random forests called bagging . In most examples the\n",
      "second level of randomization pays dividends.\n",
      "Algorithm 17.1 RANDOM FOREST .\n",
      "1 Given training data set dD.X;y/. Fixm\u0014pand the number of trees\n",
      "B.\n",
      "2 ForbD1;2;:::;B , do the following.\n",
      "(a) Create a bootstrap version of the training data d\u0003\n",
      "b, by randomly sam-\n",
      "pling thenrows with replacement ntimes. The sample can be repre-\n",
      "sented by the bootstrap frequency vector w\u0003\n",
      "b.\n",
      "(b) Grow a maximal-depth tree Orb.x/using the data in d\u0003\n",
      "b, samplingm\n",
      "of thepfeatures at random prior to making each split.\n",
      "(c) Save the tree, as well as the bootstrap sampling frequencies for each\n",
      "of the training observations.\n",
      "3 Compute the random-forest Ô¨Åt at any prediction point x0as the average\n",
      "Orrf.x0/D1\n",
      "BBX\n",
      "bD1Orb.x0/:\n",
      "4 Compute the OOB ierror for each response observation yiin the training\n",
      "data, by using the Ô¨Åt Or.i/\n",
      "rf, obtained by averaging only those Orb.xi/for\n",
      "which observation iwasnotin the bootstrap sample. The overall OOB\n",
      "error is the average of these OOB i.\n",
      "Algorithm 17.1 gives some of the details; some more are given in the\n",
      "technical notes.¬é ¬é1\n",
      "Random forests are easy to use, since there is not much tuning needed.\n",
      "The package randomForest inRsets as a default mDppfor clas-\n",
      "siÔ¨Åcation trees, and mDp=3 for regression trees, but one can use other\n",
      "values. With mD1the split variable is completely random, so all vari-\n",
      "ables get a chance. This will decorrelate the trees the most, but can create\n",
      "bias, somewhat similar to that in ridge regression. Figure 17.2 shows the328 Random Forests and Boosting\n",
      "0.00 0.02 0.04 0.06 0.08Random Forest on the Spam Data\n",
      "Number of TreesTest Error\n",
      "1 500 1000 1500 2000 2500Bagging\n",
      "Random Forest\n",
      "Single Tree\n",
      "Lasso\n",
      "Lasso (interaction)\n",
      "Figure 17.2 Test misclassiÔ¨Åcation error of random forests on the\n",
      "spam data, as a function of the number of trees. The red curve\n",
      "selectsmD7of thepD57features at random as candidates for\n",
      "the split variable, each time a split is made. The blue curve uses\n",
      "mD57, and hence amounts to bagging. Both bagging and\n",
      "random forests outperform the lasso methods, and a single tree.\n",
      "misclassiÔ¨Åcation performance of a random forest on the spam test data, as\n",
      "a function of the number of trees averaged. We see that in this case, after\n",
      "a relatively small number of trees (500), the error levels off. The number\n",
      "Bof trees averaged is not a real tuning parameter; as with the bootstrap\n",
      "(Chapters 10 and 11), we need a sufÔ¨Åcient number for the estimate to sta-\n",
      "bilize, but cannot overÔ¨Åt by having too many.\n",
      "Random forests have been described as adaptive nearest-neighbor esti-\n",
      "mators‚Äîadaptive in that they select predictors. A k-nearest-neighbor esti-\n",
      "mate Ô¨Ånds the ktraining observations closest in feature space to the target\n",
      "pointx0, and averages their responses. Each tree in the random forest drills\n",
      "down by recursive partitioning to pure terminal nodes, often consisting of a\n",
      "single observation. Hence, when evaluating the prediction from each tree,\n",
      "Orb.x0/Dy`for some`, and for many of the trees this could be the same17.1 Random Forests 329\n",
      "`. From the whole collection of Btrees, the number of distinct `s can be\n",
      "fairly small. Since the partitioning that reaches the terminal nodes involves\n",
      "only a subset of the predictors, the neighborhoods so deÔ¨Åned are adaptive.\n",
      "Out-of-Bag Error Estimates\n",
      "Random forests deliver cross-validated error estimates at virtually no ex-\n",
      "tra cost. The idea is similar to the bootstrap error estimates discussed in\n",
      "Chapter 10. The computation is described in step 4 of Algorithm 17.1.\n",
      "In making the prediction for observation pair .xi;yi/, we average all the\n",
      "random-forest trees Orb.xi/for which that pair is not in the corresponding\n",
      "bootstrap sample:\n",
      "0.000.020.040.060.08\n",
      "Number of TreesMisclassification Error\n",
      "1 500 1000 1500 2000 2500OOB Error\n",
      "Test Error\n",
      "Figure 17.3 Out-of-bag misclassiÔ¨Åcation error estimate for the\n",
      "spam data (blue) versus the test error (red), as a function of the\n",
      "number of trees.\n",
      "Or.i/\n",
      "rf.xi/D1\n",
      "BiX\n",
      "bWw\u0003\n",
      "biD0Orb.xi/; (17.1)\n",
      "whereBiis the number of times observation iwas not in the bootstrap\n",
      "sample (with expected value e\u00001B\u00190:37B ). We then compute the OOB\n",
      "error estimate\n",
      "err OOBD1\n",
      "nnX\n",
      "iD1L≈íyi;Or.i/\n",
      "rf.xi/¬ç; (17.2)330 Random Forests and Boosting\n",
      "whereLis the loss function of interest, such as misclassiÔ¨Åcation or squared-\n",
      "error loss. If Bis sufÔ¨Åciently large (about 1:58 times the number needed\n",
      "for the random forest to stabilize), we can see that the OOB error estimate\n",
      "is equivalent to leave-one-out cross-validation error.\n",
      "Standard Errors\n",
      "We can use very similar ideas to estimate the variance of a random-forest\n",
      "prediction, using the jackknife variance estimator (see (10.6) in Chapter 10).\n",
      "IfO\u0012is a statistic estimated using all ntraining observations, then the jack-\n",
      "knife estimate of the variance of O\u0012is given by\n",
      "bVjack.O\u0012/Dn\u00001\n",
      "nnX\n",
      "iD1\u0010\n",
      "O\u0012.i/\u0000O\u0012.\u0001/\u00112\n",
      "; (17.3)\n",
      "whereO\u0012.i/is the estimate using all but observation i, andO\u0012.\u0001/D1\n",
      "nP\n",
      "iO\u0012.i/.\n",
      "The natural jackknife variance estimate for a random-forest prediction\n",
      "atx0is obtained by simply plugging into this formula:\n",
      "bVjack.Orrf.x0//Dn\u00001\n",
      "nnX\n",
      "iD1\u0010\n",
      "Or.i/\n",
      "rf.x0/\u0000Orrf.x0/\u00112\n",
      ": (17.4)\n",
      "This formula is derived under the BD1 setting, in which case Orrf.x0/is\n",
      "an expectation under bootstrap sampling, and hence is free of Monte Carlo\n",
      "variability. This also makes the distinction clear: we are estimating the sam-\n",
      "pling variability of a random-forest prediction Orrf.x0/, as distinct from any\n",
      "Monte Carlo variation. In practice Bis Ô¨Ånite, and expression (17.4) will\n",
      "have Monte Carlo bias and variance. All of the Or.i/\n",
      "rf.x0/are based on B\n",
      "bootstrap samples, and they are hence noisy versions of their expectations.\n",
      "Since thenquantities summed in (17.4) are squared, by Jensen‚Äôs inequal-\n",
      "ity we will have positive bias (and it turns out that this bias dominates the\n",
      "Monte Carlo variance). Hence one would want to use a much larger value\n",
      "ofBwhen estimating variances, than was used in the original random-\n",
      "forest Ô¨Åt. Alternatively, one can use the same Bbootstrap samples as were\n",
      "used to Ô¨Åt the random forest, along with a bias-corrected version of the\n",
      "jackknife variance estimate:¬é ¬é2\n",
      "bVu\n",
      "jack.Orrf.x0//DbVjack.Orrf.x0//\u0000.e\u00001/n\n",
      "BOv.x0/; (17.5)17.1 Random Forests 331\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè ‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè ‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè ‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè ‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè ‚óè ‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè ‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè ‚óè ‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè ‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè ‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè ‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè ‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè ‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè ‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè ‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.000.050.100.150.200.250.30\n",
      "Probability PredictionStandard Error Estimate\n",
      "Figure 17.4 Jackknife Standard Error estimates (with bias\n",
      "correction) for the probability estimates in the spam test data.\n",
      "The points labeled red were misclassiÔ¨Åcations, and tend to\n",
      "concentrate near the decision boundary (0.5).\n",
      "whereeD2:718::: , and\n",
      "Ov.x0/D1\n",
      "BBX\n",
      "bD1.Orb.x0/\u0000Orrf.x0//2; (17.6)\n",
      "the bootstrap estimate of the variance of a single random-forest tree. All\n",
      "these quantities are easily computed from the output of a random forest, so\n",
      "they are immediately available. Figure 17.4 shows the predicted probabil-\n",
      "ities and their jackknife estimated standard errors for the spam test data.\n",
      "The estimates near the decision boundary tend to have higher standard er-\n",
      "rors.\n",
      "Variable-Importance Plots\n",
      "A random forest is something of a black box, giving good predictions\n",
      "but usually not much insight into the underlying surface it has Ô¨Åt. Each\n",
      "random-forest tree Orbwill have used a subset of the predictors as split-\n",
      "ting variables, and each tree is likely to use overlapping but not necessarily332 Random Forests and Boosting\n",
      "!$removefreeCAPAVEyourCAPMAXhpCAPTOTmoneyyouourgeorge000edubusinesshpl1999internet(willallrereceiveemailmailover;meeting650addressorderlabspmpeoplecreditmake#technologydatafont85[labtelnetreportoriginalprojectconferenceaddressesdirect4158573dcspartstableRandom Forest m = 7\n",
      "0 20 40 60 80 100\n",
      "Variable Importance$!removehpCAPAVEfreeCAPTOTCAPMAXgeorgeyoueduyourourbusinessmoneywill(reemail000internetreceivemailmeeting1999overpm;dataorderfontpeopleallmakeconference650reportaddresshploriginaltechnologylab3dlabs[#creditcs85telnetpartsproject857addressesdirect415tableBagging m = 57\n",
      "0 20 40 60 80 100\n",
      "Variable Importance\n",
      "Figure 17.5 Variable-importance plots for random forests Ô¨Åt to\n",
      "thespam data. On the left we have the mD7random forest; due\n",
      "to the split-variable randomization, it spreads the importance\n",
      "among the variables. On the right is the mD57random forest or\n",
      "bagging, which focuses on a smaller subset of the variables.\n",
      "identical subsets. One might conclude that any variable never used in any\n",
      "of the trees is unlikely to be important, but we would like a method of\n",
      "assessing the relative importance of variables that are included in the en-\n",
      "semble. Variable-importance plots Ô¨Åt this bill. Whenever a variable is used\n",
      "in a tree, the algorithm logs the decrease in the split-criterion due to this\n",
      "split. These are accumulated over all the trees, for each variable, and sum-\n",
      "marized as relative importance measures. Figure 17.5 demonstrates this on\n",
      "thespam data. We see that the mD7random forest, by virtue of the\n",
      "split-variable randomization, spreads the importance out much more than\n",
      "bagging, which always gets to pick the best variable for splitting. In this\n",
      "sense smallmhas some similarity to ridge regression, which also tends to\n",
      "share the coefÔ¨Åcients evenly among correlated variables.17.2 Boosting with Squared-Error Loss 333\n",
      "17.2 Boosting with Squared-Error Loss\n",
      "Boosting was originally proposed as a means for improving the perfor-\n",
      "mance of ‚Äúweak learners‚Äù in binary classiÔ¨Åcation problems. This was achiev-\n",
      "ed through resampling training points‚Äîgiving more weight to those which\n",
      "had been misclassiÔ¨Åed‚Äîto produce a new classiÔ¨Åer that would boost the\n",
      "performance in previously problematic areas of feature space. This pro-\n",
      "cess is repeated, generating a stream of classiÔ¨Åers, which are ultimately\n",
      "combined through voting1to produce the Ô¨Ånal classiÔ¨Åer. The prototypical\n",
      "weak learner was a decision tree.\n",
      "Boosting has evolved since this earliest invention, and different Ô¨Çavors\n",
      "are popular in statistics, computer science, and other areas of pattern recog-\n",
      "nition and prediction. We focus on the version popular in statistics‚Äîgradient\n",
      "boosting‚Äîand return to this early version later in the chapter.\n",
      "Algorithm 17.2 GRADIENT BOOSTING WITH SQUARED -ERROR LOSS .\n",
      "1 Given a training sample dD.X;y/. Fix the number of steps B, the\n",
      "shrinkage factor \u000fand the tree depth d. Set the initial Ô¨Åt bG0\u00110, and\n",
      "the residual vector rDy.\n",
      "2 ForbD1;2;:::;B repeat:\n",
      "(a) Fit a regression tree Qgbto the data.X;r/, grown best-Ô¨Årst to depth\n",
      "d: this means the total number of splits are d, and each successive\n",
      "split is made to that terminal node that yields the biggest reduction in\n",
      "residual sum of squares.\n",
      "(b) Update the Ô¨Åtted model with a shrunken version of Qgb:bGbDbGb\u00001C\n",
      "Ogb;withOgbD\u000f\u0001Qgb:\n",
      "(c) Update the residuals accordingly: riDri\u0000Ogb.xi/; iD1;:::;n:\n",
      "3 Return the sequence of Ô¨Åtted functions bGb; bD1;:::;B .\n",
      "Algorithm 17.2 gives the most basic version of gradient boosting, for\n",
      "squared-error loss. This amounts to building a model by repeatedly Ô¨Åtting\n",
      "a regression tree to the residuals. Importantly, the tree is typically quite\n",
      "small, involving a small number dof splits‚Äîit is indeed a weak learner .\n",
      "After each tree has been grown to the residuals, it is shrunk down by a\n",
      "factor\u000fbefore it is added to the current model; this is a means of slowing\n",
      "the learning process. Despite the obvious similarities with a random forest,\n",
      "boosting is different in a fundamental way. The trees in a random forest are\n",
      "1Each classiÔ¨ÅerOcb.x0/predicts a class label, and the class with the most ‚Äúvotes‚Äù wins.334 Random Forests and Boosting\n",
      "0.25 0.26 0.27 0.28 0.29 0.30\n",
      "Number of TreesMean‚àísquared Error\n",
      "1 100 200 300 400 500Boost\n",
      "Random Forest\n",
      "Lasso\n",
      "Figure 17.6 Test performance of a boosted regression-tree model\n",
      "Ô¨Åt to the ALS training data, with nD1197 andpD369. Shown\n",
      "is the mean-squared error on the 625designated test observations,\n",
      "as a function of the number of trees. Here the depth dD4and\n",
      "\u000fD0:02. Boosting achieves a lower test MSE than a random\n",
      "forest. We see that as the number of trees Bgets large, the test\n",
      "error for boosting starts to increase‚Äîa consequence of\n",
      "overÔ¨Åtting. The random forest does not overÔ¨Åt. The dotted blue\n",
      "horizontal line shows the best performance of a linear model, Ô¨Åt\n",
      "by the lasso. The differences are less dramatic than they appear,\n",
      "since the vertical scale does not extend to zero.\n",
      "identically distributed‚Äîthe same (random) treatment is repeatedly applied\n",
      "to the same data. With boosting, on the other hand, each tree is trying to\n",
      "amend errors made by the ensemble of previously grown trees. The number\n",
      "of termsBis important as well, because unlike random forests, a boosted\n",
      "regression model can overÔ¨Åt if Bis too large. Hence there are three tuning\n",
      "parameters,B,dand\u000f, and each can change the performance of a boosted\n",
      "model, sometimes considerably.\n",
      "Figure 17.6 shows the test performance of boosting on the ALS data.¬é ¬é3\n",
      "These data represent measurements on patients with amyotrophic lateral\n",
      "sclerosis (Lou Gehrig‚Äôs disease). The goal is to predict the rate of pro-\n",
      "gression of an ALS functional rating score ( FRS). There are 1197 training17.2 Boosting with Squared-Error Loss 335\n",
      "measurements on 369 predictors and the response, with a corresponding\n",
      "test set of size 625 observations.\n",
      "As is often the case, boosting slightly outperforms a random forest here,\n",
      "but at a price. Careful tuning of boosting requires considerable extra work,\n",
      "with time-costly rounds of cross-validation, whereas random forests are al-\n",
      "most automatic. In the following sections we explore in more detail some\n",
      "of the tuning parameters. The Rpackage gbm implements gradient boost-\n",
      "ing, with some added bells and whistles. By default it grows each new tree\n",
      "on a 50% random sub-sample of the training data. Apart from speeding up\n",
      "the computations, this has a similar effect to bagging, and results in some\n",
      "variance reduction in the ensemble.\n",
      "We can also compute a variable-importance plot, as we did for random\n",
      "forests; this is displayed in Figure 17.7 for the ALS data. Only 267 of the\n",
      "369 variables were ever used, with one variable Onset.Delta standing\n",
      "out ahead of the others. This measures the amount of time that has elapsed\n",
      "since the patient was Ô¨Årst diagnosed with ALS, and hence a larger value\n",
      "will indicate a slower progression rate.\n",
      "Tree Depth and Interaction Order\n",
      "Tree depthdis an important parameter for gradient boosted models, and\n",
      "the right choice will depend on the data at hand. Here depth dD4appears\n",
      "to be a good choice on the test data. Without test data, we could use cross-\n",
      "validation to make the selection. Apart from a general complexity measure,\n",
      "tree depth also controls the interaction order of the model.2The easiest\n",
      "case is withdD1, where each tree consists of a single split (a stump).\n",
      "Suppose we have a Ô¨Åtted boosted model bGB.x/, usingBtrees. Denote by\n",
      "Bj\u0012BDf1;2;:::;Bgthe indices of the trees that made the single split\n",
      "using variable j, forjD1;:::;p . These Bjare disjoint (some B`can be\n",
      "2A.k\u00001/th-order interaction is also known as a k-way interaction. Hence an order-one\n",
      "interaction model has two-way interactions, and an order-zero model is additive.336 Random Forests and Boosting\n",
      "Onset.Deltalast.slope.weightalsfrs.score.slopelast.slope.bp.systolicmean.slope.svc.literssum.slope.alsfrs.scoremean.slope.weightlast.slope.fvc.litersmeansquares.alsfrs.scoresum.slope.fvc.literslast.alsfrs.scorefvc.liters.slopesum.bp.diastolicweight.slopemean.speechmax.slope.fvc.litersmean.slope.handwritingslope.bp.systolic.slopemean.slope.fvc.litersmin.slope.alsfrs.scoresd.fvc.litersmeansquares.climbing.stairssd.salivationlast.speechmin.slope.bp.systolicmean.alsfrs.scoremax.slope.salivationsum.slope.climbing.stairslast.fvc.literssum.fvc.litersmean.slope.alsfrs.scorebp.diastolic.slopelast.slope.handwritingmin.slope.fvc.litersspeech.slopesd.slope.alsfrs.scoremeansquares.slope.weightslope.handwriting.slopeslope.weight.slopesum.alsfrs.scoreslope.turning.slopelast.slope.swallowingsum.slope.speechsum.weightmax.fvc.literslast.svc.liters.datemax.bp.diastolicmin.alsfrs.scoremean.climbing.stairsmax.dressingmeansquares.bp.diastoliclast.weightsum.slope.weightmeansquares.resp.rateslope.fvc.liters.slopemean.slope.climbing.stairsswallowing.slopesum.slope.handwritinglast.slope.alsfrs.score.datemeansquares.dressingsd.bp.diastolicclimbing.stairs.slopeslope.alsfrs.score.slopelast.slope.svc.liters.datesum.handwritingmean.slope.speechsum.slope.dressingsd.bp.systolicmin.slope.cuttingmax.weightlast.slope.alsfrs.scoremean.fvc.litersslope.bp.diastolic.slopeslope.salivation.slopemeansquares.slope.dressingmin.slope.speechsd.slope.fvc.litersmax.alsfrs.scoresum.slope.bp.systolicmeansquares.slope.bp.diastolicsd.slope.bp.systolicslope.dressing.slopemin.slope.dressinglast.slope.climbing.stairsmax.slope.alsfrs.scoremax.slope.bp.systolicsd.dressingsum.climbing.stairssd.slope.weightslope.resp.rate.slopesd.resp.ratemean.slope.bp.systoliclast.slope.weight.datemax.heightlast.slope.svc.litersmin.bp.systolicsd.slope.turningsum.slope.salivationfirst.slope.fvc.liters.datelast.bp.diastolicsd.slope.svc.literssd.slope.dressingmeansquares.slope.fvc.litersmeansquares.speechmeansquares.fvc.literslast.alsfrs.score.datesd.slope.resp.ratemeansquares.slope.cuttingmin.slope.turningmin.fvc.litersslope.swallowing.slopelast.slope.bp.diastolicmax.slope.weightsd.slope.swallowinglast.slope.resp.rate.datemean.turningmean.bp.diastolicsum.dressingmeansquares.cuttingsum.resp.ratemeansquares.slope.turningbp.systolic.slopemean.slope.dressingsd.cuttingmean.slope.bp.diastolicsum.heightsalivation.slopemin.slope.weightfirst.slope.weight.datesum.slope.resp.ratemax.slope.swallowinglast.swallowingmean.slope.cuttingmax.slope.svc.litersmean.slope.turningmax.cuttinglast.turningmean.slope.resp.ratesum.bp.systolicmeansquares.slope.climbing.stairsfirst.slope.resp.rate.datesd.slope.salivationslope.cutting.slopelast.resp.rate.datesd.alsfrs.scoresd.slope.cuttingSite.of.Onset.Onset..Bulbarfirst.slope.svc.liters.datemin.turningsd.slope.bp.diastolicmin.cuttingmax.slope.resp.rateAgesum.slope.swallowingsum.swallowingsd.speechlast.dressingfirst.slope.alsfrs.score.datemean.slope.salivationmeansquares.handwritingmin.weightlast.bp.systolicmean.salivationmeansquares.slope.alsfrs.scoremin.slope.climbing.stairsmean.slope.swallowinglast.slope.fvc.liters.datesum.salivationmin.slope.salivationmean.dressingslope.climbing.stairs.slopemeansquares.slope.walkingmeansquares.slope.bp.systolicresp.rate.slopelast.slope.dressinglast.slope.resp.ratemax.slope.dressingSex.Femalemin.swallowingmax.bp.systolicmin.bp.diastolicmeansquares.slope.handwritingmin.slope.bp.diastolicfirst.slope.bp.diastolic.datemean.handwritinglast.height.datesd.svc.literssd.slope.handwritingmean.slope.walkingmean.cuttingsum.slope.bp.diastoliclast.slope.salivationmin.slope.swallowinglast.fvc.liters.datewalking.slopesum.cuttingmean.swallowingsd.swallowingsum.speechmax.slope.climbing.stairshandwriting.slopemean.svc.litersmax.speechmeansquares.salivationmeansquares.slope.svc.literslast.slope.bp.diastolic.datefirst.resp.rate.datemeansquares.slope.salivationmean.resp.ratemax.slope.bp.diastolicmin.speechslope.speech.slopemean.weightSex.Malemax.svc.literssum.turningmax.slope.turninglast.svc.litersmax.swallowingmeansquares.turningmax.handwritingmeansquares.swallowingsd.slope.climbing.stairsmin.climbing.stairssvc.liters.slopelast.cuttingmeansquares.bp.systoliclast.slope.turningmax.salivationlast.weight.datemeansquares.slope.resp.ratemeansquares.weightmeansquares.slope.speechmeansquares.slope.swallowingmax.walkinglast.handwritingnum.slope.weight.visitssum.slope.cuttinglast.resp.ratesd.weightsd.climbing.stairssum.slope.turningsum.walkingslope.walking.slopeno.height.datafirst.alsfrs.score.datemin.salivationmin.slope.resp.ratefirst.slope.bp.systolic.datelast.slope.walkingStudy.Arm.PLACEBOfirst.slope.height.datemin.slope.handwritinglast.climbing.stairssd.handwritingSymptom.WEAKNESSmax.slope.handwritinglast.slope.cuttingmax.turningmin.slope.svc.litersdressing.slopesd.slope.speechMothernum.slope.resp.rate.visitsSymptom.AtrophySymptom.Swallowingcutting.slopenum.slope.bp.systolic.visitslessthan2.slope.bp.diastolicnum.slope.bp.diastolic.visitsno.slope.resp.rate.datalessthan2.slope.resp.rateno.slope.weight.datalessthan2.slope.weightslope.svc.liters.slopeno.slope.fvc.liters.datalessthan2.slope.fvc.litersnum.slope.fvc.liters.visitssd.slope.walkingsum.slope.walkingmin.slope.walkingmax.slope.walkingmax.slope.cuttinglast.slope.speechmax.slope.speechlessthan2.slope.alsfrs.scorenum.slope.alsfrs.score.visitsnum.bp.systolic.visitsmean.bp.systolicno.bp.diastolic.datalessthan2.bp.diastoliclast.bp.diastolic.datefirst.bp.diastolic.datenum.bp.diastolic.visitsno.resp.rate.datalessthan2.resp.ratenum.resp.rate.visitsmin.resp.ratemax.resp.ratesd.heightmeansquares.heightfirst.height.datenum.height.visitsmean.heightmin.heightlessthan2.weightfirst.weight.datenum.weight.visitsmeansquares.svc.literssum.svc.litersnum.svc.liters.visitsmin.svc.litersno.fvc.liters.datalessthan2.fvc.litersfirst.fvc.liters.datenum.fvc.liters.visitsmax.climbing.stairssd.walkingmeansquares.walkingmean.walkinglast.walkingmin.walkingturning.slopesd.turningmin.dressingmin.handwritinglast.salivationno.alsfrs.score.datalessthan2.alsfrs.scorenum.alsfrs.score.visitsStudy.Arm.ACTIVENeurological.Disease.STROKE.HEMORRHAGICNeurological.Disease.STROKE.ISCHEMICNeurological.Disease.BRAIN.TUMORNeurological.Disease.ALSNeurological.Disease.DATNeurological.Disease.PARKINSON.S.DISEASENeurological.Disease.DEMENTIA.NOSNeurological.Disease.STROKE.NOSNeurological.Disease.OTHERFamilyBrotherSisterDaughterSonUncle..Paternal.Uncle..Maternal.UncleGrandmother..Maternal.GrandmotherGrandfather..Maternal.FatherCousinAunt..Maternal.AuntRace...OtherRace...CaucasianRace...Black.African.AmericanRace...AsianSite.of.Onset.Onset..Limb.and.BulbarSite.of.Onset.Onset..LimbSymptom..Symptom.StiffnessSymptom.SENSORY_CHANGESSymptom.FasciculationsSymptom.CrampsSymptom.GAIT_CHANGESSymptom.OTHERSymptom.Speech\n",
      "0 20 40 60 80 100\n",
      "Variable‚àíImportance Plot for Boosting on the  ALS Data\n",
      "Figure 17.7 Variable importance plot for the ALS data. Here 267\n",
      "of the 369 variables were used in the ensemble. There are too\n",
      "many variables for the labels to be visible, so this plot serves as a\n",
      "visual guide. Variable Onset.Delta has relative importance\n",
      "100 (the lowest red bar), more than double the next two at around\n",
      "40 (last.slope.weight andalsfrs.score.slope ).\n",
      "However, the importances drop off slowly, suggesting that the\n",
      "model requires a signiÔ¨Åcant fraction of the variables.\n",
      "empty), andSp\n",
      "jD1BjDB. Then we can write\n",
      "bGB.x/DBX\n",
      "bD1Ogb.x/\n",
      "DpX\n",
      "jD1X\n",
      "b2BjOgb.x/\n",
      "DpX\n",
      "jD1Ofj.xj/: (17.7)17.2 Boosting with Squared-Error Loss 337\n",
      "0.250.260.270.280.290.30\n",
      "Number of TreesMean‚àísquared Error\n",
      "1 100 200 300 400 500Depth\n",
      "1\n",
      "2\n",
      "4\n",
      "7\n",
      "Figure 17.8 ALS test error for boosted models with different\n",
      "depth parameters d, and all using the same shrinkage parameter\n",
      "\u000fD0:02. It appears that dD1is inferior to the rest, with dD4\n",
      "about the best. With dD7, overÔ¨Åtting begins around 200 trees,\n",
      "withdD4around 300, while neither of the other two show\n",
      "evidence of overÔ¨Åtting by 500 trees.\n",
      "Hence boosted stumps Ô¨Åts an additive model, but in a fully adaptive way.\n",
      "It selects variables, and also selects how much action to devote to each\n",
      "variable. We return to additive models in Section 17.5. Figure 17.9 shows\n",
      "the three functions with highest relative importance. The Ô¨Årst function con-\n",
      "Ô¨Årms that a longer time since diagnosis (more negative Onset.Delta )\n",
      "predicts a slower decline. last.slope.weight is the difference in\n",
      "body weight at the last two visits‚Äîagain positive is good. Likewise for\n",
      "alsfrs.score.slope , which measures the local slope of the FRS\n",
      "score after the Ô¨Årst two visits.\n",
      "In a similar way, boosting with dD2Ô¨Åts a two-way interaction model;\n",
      "each tree involves at most two variables. In general, boosting with dDk\n",
      "leads to a.k\u00001/th-order interaction model. Interaction order is perhaps a\n",
      "more natural way to think of model complexity.338 Random Forests and Boosting\n",
      "‚àí2000 ‚àí1000 0‚àí0.3‚àí0.2‚àí0.10.00.10.2\n",
      "Onset.DeltaFitted Function\n",
      "‚àí20‚àí1001020‚àí0.3‚àí0.2‚àí0.10.00.10.2\n",
      "last.slope.weightFitted Function\n",
      "‚àí10 ‚àí6‚àí4‚àí2024‚àí0.3‚àí0.2‚àí0.10.00.10.2\n",
      "alsfrs.score.slopeFitted Function\n",
      "Figure 17.9 Three of the Ô¨Åtted functions (17.7) for the ALS data,\n",
      "in a boosted stumps model ( dD1), each centered to average zero\n",
      "over the training data. In terms of the outcome, bigger is better\n",
      "(slower decline in FRS). The Ô¨Årst function conÔ¨Årms that a longer\n",
      "time since diagnosis (more negative value of Onset.Delta )\n",
      "predicts a slower decline. The variable last.slope.weight\n",
      "is the difference in body weight at the last two visits‚Äîagain\n",
      "positive is good. Likewise for alsfrs.score.slope , which\n",
      "measures the local slope of the FRS score after the Ô¨Årst two visits.\n",
      "Shrinkage\n",
      "The shrinkage parameter \u000fcontrols the rate at which boosting Ô¨Åts‚Äîand\n",
      "hence overÔ¨Åts‚Äîthe data. Figure 17.10 demonstrates the effect of shrink-\n",
      "age on the ALS data. The under-shrunk ensemble (red) quickly overÔ¨Åts the\n",
      "data, leading to poor validation error. The blue ensemble uses a shrink-\n",
      "age parameter 25 times smaller, and reaches a lower validation error. The\n",
      "downside of a very small shrinkage parameter is that it can take many trees\n",
      "to adequately Ô¨Åt the data. On the other hand, the shrunken Ô¨Åts are smoother,\n",
      "take much longer to overÔ¨Åt, and hence are less sensitive to the stopping\n",
      "pointB.\n",
      "17.3 Gradient Boosting\n",
      "We now turn our attention to boosting models using other than square-error\n",
      "loss. We focus on the family of generalized models generated by the expo-\n",
      "nential family of response distributions (see Chapter 8). The most popular\n",
      "and relevant in this class is logistic regression, where we are interested in\n",
      "modeling\u0016.x/DPr.YD1jXDx/for a Bernoulli response variable.17.3 Gradient Boosting 339\n",
      "0.0 0.1 0.2 0.3 0.4\n",
      "Number of TreesMean‚àísquared Error\n",
      "1 50 100 150 200 250«´= 0 .02\n",
      "«´= 0 .50Train\n",
      "Validate\n",
      "Figure 17.10 BoosteddD3models with different shrinkage\n",
      "parameters, Ô¨Åt to a subset of the ALS data. The solid curves are\n",
      "validation errors, the dashed curves training errors, with red for\n",
      "\u000fD0:5and blue for\u000fD0:02. With\u000fD0:5, the training error\n",
      "drops rapidly with the number of trees, but the validation error\n",
      "starts to increase rapidly after an initial decrease. With \u000fD0:02\n",
      "(25 times smaller), the training error drops more slowly. The\n",
      "validation error also drops more slowly, but reaches a lower\n",
      "minimum (the horizontal dotted line) than the \u000fD0:5case. In\n",
      "this case, the slower learning has paid off.\n",
      "The idea is to Ô¨Åt a model of the form\n",
      "\u0015.x/DGB.x/DBX\n",
      "b/; (17.8)\n",
      "where\u0015.x/ is the natural parameter in the conditional distribution of YjXD\n",
      "b/are simple functions such as shallow trees. Here we\n",
      "b; for trees these wouldon by a parameter vector \n",
      "capture the identity of the split variables, their split values, and the con-\n",
      "stants in the terminal nodes. In the case of the Bernoulli response, we have\n",
      "\u0015.x/Dlog\u0012Pr.YD1jXDx/\n",
      "Pr.YD0jXDx/\u0013\n",
      "; (17.9)340 Random Forests and Boosting\n",
      "the logit link function that relates the mean to the natural parameter. In gen-\n",
      "eral, if\u0016.x/DE.YjXDx/is the conditional mean, we have \u0011≈í\u0016.x/¬çD\n",
      "\u0015.x/ , where\u0011is the monotone link function .\n",
      "Algorithm 17.3 outlines a general strategy for building a model by for-\n",
      "ward stagewise Ô¨Åtting. Lis the loss function, such as the negative log-\n",
      "likelihood for Bernoulli responses, or squared-error for Gaussian responses.\n",
      "/, thegh we are thinking of trees for the simple functions g.xI\n",
      "ideas generalize. This algorithm is easier to state than to implement. For\n",
      "Algorithm 17.3 GENERALIZED BOOSTING BY FORWARD -STAGEWISE\n",
      "FITTING\n",
      "/. Start withbG0.x/D0, and setBg.xI\n",
      "and the shrinkage parameter \u000f>0 .\n",
      "2 ForbD1;:::;B repeat the following steps.\n",
      "(a) Solve\n",
      "bDarg min\n",
      "nX\n",
      "iD1L\u0010\n",
      "/\u0011;bGb\u00001.xi/Cg.xiI\n",
      "b/. UpdatebGb.x/DbGb\u00001.x/COgb.x/;withOgb.x/D\u000f\u0001g.xIO\n",
      "3 Return the sequence bGb.x/; bD1;:::;B .\n",
      "squared-error loss, at each step we need to solve\n",
      "minimize\n",
      "nX\n",
      "//2; (17.10)\n",
      "/represents a depth- dtree,:;n . Ifg.\u0001I\n",
      "(17.10) is still difÔ¨Åcult to solve. But here we can resort to the usual greedy\n",
      "heuristic, and grow a depth- dtree to the residuals by the usual top-down\n",
      "splitting, as in step 2(a) of Algorithm 17.2. Hence in this case, we have\n",
      "exactly the squared-error boosting Algorithm 17.2. For more general loss\n",
      "functions, we rely on one more heuristic for solving step 2(a), inspired by\n",
      "gradient descent. Algorithm 17.4 gives the details. The idea is to perform\n",
      "functional gradient descent on the loss function, in the n-dimensional space\n",
      "of the Ô¨Åtted vector. However, we want to be able to evaluate our new func-\n",
      "tion everywhere, not just at the noriginal values xi. Hence once the (neg-\n",
      "ative) gradient vector has been computed, it is approximated by a depth- d\n",
      "tree (which canbe evaluated everywhere). Taking a step of length \u000fdown17.4 Adaboost: the Original Boosting Algorithm 341\n",
      "the gradient amounts to adding \u000ftimes the tree to the current function.¬é¬é4\n",
      "Gradient boosting is quite general, and can be used with any differentiable\n",
      "Algorithm 17.4 GRADIENT BOOSTING\n",
      "1 Start withOG0.x/D0, and setBand the shrinkage parameter \u000f>0 .\n",
      "2 ForbD1;:::;B repeat the following steps.\n",
      "(a) Compute the pointwise negative gradient of the loss function at the\n",
      "current Ô¨Åt:\n",
      "riD\u0000@L.yi;\u0015i/\n",
      "@\u0015iÀáÀáÀáÀá\n",
      "\u0015iDbGb\u00001.xi/; iD1;:::;n:\n",
      "(b) Approximate the negative gradient by a depth- dtree by solving\n",
      "minimize\n",
      "nX\n",
      "//2:ri\u0000g.xiI\n",
      "b/. UpdateOGb.x/DOGb\u00001.x/COgb.x/;withOgb.x/D\u000f\u0001g.xIO\n",
      "3 Return the sequence OGb.x/; bD1;:::;B .\n",
      "loss function. The Rpackage gbm implements Algorithm 17.4 for a variety\n",
      "of loss functions, including squared-error, binomial (Bernoulli), Laplace\n",
      "(`1loss), multinomial, and others. Included as well is the partial likelihood\n",
      "for the Cox proportional hazards model (Chapter 9). Figure 17.11 com-\n",
      "pares the misclassiÔ¨Åcation error of boosting on the spam data, with that of\n",
      "random forests and bagging. Since boosting has more tuning parameters, a\n",
      "careful comparison must take these into account. Using the McNemar test\n",
      "we would conclude that boosting and random forest are not signiÔ¨Åcantly\n",
      "different from each other, but both outperform bagging.\n",
      "17.4 Adaboost: the Original Boosting Algorithm\n",
      "The original proposal for boosting looked quite different from what we\n",
      "have presented so far. Adaboost was developed for the two-class classiÔ¨Å-\n",
      "cation problem, where the response is coded as -1/1. The idea was to Ô¨Åt a\n",
      "sequence of classiÔ¨Åers to modiÔ¨Åed versions of the training data, where the\n",
      "modiÔ¨Åcations give more weight to misclassiÔ¨Åed points. The Ô¨Ånal classiÔ¨Å-\n",
      "cation is by weighted majority vote. The details are rather speciÔ¨Åc, and are\n",
      "given in Algorithm 17.5. Here we distinguish a classiÔ¨Åer C.x/2f\u00001;1g,\n",
      "which returns a class label, rather than a probability. Algorithm 17.5 gives342 Random Forests and Boosting\n",
      "0.00 0.02 0.04 0.06 0.08Gradient Boosting on the Spam Data\n",
      "Number of TreesTest Error\n",
      "1 500 1000 1500 2000 2500Bagging\n",
      "Random Forest\n",
      "Boosting (depth 4)\n",
      "Figure 17.11 Test misclassiÔ¨Åcation for gradient boosting on the\n",
      "spam data, compared with a random forest and bagging.\n",
      "Although boosting appears to be better, it requires crossvaldiation\n",
      "or some other means to estimate its tuning parameters, while the\n",
      "random forest is essentially automatic.\n",
      "theAdaboost.M1 algorithm. Although the classiÔ¨Åer in step 2(a) can be ar-\n",
      "bitrary, it was intended for weak learners such as shallow trees. Steps 2(c)‚Äì\n",
      "(d) look mysterious. Its easy to check that, with the reweighted points, the\n",
      "classiÔ¨ÅerOcbjust learned would have weighted error 0.5, that of a coin Ô¨Çip.\n",
      "We also notice that, although the individual classiÔ¨Åers Ocb.x/produce val-\n",
      "uesÀô1, the ensemble bGb.x/takes values in R.\n",
      "It turns out that the Adaboost Algorithm 17.5 Ô¨Åts a logistic regression\n",
      "model via a version of the general boosting Algorithm 17.3, using an ex-\n",
      "ponential loss function. The functions bGb.x/output in step 3 of Algo-\n",
      "rithm 17.5 are estimates of (half) the logit function \u0015.x/ .\n",
      "To show this, we Ô¨Årst motivate the exponential loss, a somewhat unusual\n",
      "choice, and show how it is linked to logistic regression. For a -1/1 response\n",
      "yand function f.x/ , the exponential loss is deÔ¨Åned as LE.y;f.x//D\n",
      "exp≈í\u0000yf.x/¬ç . A simple calculation shows that the solution to the (condi-17.4 Adaboost: the Original Boosting Algorithm 343\n",
      "Algorithm 17.5 ADABOOST\n",
      "1 Initialize the observation weights wiD1=n; iD1;:::;n .\n",
      "2 ForbD1;:::;B repeat the following steps.\n",
      "(a) Fit a classiÔ¨Åer Ocb.x/to the training data, using observation weights\n",
      "wi.\n",
      "(b) Compute the weighted misclassiÔ¨Åcation error for Ocb:\n",
      "errbDPn\n",
      "iD1wiI≈íyi¬§Ocb.xi/¬çPn\n",
      "iD1wi:\n",
      "(c) Compute ÀõbDlog≈í.1\u0000errb/=errb¬ç.\n",
      "(d) Update the weights wi wi\u0001exp.Àõb\u0001I≈íyi¬§cb.xi/¬ç/; iD\n",
      "1;:::;n .\n",
      "3 Output the sequence of functions bGb.x/DPb\n",
      "`D1Àõ`Oc`.x/and corre-\n",
      "sponding classiÔ¨Åers bCb.x/Dsignh\n",
      "bGb.x/i\n",
      ",bD1;:::;B .\n",
      "tional) population minimization problem\n",
      "minimize\n",
      "f.x/E≈íe\u0000yf.x/jx¬ç (17.11)\n",
      "is given by\n",
      "f.x/D1\n",
      "2log\u0012Pr.yDC1jx/\n",
      "Pr.yD\u00001jx/\u0013\n",
      ": (17.12)\n",
      "Inverting, we get\n",
      "Pr.yDC1jx/Def.x/\n",
      "e\u0000f.x/Cef.x/and Pr.yD\u00001jx/De\u0000f.x/\n",
      "e\u0000f.x/Cef.x/;\n",
      "(17.13)\n",
      "a perfectly reasonable (and symmetric) model for a probability. The quan-\n",
      "tityyf.x/ is known as the margin (see also Chapter 19); if the margin\n",
      "is positive, the classiÔ¨Åcation using Cf.x/Dsign.f.x// is correct for y,\n",
      "else it is incorrect if the margin is negative. The magnitude of yf.x/ is\n",
      "proportional to the (signed) distance of xfrom the classiÔ¨Åcation boundary\n",
      "(exactly for linear models, approximately otherwise). For -1/1 data, we can\n",
      "also write the (negative) binomial log-likelihood in terms of the margin.344 Random Forests and Boosting\n",
      "Using (17.13) we have\n",
      "LB.y;f.x//D\u0000fI.yD\u00001/log Pr.yD\u00001jx/\n",
      "CI.yDC1/log Pr.yDC1jx/g\n",
      "Dlog\u0010\n",
      "1Ce\u00002yf.x/\u0011\n",
      ": (17.14)\n",
      "E\u0002\n",
      "log\u0000\n",
      "1Ce\u00002yf.x/\u0001\n",
      "jx\u0003\n",
      "also has population minimizer f.x/ equal to\n",
      "half the logit (17.12).3Figure 17.12 compares the exponential loss function\n",
      "with this binomial loss. They both asymptote to zero in the right tail‚Äîthe\n",
      "area of correct classiÔ¨Åcation. In the left tail, the binomial loss asymptotes\n",
      "to a linear function, much less severe than the exponential loss.\n",
      "‚àí3 ‚àí2 ‚àí1 0 1 2 30123456\n",
      "yf(x)LossBinomial\n",
      "Exponential\n",
      "Figure 17.12 Exponential loss used in Adaboost, versus the\n",
      "binomial loss used in the usual logistic regression. Both estimate\n",
      "the logit function. The exponential left tail, which punishes\n",
      "misclassiÔ¨Åcations, is much more severe than the asymptotically\n",
      "linear tail of the binomial.\n",
      "The exponential loss simpliÔ¨Åes step 2(a) in the gradient boosting Algo-\n",
      "3The half comes from the symmetric representation we use.17.5 Connections and Extensions 345\n",
      "rithm 17.3.\n",
      "nX\n",
      "iD1LE\u0010\n",
      "/\u0011;bGb\u00001.xi/Cg.xiI\n",
      "DnX\n",
      "//¬çexp≈í\u0000yi.bGb\u00001.xi/Cg.xiI\n",
      "DnX\n",
      "/¬ç (17.15)yig.xiI\n",
      "DnX\n",
      "//;wiLE.yi;g.xiI\n",
      "withwiDexp≈í\u0000yibGb\u00001.xi/¬ç. This is just a weighted exponential loss with\n",
      "the past history encapsulated in the observation weight wi(see step 2(a) in\n",
      "Algorithm 17.5). We give some more details in the chapter endnotes on\n",
      "how this reduces to the Adaboost algorithm.¬é ¬é5\n",
      "The Adaboost algorithm achieves an error rate on the spam data com-\n",
      "parable to binomial gradient boosting.\n",
      "17.5 Connections and Extensions\n",
      "Boosting is a general nonparametric function-Ô¨Åtting algorithm, and shares\n",
      "attributes with a variety of existing methods. Here we relate boosting to two\n",
      "different approaches: generalized additive models and the lasso of Chap-\n",
      "ter 16.\n",
      "Generalized Additive Models\n",
      "Boosting Ô¨Åts additive, low-order interaction models by a forward stage-\n",
      "wise strategy. Generalized additive models (GAMs) are a predecessor, a\n",
      "semi-parametric approach toward nonlinear function Ô¨Åtting. A GAM has\n",
      "the form\n",
      "\u0015.x/DpX\n",
      "jD1fj.xj/; (17.16)\n",
      "where again \u0015.x/D\u0011≈í\u0016.x/¬ç is the natural parameter in an exponential\n",
      "family. The attraction of a GAM is that the components are interpretable\n",
      "and can be visualized, and they can move us a big step up from a linear\n",
      "model.\n",
      "There are many ways to specify and Ô¨Åt additive models. For the fj, we\n",
      "could use parametric functions (e.g. polynomials), Ô¨Åxed-knot regression\n",
      "splines, or even linear functions for some terms. Less parametric options346 Random Forests and Boosting\n",
      "are smoothing splines and local regression (see Section 19.8). In the case of\n",
      "squared-error loss (the Gaussian case), there is a natural set of backÔ¨Åtting\n",
      "equations for Ô¨Åtting a GAM:\n",
      "Ofj Sj.y\u0000X\n",
      "`¬§jOf`/; jD1;:::;p: (17.17)\n",
      "HereOf`D≈íOf`.x1`/;:::;.Of`.xn`/¬ç0is then-vector of Ô¨Åtted values for the\n",
      "current estimate of function f`. Hence the term in parentheses is a partial\n",
      "residual , removing all the current function Ô¨Åts from yexcept the one about\n",
      "to be updated. Sjis a smoothing operator derived from variable xjthat\n",
      "gets applied to this residual and delivers the next estimate for function f`.\n",
      "BackÔ¨Åtting starts with all the functions zero, and then cycles through these\n",
      "equations for jD1;2;:::;p;1;2;::: in a block-coordinate fashion, until\n",
      "all the functions stabilize.\n",
      "The Ô¨Årst pass through all the variables is similar to the regression boost-\n",
      "ing Algorithm 17.2, where each new function takes the residuals from the\n",
      "past Ô¨Åts, and models them using a tree (for Sj). The difference is that\n",
      "boosting never goes back and Ô¨Åxes up past functions, but Ô¨Åts in a forward-\n",
      "stagewise fashion, leaving all past functions alone. Of course, with its adap-\n",
      "tive Ô¨Åtting mechanism, boosting can select the same variables as used be-\n",
      "fore, and thereby update that component of the Ô¨Åt. Boosting with stumps\n",
      "(single-split trees, see the discussion on tree depth on 335 in Section 17.2)\n",
      "can hence be seen as an adaptive way for Ô¨Åtting an additive model, that si-\n",
      "multaneously performs variable selection and allows for different amounts\n",
      "of smoothing for different variables.\n",
      "Boosting and the Lasso\n",
      "In Section 16.7 we drew attention to the close connection between the\n",
      "forward-stagewise Ô¨Åtting of boosting (with shrinkage ) and the lasso, via\n",
      "inÔ¨Ånitesimal forward-stagewise regression. Here we take this a step further,\n",
      "by using the lasso as a post-processor for boosting (or random forests).\n",
      "Boosting with shrinkage does a good job in building a prediction model,\n",
      "but at the end of the day can involve a lot of trees. Because of the shrink-\n",
      "age, many of these trees could be similar to each other. The idea here is\n",
      "to use the lasso to select a subset of these trees, reweight them, and hence\n",
      "produce a prediction model with far fewer trees and, one hopes, compa-\n",
      "rable accuracy. Suppose boosting has produced a sequence of Ô¨Åtted trees\n",
      "Ogb.x/; bD1;:::;B . We then solve the lasso problem17.6 Notes and Details 347\n",
      "0.250.260.270.280.290.30\n",
      "Number of TreesMean‚àísquared Error\n",
      "1 100 200 300 400 500Depth 2 Boost\n",
      "Lasso Post Fit\n",
      "Figure 17.13 Post-processing of the trees produced by boosting\n",
      "on the ALS data. Shown is the test prediction error as a function\n",
      "of the number of trees selected by the (nonnegative) lasso. We see\n",
      "that the lasso can do as good a job with one-third the number of\n",
      "trees, although selecting the correct number is critical.\n",
      "minimize\n",
      "fÀábgB\n",
      "1nX\n",
      "iD1L\"\n",
      "yi;BX\n",
      "bD1Ogb.xi/Àáb#\n",
      "C\u0015BX\n",
      "bD1jÀábj (17.18)\n",
      "for different values of \u0015. This model selects some of the trees, and as-\n",
      "signs differential weights to them. A reasonable variant is to insist that the\n",
      "weights are nonnegative. Figure 17.13 illustrates this approach on the ALS\n",
      "data. Here we could use one-third of the trees. Often the savings are much\n",
      "more dramatic.\n",
      "17.6 Notes and Details\n",
      "Random forests and boosting live at the cutting edge of modern predic-\n",
      "tion methodology. They Ô¨Åt models of breathtaking complexity compared\n",
      "with classical linear regression, or even with standard GLM modeling as\n",
      "practiced in the late twentieth century (Chapter 8). They are routinely used\n",
      "as prediction engines in a wide variety of industrial and scientiÔ¨Åc appli-\n",
      "cations. For the more cautious, they provide a terriÔ¨Åc benchmark for how\n",
      "well a traditional parametrized model is performing: if the random forests348 Random Forests and Boosting\n",
      "does much better, you probably have some work to do, by including some\n",
      "important interactions and the like.\n",
      "The regression and classiÔ¨Åcation trees discussed in Chapter 8 (Breiman\n",
      "et al. , 1984) took traditional models to a new level, with their ability to\n",
      "adapt to the data, select variables, and so on. But their prediction per-\n",
      "formance is somewhat lacking, and so they stood the risk of falling by\n",
      "the wayside. With their new use as building blocks in random forests and\n",
      "boosting, they have reasserted themselves as critical elements in the mod-\n",
      "ern toolbox.\n",
      "Random forests and bagging were introduced by Breiman (2001), and\n",
      "boosting by Schapire (1990) and Freund and Schapire (1996). There has\n",
      "been much discussion on why boosting works (Breiman, 1998; Friedman\n",
      "et al. , 2000; Schapire and Freund, 2012); the statistical interpretation given\n",
      "here can also be found in Hastie et al. (2009), and led to the gradient boost-\n",
      "ing algorithm (Friedman, 2001). Adaboost was Ô¨Årst described in Freund\n",
      "and Schapire (1997). Hastie et al. (2009, Chapter 15) is devoted to random\n",
      "forests. For the examples in this chapter we used the randomForest\n",
      "package in R(Liaw and Wiener, 2002), and for boosting the gbm (Ridge-\n",
      "way, 2005) package. The lasso post-processing idea is due to Friedman and\n",
      "Popescu (2005), which we implemented using glmnet (Friedman et al. ,\n",
      "2009). Generalized additive models are described in Hastie and Tibshirani\n",
      "(1990).\n",
      "We now give some particular technical details on topics covered in the\n",
      "chapter.\n",
      "¬é1[p. 327] Averaging trees. A maximal-depth tree splits every node until it is\n",
      "pure, meaning all the responses are the same. For very large nthis might\n",
      "be unreasonable; in practice, one can put a lower bound on the minimum\n",
      "count in a terminal node. We are deliberately vague about the response type\n",
      "in Algorithm 17.1. If it is quantitative, we would Ô¨Åt a regression tree. If it\n",
      "is binary or multilevel qualitative, we would Ô¨Åt a classiÔ¨Åcation tree. In this\n",
      "case at the averaging stage, there are at least two strategies. The original\n",
      "random-forest paper (Breiman, 2001) proposed that each tree should make\n",
      "a classiÔ¨Åcation, and then the ensemble uses a plurality vote. An alterna-\n",
      "tive reasonable strategy is to average the class probabilities produced by\n",
      "the trees; these procedures are identical if the trees are grown to maximal\n",
      "depth.\n",
      "¬é2[p. 330] Jackknife variance estimate. The jackknife estimate of variance\n",
      "for a random forest, and the bias-corrected version, is described in Wager\n",
      "et al. (2014). The jackknife formula (17.3) is applied to the BD1 ver-17.6 Notes and Details 349\n",
      "sion of the random forest, but of course is estimated by plugging in Ô¨Ånite\n",
      "Bversions of the quantities involved. Replacing Or.\u0001/\n",
      "rf.x0/by its expectation\n",
      "Orrf.x0/is not the problem; its that each of the Or.i/\n",
      "rf.x0/vary about their boot-\n",
      "strap expectations, compounded by the square in expression (17.4). Calcu-\n",
      "lating the bias requires some technical derivations, which can be found in\n",
      "that reference.\n",
      "They also describe the inÔ¨Ånitesimal jackknife estimate of variance, given\n",
      "by\n",
      "bVIJ.Orrf.x0//DnX\n",
      "iD1dcov2\n",
      "i; (17.19)\n",
      "with\n",
      "dcoviDdcov.w\u0003;Or\u0003.x0//D1\n",
      "BBX\n",
      "bD1.w\u0003\n",
      "bi\u00001/.Orb.x0/\u0000Orrf.x0//; (17.20)\n",
      "as discussed in Chapter 20. It too has a bias-corrected version, given by\n",
      "bVu\n",
      "IJ.Orrf.x0//DbVIJ.Orrf.x0//\u0000n\n",
      "BOv.x0/; (17.21)\n",
      "similar to (17.5).\n",
      "¬é3[p. 334] TheALS data. These data were kindly provided by Lester Mackey\n",
      "and Lilly Fang, who won the DREAM challenge prediction prize in 2012\n",
      "(Kuffner et al. , 2015). It includes some additional variables created by\n",
      "them. Their winning entry used Bayesian trees, not too different from ran-\n",
      "dom forests.\n",
      "¬é4[p. 341] Gradient-boosting details. In Friedman‚Äôs gradient-boosting algo-\n",
      "rithm (Hastie et al. , 2009, Chapter 10, for example), a further reÔ¨Ånement\n",
      "is implemented. The tree in step 2(b) of Algorithm 17.4 is used to de-\n",
      "Ô¨Åne the structure (split variables and splits), but the values in the terminal\n",
      "nodes are left to be updated. We can think of partitioning the parameters\n",
      "t. Here then represent the tree as g.xI\n",
      "s/is a vector of dC1binary basis functions that indicate the termi-\n",
      "tare thedC1values of the terminal\n",
      "sby approximating the gradient in step 2(b)\n",
      "tby solvingand then (re-)learn the terminal-node parameters O\n",
      "the optimization problem\n",
      "minimize\n",
      "tnX\n",
      "iD1L\u0010\n",
      "t\u00110OGb\u00001.xi/CT.xiIO\n",
      ": (17.22)\n",
      "Solving (17.22) amounts to Ô¨Åtting a simple GLM with an offset .350 Random Forests and Boosting\n",
      "¬é5[p. 345] Adaboost and gradient boosting. Hastie et al. (2009, Chapter\n",
      "10) derive Adaboost as an instance of Algorithm 17.3. One detail is that\n",
      "0/.e replaced by a simpliÔ¨Åed scaled classiÔ¨Åer Àõ\u0001c.xI\n",
      "Hence, from (17.15), in step 2(a) of Algorithm 17.3 we need to solve\n",
      "minimize\n",
      "0nX\n",
      "0/¬ç: (17.23)Àõc.xiI\n",
      "The derivation goes on to show that\n",
      "\u000fminimizing (17.23) for any value of Àõ > 0 can be achieved by Ô¨Åtting\n",
      "0/to minimize the weighted misclassiÔ¨Åcation\n",
      "error\n",
      "nX\n",
      "0/¬çIiI≈íyi¬§c.xi;\n",
      "0/,Àõis estimated as in step 2(c) of Algorithm 17.5 (and is\n",
      "non-negative);\n",
      "\u000fthe weight-update scheme in step 2(d) of Algorithm 17.5 corresponds\n",
      "exactly to the weights as computed in (17.15).18\n",
      "Neural Networks and Deep Learning\n",
      "Something happened in the mid 1980s that shook up the applied statistics\n",
      "community. Neural networks (NNs) were introduced, and they marked a\n",
      "shift of predictive modeling towards computer science and machine learn-\n",
      "ing. A neural network is a highly parametrized model, inspired by the ar-\n",
      "chitecture of the human brain, that was widely promoted as a universal\n",
      "approximator ‚Äîa machine that with enough data could learn any smooth\n",
      "predictive relationship.\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x4f(x)Hidden\n",
      "layer L2Input\n",
      "layer L1Output\n",
      "layer L3\n",
      "Figure 18.1 Neural network diagram with a single hidden layer.\n",
      "The hidden layer derives transformations of the inputs‚Äînonlinear\n",
      "transformations of linear combinations‚Äîwhich are then used to\n",
      "model the output.\n",
      "Figure 18.1 shows a simple example of a feed-forward neural network\n",
      "diagram. There are four predictors or inputs xj, Ô¨Åve hidden units a`D\n",
      "g.w.1/\n",
      "`0CP4\n",
      "jD1w.1/\n",
      "`jxj/, and a single output unit oDh.w.2/\n",
      "0CP5\n",
      "`D1w.2/\n",
      "`a`/.\n",
      "The language associated with NNs is colorful: memory units or neurons\n",
      "automatically learn new features from the data through a process called\n",
      "351352 Neural Networks\n",
      "supervised learning . Each neuron alis connected to the input layer via a\n",
      "vector of parameters or weightsfw.1/\n",
      "`jgp\n",
      "1(the.1/refers to the Ô¨Årst layer\n",
      "and`jrefers to the jth variable and `th unit). The intercept terms w.1/\n",
      "`0\n",
      "are called a bias, and the function gis a nonlinearity, such as the sigmoid\n",
      "functiong.t/D1=.1Ce\u0000t/. The idea was that each neuron will learn a\n",
      "simple binary on/off function; the sigmoid function is a smooth and dif-\n",
      "ferentiable compromise. The Ô¨Ånal or output layer also has weights, and\n",
      "an output function h. For quantitative regression his typically the identity\n",
      "function, and for a binary response it is once again the sigmoid. Note that\n",
      "without the nonlinearity in the hidden layer, the neural network would re-\n",
      "duce to a generalized linear model (Chapter 8). Typically neural networks\n",
      "are Ô¨Åt by maximum likelihood, usually with a variety of forms of regular-\n",
      "ization.\n",
      "The knee-jerk response from statisticians was ‚ÄúWhat‚Äôs the big deal? A\n",
      "neural network is just a nonlinear model, not too different from many other\n",
      "generalizations of linear models.‚Äù\n",
      "While this may be true, neural networks brought a new energy to the\n",
      "Ô¨Åeld. They could be scaled up and generalized in a variety of ways: many\n",
      "hidden units in a layer, multiple hidden layers, weight sharing, a variety\n",
      "of colorful forms of regularization, and innovative learning algorithms for\n",
      "massive data sets. And most importantly, they were able to solve problems\n",
      "on a scale far exceeding what the statistics community was used to. This\n",
      "was part computing scale and expertise, part liberated thinking and cre-\n",
      "ativity on the part of this computer science community. New journals were\n",
      "devoted to the Ô¨Åeld,¬éand several popular annual conferences (initially at ¬é1\n",
      "ski resorts) attracted their denizens, and drew in members of the statistics\n",
      "community.\n",
      "After enjoying considerable popularity for a number of years, neural\n",
      "networks were somewhat sidelined by new inventions in the mid 1990s,\n",
      "such as boosting (Chapter 17) and SVMs (Chapter 19). Neural networks\n",
      "were pass¬¥e. But then they re-emerged with a vengeance after 2010‚Äîthe\n",
      "reincarnation now being called deep learning . This renewed enthusiasm is\n",
      "a result of massive improvements in computer resources, some innovations,\n",
      "and the ideal niche learning tasks such as image and video classiÔ¨Åcation,\n",
      "and speech and text processing.18.1 Neural Networks and the Handwritten Digit Problem 353\n",
      "18.1 Neural Networks and the Handwritten Digit Problem\n",
      "Neural networks really cut their baby teeth on an optical character recogni-\n",
      "tion (OCR) task: automatic reading of handwritten digits, as in a zipcode.\n",
      "Figure 18.2 shows some examples, taken from the MNIST corpus.¬éThe¬é2\n",
      "idea is to build a classiÔ¨Åer C.x/2f0;1;:::;9gbased on the input image\n",
      "x2R28\u000228, a28\u000228grid of image intensities. In fact, as is often the\n",
      "case, it is more useful to learn the probability function Pr .yDjjx/; jD\n",
      "0;1;2;:::;9 ; this is indeed the target for our neural network. Figure 18.3\n",
      "Figure 18.2 Examples of handwritten digits from the MNIST\n",
      "corpus. Each digit is represented by a 28\u000228grayscale image,\n",
      "derived from normalized binary images of different shapes and\n",
      "sizes. The value stored for each pixel in an image is a nonnegative\n",
      "eight-bit representation of the amount of gray present at that\n",
      "location. The 784 pixels for each image are the predictors, and the\n",
      "0‚Äì9 class labels the response. There are 60,000 training images in\n",
      "the full data set, and 10,000 in the test set.\n",
      "shows a neural network with three hidden layers, a successful conÔ¨Ågura-\n",
      "tion for this digit classiÔ¨Åcation problem. In this case the output layer has\n",
      "10 nodes, one for each of the possible class labels. We use this example\n",
      "to walk the reader through some of the aspects of the conÔ¨Åguration of a\n",
      "network, and Ô¨Åtting it to training data. Since all of the layers are functions\n",
      "of their previous layers, and Ô¨Ånally functions of the input vector x, the net-\n",
      "work represents a somewhat complex function f.xIW/, where Wrepre-\n",
      "sents the entire collection of weights. Armed with a suitable loss function,\n",
      "we could simply barge right in and throw it at our favorite optimizer. In the\n",
      "early days this was not computationally feasible, especially when special354 Neural Networks\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "...\n",
      "xpy0\n",
      "y1\n",
      "...\n",
      "y9Hidden\n",
      "layer L4Hidden\n",
      "layer L3Hidden\n",
      "layer L2Input\n",
      "layer L1Output\n",
      "layer L5\n",
      "W(1)\n",
      "a(2)W(2)\n",
      "a(3)\n",
      "W(3)\n",
      "a(4)W(4)a(5)\n",
      "Figure 18.3 Neural network diagram with three hidden layers\n",
      "and multiple outputs, suitable for the MNIST handwritten-digit\n",
      "problem. The input layer has pD784units. Such a network with\n",
      "hidden layer sizes .1024;1024;2048/ , and particular choices of\n",
      "tuning parameters, achieves the state-of-the art error rate of\n",
      "0:93% on the ‚ÄúofÔ¨Åcial‚Äù test data set. This network has close to\n",
      "four million weights, and hence needs to be heavily regularized.\n",
      "structure is imposed on the weight vectors. Today there are fairly automatic\n",
      "systems for setting up and Ô¨Åtting neural networks, and this view is not too\n",
      "far from reality. They mostly use some form of gradient descent, and rely\n",
      "on an organization of parameters that leads to a manageable calculation of\n",
      "the gradient.\n",
      "The network in Figure 18.3 is complex, so it is essential to establish a\n",
      "convenient notation for referencing the different sets of parameters. We\n",
      "continue with the notation established for the single-layer network, but\n",
      "with some additional annotations to distinguish aspects of different layers.\n",
      "From the Ô¨Årst to the second layer we have\n",
      "z.2/\n",
      "`Dw.1/\n",
      "`0CpX\n",
      "jD1w.1/\n",
      "`jxj; (18.1)\n",
      "a.2/\n",
      "`Dg.2/.z.2/\n",
      "`/: (18.2)18.1 Neural Networks and the Handwritten Digit Problem 355\n",
      "We have separated the linear transformations z.2/\n",
      "`of thexjfrom the nonlin-\n",
      "ear transformation of these, and we allow for layer-speciÔ¨Åc nonlinear trans-\n",
      "formationsg.k/. More generally we have the transition from layer k\u00001to\n",
      "layerk:\n",
      "z.k/\n",
      "`Dw.k\u00001/\n",
      "`0Cpk\u00001X\n",
      "jD1w.k\u00001/\n",
      "`ja.k\u00001/\n",
      "j; (18.3)\n",
      "a.k/\n",
      "`Dg.k/.z.k/\n",
      "`/: (18.4)\n",
      "In fact (18.3)‚Äì(18.4) can serve for the input layer (18.1)‚Äì(18.2) if we adopt\n",
      "the notation that a.1/\n",
      "`\u0011x`andp1Dp, the number of input variables.\n",
      "Hence each of the arrows in Figure 18.3 is associated with a weight param-\n",
      "eter.\n",
      "It is simpler to adopt a vector notation\n",
      "z.k/DW.k\u00001/a.k\u00001/(18.5)\n",
      "a.k/Dg.k/.z.k//; (18.6)\n",
      "whereW.k\u00001/represents the matrix of weights that go from layer Lk\u00001\n",
      "to layerLk,a.k/is the entire vector of activations at layer Lk, and our\n",
      "notation assumes that g.k/operates elementwise on its vector argument.\n",
      "We have also absorbed the bias parameters w.k\u00001/\n",
      "`0into the matrix W.k\u00001/,\n",
      "which assumes that we have augmented each of the activation vectors a.k/\n",
      "with a constant element 1.\n",
      "Sometimes the nonlinearities g.k/at the inner layers are the same func-\n",
      "tion, such as the function \u001bdeÔ¨Åned earlier. In Section 18.5 we present a\n",
      "network for natural color image classiÔ¨Åcation, where a number of different\n",
      "activation functions are used.\n",
      "Depending on the response, the Ô¨Ånal transformation g.K/is usually spe-\n",
      "cial. ForM-class classiÔ¨Åcation, such as here with MD10, one typically\n",
      "uses the softmax function\n",
      "g.K/.z.K/\n",
      "mIz.K//Dez.K/\n",
      "m\n",
      "PM\n",
      "`D1ez.K/\n",
      "`; (18.7)\n",
      "which computes a number (probability) between zero and one, and all M\n",
      "of them sum to one.1\n",
      "1This is a symmetric version of the inverse link function used for multiclass logistic\n",
      "regression.356 Neural Networks\n",
      "18.2 Fitting a Neural Network\n",
      "As we have seen, a neural network model is a complex, hierarchical func-\n",
      "tionf.xIW/of the the feature vector x, and the collection of weights W.\n",
      "For typical choices for the g.k/, this function will be differentiable. Given\n",
      "a training setfxi;yign\n",
      "1and a loss function L≈íy;f.x/¬ç , along familiar lines\n",
      "we might seek to solve\n",
      "minimize\n",
      "W(\n",
      "1\n",
      "nnX\n",
      "iD1L≈íyi;f.xiIW/¬çC\u0015J.W/)\n",
      "; (18.8)\n",
      "whereJ.W/is a nonnegative regularization term on the elements of W,\n",
      "and\u0015\u00150is a tuning parameter. (In practice there may be multiple reg-\n",
      "ularization terms, each with their own \u0015.) For example an early popular\n",
      "penalty is the quadratic\n",
      "J.W/D1\n",
      "2K\u00001X\n",
      "kD1pkX\n",
      "jD1pkC1X\n",
      "`D1n\n",
      "w.k/\n",
      "`jo2\n",
      "; (18.9)\n",
      "as in ridge regression (7.41). Also known as the weight-decay penalty, it\n",
      "pulls the weights toward zero (typically the biases are not penalized). Lasso\n",
      "penalties (Chapter 16) are also popular, as are mixtures of these (an elastic\n",
      "net).\n",
      "For binary classiÔ¨Åcation we could take Lto be binomial deviance (8.14),\n",
      "in which case the neural network amounts to a penalized logistic regres-\n",
      "sion, Section 8.1, albeit a highly parametrized and penalized one. Loss\n",
      "functions are usually convex in f, but not in the elements of W, so solving\n",
      "(18.8) is difÔ¨Åcult, and at best we seek good local optima. Most methods\n",
      "are based on some form of gradient descent, with many associated bells\n",
      "and whistles. We brieÔ¨Çy discuss some elements of the current practice in\n",
      "Ô¨Ånding good solutions to (18.8).\n",
      "Computing the Gradient: Backpropagation\n",
      "The elements of Woccur in layers, since f.xIW/is deÔ¨Åned as a series\n",
      "of compositions, starting from the input layer. Computing the gradient is\n",
      "also done most naturally in layers (the chain rule for differentiation; see\n",
      "for example (18.10) in Algorithm 18.1 below), and our notation makes this\n",
      "easier to describe in a recursive fashion. We will consider computing the\n",
      "derivative ofL≈íy;f.xIW¬çwith respect to any of the elements of W, for a\n",
      "generic input‚Äìoutput pair x;y; since the loss part of the objective is a sum,18.2 Fitting a Neural Network 357\n",
      "the overall gradient will be the sum of these individual gradient elements\n",
      "over the training pairs .xi;yi/.\n",
      "The intuition is as follows. Given a training generic pair .x;y/ , we Ô¨Årst\n",
      "make a forward pass through the network, which creates activations at each\n",
      "of the nodesa.k/\n",
      "`in each of the layers, including the Ô¨Ånal output layer. We\n",
      "would then like to compute an error term ƒ±.k/\n",
      "`that measures the responsibil-\n",
      "ity of each node for the error in predicting the true output y. For the output\n",
      "activationsa.K/\n",
      "`these errors are easy: either residuals or generalized resid-\n",
      "uals, depending on the loss function. For activations at inner layers, ƒ±.k/\n",
      "`\n",
      "will be a weighted sum of the errors terms of nodes that use a.k/\n",
      "`as inputs.\n",
      "The backpropagation Algorithm 18.1 gives the details for computing the\n",
      "gradient for a single input‚Äìoutput pair x;y. We leave it to the reader to\n",
      "verify that this indeed implements the chain rule for differentiation.\n",
      "Algorithm 18.1 BACKPROPAGATION\n",
      "1 Given a pair x;y, perform a ‚Äúfeedforward pass,‚Äù computing the activa-\n",
      "tionsa.k/\n",
      "`at each of the layers L2;L3;:::;LK; i.e. compute f.xIW/at\n",
      "xusing the current W, saving each of the intermediary quantities along\n",
      "the way.\n",
      "2 For each output unit `in layerLK, compute\n",
      "ƒ±.K/\n",
      "`D@L≈íy;f.x; W/¬ç\n",
      "@z.K/\n",
      "`\n",
      "D@L≈íy;f.xIW/¬ç\n",
      "@a.K/\n",
      "`Pg.K/.z.K/\n",
      "`/; (18.10)\n",
      "wherePgdenotes the derivative of g.z/ wrtz. For example for L.y;f/D\n",
      "1\n",
      "2ky\u0000fk2\n",
      "2, (18.10) becomes\u0000.y`\u0000f`/\u0001Pg.K/.z.K/\n",
      "`/.\n",
      "3 For layerskDK\u00001; K\u00002;:::;2 , and for each node `in layerk, set\n",
      "ƒ±.k/\n",
      "`D0\n",
      "@pkC1X\n",
      "jD1w.k/\n",
      "j`ƒ±.kC1/\n",
      "j1\n",
      "APg.k/.z.k/\n",
      "`/: (18.11)\n",
      "4 The partial derivatives are given by\n",
      "@L≈íy;f.xIW/¬ç\n",
      "@w.k/\n",
      "`jDa.k/\n",
      "jƒ±.kC1/\n",
      "`: (18.12)\n",
      "One again matrix‚Äìvector notation simpliÔ¨Åes these expressions a bit:358 Neural Networks\n",
      "(18.10) becomes (for squared-error loss)\n",
      "ƒ±.K/D\u0000.y\u0000a.K//ƒ±Pg.K/.z.K//; (18.13)\n",
      "whereƒ±denotes the Hadamard (elementwise) product;\n",
      "(18.11) becomes\n",
      "ƒ±.k/D\u0010\n",
      "W.k/0ƒ±.kC1/\u0011\n",
      "ƒ±Pg.k/.z.k//I (18.14)\n",
      "(18.12) becomes\n",
      "@L≈íy;f.xIW/¬ç\n",
      "@W.k/Dƒ±.kC1/a.k/0: (18.15)\n",
      "Backpropagation was considered a breakthrough in the early days of\n",
      "neural networks, since it made Ô¨Åtting a complex model computationally\n",
      "manageable.\n",
      "Gradient Descent\n",
      "Algorithm 18.1 computes the gradient of the loss function at a single generic\n",
      "pair.x;y/ ; withntraining pairs the gradient of the Ô¨Årst part of (18.8) is\n",
      "given by\n",
      "¬ÅW.k/D1\n",
      "nnX\n",
      "iD1@L≈íyi;f.xiIW¬ç\n",
      "@W.k/: (18.16)\n",
      "With the quadratic form (18.9) for the penalty, a gradient-descent update is\n",
      "W.k/ W.k/\u0000Àõ\u0010\n",
      "¬ÅW.k/C\u0015W.k/\u0011\n",
      "; kD1;:::;K\u00001; (18.17)\n",
      "whereÀõ2.0;1¬ç is the learning rate .\n",
      "Gradient descent requires starting values for all the weights W. Zero is\n",
      "not an option, because each layer is symmetric in the weights Ô¨Çowing to the\n",
      "different neurons, hence we rely on starting values to break the symmetries.\n",
      "Typically one would use random starting weights, close to zero; random\n",
      "uniform or Gaussian weights are common.\n",
      "There are a multitude of ‚Äútricks of the trade‚Äù in Ô¨Åtting or ‚Äúlearning‚Äù a\n",
      "neural network, and many of them are connected with gradient descent.\n",
      "Here we list some of these, without going into great detail.\n",
      "Stochastic Gradient Descent\n",
      "Rather than process all the observations before making a gradient step, it\n",
      "can be more efÔ¨Åcient to process smaller batches at a time‚Äîeven batches18.2 Fitting a Neural Network 359\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "0 200 400 600 800 10000.00 0.01 0.02 0.03 0.04\n",
      "EpochsMisclassification Error‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óèTrain\n",
      "Test\n",
      "Test ‚àí RF\n",
      "Figure 18.4 Training and test misclassiÔ¨Åcation error as a\n",
      "function of the number of epochs of training, for the MNIST digit\n",
      "classiÔ¨Åcation problem. The architecture for the network is shown\n",
      "in Figure 18.3. The network was Ô¨Åt using accelerated gradient\n",
      "descent with adaptive rate control, a rectiÔ¨Åed linear activation\n",
      "function, and dropout regularization (Section 18.5). The\n",
      "horizontal broken line shows the error rate of a random forest\n",
      "(Section 17.1). A logistic regression model (Section 8.1) achieves\n",
      "only 0.072 (off the scale).\n",
      "of size one! These batches can be sampled at random, or systematically\n",
      "processed. For large data sets distributed on multiple computer cores, this\n",
      "can be essential for reasons of efÔ¨Åciency. An epoch of training means that\n",
      "allntraining samples have been used in gradient steps, irrespective of how\n",
      "they have been grouped (and hence how many gradient steps have been\n",
      "made).\n",
      "Accelerated Gradient Methods\n",
      "The idea here is to allow previous iterations to build up momentum and\n",
      "inÔ¨Çuence the current iterations. The iterations have the form\n",
      "VtC1D\u0016Vt\u0000Àõ.¬ÅWtC\u0015Wt/; (18.18)\n",
      "WtC1DWtCVtC1; (18.19)360 Neural Networks\n",
      "usingWtto represent the entire collection of weights at iteration t.Vtis\n",
      "avelocity vector that accumulates gradient information from previous iter-\n",
      "ations, and is controlled by an additional momentum parameter \u0016. When\n",
      "correctly tuned, accelerated gradient descent can achieve much faster con-\n",
      "vergence rates; however, tuning tends to be a difÔ¨Åcult process, and is typi-\n",
      "cally done adaptively.\n",
      "Rate Annealing\n",
      "A variety of creative methods have been proposed to adapt the learning rate\n",
      "to avoid jumping across good local minima. These tend to be a mixture of\n",
      "principled approaches combined with ad-hoc adaptations that tend to work\n",
      "well in practice.¬éFigure 18.4 shows the performance of our neural net ¬é3\n",
      "on the MNIST digit data. This achieves state-of-the art misclassiÔ¨Åcation\n",
      "error rates on these data (just under 0.93% errors), and outperforms random\n",
      "forests (2.8%) and a generalized linear model (7.2%). Figure 18.5 shows\n",
      "the 93 misclassiÔ¨Åed digits.\n",
      "42 53 60 82 58 97 89 65 72 46 72 94\n",
      "49 95 71 57 83 79 87 46 93 06 37 93\n",
      "23 94 53 20 37 49 61 90 91 94 24 61\n",
      "53 95 61 65 32 95 35 97 12 49 60 37\n",
      "91 64 50 85 72 46 13 46 03 97 27 32\n",
      "87 89 61 80 94 72 70 49 53 38 38 39\n",
      "89 97 71 07 95 85 05 39 85 49 72 72\n",
      "72 08 97 27 47 63 56 42 50\n",
      "Figure 18.5 All 93 misclassiÔ¨Åed digits in the MNIST test set.\n",
      "The true digit class is labeled in blue, the predicted in red.18.2 Fitting a Neural Network 361\n",
      "Other Tuning Parameters\n",
      "Apart from the many details associated with gradient descent, there are sev-\n",
      "eral other important structural and operational aspects of neural networks\n",
      "that have to be speciÔ¨Åed.\n",
      "Number of Hidden Layers, and Their Sizes\n",
      "With a single hidden layer, the number of hidden units determines the num-\n",
      "ber of parameters. In principle, one could treat this number as a tuning\n",
      "parameter, which could be adjusted to avoid overÔ¨Åtting. The current col-\n",
      "lective wisdom suggests it is better to have an abundant number of hidden\n",
      "units, and control the model complexity instead by weight regularization.\n",
      "Having deeper networks (more hidden layers) increases the complexity as\n",
      "well. The correct number tends to be task speciÔ¨Åc; having two hidden lay-\n",
      "ers with the digit recognition problem leads to competitive performance.\n",
      "Choice of Nonlinearities\n",
      "There are a number of activation functions g.k/in current use. Apart from\n",
      "the sigmoid function, which transforms its input to a values in .0;1/ , other\n",
      "popular choices are\n",
      "‚àí2 ‚àí1 0 1 2‚àí1.0‚àí0.5 0.00.51.0\n",
      "zg(z)sigmoid\n",
      "tanh\n",
      "ReLU\n",
      "leaky ReLU\n",
      "Figure 18.6 Activation functions. ReLU is a rectiÔ¨Åed linear\n",
      "(unit).\n",
      "tanh:g.z/Dez\u0000e\u0000z\n",
      "ezCe\u0000z;362 Neural Networks\n",
      "which delivers values in .\u00001;1/.\n",
      "rectiÔ¨Åed linear: g.z/DzC;\n",
      "or the positive-part function. This has the advantage of making the gra-\n",
      "dient computations cheaper to compute.\n",
      "leaky rectiÔ¨Åed linear: gÀõ.z/DzC\u0000Àõz\u0000;\n",
      "forÀõnonnegative and close to zero. The rectiÔ¨Åed linear tends to have Ô¨Çat\n",
      "spots, because of the many zero activations; this is an attempt to avoid\n",
      "these and the accompanying zero gradients.\n",
      "Choice of Regularization\n",
      "Typically this is a mixture of `2and`1regularization, each of which re-\n",
      "quires a tuning parameter. As in lasso and regression applications, the bias\n",
      "terms (intercepts) are usually not regularized. The weight regularization\n",
      "is typically light, and serves several roles. The `2reduces problems with\n",
      "collinearity, the `1can ignore irrelevant features, and both slow the rate of\n",
      "overÔ¨Åtting, especially with deep (over-parametrized) networks.\n",
      "Early Stopping\n",
      "Neural nets are typically over-parametrized, and hence are prone to overÔ¨Åt-\n",
      "ting. Originally early stopping was set up as the primary tuning parameter,\n",
      "and the stopping time was determined using a held-out set of validation\n",
      "data. In modern networks the regularization is tuned adaptively to avoid\n",
      "overÔ¨Åtting, and hence it is less of a problem. For example, in Figure 18.4\n",
      "we see that the test misclassiÔ¨Åcation error has Ô¨Çattened out, and does not\n",
      "rise again with increasing number of epochs.\n",
      "18.3 Autoencoders\n",
      "An autoencoder is a special neural network for computing a type of non-\n",
      "linear principal-component decomposition.\n",
      "The linear principal component decomposition is a popular and effective\n",
      "linear method for reducing a large set of correlated variables to a typically\n",
      "smaller number of linear combinations that capture most of the variance in\n",
      "the original set. Hence, given a collection of nvectorsxi2Rp(assumed to\n",
      "have mean zero), we produce a derived set of uncorrelated features zi2Rq18.3 Autoencoders 363\n",
      "(q\u0014p, and typically smaller) via ziDV0xi. The columns of Vare or-\n",
      "thonormal, and are derived such that the Ô¨Årst component of zihas maximal\n",
      "variance, the second has the next largest variance and is uncorrelated with\n",
      "the Ô¨Årst, and so on. It is easy to show that the columns of Vare the leading\n",
      "qeigenvectors of the sample covariance matrix SD1\n",
      "nX0X.\n",
      "Principal components can also be derived in terms of a best-approx-\n",
      "imating linear subspace, and it is this version that leads to the nonlinear\n",
      "generalization presented here. Consider the optimization problem\n",
      "minimize\n",
      "ignp\u0002q;f\n",
      "12Rq\u0002nnX\n",
      "ik2kxi\u0000A\n",
      "2; (18.20)\n",
      "forq < p . The subspace is deÔ¨Åned by the column space of A, and for\n",
      "each pointxiwe wish to locate its best approximation in the subspace (in\n",
      "terms of Euclidean distance). Without loss of generality, we can assume A\n",
      "iDA0xifor eachi(nseparatein which case O\n",
      "linear regressions). Plugging in, (18.20) reduces to\n",
      "minimize\n",
      "A2Rp\u0002q;A0ADIqnX\n",
      "iD1kxi\u0000AA0xik2\n",
      "2: (18.21)\n",
      "A solution is given by OADV, the matrix above of the Ô¨Årst qprincipal-\n",
      "component direction vectors computed from the xi. By analogy, a single-\n",
      "layer autoencoder solves a nonlinear version of this problem:\n",
      "minimize\n",
      "W2Rq\u0002pnX\n",
      "iD1kxi\u0000W0g.Wxi/k2\n",
      "2; (18.22)\n",
      "for some nonlinear activation function g; see Figure 18.7 (left panel). If g\n",
      "is the identity function, these solutions coincide (with WDV0).\n",
      "Figure 18.7 (right panel) represents the learned row of Was images,\n",
      "when the autoencoder is Ô¨Åt to the MNIST digit database. Since autoen-\n",
      "coders do not require a response (the class labels in this case), this decom-\n",
      "position is unsupervised. It is often expensive to label images, for example,\n",
      "while unlabeled images are abundant. Autoencoders provide a means for\n",
      "extracting potentially useful features from such data, which can then be\n",
      "used with labeled data to train a classiÔ¨Åer. In fact, they are often used as\n",
      "warm starts for the weights when Ô¨Åtting a supervised neural network.\n",
      "Once again there are a number of bells and whistles that make autoen-\n",
      "coders more effective.364 Neural Networks\n",
      "x1\n",
      "x2\n",
      "x3\n",
      "x4\n",
      "x5ÀÜx1\n",
      "ÀÜx2\n",
      "ÀÜx3\n",
      "ÀÜx4\n",
      "ÀÜx5Input\n",
      "layerHidden\n",
      "layerOutput\n",
      "layer\n",
      "W\n",
      "g(Wx)W‚Ä≤\n",
      "Figure 18.7 Left: Network representation of an autoencoder used\n",
      "for unsupervised learning of nonlinear principal components. The\n",
      "middle layer of hidden units creates a bottleneck, and learns\n",
      "nonlinear representations of the inputs. The output layer is the\n",
      "transpose of the input layer, so the network tries to reproduce the\n",
      "input data using this restrictive representation. Right: Images\n",
      "representing the estimated rows of Wusing the MNIST database;\n",
      "the images can be seen as Ô¨Ålters that detect local gradients in the\n",
      "image pixels. In each image, most of the weights are zero, and the\n",
      "nonzero weights are localized in the two-dimensional image\n",
      "space.\n",
      "\u000f`1regularization applied to the rows of Wlead to sparse weight vectors,\n",
      "and hence local features, as was the case in our example.\n",
      "\u000fDenoising is a process where noise is added to the input layer (but not the\n",
      "output), resulting in features that do not focus on isolated values, such\n",
      "as pixels, but instead have some volume . We discuss denoising further in\n",
      "Section 18.5.\n",
      "\u000fWith regularization, the bottleneck is not necessary, as in the Ô¨Ågure or in\n",
      "principal components. In fact we can learn many more than pcompo-\n",
      "nents.\n",
      "\u000fAutoencoders can also have multiple layers, which are typically learned\n",
      "sequentially. The activations learned in the Ô¨Årst layer are treated as the\n",
      "input (and output) features, and a model like (18.22) is Ô¨Åt to them.\n",
      "18.4 Deep Learning\n",
      "Neural networks were reincarnated around 2010 with ‚Äúdeep learning‚Äù as a\n",
      "Ô¨Çashier name, largely a result of much faster and larger computing systems,\n",
      "plus a few new ideas. They have been shown to be particularly successful18.4 Deep Learning 365\n",
      "in the difÔ¨Åcult task of classifying natural images, using what is known as a\n",
      "convolutional architecture. Initially autoencoders were considered a crucial\n",
      "aspect of deep learning, since unlabeled images are abundant. However,\n",
      "as labeled corpora become more available, the word on the street is that\n",
      "supervised learning is sufÔ¨Åcient.\n",
      "Figure 18.8 shows examples of natural images, each with a class label\n",
      "such as beaver ,sunflower ,trout etc. There are 100 class labels in\n",
      "Figure 18.8 Examples of natural images. The CIFAR-100\n",
      "database consists of 100 color image classes, with 600 examples\n",
      "in each class (500 train, 100 test). Each image is 32\u000232\u00023(red,\n",
      "green, blue). Here we display a randomly chosen image from each\n",
      "class. The classes are organized by hierarchical structure, with 20\n",
      "coarse levels and Ô¨Åve subclasses within each. So, for example, the\n",
      "Ô¨Årst Ô¨Åve images in the Ô¨Årst column are aquatic mammals ,\n",
      "namely beaver ,dolphin ,otter ,seal andwhale .366 Neural Networks\n",
      "all, and 500 training images and 100 test images per class. The goal is to\n",
      "build a classiÔ¨Åer to assign a label to an image. We present the essential\n",
      "details of a deep-learning network for this task‚Äîone that achieves a re-\n",
      "spectable classiÔ¨Åcation performance of 35% errors on the designated test\n",
      "set.2Figure 18.9 shows a typical deep-learning architecture, with many\n",
      "1003232\n",
      "216\n",
      "5004 8\n",
      "convolve pool convolve pool convolve poolconnect \n",
      "fully\n",
      "Figure 18.9 Architecture of a deep-learning network for the\n",
      "CIFAR-100 image classiÔ¨Åcation task. The input layer and\n",
      "hidden layers are all represented as images, except for the last\n",
      "hidden layer, which is ‚ÄúÔ¨Çattened‚Äù (vectorized). The input layer\n",
      "consists of the p1D3color (red, green, and blue) versions of an\n",
      "input image (unlike earlier, here we use the pkto refer to the\n",
      "number of images rather than the totality of pixels). Each of these\n",
      "color panes is 32\u000232pixels in dimension. The Ô¨Årst hidden layer\n",
      "computes a convolution using a bank of p2distinctq\u0002q\u0002p1\n",
      "learned Ô¨Ålters, producing an array of images of dimension\n",
      "p2\u000232\u000232. The next pool layer reduces each non-overlapping\n",
      "block of`\u0002`numbers in each pane of the Ô¨Årst hidden layer to a\n",
      "single number using a ‚Äúmax‚Äù operation. Both qand`are\n",
      "typically small; each was 2 for us. These convolve and pool layers\n",
      "are repeated here three times, with changing dimensions (in our\n",
      "actual implementation, there are 13 layers in total). Finally the\n",
      "500 derived features are Ô¨Çattened, and a fully connected layer\n",
      "maps them to the 100 classes via a ‚Äúsoftmax‚Äù activation.\n",
      "hidden layers. These consist of two special types of layers: ‚Äúconvolve‚Äù and\n",
      "‚Äúpool.‚Äù We describe each in turn.\n",
      "Convolve Layer\n",
      "Figure 18.10 illustrates a convolution layer, and some details are given in\n",
      "2ClassiÔ¨Åcation becomes increasingly difÔ¨Åcult as the number of classes grows. With equal\n",
      "representation in each class, the NULL or random error rate for Kclasses is\n",
      ".K\u00001/=K ; 50% for two classes, 99% for 100.18.4 Deep Learning 367\n",
      "the caption. If an image xis represented by a k\u0002kmatrix, and a Ô¨Ålter f\n",
      "+\n",
      "+\n",
      "... ... ...\n",
      "Figure 18.10 Convolution layer for the input images. The input\n",
      "image is split into its three color components. A single Ô¨Ålter is a\n",
      "q\u0002q\u0002p1array (here one q\u0002qfor each of the p1D3color\n",
      "panes), and is used to compute an inner product with a\n",
      "correspondingly sized subimage in each pane, and summed across\n",
      "thep1panes. We used qD2, and small values are typical. This is\n",
      "repeated over all (overlapping) q\u0002qsubimages (with boundary\n",
      "padding), and hence produces an image of the same dimension as\n",
      "one of the input panes. This is the convolution operation. There\n",
      "arep2different versions of this Ô¨Ålter, and hence p2new panes are\n",
      "produced. Each of the p2Ô¨Ålters hasp1q2weights, which are\n",
      "learned via backpropagation.\n",
      "is aq\u0002qmatrix withq\u001ck, the convolved image is another k\u0002kmatrix\n",
      "Qxwith elementsQxi;jDPq\n",
      "`D1Pq\n",
      "`0D1xiC`;jC`0f`;`0(with edge padding to\n",
      "achieve a full-sized k\u0002koutput image). In our application we used 2\u00022,\n",
      "but other sizes such as 3\u00023are popular. It is most natural to represent\n",
      "the structure in terms of these images as in Figure 18.9, but they could all\n",
      "be vectorized into a massive network diagram as in Figures 18.1 and 18.3.\n",
      "However, the weights would have special sparse structure, with most being\n",
      "zero, and the nonzero values repeated (‚Äúweight sharing‚Äù).368 Neural Networks\n",
      "Pool Layer\n",
      "The pool layer corresponds to a kind of nonlinear activation. It reduces\n",
      "each nonoverlapping block of r\u0002rpixels (rD2for us) to a single number\n",
      "by computing their maximum. Why maximum? The convolution Ô¨Ålters are\n",
      "themselves small image patches, and are looking to identify similar patches\n",
      "in the target image (in which case the inner product will be high). The max\n",
      "operation introduces an element of local translation invariance. The pool\n",
      "operation reduces the size of each image by a factor rin each dimension.\n",
      "To compensate, the number of tiles in the next convolution layer is typically\n",
      "increased accordingly. Also, as these tiles get smaller, the effective weights\n",
      "resulting from the convolution operator become denser. Eventually the tiles\n",
      "are the same size as the convolution Ô¨Ålter, and the layer becomes fully\n",
      "connected.\n",
      "18.5 Learning a Deep Network\n",
      "Despite the additional structure imposed by the convolution layers, deep\n",
      "networks are learned by gradient descent. The gradients are computed by\n",
      "backpropagation as before, but with special care taken to accommodate the\n",
      "tied weights in the convolution Ô¨Ålters. However, a number of additional\n",
      "tricks have been introduced that appear to improve the performance of\n",
      "modern deep learning networks. These are mostly aimed at regularization;\n",
      "indeed, our 100-class image network has around 50 million parameters, so\n",
      "regularization is essential to avoid overÔ¨Åtting. We brieÔ¨Çy discuss some of\n",
      "these.\n",
      "Dropout\n",
      "This is a form of regularization that is performed when learning a network,\n",
      "typically at different rates at the different layers. It applies to all networks,\n",
      "not just convolutional; in fact, it appears to work better when applied at the\n",
      "deeper, denser layers. Consider computing the activation z.k/\n",
      "`in layerkas\n",
      "in (18.3) for a single observation during the feed-forward stage. The idea\n",
      "is to randomly set each of the pk\u00001nodesa.k\u00001/\n",
      "j to zero with probability\n",
      "\u001e, and inÔ¨Çate the remaining ones by a factor 1=.1\u0000\u001e/. Hence, for this ob-\n",
      "servation, those nodes that survive have to stand in for those omitted. This\n",
      "can be shown to be a form of ridge regularization, and when done correctly\n",
      "improves performance.¬éThe fraction\u001eomitted is a tuning parameter, and ¬é4\n",
      "for convolutional networks it appears to be better to use different values at18.5 Learning a Deep Network 369\n",
      "different layers. In particular, as the layers become denser, \u001eis increased:\n",
      "from0in the input layer to 0:5in the Ô¨Ånal, fully connected layer.\n",
      "Input Distortion\n",
      "This is another form of regularization that is particularly suitable for tasks\n",
      "like image classiÔ¨Åcation. The idea is to augment the training set with many\n",
      "distorted copies of an input image (but of course the same label). These\n",
      "distortions can be location shifts and other small afÔ¨Åne transformations, but\n",
      "also color and shading shifts that might appear in natural images. We show\n",
      "Figure 18.11 Each column represents distorted versions of an\n",
      "input image, including afÔ¨Åne and color distortions. The input\n",
      "images are padded on the boundary to increase the size, and\n",
      "hence allow space for some of the distortions.\n",
      "some distorted versions of input images in Figure 18.11. The distortions are\n",
      "such that a human would have no trouble identifying any of the distorted\n",
      "images if they could identify the original.¬éThis both enriches the training ¬é5\n",
      "data with hints , and also prevents overÔ¨Åtting to the original image. One\n",
      "could also apply distortions to a test image, and then ‚Äúpoll‚Äù the results to\n",
      "produce a Ô¨Ånal classiÔ¨Åcation.\n",
      "ConÔ¨Åguration\n",
      "Designing the correct architecture for a deep-learning network, along with\n",
      "the various choices at each layer, appears to require experience and trial370 Neural Networks\n",
      "and error. We summarize the third and Ô¨Ånal architecture which we built\n",
      "for classifying the CIFAR-100 data set in Algorithm 18.2.¬éIn addition to ¬é6\n",
      "these size parameters for each layer, we must select the activation functions\n",
      "and additional regularization. In this case we used the leaky rectiÔ¨Åed linear\n",
      "functionsgÀõ.z/(Section 18.2), with Àõincreasing from 0:05 in layer 5 up to\n",
      "0:5in layer 13. In addition a type of `2regularization was imposed on the\n",
      "weights, restricting all incoming weight vectors to a node to have `2norm\n",
      "bounded by one. Figure 18.12 shows both the progress of the optimization\n",
      "objective (red) and the test misclassiÔ¨Åcation error (blue) as the gradient-\n",
      "descent algorithm proceeds. The accelerated gradient method maintains a\n",
      "memory, which we can see was restarted twice to get out of local minima.\n",
      "Our network achieved a test error rate of 35% on the 10,000 test images\n",
      "(100 images per class). The best reported error rate we have seen is 25%,\n",
      "so apparently we have some way to go!\n",
      "0 50 100 150 200 250 30040 50 60708090\n",
      "EpochTest Misclassification Error\n",
      "300 1900 3900 6300 9200\n",
      "ObjectiveObjective Cost\n",
      "Misclassification Error\n",
      "Figure 18.12 Progress of the algorithm as a function of the\n",
      "number of epochs. The accelerated gradient algorithm is\n",
      "‚Äúrestarted‚Äù every 100 epochs, meaning the long-term memory is\n",
      "forgotten, and a new trail is begun, starting at the current solution.\n",
      "The red curve shows the objective (negative penalized\n",
      "log-likelihood on the training data). The blue curve shows test-set\n",
      "misclassiÔ¨Åcation error. The vertical axis is on the log scale, so\n",
      "zero cannot be included.18.6 Notes and Details 371\n",
      "Algorithm 18.2 CONFIGURATION PARAMETERS FOR DEEP -LEARNING\n",
      "NETWORK USED ON THE CIFAR-100 DATA .\n",
      "Layer 1: 100 convolution maps each with 2\u00022\u00023kernel (the 3 for three\n",
      "colors). The input image is padded from 32\u000232to40\u000240to accom-\n",
      "modate input distortions.\n",
      "Layers 2 and 3: 100 convolution maps each 2\u00022\u0002100. Compositions of\n",
      "convolutions are roughly equivalent to convolutions with a bigger band-\n",
      "width, and the smaller ones have fewer parameters.\n",
      "Layer 4: Max pool2\u00022layer, pooling nonoverlapping 2\u00022blocks of\n",
      "pixels, and hence reducing the images to size 20\u000220.\n",
      "Layer 5: 300 convolution maps each 2\u00022\u0002100, with dropout learning\n",
      "with rate\u001e5D0:05.\n",
      "Layer 6: Repeat of Layer 5.\n",
      "Layer 7: Max pool2\u00022layer (down to 10\u000210images).\n",
      "Layer 8: 600 convolution maps each 2\u00022\u0002300, with dropout rate \u001e8D\n",
      "0:10.\n",
      "Layer 9: 800 convolution maps each 2\u00022\u0002600, with dropout rate \u001e9D\n",
      "0:10.\n",
      "Layer 10: Max pool2\u00022layer (down to 5\u00025images).\n",
      "Layer 11: 1600 convolution maps, each 1\u00021\u0002800. This is a pixelwise\n",
      "weighted sum across the 800 images from the previous layer.\n",
      "Layer 12: 2000 fully connected units, with dropout rate \u001e12D0:25.\n",
      "Layer 13: Final 100 output units, with softmax activation, and dropout rate\n",
      "\u001e13D0:5.\n",
      "18.6 Notes and Details\n",
      "The reader will notice that probability models have disappeared from the\n",
      "development here. Neural nets are elaborate regression methods aimed\n",
      "solely at prediction‚Äînot estimation or explanation in the language of Sec-\n",
      "tion 8.4. In place of parametric optimality criteria, the machine learning\n",
      "community has focused on a set of speciÔ¨Åc prediction data sets, like the\n",
      "digits MNIST corpus and CIFAR-100 , as benchmarks for measuring per-\n",
      "formance.\n",
      "There is a vast literature on neural networks, with hundreds of books and\n",
      "thousands of papers. With the resurgence of deep learning, this literature is\n",
      "again growing. Two early statistical references on neural networks are Rip-\n",
      "ley (1996) and Bishop (1995), and Hastie et al. (2009) devote one chapter\n",
      "to the topic. Part of our description of backpropagation in Section 18.2 was372 Neural Networks\n",
      "guided by Andrew Ng‚Äôs online Stanford lecture notes (Ng, 2015). Bengio\n",
      "et al. (2013) provide a useful review of autoencoders. LeCun et al. (2015)\n",
      "give a brief overview of deep learning, written by three pioneers of this\n",
      "Ô¨Åeld: Yann LeCun, Yoshua Bengio and Geoffrey Hinton; we also bene-\n",
      "Ô¨Åted from reading Ngiam et al. (2010). Dropout learning (Srivastava et al. ,\n",
      "2014) is a relatively new idea, and its connections with ridge regression\n",
      "were most usefully described in Wager et al. (2013). The most popular\n",
      "version of accelerated gradient descent is due to Nesterov (2013). Learn-\n",
      "ing with hints is due to Abu-Mostafa (1995). The material in Sections 18.4\n",
      "and 18.5 beneÔ¨Åted greatly from discussions with Rakesh Achanta (Achanta\n",
      "and Hastie, 2015), who produced some of the color images and diagrams,\n",
      "and designed and Ô¨Åt the deep-learning network to the CIFAR-100 data.\n",
      "¬é1[p. 352] The Neural Information Processing Systems (NIPS, now NeurIPS)\n",
      "conferences started in late Fall 1987 in Denver, Colorado, and post-confer-\n",
      "ence workshops were held at the nearby ski resort at Vail. These are still\n",
      "very popular today, although the venue has changed over the years. The\n",
      "NeurIPS proceedings are refereed, and NeurIPS papers count as publica-\n",
      "tions in most Ô¨Åelds, especially Computer Science and Engineering. Al-\n",
      "though neural networks were initially the main topic of the conferences,\n",
      "a modern NeurIPS conference covers all the latest ideas in machine learn-\n",
      "ing.\n",
      "¬é2[p. 353] MNIST is a curated database of images of handwritten digits\n",
      "(LeCun and Cortes, 2010). There are 60,000 training images, and 10,000\n",
      "test images, each a 28\u000228grayscale image. These data have been used as\n",
      "a testbed for many different learning algorithms, so the reported best error\n",
      "rates might be optimistic.\n",
      "¬é3[p. 360] Tuning parameters. Typical neural network implementations have\n",
      "dozens of tuning parameters, and many of these are associated with the Ô¨Åne\n",
      "tuning of the descent algorithm. We used the h2o.deepLearning func-\n",
      "tion in the Rpackage h2o to Ô¨Åt our model for the MNIST data set. It has\n",
      "around 20 such parameters, although most default to factory-tuned con-\n",
      "stants that have been found to work well on many examples. Arno Candel\n",
      "was very helpful in assisting us with the software.\n",
      "¬é4[p. 368] Dropout and ridge regression. Dropout was originally proposed in\n",
      "Srivastava et al. (2014), and reinterpreted in Wager et al. (2013). Dropout\n",
      "was inspired by the random selection of variables at each tree split in a\n",
      "random forest (Section 17.1). Consider a simple version of dropout for\n",
      "the linear regression problem with squared-error loss. We have an n\u0002p\n",
      "regression matrix X, and a response n-vector y. For simplicity we assume\n",
      "all variables have mean zero, so we can ignore intercepts. Consider the18.6 Notes and Details 373\n",
      "following random least-squares criterion:\n",
      "LI.Àá/D1\n",
      "2nX\n",
      "iD10\n",
      "@yi\u0000pX\n",
      "jD1xijIijÀáj1\n",
      "A2\n",
      ":\n",
      "Here theIijare i.i.d variables8i;jwith\n",
      "IijD\u001a0with probability \u001e;\n",
      "1=.1\u0000\u001e/ with probability 1\u0000\u001e;\n",
      "(this particular form is used so that E≈íIij¬çD1). Using simple probability\n",
      "it can be shown that the expected score equations can be written\n",
      "E\u0014@LI.Àá/\n",
      "@Àá\u0015\n",
      "D\u0000X0yCX0XÀáC\u001e\n",
      "1\u0000\u001eDÀáD0; (18.23)\n",
      "withDDdiagfkx1k2;kx2k2;:::;kxpk2g. Hence the solution is given by\n",
      "OÀáD\u0012\n",
      "X0XC\u001e\n",
      "1\u0000\u001eD\u0013\u00001\n",
      "X0y; (18.24)\n",
      "a generalized ridge regression. If the variables are standardized, the term\n",
      "Dbecomes a scalar, and the solution is identical to ridge regression. With\n",
      "a nonlinear activation function, the interpretation changes slightly; see Wa-\n",
      "geret al. (2013) for details.\n",
      "¬é5[p. 369] Distortion and ridge regression. We again show in a simple ex-\n",
      "ample that input distortion is similar to ridge regression. Assume the same\n",
      "setup as in the previous example, except a different randomized version of\n",
      "the criterion:\n",
      "LN.Àá/D1\n",
      "2nX\n",
      "iD10\n",
      "@yi\u0000pX\n",
      "jD1.xijCnij/Àáj1\n",
      "A2\n",
      ":\n",
      "Here we have added random noise to the prediction variables, and we as-\n",
      "sume this noise is i.i.d .0;\u0015/ . Once again the expected score equations can\n",
      "be written\n",
      "E\u0014@LN.Àá/\n",
      "@Àá\u0015\n",
      "D\u0000X0yCX0XÀáC\u0015ÀáD0; (18.25)\n",
      "because of the independence of all the nijand E.n2\n",
      "ij/D\u0015. Once again\n",
      "this leads to a ridge regression. So replacing each observation pair xi;yi\n",
      "by the collectionfx\u0003b\n",
      "i;yigB\n",
      "bD1, where each x\u0003b\n",
      "iis a noisy version of xi, is\n",
      "approximately equivalent to a ridge regression on the original data.374 Neural Networks\n",
      "¬é6[p. 370] Software for deep learning. Our deep learning convolutional net-\n",
      "work for the CIFAR-100 data was constructed and run by Rakesh Achanta\n",
      "inTheano , a Python-based system (Bastien et al. , 2012; Bergstra et al. ,\n",
      "2010). Theano has a user-friendly language for specifying the host of\n",
      "parameters for a deep-learning network, and uses symbolic differentiation\n",
      "for computing the gradients needed in stochastic gradient descent. In 2015\n",
      "Google announced an open-source version of their TensorFlow software\n",
      "for Ô¨Åtting deep networks.19\n",
      "Support-Vector Machines and Kernel\n",
      "Methods\n",
      "While linear logistic regression has been the mainstay in biostatistics and\n",
      "epidemiology, it has had a mixed reception in the machine-learning com-\n",
      "munity. There the goal is often classiÔ¨Åcation accuracy, rather than statistical\n",
      "inference. Logistic regression builds a classiÔ¨Åer in two steps: Ô¨Åt a condi-\n",
      "tional probability model for Pr .YD1jXDx/, and then classify as a\n",
      "one ifbPr.YD1jXDx/\u00150:5. SVMs bypass the Ô¨Årst step, and build a\n",
      "classiÔ¨Åer directly.\n",
      "Another rather awkward issue with logistic regression is that it fails if\n",
      "the training data are linearly separable! What this means is that, in the\n",
      "feature space, one can separate the two classes by a linear boundary. In\n",
      "cases such as this, maximum likelihood fails and some parameters march\n",
      "off to inÔ¨Ånity. While this might have seemed an unlikely scenario to the\n",
      "early users of logistic regression, it becomes almost a certainty with mod-\n",
      "ernwide genomics data. When p\u001dn(more features than observations),\n",
      "we can typically always Ô¨Ånd a separating hyperplane. Finding an optimal\n",
      "separating hyperplane was in fact the launching point for SVMs. As we\n",
      "will see, they have more than this to offer, and in fact live comfortably\n",
      "alongside logistic regression.\n",
      "SVMs pursued an age-old approach in statistics, of enriching the feature\n",
      "space through nonlinear transformations and basis expansions; a classical\n",
      "example being augmenting a linear regression with interaction terms. A\n",
      "linear model in the enlarged space leads to a nonlinear model in the ambient\n",
      "space. This is typically achieved via the ‚Äúkernel trick,‚Äù which allows the\n",
      "computations to be performed in the n-dimensional space for an arbitrary\n",
      "number of predictors p. As the Ô¨Åeld matured, it became clear that in fact\n",
      "this kernel trick amounted to estimation in a reproducing-kernel Hilbert\n",
      "space.\n",
      "Finally, we contrast the kernel approach in SVMs with the nonparame-\n",
      "teric regression techniques known as kernel smoothing.\n",
      "375376 SVMs and Kernel Methods\n",
      "19.1 Optimal Separating Hyperplane\n",
      "Figure 19.1 shows a small sample of points in R2, each belonging to one of\n",
      "two classes (blue or orange). Numerically we would score these classes as\n",
      "yDC1for say blue, and yD\u00001for orange.1We deÔ¨Åne a two-class lin-\n",
      "ear classiÔ¨Åer via a function f.x/DÀá0Cx0Àá, with the convention that we\n",
      "classify a point x0as +1 iff.x0/>0 , and as -1 iff.x0/<0 (on the fence\n",
      "we Ô¨Çip a coin). Hence the classiÔ¨Åer itself is C.x/Dsign≈íf.x/¬ç . The deci-\n",
      "‚àí1 0 1 2 3‚àí1 0 1 2 3\n",
      "X1X2\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚àí1 0 1 2 3‚àí1 0 1 2 3\n",
      "X1X2\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "Figure 19.1 Left panel: data in two classes in R2. Three potential\n",
      "decision boundaries are shown; each separate the data perfectly.\n",
      "Right panel: the optimal separating hyperplane (a line in R2)\n",
      "creates the biggest margin between the two classes.\n",
      "sion boundary is the set fxjf.x/D0g. We see three different classiÔ¨Åers\n",
      "in the left panel of Figure 19.1, and they all classify the points perfectly.\n",
      "The optimal separating hyperplane is the linear classiÔ¨Åer that creates the\n",
      "largest margin between the two classes, and is shown in the right panel\n",
      "(it is also known as an optimal-margin classiÔ¨Åer). The underlying hope is\n",
      "that, by making a big margin on the training data, it will also classify future\n",
      "observations well.\n",
      "Some elementary geometry¬éshows that the (signed) Euclidean distance ¬é1\n",
      "from a pointx0to the linear decision boundary deÔ¨Åned by fis given by\n",
      "1\n",
      "kÀák2f.x0/: (19.1)\n",
      "With this in mind, for a separating hyperplane the quantity1\n",
      "kÀák2yif.xi/is\n",
      "1In this chapter, theÀô1scoring leads to convenient notation.19.1 Optimal Separating Hyperplane 377\n",
      "the distance of xifrom the decision boundary.2This leads to an optimiza-\n",
      "tion problem for creating the optimal margin classiÔ¨Åer:\n",
      "maximize\n",
      "Àá0;ÀáM (19.2)\n",
      "subject to1\n",
      "kÀák2yi.Àá0Cx0Àá/\u0015M; iD1;:::;n:\n",
      "A rescaling argument reduces this to the simpler form\n",
      "minimize\n",
      "Àá0;ÀákÀák2 (19.3)\n",
      "subject toyi.Àá0Cx0Àá/\u00151; iD1;:::;n:\n",
      "This is a quadratic program, which can be solved by standard techniques\n",
      "in convex optimization.¬éOne noteworthy property of the solution is that ¬é2\n",
      "OÀáDX\n",
      "i2SOÀõixi; (19.4)\n",
      "where Sis the support set . We can see in Figure 19.1 that the margin\n",
      "touches three points (vectors); in this case there are jSjD3support vec-\n",
      "tors, and clearly the orientation of OÀáis determined by them. However, we\n",
      "still have to solve the optimization problem to identify the three points\n",
      "inS, and their coefÔ¨Åcients Àõi; i2S. Figure 19.2 shows an optimal-\n",
      "margin classiÔ¨Åer Ô¨Åt to wide data, that is data where p\u001dn. These are\n",
      "gene-expression measurements on pD3571 genes measured on blood\n",
      "samples from nD72leukemia patients (Ô¨Årst seen in Chapter 1). They\n",
      "were classiÔ¨Åed into two classes, 47 acute lymphoblastic leukemia ( ALL)\n",
      "and 25 myeloid leukemia ( AML). In cases like this, we are typically guar-\n",
      "anteed a separating hyperplane3. In this case 42 of the 72 points are support\n",
      "points. One might be justiÔ¨Åed in thinking that this solution is overÔ¨Åt to this\n",
      "small amount of data. Indeed, when broken into a training and test set, we\n",
      "see that the test data encroaches well into the margin region, but in this\n",
      "case none are misclassiÔ¨Åed. Such classiÔ¨Åers are very popular in the wide-\n",
      "data world of genomics, largely because they seem to work very well. They\n",
      "offer a simple alternative to logistic regression, in a situation where the lat-\n",
      "ter fails. However, sometimes the solution is overÔ¨Åt, and a modiÔ¨Åcation is\n",
      "called for. This same modiÔ¨Åcation takes care of nonseparable situations as\n",
      "well.\n",
      "2Since all the points are correctly classiÔ¨Åed, the sign of f.xi/agrees withyi, hence this\n",
      "quantity is always positive.\n",
      "3Ifn\u0014pC1we can always Ô¨Ånd a separating hyperplane, unless there are exact feature\n",
      "ties across the class barrier!378 SVMs and Kernel Methods\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚àí1.5‚àí1.0‚àí0.50.00.51.01.5‚àí0.2‚àí0.10.00.10.20.3Leukemia: All Data\n",
      "SVM ProjectionPCA 5 Projection‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè ‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚àí1.5‚àí1.0‚àí0.5 0.00.51.0‚àí0.2‚àí0.10.00.10.20.3Leukemia: Train and Test\n",
      "SVM ProjectionPCA 5 Projection‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè ‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "Figure 19.2 Left panel: optimal margin classiÔ¨Åer Ô¨Åt to\n",
      "leukemia data. There are 72 observations from two\n",
      "classes‚Äî47 ALL and 25 AML‚Äîand 3571 gene-expression\n",
      "variables. Of the 72 observations, 42 are support vectors, sitting\n",
      "on the margin. The points are plotted against their Ô¨Åtted classiÔ¨Åer\n",
      "functionOf.x/ , labeled SVM projection, and the Ô¨Åfth principal\n",
      "component of the data (chosen for display purposes, since it has\n",
      "low correlation with the former). Right panel: here the optimal\n",
      "margin classiÔ¨Åer was Ô¨Åt to a random subset of 50 of the 72\n",
      "observations, and then used to classify the remaining 22 (shown\n",
      "in color). Although these points fall on the wrong sides of their\n",
      "respective margins, they are all correctly classiÔ¨Åed.\n",
      "19.2 Soft-Margin ClassiÔ¨Åer\n",
      "Figure 19.3 shows data in R2that are not separable. The generalization to\n",
      "asoftmargin allows points to violate their margin. Each of the violators\n",
      "has a line segment connecting it to its margin, showing the extent of the\n",
      "violation. The soft-margin classiÔ¨Åer solves\n",
      "minimize\n",
      "Àá0;ÀákÀák2\n",
      "subject toyi.Àá0Cx0\n",
      "iÀá/\u00151\u0000\u000fi;\n",
      "\u000fi\u00150; iD1;:::;n; andnX\n",
      "iD1\u000fi\u0014B:(19.5)\n",
      "HereBis the budget for the total amount of overlap. Once again, the solu-\n",
      "tion has the form (19.4), except now the support set Sincludes any vectors\n",
      "on the margin as well as those that violate the margin. The bigger B, the19.3 SVM Criterion as Loss Plus Penalty 379\n",
      "‚àí2‚àí101234‚àí101234\n",
      "X1X2\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚àí2‚àí101234‚àí101234\n",
      "X1X2\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "Figure 19.3 For data that are not separable, such as here, the\n",
      "soft-margin classiÔ¨Åer allows margin violations. The budget Bfor\n",
      "the total measure of violation becomes a tuning parameter. The\n",
      "bigger the budget, the wider the soft margin and the more support\n",
      "points there are involved in the Ô¨Åt.\n",
      "bigger the support set, and hence the more points that have a say in the\n",
      "solution. Hence bigger Bmeans more stability and lower variance. In fact,\n",
      "even for separable data, allowing margin violations via Blets us regularize\n",
      "the solution by tuning B.\n",
      "19.3 SVM Criterion as Loss Plus Penalty\n",
      "It turns out that one can reformulate (19.5) and (19.3) in more traditional\n",
      "terms as the minimization of a loss plus a penalty:\n",
      "minimize\n",
      "Àá0;ÀánX\n",
      "iD1≈í1\u0000yi.Àá0Cx0\n",
      "iÀá/¬çCC\u0015kÀák2\n",
      "2: (19.6)\n",
      "Here the hinge lossLH.y;f.x//D≈í1\u0000yf.x/¬çCoperates on the margin\n",
      "quantityyf.x/ , and is piecewise linear as in Figure 19.4.¬éThe same margin ¬é3\n",
      "quantity came up in boosting in Section 17.4. The quantity ≈í1\u0000yi.Àá0C\n",
      "x0\n",
      "iÀá/¬çCis the cost for xibeing on the wrong side of its margin (the cost\n",
      "is zero if it‚Äôs on the correct side). The correspondence between (19.6) and\n",
      "(19.5) is exact; large \u0015corresponds to large B, and this formulation makes\n",
      "explicit the form of regularization. For separable data, the optimal separat-\n",
      "ing hyperplane solution (19.3) corresponds to the limiting minimum-norm\n",
      "solution as\u0015#0. One can show that the population minimizer of the380 SVMs and Kernel Methods\n",
      "‚àí3 ‚àí2 ‚àí1 0 1 2 30.00.51.01.52.02.53.0\n",
      "yf(x)LossBinomial\n",
      "SVM\n",
      "Figure 19.4 The hinge loss penalizes observation margins yf.x/\n",
      "less thanC1linearly, and is indifferent to margins greater than\n",
      "C1. The negative binomial log-likelihood (deviance) has the\n",
      "same asymptotes, but operates in a smoother fashion near the\n",
      "elbow atyf.x/D1.\n",
      "hinge loss is in fact the Bayes classiÔ¨Åer.4This shows that the SVM is in\n",
      "fact directly estimating the classiÔ¨Åer C.x/2f\u00001;C1g.¬é ¬é4\n",
      "The red curve in Figure 19.4 is (half) the binomial deviance for logistic\n",
      "regression (i.e. f.x/DÀá0Cx0Àáis now modeling logit Pr .YDC1jXD\n",
      "x/). WithYDÀô1, the deviance can also be written in terms of the margin,\n",
      "and the ridged logistic regression corresponding to (19.6) has the form\n",
      "minimize\n",
      "Àá0;ÀánX\n",
      "iD1log≈í1Ce\u0000yi.Àá0Cx0\n",
      "iÀá/¬çC\u0015kÀák2\n",
      "2: (19.7)\n",
      "Logistic regression is discussed in Section 8.1, as well as Sections 16.5 and\n",
      "17.4. This form of the binomial deviance is derived in (17.13) on page 343.\n",
      "These loss functions have some features in common, as can be seen in the\n",
      "Ô¨Ågure. The binomial loss asymptotes to zero for large positive margins, and\n",
      "to a linear loss for large negative margins, matching the hinge loss in this\n",
      "regard. The main difference is that the hinge has a sharp elbow at +1, while\n",
      "the binomial bends smoothly. A consequence of this is that the binomial\n",
      "solution involves all the data, via weights pi.1\u0000pi/that fade smoothly\n",
      "with distance from the decision boundary, as opposed to the binary nature\n",
      "4The Bayes classiÔ¨Åer C.x/ for a two-class problem using equal costs for\n",
      "misclassiÔ¨Åcation errors assigns xto the class for which Pr .yjx/is largest.19.4 Computations and the Kernel Trick 381\n",
      "of support points. Also, as seen in Section 17.4 as well, the population\n",
      "minimizer of the binomial deviance is the logit of the class probability\n",
      "\u0015.x/Dlog\u0012Pr.yDC1jx/\n",
      "Pr.yD\u00001jx/\u0013\n",
      "; (19.8)\n",
      "while that of the hinge loss is its sign C.x/Dsign≈í\u0015.x/¬ç . Interestingly,\n",
      "as\u0015#0the solution direction OÀáto the ridged logistic regression prob-\n",
      "lem (19.7) converges to that of the SVM.¬é ¬é5\n",
      "These forms immediately suggest other generalizations of the linear\n",
      "SVM. In particular, we can replace the ridge penalty kÀák2\n",
      "2by the sparsity-\n",
      "inducing lasso penalty kÀák1, which will set some coefÔ¨Åcients to zero and\n",
      "hence perform feature selection. Publicly available software (e.g. package\n",
      "liblineaR inR) is available for Ô¨Åtting such lasso-regularized support-\n",
      "vector classiÔ¨Åers.\n",
      "19.4 Computations and the Kernel Trick\n",
      "The form of the solution OÀáDP\n",
      "i2SOÀõixifor the optimal- and soft-margin\n",
      "classiÔ¨Åer has some important consequences. For starters, we can write the\n",
      "Ô¨Åtted function evaluated at a point xas\n",
      "Of.x/DOÀá0Cx0OÀá\n",
      "DOÀá0CX\n",
      "i2SOÀõihx;xii;(19.9)\n",
      "where we have deliberately replaced the transpose notation with the more\n",
      "suggestive inner product. Furthermore, we show in (19.23) in Section 19.9\n",
      "that the Lagrange dual involves the data only through the n2pairwise inner\n",
      "productshxi;xji(the elements of the n\u0002ngram matrixXX0). This means\n",
      "that the computations for computing the SVM solution scale linearly with\n",
      "p, although potentially cubic5inn. With very large p(in the tens of thou-\n",
      "sands and even millions as we will see), this can be convenient.\n",
      "It turns out that all ridge-regularized linear models with wide data can\n",
      "be reparametrized in this way. Take ridge regression, for example:\n",
      "minimize\n",
      "Àáky\u0000XÀák2\n",
      "2C\u0015kÀák2\n",
      "2: (19.10)\n",
      "This has solutionOÀáD.X0XC\u0015Ip/\u00001X0y, and withplarge requires\n",
      "inversion of a p\u0002pmatrix. However, it can be shown that OÀáDX0OÀõD\n",
      "5In practiceO.n2jSj/, and, with modern approximate solutions, much faster than that.382 SVMs and Kernel Methods\n",
      "Pn\n",
      "iD1OÀõixi, withOÀõD.XX0C\u0015In/\u00001y, which means the solution can\n",
      "be obtained in O.n2p/rather thanO.np2/computations. Again the gram\n",
      "matrix has played a role, and OÀáhas the same form as for the SVM.¬é ¬é6\n",
      "We now imagine expanding the p-dimensional feature vector xinto a\n",
      "potentially much larger set h.x/D≈íh1.x/;h2.x/;:::;h m.x/¬ç; for an ex-\n",
      "ample to latch onto, think polynomial basis of total degree d. As long as we\n",
      "have an efÔ¨Åcient way to compute the inner products hh.x/;h.xj/ifor any\n",
      "x, we can compute the SVM solution in this enlarged space just as easily\n",
      "as in the original. It turns out that convenient kernel functions exist that do\n",
      "just that. For example Kd.x;z/D.1Chx;zi/dcreates a basis expansion\n",
      "hdof polynomials of total degree d, andKd.x;z/Dhhd.x/;hd.z/i.¬é ¬é7\n",
      "The polynomial kernels are mainly useful as existence proofs; in practice\n",
      "other more useful kernels are used. Probably the most popular is the radial\n",
      "kernel\n",
      "kx\u0000zk2De\u0000\n",
      "2: (19.11)\n",
      "This is a positive-deÔ¨Ånite function, and can be thought of as computing an\n",
      "inner product in some feature space. Here the feature space is in principle\n",
      "inÔ¨Ånite-dimensional, but of course effectively Ô¨Ånite.6Now one can think\n",
      "of the representation (19.9) in a different light;\n",
      "Of.x/DOÀõ0CX\n",
      "i2SOÀõiK.x;xi/; (19.12)\n",
      "an expansion of radial basis functions, each centered on one of the train-\n",
      "ing examples. Figure 19.5 illustrates such an expansion in R1. Using such\n",
      "nonlinear kernels expands the scope of SVMs considerably, allowing one\n",
      "to Ô¨Åt classiÔ¨Åers with nonlinear decision boundaries.\n",
      "One may ask what objective is being optimized when we move to this\n",
      "kernel representation. This is covered in the next section, but as a sneak\n",
      "preview we present the criterion\n",
      "minimize\n",
      "Àõ0;ÀõnX\n",
      "jD1\"\n",
      "1\u0000yj \n",
      "Àõ0CnX\n",
      "iD1ÀõiK.xj;xi/!#\n",
      "CC\u0015Àõ0KÀõ;(19.13)\n",
      "where then\u0002nmatrixKhas entriesK.xj;xi/.\n",
      "As an illustrative example in R2(so we can visualize the nonlinear\n",
      "boundaries), we generated the data in Figure 19.6. We show two SVM\n",
      "6A bivariate function K.x;z/ (Rp\u0002Rp7!R1) is positive-deÔ¨Ånite if, for every q,\n",
      "everyq\u0002qmatrixKDfK.xi;xj/gformed using distinct entries x1;x2;:::;xqis\n",
      "positive deÔ¨Ånite. The feature space is deÔ¨Åned in terms of the eigen-functions of the\n",
      "kernel.19.4 Computations and the Kernel Trick 383\n",
      "‚àí2 ‚àí1 0 1 20.0 0.5 1.0 1.5\n",
      "‚àí2 ‚àí1 0 1 2‚àí0.4 0.0 0.2 0.4Radial Basis Functions\n",
      "f(x)K(x, x j)f(x) =Œ±0+‚àë\n",
      "jŒ±jK(x, x j)\n",
      "x x\n",
      "Figure 19.5 Radial basis functions in R1. The left panel shows a\n",
      "collection of radial basis functions, each centered on one of the\n",
      "seven observations. The right panel shows a function obtained\n",
      "from a particular linear expansion of these basis functions.\n",
      "solutions, both using a radial kernel. In the left panel, some margin errors\n",
      "are committed, but the solution looks reasonable. However, with the Ô¨Çex-\n",
      "ibility of the enlarged feature space, by decreasing the budget Bwe can\n",
      "typically overÔ¨Åt the training data, as is the case in the right panel. A sepa-\n",
      "rate little blue island was created to accommodate the one blue point in a\n",
      "sea of brown.\n",
      "‚àí4 ‚àí2 0 2 4‚àí4 ‚àí2 0 2 4\n",
      "X1X2\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè  \n",
      "  \n",
      "    \n",
      "‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè\n",
      "‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚àí4 ‚àí2 0 2 4‚àí4 ‚àí2 0 2 4\n",
      "X1X2\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè      \n",
      "    \n",
      "  \n",
      "    \n",
      "    \n",
      "‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè\n",
      "‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè ‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "Figure 19.6 Simulated data in two classes in R2, with SVM\n",
      "classiÔ¨Åers computed using the radial kernel (19.11). The left\n",
      "panel uses a larger value of Bthan the right. The solid lines are\n",
      "the decision boundaries in the original space (linear boundaries in\n",
      "the expanded feature space). The dashed lines are the projected\n",
      "margins in both cases.384 SVMs and Kernel Methods\n",
      "19.5 Function Fitting Using Kernels\n",
      "The analysis in the previous section is heuristic‚Äîreplacing inner products\n",
      "by kernels that compute inner products in some (implicit) feature space.\n",
      "Indeed, this is how kernels were Ô¨Årst introduced in the SVM world. There\n",
      "is however a rich literature behind such approaches, which goes by the\n",
      "name function Ô¨Åtting in reproducing-kernel Hilbert spaces (RKHSs) . We\n",
      "give a very brief overview here. One starts with a bivariate positive-deÔ¨Ånite\n",
      "kernelKWRp\u0002Rp!R1, and we consider a space HKof functions\n",
      "fWRp!R1generated by the kernel: f2spanfK.\u0001;z/; z2Rpg7The\n",
      "kernel also induces a norm on the space kfkHK,¬éwhich can be thought of ¬é8\n",
      "as a roughness measure.\n",
      "We can now state a very general optimization problem for Ô¨Åtting a func-\n",
      "tion to data, when restricted to this class;\n",
      "minimize\n",
      "f2HK(nX\n",
      "iD1L.yi;Àõ0Cf.xi//C\u0015kfk2\n",
      "HK)\n",
      "; (19.14)\n",
      "a search over a possibly inÔ¨Ånite-dimensional function space. Here Lis an\n",
      "arbitrary loss function. The ‚Äúmagic‚Äù of these spaces in the context of this\n",
      "problem is that one can show that the solution is Ô¨Ånite-dimensional:\n",
      "Of.x/DnX\n",
      "iD1OÀõiK.x;xi/; (19.15)\n",
      "a linear basis expansion with basis functions ki.x/DK.x;xi/anchored\n",
      "at each of the observed ‚Äúvectors‚Äù xiin the training data. Moreover, using\n",
      "the ‚Äúreproducing‚Äù property of the kernel in this space, one can show that\n",
      "the penalty reduces to\n",
      "kOfk2\n",
      "HKDnX\n",
      "iD1nX\n",
      "jD1OÀõiOÀõjK.xi;xj/DOÀõ0KOÀõ: (19.16)\n",
      "HereKis then\u0002ngram matrix of evaluations of the kernel, equivalent to\n",
      "theXX0matrix for the linear case.\n",
      "Hence the abstract problem (19.14) reduces to the generalized ridge\n",
      "problem\n",
      "minimize\n",
      "Àõ2Rn8\n",
      "<\n",
      ":nX\n",
      "iD1L0\n",
      "@yi;Àõ0CnX\n",
      "jD1ÀõiK.xi;xj/1\n",
      "AC\u0015Àõ0KÀõ9\n",
      "=\n",
      ";:(19.17)\n",
      "7HerekzDK.\u0001;z/is considered a function of the Ô¨Årst argument, and the second\n",
      "argument is a parameter.19.6 Example: String Kernels for Protein ClassiÔ¨Åcation 385\n",
      "Indeed, ifLis the hinge loss as in (19.6), this is the equivalent ‚Äúloss plus\n",
      "penalty‚Äù criterion being Ô¨Åt by the kernel SVM. Alternatively, if Lis the bi-\n",
      "nomial deviance loss as in (19.7), this would Ô¨Åt a kernel version of logistic\n",
      "regression. Hence most Ô¨Åtting methods can be generalized to accommodate\n",
      "kernels.\n",
      "This formalization opens the door to a wide variety of applications, de-\n",
      "pending on the kernel function used. Alternatively, as long as we can com-\n",
      "pute suitable similarities between objects, we can build sophisticated clas-\n",
      "siÔ¨Åers and other models for making predictions about other attributes of\n",
      "the objects.8In the next section we consider a particular example.\n",
      "19.6 Example: String Kernels for Protein ClassiÔ¨Åcation\n",
      "One of the important problems in computational biology is to classify pro-\n",
      "teins into functional and structural classes based on their sequence simi-\n",
      "larities. Protein molecules can be thought of as strings of amino acids, and\n",
      "differ in terms of length and composition. In the example we consider, the\n",
      "lengths vary between 75 and 160 amino-acid molecules, each of which can\n",
      "be one of 20 different types, labeled using the letters of the alphabet.\n",
      "Here follow two protein examples x1andx2, of length 110 and 153\n",
      "respectively:\n",
      "IPTSALVKETLALLSTHRTLLIANETLRIPVPVHKNHQLCTEEIFQGIGTLESQTVQGGTV\n",
      "ERLFKNLSLIKKYIDGQKKKCGEERRRVNQFLDY LQE FLGVMNTEWI\n",
      "PHRRDLCSRSIWLARKIRSDLTALTESYVKHQGLWSELTEAER LQE NLQAYRTFHVLLA\n",
      "RLLEDQQVHFTPTEGDFHQAIHTLLLQV AAFAYQIEELMILLEYKIPRNEADGMLFEKK\n",
      "LWGLKV LQE LSQWTVRSIHDLRFISSHQTGIP\n",
      "We treat the proteins xas documents consisting of letters, with a dictio-\n",
      "nary of size 20. Our feature vector hm.x/will consist of the counts for all\n",
      "m-grams in the protein‚Äîthat is, distinct sequences of consecutive letters of\n",
      "lengthm. As an illustration, we use mD3, which results in 203D8,000\n",
      "possible sub-sequences; hence h3.x/will be a vector of length 8,000, with\n",
      "each element the number of times that particular sub-sequence occurs in\n",
      "the proteinx. In our example, the sub-sequence LQE occurs once in the\n",
      "Ô¨Årst, and twice in the second protein, so h3\n",
      "LQE.x1/D1andh3\n",
      "LQE.x2/D2.\n",
      "The number of possible sequences of length mis20m, which can be very\n",
      "8As long as the similarities behave like inner products; i.e. they form positive\n",
      "semi-deÔ¨Ånite matrices.386 SVMs and Kernel Methods\n",
      "large for moderate m. Also the vast majority of the sub-sequences do not\n",
      "match the strings in our training set, which means hm.x/will be sparse. It\n",
      "turns out that we can compute the n\u0002ninner product matrix or string kernel\n",
      "Km.x1;x2/Dhhm.x1/;hm.x2/iefÔ¨Åciently using tree structures, without\n",
      "actually computing the individual vectors.¬éArmed with the kernel, we ¬é9\n",
      "Protein Classification\n",
      "False-positive rateTrue-positive rate\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.0 0.2 0.4 0.6 0.8 1.0AUC\n",
      "SVM  0.84\n",
      "KLR  0.78\n",
      "Figure 19.7 ROC curves for two classiÔ¨Åers Ô¨Åt to the protein data.\n",
      "The ROC curves were computed using 10-fold cross-validation,\n",
      "and trace the trade-off between false-positive and true-positive\n",
      "error rates as the classiÔ¨Åer threshold is varied. The area under the\n",
      "curve (AUC) summarizes the overall performance of each\n",
      "classiÔ¨Åer. Here the SVM is slightly superior to kernel logistic\n",
      "regression.\n",
      "can now use it to Ô¨Åt a regularized SVM or logistic regression model, as\n",
      "outlined in the previous section. The data consist of 1708 proteins in two\n",
      "classes‚Äînegative (1663) and positive (45). We Ô¨Åt both the kernel SVM\n",
      "and kernel logistic regression models. For both methods, cross-validation\n",
      "suggested a very small value for \u0015. Figure 19.7 shows the ROC trade-off\n",
      "curve for each, using 10-fold cross-validation. Here the SVM outperforms\n",
      "logistic regression.19.7 SVMs: Concluding Remarks 387\n",
      "19.7 SVMs: Concluding Remarks\n",
      "SVMs have been wildly successful, and are one of the ‚Äúmust have‚Äù tools in\n",
      "any machine-learning toolbox. They have been extended to cover many dif-\n",
      "ferent scenarios, other than two-class classiÔ¨Åcation, with some awkward-\n",
      "ness in cases. The extension to nonlinear function-Ô¨Åtting via kernels (in-\n",
      "spiring the ‚Äúmachine‚Äù in the name) generated a mini industry. Kernels are\n",
      "parametrized, learned from data, with special problem-speciÔ¨Åc structure,\n",
      "and so on.\n",
      "On the other hand, we know that Ô¨Åtting high-dimensional nonlinear\n",
      "functions is intrinsically difÔ¨Åcult (the ‚Äúcurse of dimensionality‚Äù), and SVMs\n",
      "are not immune. The quadratic penalty implicit in kernel methodology\n",
      "means all features are included in the model, and hence sparsity is gen-\n",
      "erally not an option. Why then this unbridled enthusiasm? ClassiÔ¨Åers are\n",
      "far less sensitive to bias‚Äìvariance tradeoffs, and SVMs are mostly popular\n",
      "for their classiÔ¨Åcation performance. The ability to deÔ¨Åne a kernel for mea-\n",
      "suring similarities between abstract objects, and then train a classiÔ¨Åer, is a\n",
      "novelty added by these approaches that was missed in the past.\n",
      "19.8 Kernel Smoothing and Local Regression\n",
      "The phrase ‚Äúkernel methodology‚Äù might mean something a little different\n",
      "to statisticians trained in the 1970‚Äì90 period. Kernel smoothing represents\n",
      "a broad range of tools for performing non- and semi-parametric regres-\n",
      "sion. Figure 19.8 shows a Gaussian kernel smooth Ô¨Åt to some artiÔ¨Åcial data\n",
      "fxi;yign\n",
      "1. It computes at each point x0a weighted average of the y-values\n",
      "of neighboring points, with weights given by the height of the kernel. In its\n",
      "simplest form, this estimate can be written as\n",
      "Of.x0/DnX\n",
      ".x0;xi/; (19.18)\n",
      ".90;xi/represents the radial kernel with width parameter \n",
      "Notice the similarity to (19.15); here the OÀõiDyi, and the complexity of\n",
      ". Despite this similarity, and the use of the\n",
      "same kernel, these methodologies are rather different.\n",
      "The focus here is on local estimation, and the kernel does the local-\n",
      "izing. Expression (19.18) is almost a weighted average‚Äîalmost because\n",
      ".388 SVMs and Kernel Methodssian density with mean \u0016and variance1=\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "0.0 0.2 0.4 0.6 0.8 1.0‚àí1.0‚àí0.50.00.51.01.5Gaussian Kernel\n",
      "XY‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "x0\n",
      "Figure 19.8 A Gaussian kernel smooth of simulated data. The\n",
      "points come from the blue curve with added random errors. The\n",
      "kernel smoother Ô¨Åts a weighted mean of the observations, with\n",
      "the weighting kernel centered at the target point, x0in this case.\n",
      "The points shaded orange contribute to the Ô¨Åt at x0. Asx0moves\n",
      "across the domain, the smoother traces out the green curve. The\n",
      "width of the kernel is a tuning parameter. We have depicted the\n",
      "Gaussian weighting kernel in this Ô¨Ågure for illustration; in fact its\n",
      "vertical coordinates are all positive and integrate to one.\n",
      "Pn\n",
      ".x0;xi/\u00191. In fact, the Nadaraya‚ÄìWatson estimator is more ex-\n",
      "plicit:\n",
      "OfNW.x0/DPn\n",
      ".x0;xi/Pn\n",
      ".x0;xi/: (19.19)\n",
      "Although Figure 19.8 is one-dimensional, the same formulation applies to\n",
      "xin higher dimensions.\n",
      "Weighting kernels other than the Gaussian are typically favored; in par-\n",
      "ticular, near-neighbor kernels with compact support. For example, the tricube\n",
      "kernel used by the lowess smoother in Ris deÔ¨Åned as follows:\n",
      "1 DeÔ¨ÅnediDkx0\u0000xik2; iD1;:::;n , and letd.m/be themth smallest\n",
      "(the distance of the mth nearest neighbor to x0). LetuiDdi=d.m/; iD\n",
      "1;:::;n .19.8 Kernel Smoothing and Local Regression 389\n",
      "2 The tricube kernel is given by\n",
      "Ks.x0;xi/D(\u0000\n",
      "1\u0000u3\n",
      "i\u00013ifui\u00141;\n",
      "0otherwise,(19.20)\n",
      "wheresDm=n , the span of the kernel. Near-neighbor kernels such as\n",
      "this adapt naturally to the local density of the xi; wider in low-density\n",
      "regions, narrower in high-density regions. A tricube kernel is illustrated\n",
      "in Figure 19.9.\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "0.0 0.2 0.4 0.6 0.8 1.0‚àí1.0‚àí0.50.00.51.01.5Local Regression (tricube)\n",
      "XY\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "x0\n",
      "Figure 19.9 Local regression Ô¨Åt to the simulated data. At each\n",
      "pointx0, we Ô¨Åt a locally weighted linear least-squares model, and\n",
      "use the Ô¨Åtted value to estimate OfLR.x0/. Here we use the tricube\n",
      "kernel (19.20), with a span of 25%. The orange points are in the\n",
      "weighting neighborhood, and we see the orange linear Ô¨Åt\n",
      "computed by kernel weighted least squares. The green dot is the\n",
      "Ô¨Åtted value at x0from this local linear Ô¨Åt.\n",
      "Weighted means suffer from boundary bias‚Äîwe can see in Figure 19.8 that\n",
      "the estimate appears biased upwards at both boundaries. The reason is that,\n",
      "for example on the left, the estimate for the function on the boundary aver-\n",
      "ages points always to the right, and since the function is locally increasing,\n",
      "there is an upward bias. Local linear regression is a natural generalization\n",
      "that Ô¨Åxes such problems. At each point x0we solve the following weighted390 SVMs and Kernel Methods\n",
      "least-squares problem\n",
      ".OÀá0.x0/;OÀá.x0//Darg min\n",
      "Àá0;ÀánX\n",
      "iD1Ks.x0;xi/.yi\u0000Àá0\u0000xiÀá/2:(19.21)\n",
      "ThenOfLR.x0/DOÀá0.x0/Cx0OÀá.x0/. One can show that, to Ô¨Årst order,\n",
      "OfLR.x0/removes the boundary bias exactly.¬é ¬é10\n",
      "Figure 19.9 illustrates the procedure on our simulated data, using the\n",
      "tricube kernel with a span of 25% of the data. In practice, the width of the\n",
      "kernel (the span here) has to be selected by some means; typically we use\n",
      "cross-validation.\n",
      "Local regression works in any dimension; that is, we can Ô¨Åt two- or\n",
      "higher-dimensional surfaces using exactly the same technique. Here the\n",
      "ability to remove boundary bias really pays off, since the boundaries can\n",
      "be complex. These are referred to as memory-based methods , since there\n",
      "is no Ô¨Åtted model. We have to save all the training data, and recompute the\n",
      "local Ô¨Åt every time we make a prediction.\n",
      "Like kernel SVMs and their relatives, kernel smoothing and local regres-\n",
      "sion break down in high dimensions. Here the near neighborhoods become\n",
      "so wide that they are no longer local.\n",
      "19.9 Notes and Details\n",
      "In the late 1980s and early 1990s, machine-learning research was largely\n",
      "driven by prediction problems, and the neural-network community at AT&T\n",
      "Bell laboratories was amongst the leaders. The problem of the day was\n",
      "the US Post-OfÔ¨Åce handwritten zip-code OCR challenge‚Äîa 10-class im-\n",
      "age classiÔ¨Åcation problem. Vladimir Vapnik was part of this team, and\n",
      "along with colleagues invented a more direct approach to classiÔ¨Åcation, the\n",
      "support-vector machine. This started with the seminal paper by Boser et al.\n",
      "(1992), which introduced the optimal margin classiÔ¨Åer (optimal separating\n",
      "hyperplane); see also Vapnik (1996). The ideas took off quite rapidly, at-\n",
      "tracting a large cohort of researchers, and evolved into the more general\n",
      "class of ‚Äúkernel‚Äù methods‚Äîthat is, models framed in reproducing-kernel\n",
      "Hilbert spaces. A good general reference is Sch ¬®olkopf and Smola (2001).\n",
      "¬é1[p. 376] Geometry of separating hyperplanes. Letf.x/DÀá0xCÀá0de-\n",
      "Ô¨Åne a linear decision boundary fxjf.x/D0ginRp(an afÔ¨Åne set of co-\n",
      "dimension one). The unit vector normal to the boundary is Àá=kÀák2, where\n",
      "k\u0001k2denotes the`2or Euclidean norm. How should one compute the dis-\n",
      "tance from a point xto this boundary? If x0is any point on the boundary19.9 Notes and Details 391\n",
      "(i.e.f.x0/D0), we can project x\u0000x0onto the normal, giving us\n",
      "Àá0.x\u0000x0/\n",
      "kÀák2D1\n",
      "kÀák2f.x/;\n",
      "as claimed in (19.1). Note that this is the signed distance, since f.x/ will\n",
      "be positive or negative depending on what side of the boundary it lies on.\n",
      "¬é2[p. 377] The ‚Äúsupport‚Äù in SVM. The Lagrange primal problem correspond-\n",
      "ing to (19.3) can be written as\n",
      "minimize\n",
      "Àá0;Àá(\n",
      "1\n",
      "2Àá0ÀáCnX\n",
      "i≈í1\u0000yi.Àá0Cx0\n",
      "iÀá/¬ç)\n",
      "; (19.22)\n",
      "i\u00150are the Lagrange multipliers. On differentiating we Ô¨Ånd that\n",
      "ÀáDPn\n",
      "iyixiandPn\n",
      "i, we get (19.4), and\n",
      "iwill lead to some of the Àõibeingnt on \n",
      "zero. Plugging into (19.22) we obtain the Lagrange dual problem\n",
      "maximize\n",
      "ign\n",
      "1(nX\n",
      "i\u00001\n",
      "2nX\n",
      "iD1nX\n",
      "jyiyjx0\n",
      "ixj9\n",
      "=\n",
      ";\n",
      "i\u00150;nXt to\n",
      "iD0:(19.23)\n",
      "¬é3[p. 379] The SVM loss function. The constraint in (19.5) can be succinctly\n",
      "captured via the expression\n",
      "nX\n",
      "iD1≈í1\u0000yi.Àá0Cx0\n",
      "iÀá/¬çC\u0014B: (19.24)\n",
      "We only require a (positive) \u000fiif our margin is less than 1, and we get\n",
      "charged for the sum of these \u000fi. We now use a Lagrange multiplier to en-\n",
      "force the constraint, leading to\n",
      "minimize\n",
      "Àá0;ÀákÀák2\n",
      "nX\n",
      "iD1≈í1\u0000yi.Àá0Cx0\n",
      "iÀá/¬çC: (19.25)\n",
      "gives us (19.6).D1=\n",
      "¬é4[p. 380] The SVM estimates a classiÔ¨Åer. The following derivation is due to\n",
      "Wahba et al. (2000). Consider\n",
      "minimize\n",
      "f.x/EYjXDxf≈í1\u0000Yf.x/¬çCg: (19.26)392 SVMs and Kernel Methods\n",
      "Dropping the dependence on x, the objective can be written as PC≈í1\u0000\n",
      "f¬çCCP\u0000≈í1Cf¬çC, wherePCDPr.YDC1jXDx/, andP\u0000DPr.YD\n",
      "\u00001jXDx/D1\u0000PC. From this we see that\n",
      "fD\u001aC1ifPC>1\n",
      "2\n",
      "\u00001ifP\u0000<1\n",
      "2:(19.27)\n",
      "¬é5[p. 381] SVM and ridged logistic regression. Rosset et al. (2004) show\n",
      "that the limiting solution as \u0015#0to (19.7) for separable data coincides\n",
      "with that of the SVM, in the sense that OÀá=kOÀák2converges to the same\n",
      "quantity for the SVM. However, because of the required normalization\n",
      "for logistic regression, the SVM solution is preferable. On the other hand,\n",
      "for overlapped situations, the logistic-regression solution has some advan-\n",
      "tages, since its target is the logit of the class probabilities.\n",
      "¬é6[p. 382] The kernel trick. The trick here is to observe that from the score\n",
      "equations we have\u0000X0.y\u0000XÀá/C\u0015ÀáD0, which means we can write\n",
      "OÀáDX0Àõfor someÀõ. We now plug this into the score equations, and\n",
      "some simple manipulation gives the result. A similar result holds for ridged\n",
      "logistic regression, and in fact any linear model with a ridge penalty on the\n",
      "coefÔ¨Åcients (Hastie and Tibshirani, 2004).\n",
      "¬é7[p. 382] Polynomial kernels. ConsiderK2.x;z/D.1Chx;zi/2, forx\n",
      "(andz) inR2. Expanding we get\n",
      "K2.x;z/D1C2x1z1C2x2z2C2x1x2z1z2Cx2\n",
      "1z2\n",
      "1Cx2\n",
      "2z2\n",
      "2:\n",
      "This corresponds to hh2.x/;h2.z/iwith\n",
      "h2.x/D.1;p\n",
      "2x1;p\n",
      "2x2;p\n",
      "2x1x2;x2\n",
      "1;x2\n",
      "2/:\n",
      "The same is true for p>2 and for degree d >2 .\n",
      "¬é8[p. 384] Reproducing kernel Hilbert spaces. SupposeKhas eigen expan-\n",
      "sionK.x;z/DP1\n",
      "i\u00150andP1.z/, with\n",
      "i<1. Then\n",
      "we sayf2HKiff.x/DP1\n",
      "iD1ci\u001ei.x/, with\n",
      "kfk2\n",
      "HK\u00111X\n",
      "iD1c2\n",
      "i\n",
      "i<1: (19.28)\n",
      "OftenkfkHKbehaves like a roughness penalty, in that it penalizes unlikely\n",
      "members in the span of K.\u0001;z/(assuming that these correspond to ‚Äúrough‚Äù\n",
      "functions). If fhas some high loadings cjon functions \u001ejwith small\n",
      "j(i.e. not prominent members of the span), the norm becomes\n",
      "large. Smoothing splines and their generalizations correspond to function\n",
      "Ô¨Åtting in a RKHS (Wahba, 1990).19.9 Notes and Details 393\n",
      "¬é9[p. 386] This methodology and the data we use in our example come from\n",
      "Leslie et al. (2003).\n",
      "¬é10[p. 390] Local regression and bias reduction. By expanding the unknown\n",
      "truef.x/ in a Ô¨Årst-order Taylor expansion about the target point x0, one\n",
      "can show that EOfLR.x0/\u0019f.x0/(Hastie and Loader, 1993).20\n",
      "Inference After Model Selection\n",
      "The classical theory of model selection focused on ‚Äú Ftests‚Äù performed\n",
      "within Gaussian regression models. Inference after model selection (for in-\n",
      "stance, assessing the accuracy of a Ô¨Åtted regression curve) was typically\n",
      "done ignoring the model selection process. This was a matter of neces-\n",
      "sity: the combination of discrete model selection and continuous regression\n",
      "analysis was too awkward for simple mathematical description. Electronic\n",
      "computation has opened the door to a more honest analysis of estimation\n",
      "accuracy, one that takes account of the variability induced by data-based\n",
      "model selection.\n",
      "Figure 20.1 displays the cholesterol data, an example we will use\n",
      "for illustration in what follows: cholestyramine, a proposed cholesterol-\n",
      "lowering drug, was administered to nD164men for an average of seven\n",
      "years each. The response variable diwas theith man‚Äôs decrease in choles-\n",
      "terol level over the course of the experiment. Also measured was ci, his\n",
      "compliance or the proportion of the intended dose actually taken, ranging\n",
      "from 1 for perfect compliers to zero for the four men who took none at all.\n",
      "Here the 164 civalues have been transformed to approximately follow a\n",
      "standard normal distribution,\n",
      "ciP \u0018N.0;1/: (20.1)\n",
      "We wish to predict cholesterol decrease from compliance. Polynomial\n",
      "regression models, with diaJth-order polynomial in ci, were considered,\n",
      "for degreesJD1;2;3;4;5 , or 6. TheCpcriterion (12.51) was applied\n",
      "and selected a cubic model, JD3, as best. The curve in Figure 20.1 is the\n",
      "OLS (ordinary least squares) cubic regression curve Ô¨Åt to the cholesterol\n",
      "data set\n",
      "f.ci;di/; iD1;2;:::;164g: (20.2)\n",
      "We are interested in answering the following question: how accurate is the\n",
      "39420.1 Simultaneous ConÔ¨Ådence Intervals 395\n",
      "*\n",
      "***\n",
      "**\n",
      "****\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "****\n",
      "*\n",
      "*\n",
      "***\n",
      "*\n",
      "*****\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "****\n",
      "*\n",
      "***\n",
      "*****\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "***\n",
      "**\n",
      "**\n",
      "**\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "*****\n",
      "***\n",
      "**\n",
      "*\n",
      "**\n",
      "**\n",
      "**\n",
      "***\n",
      "*\n",
      "***\n",
      "***\n",
      "**\n",
      "**\n",
      "*\n",
      "***\n",
      "**\n",
      "***\n",
      "*\n",
      "*****\n",
      "**\n",
      "****\n",
      "***\n",
      "*\n",
      "*\n",
      "****\n",
      "****\n",
      "*\n",
      "*\n",
      "****\n",
      "*\n",
      "**\n",
      "**\n",
      "*\n",
      "*\n",
      "‚àí2 ‚àí1 0 1 2‚àí20 020406080100\n",
      "Adjusted complianceCholesterol decrease\n",
      "Figure 20.1 Cholesterol data: cholesterol decrease plotted\n",
      "versus adjusted compliance for 164 men taking\n",
      "cholestyramine . The green curve is OLS cubic regression,\n",
      "with ‚Äúcubic‚Äù selected by the Cpcriterion. How accurate is the\n",
      "Ô¨Åtted curve?\n",
      "Ô¨Åtted curve, taking account of Cpselection as well as OLS estimation?\n",
      "(See Section 20.2 for an answer.)\n",
      "Currently, there is no overarching theory for inference after model selec-\n",
      "tion. This chapter, more modestly, presents a short series of vignettes that\n",
      "illustrate promising analyses of individual situations. See also Section 16.6\n",
      "for a brief report on progress in post-selection inference for the lasso.\n",
      "20.1 Simultaneous ConÔ¨Ådence Intervals\n",
      "In the early 1950s, just before the beginnings of the computer revolution,\n",
      "substantial progress was made on the problem of setting simultaneous con-\n",
      "Ô¨Ådence intervals. ‚ÄúSimultaneous‚Äù here means that there exists a catalog of\n",
      "parameters of possible interest,\n",
      "CDf\u00121;\u00122;:::;\u0012Jg; (20.3)\n",
      "and we wish to set a conÔ¨Ådence interval for each of them with some Ô¨Åxed\n",
      "probability, typically 0.95, that allof the intervals will contain their respec-\n",
      "tive parameters.396 Inference After Model Selection\n",
      "As a Ô¨Årst example, we return to the diabetes data of Section 7.3: nD\n",
      "442diabetes patients each have had pD10medical variables measured at\n",
      "baseline, with the goal of predicting prog , disease progression one year\n",
      "later. LetXbe the442\u000210matrix withith rowx0\n",
      "ithe 10 measurements\n",
      "for patienti;Xhas been standardized so that each of its columns has mean\n",
      "0 and sum of squares 1. Also let ybe the 442-vector of centered prog\n",
      "measurements (that is, subtracting off the mean of the prog values).\n",
      "Ordinary least squares applied to the normal linear model,\n",
      "y\u0018Nn.XÀá;\u001b2I/; (20.4)\n",
      "yields MLE\n",
      "OÀáD.X0X/\u00001X0y; (20.5)\n",
      "satisfying\n",
      "OÀá\u0018Np.Àá;\u001b2V/;VD.X0X/\u00001; (20.6)\n",
      "as at (7.34).\n",
      "The 95% Student- tconÔ¨Ådence interval (11.49) for Àáj, thejth compo-\n",
      "nent ofÀá, is\n",
      "OÀájÀôO\u001bV1=2\n",
      "jjt:975\n",
      "q; (20.7)\n",
      "whereO\u001bD54:2 is the usual unbiased estimate of \u001b,\n",
      "O\u001b2Dky\u0000XOÀák2=q; qDn\u0000pD432; (20.8)\n",
      "andt:975\n",
      "qD1:97 is the 0.975 quantile of a Student- tdistribution with q\n",
      "degrees of freedom.\n",
      "The catalogCin (20.3) is nowfÀá1;Àá2;:::;Àá10g. The individual inter-\n",
      "vals (20.7), shown in Table 20.1, each have 95% coverage, but they are not\n",
      "simultaneous: there is a greater than 5% chance that at least one of the Àáj\n",
      "values lies outside its claimed interval.\n",
      "Valid 95% simultaneous intervals for the 10 parameters appear on the\n",
      "right side of Table 20.1. These are the Scheff ¬¥e intervals\n",
      "OÀájÀôO\u001bV1=2\n",
      "jjk.Àõ/\n",
      "p;q; (20.9)\n",
      "discussed next. The crucial constant k.Àõ/\n",
      "p;qequals 4.30 for pD10,qD\n",
      "432, andÀõD0:95. That makes the Scheff ¬¥e intervals wider than the t\n",
      "intervals (20.7) by a factor of 2.19. One expects to pay an extra price for\n",
      "simultaneous coverage, but a factor greater than two induces sticker shock.\n",
      "Scheff ¬¥e‚Äôs method depends on the pivotal quantity\n",
      "QD\u0010\n",
      "OÀá\u0000Àá\u00110\n",
      "V\u00001\u0010\n",
      "OÀá\u0000Àá\u0011.\n",
      "O\u001b2; (20.10)20.1 Simultaneous ConÔ¨Ådence Intervals 397\n",
      "Table 20.1 Maximum likelihood estimates OÀáfor 10 diabetes predictor\n",
      "variables (20.6) ; separate 95% Student- tconÔ¨Ådence limits, also\n",
      "simultaneous 95% Scheff ¬¥e intervals. The Scheff ¬¥e intervals are wider by a\n",
      "factor of 2.19.\n",
      "Student-t Scheff ¬¥e\n",
      "OÀá Lower Upper Lower Upper\n",
      "age\u00000.5\u00006.1 5.1\u000012.7 11.8\n",
      "sex\u000011.4\u000017.1\u00005.7\u000024.0 1.1\n",
      "bmi 24.8 18.5 31.0 11.1 38.4\n",
      "map 15.4 9.3 21.6 2.1 28.8\n",
      "tc\u000037.7\u000076.7 1.2\u0000123.0 47.6\n",
      "ldl 22.7\u00009.0 54.4\u000046.7 92.1\n",
      "hdl 4.8\u000015.1 24.7\u000038.7 48.3\n",
      "tch 8.4\u00006.7 23.5\u000024.6 41.5\n",
      "ltg 35.8 19.7 51.9 0.6 71.0\n",
      "glu 3.2\u00003.0 9.4\u000010.3 16.7\n",
      "which under model (20.4) has a scaled ‚Äú Fdistribution,‚Äù1\n",
      "Q\u0018pFp;q: (20.11)\n",
      "Ifk.Àõ/2\n",
      "p;qis theÀõth quantile of a pFp;qdistribution then PrfQ\u0014k.Àõ/2\n",
      "p;qgDÀõ\n",
      "yields\n",
      "Pr8\n",
      "ÀÜ<\n",
      "ÀÜ:\u0010\n",
      "Àá\u0000OÀá\u00110\n",
      "V\u00001\u0010\n",
      "Àá\u0000OÀá\u0011\n",
      "O\u001b2\u0014k.Àõ/2\n",
      "p;q9\n",
      ">=\n",
      ">;DÀõ (20.12)\n",
      "for any choice of Àáand\u001bin model (20.4). Having observed OÀáandO\u001b,\n",
      "(20.12) deÔ¨Ånes an elliptical conÔ¨Ådence region Efor the parameter vector\n",
      "Àá.\n",
      "Suppose we are interested in a particular linear combination of the coor-\n",
      "dinates ofÀá, say\n",
      "ÀácDc0Àá; (20.13)\n",
      "1Fp;qis distributed as .\u001f2\n",
      "p=p/=.\u001f2\n",
      "q=q/, the two chi-squared variates being\n",
      "independent. Calculating the percentiles of Fp;qwas a major project of the pre-war\n",
      "period.398 Inference After Model Selection\n",
      " \n",
      "‚óèc\n",
      "Œ≤^\n",
      "Figure 20.2 Ellipsoid of possible vectors ÀádeÔ¨Åned by (20.12)\n",
      "determines conÔ¨Ådence intervals for ÀácDc0Àáaccording to the\n",
      "‚Äúbounding hyperplane‚Äù construction illustrated. The red line\n",
      "shows the conÔ¨Ådence interval for Àácifcis a unit vector,\n",
      "c0VcD1.\n",
      "wherecis a Ô¨Åxedp-dimensional vector. If Àáexists in Ethen we must have\n",
      "Àác2\u0014\n",
      "min\n",
      "Àá2E.c0Àá/;max\n",
      "Àá2E.c0Àá/\u0015\n",
      "; (20.14)\n",
      "which turns out¬éto be the interval centered at OÀácDc0OÀá, ¬é1\n",
      "Àác2OÀácÀôO\u001b.c0Vc/1=2k.Àõ/\n",
      "p;q: (20.15)\n",
      "(This agrees with (20.9) where cis thejth coordinate vector .0;:::;0;1;0 ,\n",
      ":::;0/0.) The construction is illustrated in Figure 20.2.\n",
      "Theorem (Scheff ¬¥e)IfOÀá\u0018Np.Àá;\u001b2V/independently ofO\u001b2\u0018\u001b2\u001f2\n",
      "q=q,\n",
      "then with probability Àõthe conÔ¨Ådence statement (20.15) forÀácDc0Àáwill\n",
      "be simultaneously true for all choices of the vector c.\n",
      "Here we can think of ‚Äúmodel selection‚Äù as the choice of the linear com-\n",
      "bination of interest \u0012cDc0Àá. Scheff ¬¥e‚Äôs theorem allows ‚Äúdata snooping‚Äù:\n",
      "the statistician can examine the data and then choose which \u0012c(or many\n",
      "\u0012c‚Äôs) to estimate, without invalidating the resulting conÔ¨Ådence intervals.\n",
      "An important application has the OÀáj‚Äôs as independent estimates of efÔ¨Å-\n",
      "cacy for competing treatments‚Äîperhaps different experimental drugs for\n",
      "the same target disease:\n",
      "OÀájind\u0018N.Àáj;\u001b2=nj/; forjD1;2;:::;J; (20.16)20.1 Simultaneous ConÔ¨Ådence Intervals 399\n",
      "thenjbeing known sample sizes. In this case the catalog Cmight comprise\n",
      "all pairwise differences Àái\u0000Àáj, as the statistician tries to determine which\n",
      "treatments are better or worse than the others.\n",
      "The fact that Scheff ¬¥e‚Äôs limits apply to allpossible linear combinations\n",
      "c0Àáis a blessing and a curse, the curse being their very large width, as seen\n",
      "in Table 20.1. Narrower simultaneous limits¬éare possible if we restrict the ¬é2\n",
      "catalogC, for instance to just the pairwise differences Àái\u0000Àáj.\n",
      "A serious objection, along Fisherian lines, is that the Scheff ¬¥e conÔ¨Ådence\n",
      "limits are accurate without being correct . That is, the intervals have the\n",
      "claimed overall frequentist coverage probability, but may be misleading\n",
      "when applied to individual cases. Suppose for instance that \u001b2=njD1for\n",
      "jD1;2;:::;J in (20.16) and that we observe OÀá1D10, withjOÀájj<2for\n",
      "all the others. Even if we looked at the data before singling out OÀá1for at-\n",
      "tention, the usual Student- tinterval (20.7) seems more appropriate than its\n",
      "much longer Scheff ¬¥e version (20.9). This point is made more convincingly\n",
      "in our next vignette.\n",
      "\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001\n",
      "A familiar but pernicious abuse of model selection concerns multiple\n",
      "hypothesis testing. Suppose we observe Nindependent normal variates zi,\n",
      "each with its own effect size\u0016i,\n",
      "ziind\u0018N.\u0016i;1/ foriD1;2;:::;N; (20.17)\n",
      "and, as in Section 15.1, we wish to test the null hypotheses\n",
      "H0iW\u0016iD0: (20.18)\n",
      "Being alert to the pitfalls of simultaneous testing, we employ a false-discov-\n",
      "ery rate control algorithm (15.14), which rejects Rof theNnull hypothe-\n",
      "ses, say for cases i1;i2;:::;iR. (Requaled 28 in the example of Fig-\n",
      "ure 15.3.)\n",
      "So far so good. The ‚Äúfamiliar abuse‚Äù comes in then setting the usual\n",
      "conÔ¨Ådence intervals\n",
      "\u0016i2O\u0016iÀô1:96 (20.19)\n",
      "(95% coverage) for the Rselected cases. This ignores the model selection\n",
      "process: the data-based selection of the Rcases must be taken into account\n",
      "in making legitimate inferences, even if Ris only 1 so multiplicity is not a\n",
      "concern.\n",
      "This problem is addressed by the theory of false-coverage control . Sup-\n",
      "pose algorithm Asets conÔ¨Ådence intervals for Rof theNcases, of which400 Inference After Model Selection\n",
      "rare actually false coverages, i.e., ones not containing the true effect size\n",
      "\u0016i. The false-coverage rate (FCR) of Ais the expected proportion of non-\n",
      "coverages\n",
      "FCR.A/DEfr=Rg; (20.20)\n",
      "the expectation being with respect to model (20.17). The goal, as with the\n",
      "FDR theory of Section 15.2, is to construct algorithm Ato control FCR\n",
      "below some Ô¨Åxed value q.\n",
      "The BYqalgorithm2controls FCR below level qin three easy steps,\n",
      "beginning with model (20.17).\n",
      "1 Letpibe thep-value corresponding to zi,\n",
      "piDÀÜ.zi/ (20.21)\n",
      "for left-sided signiÔ¨Åcance testing, and order the p.i/values in ascending\n",
      "order,\n",
      "p.1/\u0014p.2/\u0014p.3/\u0014:::\u0014p.N/: (20.22)\n",
      "2 CalculateRDmaxfiWp.i/\u0014i\u0001q=Ng, and (as in the BH qalgorithm\n",
      "(15.14)‚Äì(15.15)) declare the Rcorresponding null hypotheses false.\n",
      "3 For each of the Rcases, construct the conÔ¨Ådence interval\n",
      "\u0016i2ziÀôz.ÀõR/; whereÀõRD1\u0000Rq=N (20.23)\n",
      "(z.Àõ/DÀÜ\u00001.Àõ/).\n",
      "Theorem 20.1 Under model (20.17) , BYqhas FCR\u0014q; moreover, none\n",
      "of the intervals (20.23) contain\u0016iD0.\n",
      "A simulated example of BY qwas run according to these speciÔ¨Åcations:\n",
      "ND10,000; qD0:05; zi\u0018N.\u0016i;1/\n",
      "\u0016iD0 foriD1;2;:::;9000;\n",
      "\u0016i\u0018N.\u00003;1/ foriD9001;:::; 10,000:(20.24)\n",
      "In this situation we have 9000 null cases and 1000 non-null cases (all but 2\n",
      "of which had \u0016i<0).\n",
      "Because this is a simulation, we can plot the pairs .zi;\u0016i/to assess the\n",
      "BYqalgorithm‚Äôs performance. This is done in Figure 20.3 for the 1000\n",
      "non-null cases (the green points). BY qdeclaredRD565cases non-null,\n",
      "those having zi\u0014\u00002:77 (the circled points); 14 of the 565 declarations\n",
      "2Short for ‚ÄúBenjamini‚ÄìYekutieli;‚Äù see the chapter endnotes.20.1 Simultaneous ConÔ¨Ådence Intervals 401\n",
      "‚àí8 ‚àí6 ‚àí4 ‚àí2 0‚àí10 ‚àí8 ‚àí6 ‚àí4 ‚àí2 0\n",
      "Observed z¬µ‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "BY.loBY.up‚àí2.77\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "Bayes.loBayes.up\n",
      "Figure 20.3 Simulation experiment (20.24) with ND10,000\n",
      "cases, of which 1000 are non-null; the green points .zi;\u0016i/are\n",
      "these non-null cases. The FDR control algorithm BH q(qD0:05)\n",
      "declared the 565 circled cases having zi\u0014\u00002:77 to be non-null,\n",
      "of which the 14 red points were actually null. The heavy black\n",
      "lines show BY q95% conÔ¨Ådence intervals for the 565 cases, only\n",
      "17 of which failed to contain \u0016i. Actual Bayes posterior 95%\n",
      "intervals for non-null cases (20.26), dotted lines, have half the\n",
      "width and slope of BY qlimits.\n",
      "were actually null cases (the red circled points), giving false-discovery pro-\n",
      "portion14=565D0:025 . The heavy black lines trace the BY qconÔ¨Ådence\n",
      "limits (20.23) as a function of z\u0014\u00002:77.\n",
      "The Ô¨Årst thing to notice is that FCR control has indeed been achieved:\n",
      "only 17 of the declared cases lie outside their limits (the 14 nulls and 3\n",
      "non-nulls), for a false-coverage rate of 17=565D0:030 , safely less than\n",
      "qD0:05. The second thing, however, is that the BY qlimits provide a\n",
      "misleading idea of the location of \u0016igivenzi: they are much too wide and\n",
      "slope too low, especially for more negative zivalues.\n",
      "In this situation we can describe precisely the posterior distribution of\n",
      "\u0016igivenzifor the non-null cases,\n",
      "\u0016ijzi\u0018N\u0012zi\u00003\n",
      "2;1\n",
      "2\u0013\n",
      "; (20.25)402 Inference After Model Selection\n",
      "this following from \u0016i\u0018N.\u00003;1/,zij\u0016i\u0018N.\u0016i;1/, and Bayes‚Äô rule\n",
      "(5.20)‚Äì(5.21). The Bayes credible 95% limits\n",
      "\u0016i2zi\u00003\n",
      "2Àô1p\n",
      "21:96 (20.26)\n",
      "are indicated by the dotted lines in Figure 20.3. They are half as wide as\n",
      "the BYqlimits, and have slope 1=2rather than 1.\n",
      "In practice, of course, we would only see the zi, not the\u0016i, making\n",
      "(20.26) unavailable to us. We return to this example in Chapter 21, where\n",
      "empirical Bayes methods will be seen to provide a good approximation to\n",
      "the Bayes limits. (See Figure 21.5.)\n",
      "As with Scheff ¬¥e‚Äôs method, the BY qintervals can be accused of being\n",
      "accurate but not correct. ‚ÄúCorrect‚Äù here has a Bayesian/Fisherian Ô¨Çavor\n",
      "that is hard to pin down, except perhaps in large-scale applications, where\n",
      "empirical Bayes analyses can suggest appropriate inferences.\n",
      "20.2 Accuracy After Model Selection\n",
      "The cubic regression curve for the cholesterol data seen in Figure 20.1\n",
      "was selected according to the Cpcriterion of Section 12.3. Polynomial\n",
      "regression models, predicting cholesterol decrease diin terms of powers\n",
      "(‚Äúdegrees‚Äù) of adjusted compliance ci, were Ô¨Åt by ordinary least squares\n",
      "for degrees0;1;2;:::;6 . Table 20.2 shows Cpestimates (12.51) being\n",
      "minimized at degree 3.\n",
      "Table 20.2Cptable for cholesterol data of Figure 20.1, comparing OLS\n",
      "polynomial models of degrees 0 through 6. The cubic model, degree D3,\n",
      "is the minimizer (80,000 subtracted from the Cpvalues for easier\n",
      "comparison; assumes \u001bD22:0).\n",
      "Degree Cp\n",
      "0 71887\n",
      "1 1132\n",
      "2 1412\n",
      "3 667\n",
      "4 1591\n",
      "5 1811\n",
      "6 2758\n",
      "We wish to assess the accuracy of the Ô¨Åtted curve, taking account of both\n",
      "theCpmodel selection method and the OLS Ô¨Åtting process. The bootstrap20.2 Accuracy After Model Selection 403\n",
      "is a natural candidate for the job. Here we will employ the nonparamet-\n",
      "ric bootstrap of Section 10.2 (rather than the parametric bootstrap of Sec-\n",
      "tion 10.4, though this would be no more difÔ¨Åcult to carry out).\n",
      "Thecholesterol data set (20.2) comprises nD164 pairsxiD\n",
      ".ci;di/; a nonparametric bootstrap sample x\u0003(10.13) consists of 164 pairs\n",
      "chosen at random and with replacement from the original 164. Let t.x/be\n",
      "the curve obtained by applying the Cp/OLS algorithm to the original data\n",
      "setxand likewise t.x\u0003/for the algorithm applied to x\u0003; and for a given\n",
      "pointcon the compliance scale let\n",
      "O\u0012\u0003\n",
      "cDt.c;x\u0003/ (20.27)\n",
      "be the value of t.x\u0003/evaluated at compliance Dc.\n",
      " \n",
      "Cholesterol decrease Œ∏^\n",
      "‚àí2‚àóFrequency\n",
      "‚àí20 ‚àí10 0 10 200 100 200 300 400\n",
      "fixed degree (3)\n",
      "adaptive degree\n",
      "1.27\n",
      "Figure 20.4 A histogram of 4000 nonparametric bootstrap\n",
      "replications for polynomial regression estimates of cholesterol\n",
      "decreasesdat adjusted compliance cD\u00002. Blue histogram,\n",
      "adaptive estimatorO\u0012\u0003\n",
      "c(20.27), using full Cp/OLS algorithm for\n",
      "each bootstrap data set; line histogram, using OLS only with\n",
      "degree 3 for each bootstrap data set. Bootstrap standard errors are\n",
      "5.98 and 3.97.404 Inference After Model Selection\n",
      "BD4000 nonparametric bootstrap replications t.x\u0003/were generated.3\n",
      "Figure 20.4 shows the histogram of the 4000 O\u0012\u0003\n",
      "creplications for cD\u00002:0.\n",
      "It is labeled ‚Äúadaptive‚Äù to indicate that Cpmodel selection, as well as OLS\n",
      "Ô¨Åtting, was carred out anew for each x\u0003. This is as opposed to the ‚ÄúÔ¨Åxed‚Äù\n",
      "histogram, where there was no Cpselection, cubic OLS regression always\n",
      "being used.\n",
      "‚àí2 ‚àí1 0 1 201234567\n",
      "Adjusted compliance cStandard errors of Œ∏^\n",
      "c\n",
      "fixedadaptivesmoothed\n",
      "Figure 20.5 Bootstrap standard-error estimates of O\u0012c, for\n",
      "\u00002:2\u0014c\u00142. Solid black curve, adaptive estimator (20.27)\n",
      "using fullCp/OLS model selection estimate; red dashed curve,\n",
      "using OLS only with polynomial degree Ô¨Åxed at 3;\n",
      "blue dotted curve, ‚Äúbagged estimator‚Äù using bootstrap smoothing\n",
      "(20.28). Average standard-error ratios: adaptive/Ô¨Åxed D1:43,\n",
      "adaptive/smoothedD1:14.\n",
      "The bootstrap estimate of standard error (10.16) obtained from the adap-\n",
      "tive valuesO\u0012\u0003\n",
      "cwas 5.98, compared with 3.97 for the Ô¨Åxed values.4In this\n",
      "case, accounting for model selection (‚Äúadaptation‚Äù) adds more than 50% to\n",
      "the standard error estimates. The same comparison was made at all values\n",
      "3Ten times more than needed for assessing standard errors, but helpful for the\n",
      "comparisons that follow.\n",
      "4The latter is not the usual OLS assessment, following (8.30), that would be appropriate\n",
      "for a parametric bootstrap comparison. Rather, it‚Äôs the nonparametric one-sample\n",
      "bootstrap assessment, resampling pairs .xi;yi/as individual sample points.20.2 Accuracy After Model Selection 405\n",
      "of the adjusted compliance c. Figure 20.5 graphs the results: the adaptive\n",
      "standard errors averaged 43% greater than the Ô¨Åxed values. The standard\n",
      "95% conÔ¨Ådence intervals O\u0012cÀôbse\u00011:96 would be roughly 43% too short if\n",
      "we ignored model selection in assessing bse.\n",
      " \n",
      "Cholesterol decrease Œ∏^\n",
      "‚àí2‚àóFrequency\n",
      "‚àí20 ‚àí10 0 10 200 20 40 60 80\n",
      "1.27adaptive\n",
      "m* = 1\n",
      "adaptive\n",
      "m* > 1fixed\n",
      " m = 3\n",
      "Figure 20.6 ‚ÄúAdaptive‚Äù histogram of Figure 20.4 now split into\n",
      "19% of 4000 bootstrap replications where Cpselected linear\n",
      "regression (m\u0003D1) as best, versus 81% having m\u0003>1.m\u0003D1\n",
      "cases are shifted about 10 units downward. (The m\u0003>1cases\n",
      "resemble the ‚ÄúÔ¨Åxed‚Äù histogram in Figure 20.4.) Histograms are\n",
      "scaled to have equal areas.\n",
      "Having an honest assessment of standard error doesn‚Äôt mean that t.c;x/\n",
      "(20.27) is a good estimator. Model selection can induce an unpleasant\n",
      "‚Äújumpiness‚Äù in an estimator, as the original data vector xcrosses deÔ¨Åni-\n",
      "tional boundaries. This happened in our example: for 19% of the 4000\n",
      "bootstrap samples x\u0003, theCpalgorithm selected linear regression, m\u0003D1,\n",
      "as best, and in these cases O\u0012\u0003\n",
      "\u00002:0tended toward smaller values. Figure 20.6\n",
      "shows them\u0003D1histogram shifted about 10 units down from the m\u0003>1\n",
      "histogram (which now resembles the ‚ÄúÔ¨Åxed‚Äù histogram in Figure 20.4).\n",
      "Discontinuous estimators such as t.c;x/can‚Äôt be Bayesian, Bayes pos-\n",
      "terior expectations being continuous. They can also suffer frequentist difÔ¨Å-\n",
      "culties,¬éincluding excess variability and overly long conÔ¨Ådence intervals. ¬é3406 Inference After Model Selection\n",
      "Bagging , orbootstrap smoothing , is a tactic for improving a discontinuous\n",
      "estimation rule by averaging (as in (12.80) and Chapter 17).\n",
      "Supposet.x/is any estimator for which we have obtained bootstrap\n",
      "replicationsft.x\u0003b/,bD1;2;:::;Bg. The bagged version of t.x/is the\n",
      "average\n",
      "s.x/D1\n",
      "BBX\n",
      "bD1t.x\u0003b/: (20.28)\n",
      "The lettershere stands for ‚Äúsmooth.‚Äù Small changes in x, even ones that\n",
      "move across a model selection deÔ¨Ånitional boundary, produce only small\n",
      "changes in the bootstrap average s.x/.\n",
      "Averaging over the 4000 bootstrap replications of t.c;x\u0003/(20.27) gave\n",
      "a bagged estimate sc.x/for each value of c. Bagging reduced the standard\n",
      "errors of theCp/OLS estimates t.c;x/by about 12%, as indicated by the\n",
      "blue dotted curve in Figure 20.5.\n",
      "Where did the blue dotted curve come from? All 4000 bootstrap values\n",
      "t.c;x\u0003b/were needed to produce the single value sc.x/. It seems as if\n",
      "we would need to bootstrap the bootstrap in order to compute bse≈ísc.x/¬ç.\n",
      "Fortunately, a more economical calculation is possible, one that requires\n",
      "only the original Bbootstrap computations for t.c;x/.\n",
      "DeÔ¨Åne\n",
      "NbjD#ftimesxjoccurs inx\u0003bg; (20.29)\n",
      "forbD1;2;:::;B andjD1;2;:::;n . For instance N4000;7D3says\n",
      "that data point x7occurred three times in nonparametric bootstrap sample\n",
      "x\u00034000. TheBbynmatrixfNbjgcompletely describes the Bbootstrap\n",
      "samples. Also denote\n",
      "t\u0003bDt.x\u0003b/ (20.30)\n",
      "and let cov jindicate the covariance in the bootstrap sample between Nbj\n",
      "andt\u0003b,\n",
      "covjD1\n",
      "BBX\n",
      "bD1.Nbj\u0000N\u0001j/.t\u0003b\u0000t\u0003\u0001/; (20.31)\n",
      "where dots denote averaging over B:N\u0001jD1\n",
      "BP\n",
      "bNbjandt\u0003\u0001D1\n",
      "BP\n",
      "bt\u0003b.\n",
      "Theorem 20.2¬éThe inÔ¨Ånitesimal jackknife estimate of standard error ¬é420.2 Accuracy After Model Selection 407\n",
      "(10.41) for the bagged estimate (20.28) is\n",
      "bseIJ≈ísc.x/¬çD0\n",
      "@nX\n",
      "jD1cov2\n",
      "j1\n",
      "A1=2\n",
      ": (20.32)\n",
      "Keeping track of Nbjas we generate the bootstrap replications t\u0003ballows\n",
      "us to compute covjandbse≈ísc.x/¬çwithout any additional computational\n",
      "effort.\n",
      "We expect averaging to reduce variability, and this is seen to hold true in\n",
      "Figure 20.5, the ratio of bseIJ≈ísc.x/¬ç=bseboot≈ít.c;x/¬çaveraging 0.88. In fact,\n",
      "we have the following general result.\n",
      "Corollary The ratiobseIJ≈ísc.x/¬ç=bseboot≈ít.c;x/¬çis always\u00141.\n",
      "The savings due to bagging increase with the nonlinearity of t.x\u0003/as\n",
      "a function of the counts Nbj(or, in the language of Section 10.3, in the\n",
      "nonlinearity of S.P/as a function of P). Model-selection estimators such\n",
      "as theCp/OLS rule tend toward greater nonlinearity and bigger savings.\n",
      "Table 20.3 Proportion of 4000 nonparametric bootstrap replications of\n",
      "Cp/OLS algorithm that selected degrees mD1;2;:::;6 ; also\n",
      "inÔ¨Ånitesimal jackknife standard deviations for proportions (20.32) , which\n",
      "mostly exceed the estimates themselves.\n",
      "mD1 2 3 4 5 6\n",
      "proportion .19 .12 .35 .07 .20 .06\n",
      "bsdIJ .24 .20 .24 .13 .26 .06\n",
      "The Ô¨Årst line of Table 20.3 shows the proportions in which the various\n",
      "degrees were selected in the 4000 cholesterol bootstrap replications, 19%\n",
      "for linear, 12% for quadratic, 35% for cubic, etc. With BD4000 , the\n",
      "proportions seem very accurate, the binomial standard error for 0.19 being\n",
      "just.0:19\u00010:81=4000/1=2D0:006 , for instance.\n",
      "Theorem 20.2 suggests otherwise. Now let t\u0003b(20.30) indicate whether\n",
      "thebth bootstrap sample x\u0003made theCpchoicem\u0003D1,\n",
      "t\u0003bD(\n",
      "1ifm\u0003bD1\n",
      "0ifm\u0003b>1:(20.33)\n",
      "The bagged value of ft\u0003b;bD1;2;:::;Bgis the observed proportion408 Inference After Model Selection\n",
      "0.19. Applying the bagging theorem yielded bseIJD0:24, as seen in the\n",
      "second line of the table, with similarly huge standard errors for the other\n",
      "proportions.\n",
      "The binomial standard errors are internal , saying how quickly the boot-\n",
      "strap resampling process is converging to its ultimate value as B!1 .\n",
      "The inÔ¨Ånitesimal jackknife estimates are external : if we collected a new\n",
      "set of 164 data pairs .ci;di/(20.2) the new proportion table might look\n",
      "completely different than the top line of Table 20.3.\n",
      "Frequentist statistics has the advantage of being applicable to any algo-\n",
      "rithmic procedure, for instance to our Cp/OLS estimator. This has great\n",
      "appeal in an era of enormous data sets and fast computation. The draw-\n",
      "back, compared with Bayesian statistics, is that we have no guarantee that\n",
      "our chosen algorithm is best in any way. Classical statistics developed a\n",
      "theory of best for a catalog of comparatively simple estimation and testing\n",
      "problems. In this sense, modern inferential theory has not yet caught up\n",
      "with modern problems such as data-based model selection, though tech-\n",
      "niques such as model averaging (e.g., bagging) suggest promising steps\n",
      "forward.\n",
      "20.3 Selection Bias\n",
      "Many a sports fan has been victimized by selection bias. Your team does\n",
      "wonderfully well and tops the league standings. But the next year, with\n",
      "the same players and the same opponents, you‚Äôre back in the pack. This\n",
      "is the winner‚Äôs curse , a more picturesque name for selection bias, the ten-\n",
      "dency of unusually good (or bad) comparative performances not to repeat\n",
      "themselves.\n",
      "Modern scientiÔ¨Åc technology allows the simultaneous investigation of\n",
      "hundreds or thousands of candidate situations, with the goal of choosing\n",
      "the top performers for subsequent study. This is a setup for the heartbreak\n",
      "of selection bias. An apt example is offered by the prostate study data of\n",
      "Section 15.1, where we observe statistics zimeasuring patient‚Äìcontrol dif-\n",
      "ferences forND6033 genes,\n",
      "zi\u0018N.\u0016i;1/; iD1;2;:::;N: (20.34)\n",
      "Here\u0016iis the effect size for genei, the true difference between the patient\n",
      "and control populations.\n",
      "Genes with large positive or negative values of \u0016iwould be promising\n",
      "targets for further investigation. Gene number 610, with z610D5:29, at-20.3 Selection Bias 409\n",
      "tained the biggest z-value; (20.34) says that z610is unbiased for \u0016610. Can\n",
      "we believe the obvious estimate O\u0016610D5:29?\n",
      "‚ÄúNo‚Äù is the correct selection bias answer. Gene 610 has won a contest for\n",
      "bigness among 6033 contenders. In addition to being good (having a large\n",
      "value of\u0016) it has almost certainly been lucky , with the noise in (20.34)\n",
      "pushingz610in the positive direction‚Äîor else it would not have won the\n",
      "contest. This is the essence of selection bias.\n",
      "False-discovery rate theory, Chapter 15, provided a way to correct for\n",
      "selection bias in simultaneous hypothesis testing. This was extended to\n",
      "false-coverage rates in Section 20.1. Our next vignette concerns the re-\n",
      "alistic estimation of effect sizes \u0016iin the face of selection bias.\n",
      "We begin by assuming that an effect size \u0016has been obtained from a\n",
      "prior density g.\u0016/ (which might include discrete atoms) and then z\u0018\n",
      "N.\u0016;\u001b2/observed,\n",
      "\u0016\u0018g.\u0001/andzj\u0016\u0018N.\u0016;\u001b2/ (20.35)\n",
      "(\u001b2is assumed known for this discussion). The marginal density of zis\n",
      "f.z/DZ1\n",
      "\u00001g.\u0016/\u001e\u001b.z\u0000\u0016/d\u0016;\n",
      "where\u001e\u001b.z/D.2\u0019\u001b2/\u00001=2exp\u0012\n",
      "\u00001\n",
      "2z2\n",
      "\u001b2\u0013\n",
      ":(20.36)\n",
      "Tweedie‚Äôs formula¬éis an intriguing expression for the Bayes expectation ¬é5\n",
      "of\u0016givenz.\n",
      "Theorem 20.3 In model (20.35) , the posterior expectation of \u0016having\n",
      "observedzis\n",
      "Ef\u0016jzgDzC\u001b2l0.z/ withl0.z/Dd\n",
      "dzlogf.z/: (20.37)\n",
      "The especially convenient feature of Tweedie‚Äôs formula is that Ef\u0016jzg\n",
      "is expressed directly in terms of the marginal density f.z/ . This is a setup\n",
      "for empirical Bayes estimation. We don‚Äôt know g.\u0016/ , but in large-scale\n",
      "situations we can estimate the marginal density f.z/ from the observa-\n",
      "tionszD.z1;z2;:::;zN/, perhaps by Poisson regression as in Table 15.1,\n",
      "yielding\n",
      "OEf\u0016ijzigDziC\u001b2Ol0.zi/ withOl0.z/Dd\n",
      "dzlogOf.z/: (20.38)\n",
      "The solid curve in Figure 20.7 shows OEf\u0016jzgfor the prostate study data,410 Inference After Model Selection\n",
      "‚àí4 ‚àí2 0 2 4‚àí2 0 2 4\n",
      "z‚àívalueE^( ¬µ | z)\n",
      "‚óè‚óè‚óè\n",
      "z = 3.51.96\n",
      ".15\n",
      "00.20.40.60.81\n",
      " fdr\n",
      "Tweediefdr\n",
      "Figure 20.7 The solid curve is Tweedie‚Äôs estimate OEf\u0016jzg\n",
      "(20.38) for the prostate study data. The dashed line shows the\n",
      "local false-discovery rate cfdr.z/from Figure 15.5 (red scale on\n",
      "right). AtzD3:5,OEf\u0016jzgD1:96 andcfdr.z/D0:15. For gene\n",
      "610, withz610D5:29, Tweedie‚Äôs estimate is 4.03.\n",
      "with\u001b2D1andOf.z/ obtained using fourth-degree log polynomial re-\n",
      "gression as in Section 15.4. The curve has Ef\u0016jzghovering near zero for\n",
      "jzij\u00142, agreeing with the local false-discovery rate curve cfdr.z/of Fig-\n",
      "ure 15.5 that says these are mostly null genes.\n",
      "OEf\u0016jzgincreases for z > 2 , equaling 1.96 for zD3:5. At that point\n",
      "cfdr.z/D0:15. So even though ziD3:5has a one-sided p-value of 0.0002,\n",
      "with 6033 genes to consider at once, it still is not a sure thing that gene i\n",
      "is non-null. About 85% of the genes with zinear 3.5 will be non-null,\n",
      "and these will have effect sizes averaging about 2.31 ( D1:96=0:85 ). All of\n",
      "this nicely illustrates the combination of frequentist and Bayesian inference\n",
      "possible in large-scale studies, and also the combination of estimation and\n",
      "hypothesis-testing ideas in play.\n",
      "If the prior density g.\u0016/ in (20.35) is assumed to be normal, Tweedie‚Äôs\n",
      "formula (20.38) gives (almost) the James‚ÄìStein estimator (7.13). The cor-\n",
      "responding curve in Figure 20.7 in that case would be a straight line pass-\n",
      "ing through the origin at slope 0.22. Like the James‚ÄìStein estimator, ridge\n",
      "regression, and the lasso of Chapter 16, Tweedie‚Äôs formula is a shrink-\n",
      "age estimator. For z610D5:29, the most extreme observation, it gave20.3 Selection Bias 411\n",
      "O\u0016610D4:03, shrinking the maximum likelihood estimate more than one \u001b\n",
      "unit toward the origin.\n",
      "Bayes estimators are immune to selection bias, as discussed in Sections\n",
      "3.3 and 3.4. This offers some hope that Tweedie‚Äôs empirical Bayes esti-\n",
      "mates might be a realistic cure for the winners‚Äô curse. A small simulation\n",
      "experiment was run as a test.\n",
      "\u000fA hundred data sets z, each of length ND1000 , were generated accord-\n",
      "ing to a combination of exponential and normal sampling,\n",
      "\u0016iind\u0018e\u0000\u0016.\u0016>0/ andzij\u0016iind\u0018N.\u0016i;1/; (20.39)\n",
      "foriD1;2;:::;1000 .\n",
      "\u000fFor eachz,Ol.z/ was computed as in Section 15.4, now using a natural\n",
      "spline model with Ô¨Åve degrees of freedom.\n",
      "\u000fThis gave Tweedie‚Äôs estimates\n",
      "O\u0016iDziCOl0.zi/; iD1;2;:::;1000; (20.40)\n",
      "for that data set z.\n",
      "\u000fFor each data set z, the 20 largest zivalues and the corresponding O\u0016iand\n",
      "\u0016ivalues were recorded, yielding the\n",
      "uncorrected differences zi\u0000\u0016i\n",
      "and corrected differences O\u0016i\u0000\u0016i;(20.41)\n",
      "the hope being that empirical Bayes shrinkage would correct the selection\n",
      "bias in thezivalues.\n",
      "\u000fFigure 20.8 shows the 2000 (100 data sets, 20 top cases each) uncorrected\n",
      "and corrected differences. Selection bias is quite obvious, with the uncor-\n",
      "rected differences shifted one unit to the right of zero. In this case at least,\n",
      "the empirical Bayes corrections have worked well, the corrected differ-\n",
      "ences being nicely centered at zero. Bias correction often adds variance,\n",
      "but in this case it hasn‚Äôt.\n",
      "Finally, it is worth saying that the ‚Äúempirical‚Äù part of empirical Bayes is\n",
      "less the estimation of Bayesian rules from the aggregate data than the appli-\n",
      "cation of such rules to individual cases. For the prostate data we began with\n",
      "no deÔ¨Ånite prior opinions but arrived at strong (i.e., not‚Äúuninformative‚Äù)\n",
      "Bayesian conclusions for, say, \u0016610in the prostate study.412 Inference After Model Selection\n",
      " \n",
      "DifferencesFrequency\n",
      "‚àí4 ‚àí2 0 2 4020406080100120140\n",
      "uncorrected\n",
      "differences\n",
      "corrected\n",
      "differences\n",
      "Figure 20.8 Corrected and uncorrected differences for 20 top\n",
      "cases in each of 100 simulations (20.39)‚Äì(20.41). Tweedie\n",
      "corrections effectively counteracted selection bias.\n",
      "20.4 Combined Bayes‚ÄìFrequentist Estimation\n",
      "As mentioned previously, Bayes estimates are, at least theoretically, im-\n",
      "mune from selection bias. Let zD.z1;z2;:::,zN/represent the prostate\n",
      "study data of the previous section, with parameter vector \u0016D.\u00161;\u00162;:::;\n",
      "\u0016N/. Bayes‚Äô rule (3.5)\n",
      "g.\u0016jz/Dg.\u0016/f\u0016.z/=f.z/ (20.42)\n",
      "yields the posterior density of \u0016givenz. A data-based model selection\n",
      "rule such as ‚Äúestimate the \u0016icorresponding to the largest observation zi‚Äù\n",
      "has no effect on the likelihood function f\u0016.z/(withzÔ¨Åxed) or ong.\u0016jz/.\n",
      "Having chosen a prior g.\u0016/, our posterior estimate of \u0016610is unaffected\n",
      "by the fact that z610D5:29 happens to be largest.\n",
      "This same argument applies just as well to any data-based model selec-\n",
      "tion procedure, for instance a preliminary screening of possible variables\n",
      "to include in a regression analysis‚Äîthe Cpchoice of a cubic regression in\n",
      "Figure 20.1 having no effect on its Bayes posterior accuracy.\n",
      "There is a catch: the chosen prior g.\u0016/must apply to the entire param-\n",
      "eter vector\u0016and not just the part we are interested in (e.g., \u0016610). This is20.4 Combined Bayes‚ÄìFrequentist Estimation 413\n",
      "feasible in one-parameter situations like the stopping rule example of Fig-\n",
      "ure 3.3. It becomes difÔ¨Åcult and possibly dangerous in higher dimensions.\n",
      "Empirical Bayes methods such as Tweedie‚Äôs rule can be thought of as al-\n",
      "lowing the data vector zto assist in the choice of a high-dimensional prior,\n",
      "an effective collaboration between Bayesian and frequentist methodology.\n",
      "Our chapter‚Äôs Ô¨Ånal vignette concerns another Bayes‚Äìfrequentist estima-\n",
      "tion technique. Dropping the boldface notation, suppose that FDffÀõ.x/g\n",
      "is a multi-dimensional family of densities (5.1) (now with Àõplaying the\n",
      "role of\u0016), and that we are interested in estimating a particular parameter\n",
      "\u0012Dt.Àõ/. A priorg.Àõ/ has been chosen, yielding posterior expectation\n",
      "O\u0012DEft.Àõ/jxg: (20.43)\n",
      "How accurate isO\u0012? The usual answer would be calculated from the pos-\n",
      "terior distribution of \u0012givenx. This is obviously the correct answer if g.Àõ/\n",
      "is based on genuine prior experience. Most often though, and especially in\n",
      "high-dimensional problems, the prior reÔ¨Çects mathematical convenience\n",
      "and a desire to be uninformative, as in Chapter 13. There is a danger of\n",
      "circular reasoning in using a self-selected prior distribution to calculate the\n",
      "accuracy of its own estimator.\n",
      "An alternate approach, discussed next, is to calculate the frequentist ac-\n",
      "curacy ofO\u0012; that is, even though (20.43) is a Bayes estimate, we consider\n",
      "O\u0012simply as a function of x, and compute its frequentist variability. The\n",
      "next theorem leads to a computationally efÔ¨Åcient way of doing so. (The\n",
      "Bayes and frequentist standard errors for O\u0012operate in conceptually orthog-\n",
      "onal directions as pictured in Figure 3.5. Here we are supposing that the\n",
      "priorg.\u0001/is unavailable or uncertain, forcing more attention on frequentist\n",
      "calculations.)\n",
      "For convenience, we will take the family Fto be ap-parameter expo-\n",
      "nential family (5.50),\n",
      "fÀõ.x/DeÀõ0x\u0000 .Àõ/f0.x/; (20.44)\n",
      "now withÀõbeing the parameter vector called \u0016above. Thep\u0002pcovari-\n",
      "ance matrix of x(5.59) is denoted\n",
      "VÀõDcovÀõ.x/: (20.45)\n",
      "Let Covxindicate the posterior covariance given xbetween\u0012Dt.Àõ/, the\n",
      "parameter of interest, and Àõ,\n",
      "CovxDcovfÀõ;t.Àõ/jxg; (20.46)414 Inference After Model Selection\n",
      "ap\u00021vector. Cov xleads directly to a frequentist estimate of accuracy for\n",
      "O\u0012.\n",
      "Theorem 20.4¬éThe delta method estimate of standard error for O\u0012D ¬é6\n",
      "Eft.Àõ/jxg(20.43) is\n",
      "bsedeltan\n",
      "O\u0012o\n",
      "D\u0000\n",
      "Cov0\n",
      "xVOÀõCovx\u00011=2; (20.47)\n",
      "whereVOÀõisVÀõevaluated at the MLE OÀõ.\n",
      "The theorem allows us to calculate the frequentist accuracy estimate\n",
      "bsedeltafO\u0012gwith hardly any additional computational effort beyond that re-\n",
      "quired forO\u0012itself. Suppose we have used an MCMC or Gibbs sampling\n",
      "algorithm, Section 13.4, to generate a sample from the Bayes posterior dis-\n",
      "tribution ofÀõgivenx,\n",
      "Àõ.1/;Àõ.2/;:::;Àõ.B/: (20.48)\n",
      "These yield the usual estimate for Eft.Àõ/jxg,\n",
      "O\u0012D1\n",
      "BBX\n",
      "bD1t\u0010\n",
      "Àõ.b/\u0011\n",
      ": (20.49)\n",
      "They also give a similar expression for cov fÀõ;t.Àõ/jxg,\n",
      "CovxD1\n",
      "BBX\n",
      "bD1\u0010\n",
      "Àõ.b/\u0000Àõ.\u0001/\u0011\u0010\n",
      "t.b/\u0000t.\u0001/\u0011\n",
      "; (20.50)\n",
      "t.b/Dt.Àõ.b//,t.\u0001/DP\n",
      "bt.b/=B, andÀõ.\u0001/DP\n",
      "bÀõ.b/=B, from which we\n",
      "can calculate5bsedelta.O\u0012/(20.47).\n",
      "For an example of Theorem 20.4 in action we consider the diabetes\n",
      "data of Section 20.1, with x0\n",
      "itheith row ofX, the442\u000210matrix of\n",
      "prediction, so xiis the vector of 10 predictors for patient i. The response\n",
      "vectoryof progression scores has now been rescaled to have \u001b2D1in\n",
      "the normal regression model,6\n",
      "y\u0018Nn.XÀá;I/: (20.51)\n",
      "The prior distribution g.Àá/ was taken to be\n",
      "g.Àá/Dce\u0000\u0015kÀák1; (20.52)\n",
      "5VOÀõmay be known theoretically, calculated by numerical differentiation in (5.57), or\n",
      "obtained from parametric bootstrap resampling‚Äîtaking the empirical covariance matrix\n",
      "of bootstrap replications OÀá\u0003\n",
      "i.\n",
      "6By dividing the original data vector yby its estimated standard error from the linear\n",
      "modelEfygDXÀá.20.4 Combined Bayes‚ÄìFrequentist Estimation 415\n",
      "with\u0015D0:37 andcthe constant that makes g.Àá/ integrate to 1. This is\n",
      "the ‚ÄúBayesian lasso prior,‚Äù¬éso called because of its connection to the lasso, ¬é7\n",
      "(7.42) and (16.1). (The lasso plays no part in what follows).\n",
      "An MCMC algorithm generated BD10,000 samples (20.48) from the\n",
      "posterior distribution g.Àájy/. Let\n",
      "\u0012iDx0\n",
      "iÀá; (20.53)\n",
      "the (unknown) expectation of the ith patient‚Äôs response yi. The Bayes pos-\n",
      "terior expectation of \u0012iis\n",
      "O\u0012iD1\n",
      "BBX\n",
      "bD1x0\n",
      "iÀá.b/: (20.54)\n",
      "It has Bayes posterior standard error\n",
      "bseBayes\u0010\n",
      "O\u0012i\u0011\n",
      "D\"\n",
      "1\n",
      "BBX\n",
      "bD1\u0010\n",
      "x0\n",
      "iÀá.b/\u0000O\u0012i\u00112#1=2\n",
      "; (20.55)\n",
      "which we can compare with bsedelta.O\u0012i/, the frequentist standard error (20.47).\n",
      "Figure 20.9 shows the 10,000 MCMC replications O\u0012.b/\n",
      "iDx0\n",
      "iÀá.b/for\n",
      "patientiD322. The point estimate O\u0012iequaled 2.41, with Bayes and fre-\n",
      "quentist standard error estimates\n",
      "bseBayesD0:203 andbsedeltaD0:186: (20.56)\n",
      "The frequentist standard error is 9% smaller in this case; bsedeltawas less\n",
      "thanbseBayes for all 442 patients, the difference averaging a modest 5%.\n",
      "Things can work out differently. Suppose we are interested in the poste-\n",
      "rior cdf of\u0012332giveny. For any given value of clet\n",
      "t\u0010\n",
      "c;Àá.b/\u0011\n",
      "D(\n",
      "1ifx0\n",
      "322Àá.b/\u0014c\n",
      "0ifx0\n",
      "332Àá.b/>c;(20.57)\n",
      "so\n",
      "cdf.c/D1\n",
      "BBX\n",
      "bD1t\u0010\n",
      "c;Àá.b/\u0011\n",
      "(20.58)\n",
      "is our MCMC assessment of Pr f\u0012322\u0014cjyg. The solid curve in Fig-\n",
      "ure 20.10 graphs cdf .c/.\n",
      "If we believe prior (20.52) then the curve exactly represents the posterior\n",
      "distribution of \u0012322giveny(except for the simulation error due to stopping\n",
      "atBD10,000 replications). Whether or not we believe the prior we can use416 Inference After Model Selection\n",
      " \n",
      "MCMC Œ∏322 valuesFrequency\n",
      "2.0 2.5 3.00100 200 300 400 500 600\n",
      "2.41Standard Errors\n",
      "Bayes Posterior .205\n",
      "Frequentist .186\n",
      "Figure 20.9 A histogram of 10,000 MCMC replications for\n",
      "posterior distribution of \u0012322, expected progression for patient\n",
      "322 in the diabetes study; model (20.51) and prior (20.52).\n",
      "The Bayes posterior expectation is 2.41. Frequentist standard\n",
      "error (20.47) forO\u0012322D2:41 was 9% smaller than Bayes\n",
      "posterior standard error (20.55).\n",
      "Theorem 20.4, with t.b/Dt.c;Àá.b//in (20.50), to evaluate the frequentist\n",
      "accuracy of the curve.\n",
      "The dashed vertical red lines show cdf .c/plus or minus one bsedeltaunit.\n",
      "The standard errors are disturbingly large, for instance 0:687Àô0:325 at\n",
      "cD2:5. The central 90% credible interval for \u0012322(thec-values between\n",
      "cdf.c/0.05 and 0.95),\n",
      ".2:08;2:73/ (20.59)\n",
      "has frequentist standard errors about 0.185 for each endpoint‚Äî28% of the\n",
      "interval‚Äôs length.\n",
      "If we believe prior (20.52) then .2:08;2:73/ is an (almost) exact 90%\n",
      "credible interval for \u0012322, and moreover is immune to any selection bias\n",
      "involved in our focus on \u0012322. If not, the large frequentist standard errors\n",
      "are a reminder that calculation (20.59) might turn out much differently in\n",
      "a new version of the diabetes study, even ignoring selection bias.\n",
      "To return to our main theme, Bayesian calculations encourage a disre-\n",
      "gard for model selection effects. This can be dangerous in objective Bayes20.5 Notes and Details 417\n",
      "2.0 2.2 2.4 2.6 2.80.0 0.2 0.4 0.6 0.8 1.0\n",
      "c‚àívaluePr(Œ∏322 < c)\n",
      "Figure 20.10 The solid curve is the posterior cdf of \u0012322. Vertical\n",
      "red bars indicateÀôone frequentist standard error, as obtained\n",
      "from Theorem 20.4. Black triangles are endpoints of the 0.90\n",
      "central credible interval.\n",
      "settings where one can‚Äôt rely on genuine prior experience. Theorem 20.4\n",
      "serves as a frequentist checkpoint, offering some reassurance as in Fig-\n",
      "ure 20.9, or sounding a warning as in Figure 20.10.\n",
      "20.5 Notes and Details\n",
      "Optimality theories‚Äîstatements of best possible results‚Äîare marks of ma-\n",
      "turity in applied mathematics. Classical statistics achieved two such theo-\n",
      "ries: for unbiased or asymptotically unbiased estimation, and for hypothe-\n",
      "sis testing. Most of this book and all of this chapter venture beyond these\n",
      "safe havens. How far from best are theCp/OLS bootstrap smoothed esti-\n",
      "mates of Section 20.2? At this time we can‚Äôt answer such questions, though\n",
      "we can offer appealing methodologies in their pursuit, a few of which have\n",
      "been highlighted here.\n",
      "The cholestyramine example comes from Efron and Feldman (1991)\n",
      "where it is discussed at length. Data for a control group is also analyzed\n",
      "there.\n",
      "¬é1[p. 398] Scheff ¬¥e intervals. Scheff ¬¥e‚Äôs 1953 paper came at the beginning418 Inference After Model Selection\n",
      "of a period of healthy development in simultaneous inference techniques,\n",
      "mostly in classical normal theory frameworks. Miller (1981) gives a clear\n",
      "and thorough summary. The 1980s followed with a more computer-intensive\n",
      "approach, nicely developed in Westfall and Young‚Äôs 1993 book, leading up\n",
      "to Benjamini and Hochberg‚Äôs 1995 false-discovery rate paper (Chapter 15\n",
      "here), and Benjamini and Yekutieli‚Äôs (2005) false-coverage rate algorithm.\n",
      "Scheff ¬¥e‚Äôs construction (20.15) is derived by transforming (20.6) to the\n",
      "caseVDIusing the inverse square root of matrix V,\n",
      "DV\u00001=2Àá (20.60)\n",
      "(.V\u00001=2/2DV\u00001), which makes the ellipsoid of Figure 20.2 into a circle.\n",
      "dDd0\u001b2in (20.10), and for a linear combination \n",
      "it is straightforward to see that Pr fQ\u0014k.Àõ/2\n",
      "p;qgDÀõamounts to\n",
      "dÀôO\u001bkdkk.Àõ/\n",
      "p;q (20.61)\n",
      "for all choices of d, the geometry of Figure 20.2 now being transparent.\n",
      ", andcDV\u00001=2ddinates back to OÀáDV1=2O\n",
      "yields (20.15).\n",
      "¬é2[p. 399] Restricting the catalog C.Suppose that all the sample sizes njin\n",
      "(20.16) take the same value n, and that we wish to set simultaneous con-\n",
      "Ô¨Ådence intervals for all pairwise differences Àái\u0000Àáj. Tukey‚Äôs studentized\n",
      "range pivotal quantity (1952, unpublished)\n",
      "TDmax\n",
      "i¬§jÀáÀáÀá\u0010\n",
      "OÀái\u0000OÀáj\u0011\n",
      "\u0000.Àái\u0000Àáj/ÀáÀáÀá\n",
      "O\u001b(20.62)\n",
      "has a distribution not depending on \u001borÀá. This implies that\n",
      "Àái\u0000Àáj2OÀái\u0000OÀájÀôO\u001bpnT.Àõ/(20.63)\n",
      "is a set of simultaneous level- ÀõconÔ¨Ådence intervals for all pairwise dif-\n",
      "ferencesÀái\u0000Àáj, whereT.Àõ/is theÀõth quantile of T. (The factor 1=pn\n",
      "comes fromOÀáj\u0018N.Àáj;\u001b2=n/in (20.16).)\n",
      "Table 20.4 Half-width of Tukey studentized range simultaneous 95%\n",
      "conÔ¨Ådence intervals for pairwise differences Àái\u0000Àáj(in units ofO\u001b=pn)\n",
      "forpD2;3;:::;6 andnD20; compared with Scheff ¬¥e intervals (20.15) .\n",
      "p 2 3 4 5 6\n",
      "Tukey 2.95 3.58 3.96 4.23 4.44\n",
      "Scheff ¬¥e 3.74 4.31 4.79 5.21 5.5820.5 Notes and Details 419\n",
      "Reducing the catalog Cfrom all linear combinations c0Àáto only pair-\n",
      "wise differences shortens the simultaneous intervals. Table 20.4 shows the\n",
      "comparison between the Tukey and Scheff ¬¥e 95% intervals for pD2;3;\n",
      ":::;6 andnD20.\n",
      "CalculatingT.Àõ/was a substantial project in the early 1980s. Berk et al.\n",
      "(2013) now carry out the analogous computations for general catalogs of\n",
      "linear constraints. They discuss at length the inferential basis of such pro-\n",
      "cedures.\n",
      "¬é3[p. 405] Discontinuous estimators. Looking at Figure 20.6 suggests that a\n",
      "conÔ¨Ådence interval for \u0012\u00002:0will move far left for data sets xwhereCp\n",
      "selects linear regression ( mD1) as best. This kind of ‚Äújumpy‚Äù behav-\n",
      "ior lengthens the intervals needed to attain a desired coverage level. More\n",
      "seriously, intervals for mD1may give misleading inferences, another ex-\n",
      "ample of ‚Äúaccurate but incorrect‚Äù behavior. Bagging (20.28), in addition to\n",
      "reducing interval length, improves inferential correctness, as discussed in\n",
      "Efron (2014a).\n",
      "¬é4[p. 406] Theorem 20.2 and its corollary. Theorem 20.2 is proved in Sec-\n",
      "tion 3 of Efron (2014a), with a parametric bootstrap version appearing in\n",
      "Section 4. The corollary is a projection result illustrated in Figure 4 of\n",
      "that paper: let L.N/be then-dimensional subspace of B-dimensional Eu-\n",
      "clidean space spanned by the columns of the B\u0002nmatrix.Nbj/(20.29)\n",
      "andt\u0003theB-vector with components t\u0003b\u0000t\u0003\u0001; then\n",
      "bseIJ.s/ƒ±\n",
      "ƒ±t\u0003boot.t/D\n",
      "kt\u0003k; (20.64)\n",
      "whereOt\u0003is the projection of t\u0003intoL.N/. In the language of Section 10.3,\n",
      "ifO\u0012\u0003DS.P/is very nonlinear as a function of P, then the ratio in (20.64)\n",
      "will be substantially less than 1.\n",
      "¬é5[p. 409] Tweedie‚Äôs formula. For convenience, take \u001b2D1in (20.35).\n",
      "Bayes‚Äô rule (3.5) can then be arranged to give\n",
      "g.\u0016jz/De\u0016z\u0000 .z/g.\u0016/e\u00001\n",
      "2\u00162ƒ±p\n",
      "2\u0019 (20.65)\n",
      "with\n",
      " .z/D1\n",
      "2z2Clogf.z/: (20.66)\n",
      "This is a one-parameter exponential family (5.46) having natural parameter\n",
      "Àõequal toz. Differentiating  as in (5.55) gives\n",
      "Ef\u0016jzgDd \n",
      "dzDzCd\n",
      "dzlogf.z/; (20.67)420 Inference After Model Selection\n",
      "which is Tweedie‚Äôs formula (20.37) when \u001b2D1. The formula Ô¨Årst ap-\n",
      "pears in Robbins (1956), who credits it to a personal communication from\n",
      "M. K. Tweedie. Efron (2011) discusses general exponential family versions\n",
      "of Tweedie‚Äôs formula, and its application to selection bias situations.\n",
      "¬é6[p. 414] Theorem 20.4. The delta method standard error approximation for\n",
      "a statisticT.x/ is\n",
      "bsedeltaDh\n",
      ".rT.x//0OV.rT.x//i1=2\n",
      "; (20.68)\n",
      "whererT.x/ is the gradient vector .@T=@xj/andOVis an estimate of the\n",
      "covariance matrix of x. Other names include the ‚ÄúTaylor series method,‚Äù\n",
      "as in (2.10), and ‚Äúpropagation of errors‚Äù in the physical sciences literature.\n",
      "The proof of Theorem 20.4 in Section 2 of Efron (2015) consists of show-\n",
      "ing that Cov xDrT.x/ whenT.x/DEft.Àõ/jxg. Standard deviations are\n",
      "only a Ô¨Årst step in assessing the frequentist accuracy of T.x/ . The paper\n",
      "goes on to show how Theorem 20.4 can be improved to give conÔ¨Ådence\n",
      "intervals, correcting the impression in Figure 20.10 that cdf .c/can range\n",
      "outside≈í0;1¬ç .\n",
      "¬é7[p. 415] Bayesian lasso. Applying Bayes‚Äô rule (3.5) with density (20.51)\n",
      "and prior (20.52) gives\n",
      "logg.Àájy/D\u0000\u001aky\u0000XÀák2\n",
      "2C\u0015kÀák1\u001b\n",
      "; (20.69)\n",
      "as discussed in Tibshirani (2006). Comparison with (7.42) shows that the\n",
      "maximizing value of Àá(the ‚ÄúMAP‚Äù estimate) agrees with the lasso esti-\n",
      "mate. Park and Casella (2008) named the ‚ÄúBayesian lasso‚Äù and suggested\n",
      "an appropriate MCMC algorithm. Their choice \u0015D0:37 was based on\n",
      "marginal maximum likelihood calculations, giving their analysis an empir-\n",
      "ical Bayes aspect ignored in their and our analyses.21\n",
      "Empirical Bayes Estimation Strategies\n",
      "Classic statistical inference was focused on the analysis of individual cases:\n",
      "a single estimate, a single hypothesis test. The interpretation of direct evi-\n",
      "dence bearing on the case of interest‚Äîthe number of successes and failures\n",
      "of a new drug in a clinical trial as a familiar example‚Äîdominated statistical\n",
      "practice.\n",
      "The story of modern statistics very much involves indirect evidence,\n",
      "‚Äúlearning from the experience of others‚Äù in the language of Sections 7.4\n",
      "and 15.3, carried out in both frequentist and Bayesian settings. The computer-\n",
      "intensive prediction algorithms described in Chapters 16‚Äì19 use regression\n",
      "theory, the frequentist‚Äôs favored technique, to mine indirect evidence on a\n",
      "massive scale. False-discovery rate theory, Chapter 15, collects indirect ev-\n",
      "idence for hypothesis testing by means of Bayes‚Äô theorem as implemented\n",
      "through empirical Bayes estimation.\n",
      "Empirical Bayes methodology has been less studied than Bayesian or\n",
      "frequentist theory. As with the James‚ÄìStein estimator (7.13), it can seem to\n",
      "be little more than plugging obvious frequentist estimates into Bayes esti-\n",
      "mation rules. This conceals a subtle and difÔ¨Åcult task: learning the equiva-\n",
      "lent of a Bayesian prior distribution from ongoing statistical observations.\n",
      "Our Ô¨Ånal chapter concerns the empirical Bayes learning process, both as an\n",
      "exercise in applied deconvolution and as a relatively new form of statistical\n",
      "inference. This puts us back where we began in Chapter 1, examining the\n",
      "two faces of statistical analysis, the algorithmic and the inferential.\n",
      "21.1 Bayes Deconvolution\n",
      "A familiar formulation of empirical Bayes inference begins by assuming\n",
      "that an unknown prior density g.\u0012/ , our object of interest, has produced a\n",
      "random sample of real-valued variates ‚Äö1;‚Äö2;:::;‚ÄöN,\n",
      "‚Äöiiid\u0018g.\u0012/; iD1;2;:::;N: (21.1)\n",
      "421422 Empirical Bayes Estimation Strategies\n",
      "(The ‚Äúdensity‚Äù g.\u0001/may include discrete atoms of probability.) The ‚Äöiare\n",
      "unobservable, but each yields an observable random variable Xiaccording\n",
      "to a known family of density functions\n",
      "Xiind\u0018pi.Xij‚Äöi/: (21.2)\n",
      "From the observed sample X1;X2;:::;XNwe wish to estimate the prior\n",
      "densityg.\u0012/ .\n",
      "A famous example has pi.Xij‚Äöi/the Poisson family,\n",
      "Xi\u0018Poi.‚Äöi/; (21.3)\n",
      "as in Robbins‚Äô formula, Section 6.1. Still more familiar is the normal model\n",
      "(3.28),\n",
      "Xi\u0018N.‚Äöi;\u001b2/; (21.4)\n",
      "often with\u001b2D1. A binomial model was used in the medical example of\n",
      "Section 6.3,\n",
      "Xi\u0018Bi.ni;‚Äöi/: (21.5)\n",
      "There thenidiffer from case to case, accounting for the need for the Ô¨Årst\n",
      "subscriptiinpi.Xij‚Äöi/(21.2).\n",
      "Letfi.Xi/denote the marginal density ofXiobtained from (21.1)‚Äì\n",
      "(21.2),\n",
      "fi.Xi/DZ\n",
      "Tpi.Xij\u0012i/g.\u0012i/d\u0012i; (21.6)\n",
      "the integral being over the space Tof possible‚Äövalues. The statistician\n",
      "has only the marginal observations available,\n",
      "Xiind\u0018fi.\u0001/; iD1;2;:::;N; (21.7)\n",
      "from which he or she wishes to estimate the density g.\u0001/in (21.6).\n",
      "In the normal model (21.4), fiis the convolution of the unknown g.\u0012/\n",
      "with a known normal density, denoted\n",
      "fDg\u0003N.0;\u001b2/ (21.8)\n",
      "(nowfinot depending on i). Estimating gusing a sample X1;X2;:::;XN\n",
      "fromfis a problem in deconvolution . In general we might call the estima-\n",
      "tion ofgin model (21.1)‚Äì(21.2) the ‚ÄúBayes deconvolution problem.‚Äù\n",
      "An artiÔ¨Åcial example appears in Figure 21.1, where g.\u0012/ is a mixture\n",
      "distribution: seven-eighths N.0;0:52/and one-eighth uniform over the in-\n",
      "terval≈í\u00003;3¬ç. A normal sampling model Xiind\u0018N.‚Äöi;1/is assumed, yield-\n",
      "ingfby convolution as in (21.8). The convolution process makes fwider21.1 Bayes Deconvolution 423\n",
      "‚àí4 ‚àí2 0 2 40.00 0.05 0.10 0.15\n",
      "Œ∏ and xg(Œ∏) and f(x)\n",
      "f(x)g(Œ∏)\n",
      "Figure 21.1 An artiÔ¨Åcial example of the Bayes deconvolution\n",
      "problem. The solid curve is g.\u0012/ , the prior density of ‚Äö(21.1);\n",
      "the dashed curve is the density of an observation Xfrom marginal\n",
      "distributionfDg\u0003N.0;1/ (21.8). We wish to estimate g.\u0012/ on\n",
      "the basis of a random sample X1;X2;:::;XNfromf.x/ .\n",
      "and smoother than g, as illustrated in the Ô¨Ågure. Having observed a ran-\n",
      "dom sample from f, we wish to estimate the deconvolute g, which begins\n",
      "to look difÔ¨Åcult in the Ô¨Ågure‚Äôs example.\n",
      "Deconvolution has a well-deserved reputation for difÔ¨Åculty. It is the\n",
      "classic ill-posed problem: because of the convolution process (21.6), large\n",
      "changes ing.\u0012/ are smoothed out, often yielding only small changes in\n",
      "f.x/ . Deconvolution operates in the other direction, with small changes in\n",
      "the estimation of fdisturbingly magniÔ¨Åed on the gscale. Nevertheless,\n",
      "modern computation, modern theory, and most of all modern sample sizes,\n",
      "together can make empirical deconvolution a practical reality.\n",
      "Why would we want to estimate g.\u0012/ ? In the prostate data example\n",
      "(3.28) (where ‚Äöis called\u0016) we might wish to know Pr f‚ÄöD0g, the proba-\n",
      "bility of a nullgene, ones whose effect size is zero; or perhaps Pr fj‚Äöj\u00152g,\n",
      "the proportion of genes that are substantially non-null. Or we might want to\n",
      "estimate Bayesian posterior expectations like Ef‚ÄöjXDxgin Figure 20.7,\n",
      "or posterior densities as in Figure 6.5.\n",
      "Two main strategies have developed for carrying out empirical Bayes\n",
      "estimation: modeling on the \u0012scale, calledg-modeling here, and modeling424 Empirical Bayes Estimation Strategies\n",
      "on thexscale, called f-modeling . We begin in the next section with g-\n",
      "modeling.\n",
      "21.2g-Modeling and Estimation\n",
      "There has been a substantial amount of work on the asymptotic accuracy\n",
      "of estimatesOg.\u0012/ in the empirical Bayes model (21.1)‚Äì(21.2), most often\n",
      "in the normal sampling framework (21.4). The results are discouraging,\n",
      "with the rate of convergence of Og.\u0012/ tog.\u0012/ as slow as.logN/\u00001. In our\n",
      "terminology, much of this work has been carried out in a nonparametric g-\n",
      "modeling framework, allowing the unknown prior density g.\u0012/ to be virtu-\n",
      "ally anything at all. More optimistic results are possible if the g-modeling\n",
      "is pursued parametrically, that is, by restricting g.\u0012/ to lie within some\n",
      "parametric family of possibilities.\n",
      "We assume, for the sake of simpler exposition, that the space Tof pos-\n",
      "sible‚Äövalues is Ô¨Ånite and discrete, say\n",
      "TDÀö\n",
      "\u0012.1/;\u0012.2/;:::;\u0012.m/\t\n",
      ": (21.9)\n",
      "The prior density g.\u0012/ is now represented by a vector gD.g1;g2;:::;gm/0,\n",
      "with components\n",
      "gjDPrÀö\n",
      "‚ÄöD\u0012.j/\t\n",
      "forjD1;2;:::;m: (21.10)\n",
      "Ap-parameter exponential family (5.50) for gcan be written as\n",
      "gDg.Àõ/DeQÀõ\u0000 .Àõ/; (21.11)\n",
      "where thep-vectorÀõis the natural parameter and Qis a knownm\u0002p\n",
      "structure matrix . Notation (21.11) means that the jth component of g.Àõ/\n",
      "is\n",
      "gj.Àõ/DeQ0\n",
      "jÀõ\u0000 .Àõ/; (21.12)\n",
      "withQ0\n",
      "jthejth row ofQ; the function  .Àõ/ is the normalizer that makes\n",
      "g.Àõ/sum to 1,\n",
      " .Àõ/Dlog0\n",
      "@mX\n",
      "jD1eQ0\n",
      "jÀõ1\n",
      "A: (21.13)\n",
      "In the nodes example of Figure 6.4, the set of possible ‚Äövalues was\n",
      "TDf0:01;0:02;:::;0:99 g, andQwas a Ô¨Åfth-degree polynomial matrix,\n",
      "QDpoly( T,5) (21.14)21.2g-Modeling and Estimation 425\n",
      "inRnotation, indicating a Ô¨Åve-parameter exponential family for g, (6.38)‚Äì\n",
      "(6.39).\n",
      "In the development that follows we will assume that the kernel pi.\u0001j\u0001/in\n",
      "(21.2) does not depend on i, i.e., thatXihas the same family of conditional\n",
      "distributions p.Xij‚Äöi/for alli, as in the Poisson and normal situations\n",
      "(21.3) and (21.4), but not the binomial case (21.5). And moreover we as-\n",
      "sume that the sample space Xfor theXiobservations is Ô¨Ånite and discrete,\n",
      "say\n",
      "XDÀö\n",
      "x.1/;x.2/;:::;x.n/\t\n",
      ": (21.15)\n",
      "None of this is necessary, but it simpliÔ¨Åes the exposition.\n",
      "DeÔ¨Åne\n",
      "pkjDPrÀö\n",
      "XiDx.k/j‚ÄöiD\u0012.j/\t\n",
      "; (21.16)\n",
      "forkD1;2;:::;n andjD1;2;:::;m , and the corresponding n\u0002m\n",
      "matrix\n",
      "PD.pkj/; (21.17)\n",
      "havingkth rowPkD.pk1;pk2;:::;pkm/0. The convolution-type for-\n",
      "mula (21.6) for the marginal density f.x/ now reduces to an inner product,\n",
      "fk.Àõ/DPrÀõÀö\n",
      "XiDx.k/\t\n",
      "DPm\n",
      "jD1pkjgj.Àõ/\n",
      "DP0\n",
      "kg.Àõ/:(21.18)\n",
      "In fact we can write the entire marginal density f.Àõ/D.f1.Àõ/;f2.Àõ/;::: ,\n",
      "fn.Àõ//0in terms of matrix multiplication,\n",
      "f.Àõ/DPg.Àõ/: (21.19)\n",
      "The vector of counts yD.y1;y2;:::;yn/, with\n",
      "ykD#Àö\n",
      "XiDx.k/\t\n",
      "; (21.20)\n",
      "is a sufÔ¨Åcient statistic in the iid situation. It has a multinomial distribution\n",
      "(5.38),\n",
      "y\u0018Multn.N;f.Àõ//; (21.21)\n",
      "indicatingNindependent draws for a density f.Àõ/onncategories.\n",
      "All of this provides a concise description of the g-modeling probability\n",
      "model:\n",
      "Àõ!g.Àõ/DeQÀõ\u0000 .Àõ/!f.Àõ/DPg.Àõ/!y\u0018Multn.N;f.Àõ//:\n",
      "(21.22)426 Empirical Bayes Estimation Strategies\n",
      "The inferential task goes in the reverse direction,\n",
      "y!OÀõ!f.OÀõ/!g.OÀõ/DeQOÀõ\u0000 .OÀõ/: (21.23)\n",
      "Figure 21.2 A schematic diagram of empirical Bayes estimation,\n",
      "as explained in the text. Snis then-dimensional simplex,\n",
      "containing the p-parameter family Fof allowable probability\n",
      "distributionsf.Àõ/. The vector of observed proportions y=N\n",
      "yields MLEf.OÀõ/, which is then deconvolved to obtain estimate\n",
      "g.OÀõ/.\n",
      "A schematic diagram of the estimation process appears in Figure 21.2.\n",
      "\u000fThe vector of observed proportions y=Nis a point in Sn, the simplex\n",
      "(5.39) of all possible probability vectors fonncategories;y=Nis the\n",
      "usual nonparametric estimate of f.\n",
      "\u000fThe parametric family of allowable fvectors (21.19)\n",
      "FDff.Àõ/; Àõ2Ag; (21.24)\n",
      "indicated by the red curve, is a curved p-dimensional surface in Sn. Here\n",
      "Ais the space of allowable vectors Àõin family (21.11).\n",
      "\u000fThe nonparametric estimate y=Nis ‚Äúprojected‚Äù down to the parametric\n",
      "estimatef.OÀõ/; if we are using MLE estimation, f.OÀõ/will be the closest\n",
      "point in Ftoy=Nmeasured according to a deviance metric, as in (8.35).\n",
      "\u000fFinally,f.OÀõ/is mapped back to the estimate g.OÀõ/, by inverting map-\n",
      "ping (21.19). (Inversion is not actually necessary with g-modeling since,\n",
      "having foundOÀõ,g.OÀõ/is obtained directly from (21.11); the inversion\n",
      "step is more difÔ¨Åcult for f-modeling, Section 21.6.)21.3 Likelihood, Regularization, and Accuracy 427\n",
      "The maximum likelihood estimation process for g-modeling is discussed\n",
      "in more detail in the next section, where formulas for its accuracy will be\n",
      "developed.\n",
      "21.3 Likelihood, Regularization, and Accuracy1\n",
      "Parametricg-modeling, as in (21.11), allows us to work in low-dimensional\n",
      "parametric families‚Äîjust Ô¨Åve parameters for the nodes example (21.14)‚Äî\n",
      "where classic maximum likelihood methods can be more conÔ¨Ådently ap-\n",
      "plied. Even here though, some regularization will be necessary for stable\n",
      "estimation, as discussed in what follows.\n",
      "Theg-model probability mechanism (21.22) yields a log likelihood for\n",
      "the multinomial vector yof counts as a function of Àõ, sayly.Àõ/;\n",
      "ly.Àõ/Dlog nY\n",
      "kD1fk.Àõ/yk!\n",
      "DnX\n",
      "kD1yklogfk.Àõ/: (21.25)\n",
      "Its score functionPly.Àõ/, the vector of partial derivatives @ly.Àõ/=@Àõhfor\n",
      "hD1;2;:::;p , determines the MLE OÀõaccording toPly.OÀõ/D0. The\n",
      "p\u0002pmatrix of second derivatives Rly.Àõ/D.@2ly.Àõ/=@Àõh@Àõl/gives the\n",
      "Fisher information matrix (5.26)\n",
      "I.Àõ/DEf\u0000Rly.Àõ/g: (21.26)\n",
      "The exponential family model (21.11) yields simple expressions for Ply.Àõ/\n",
      "andI.Àõ/. DeÔ¨Åne\n",
      "wkjDgj.Àõ/\u0012pkj\n",
      "fk.Àõ/\u00001\u0013\n",
      "(21.27)\n",
      "and the corresponding m-vector\n",
      "Wk.Àõ/D.wk1.Àõ/;wk2.Àõ/;:::;w km.Àõ//0: (21.28)\n",
      "Lemma 21.1 The score functionPly.Àõ/under model (21.22) is\n",
      "Ply.Àõ/DQWC.Àõ/; whereWC.Àõ/DnX\n",
      "kD1Wk.Àõ/yk (21.29)\n",
      "andQis them\u0002pstructure matrix in (21.11) .\n",
      "1The technical lemmas in this section are not essential to following the subsequent\n",
      "discussion.428 Empirical Bayes Estimation Strategies\n",
      "Lemma 21.2 The Fisher information matrix I.Àõ/, evaluated at ÀõDOÀõ,\n",
      "is\n",
      "I.OÀõ/DQ0(nX\n",
      "kD1Wk.OÀõ/Nfk.OÀõ/Wk.OÀõ/0)\n",
      "Q; (21.30)\n",
      "whereNDPn\n",
      "1ykis the sample size in the empirical Bayes model (21.1) ‚Äì\n",
      "(21.2) .\n",
      "See the chapter endnotes¬éfor a brief discussion of Lemmas 21.1 and ¬é1\n",
      "21.2.I.OÀõ/\u00001is the usual maximum likelihood estimate of the covariance\n",
      "matrix ofOÀõ, but we will use a regularized version of the MLE that is less\n",
      "variable.\n",
      "In the examples that follow, OÀõwas found by numerical maximization.2\n",
      "Even thoughg.Àõ/is an exponential family, the marginal density f.Àõ/in\n",
      "(21.22) is not . As a result, some care is needed in avoiding local maxima of\n",
      "ly.Àõ/. These tend to occur at ‚Äúcorner‚Äù values of Àõ, where one of its compo-\n",
      "nents goes to inÔ¨Ånity. A small amount of regularization pulls OÀõaway from\n",
      "the corners, decreasing its variance at the possible expense of increased\n",
      "bias.\n",
      "Instead of maximizing ly.Àõ/we maximize a penalized likelihood\n",
      "m.Àõ/Dly.Àõ/\u0000s.Àõ/; (21.31)\n",
      "wheres.Àõ/ is a positive penalty function. Our examples use\n",
      "s.Àõ/Dc0kÀõkDc0 pX\n",
      "hD1Àõ2\n",
      "h!1=2\n",
      "(21.32)\n",
      "(withc0equal 1), which prevents the maximizer OÀõofm.Àõ/ from venturing\n",
      "too far into corners.\n",
      "The following lemma is discussed in the chapter endnotes.\n",
      "Lemma 21.3¬éThe maximizerOÀõofm.Àõ/ has approximate bias vector and ¬é2\n",
      "covariance matrix\n",
      "Bias.OÀõ/D\u0000.I.OÀõ/CRs.OÀõ//\u00001Ps.OÀõ/\n",
      "and Var.OÀõ/D.I.OÀõ/CRs.OÀõ//\u00001I.OÀõ/.I.OÀõ/CRs.OÀõ//\u00001;(21.33)\n",
      "whereI.OÀõ/is given in (21.30) .\n",
      "Withs.Àõ/\u00110(no regularization) the bias is zero and Var .OÀõ/DI.OÀõ/\u00001,\n",
      "2Using the nonlinear maximizer nlm in R.21.3 Likelihood, Regularization, and Accuracy 429\n",
      "the usual MLE approximations: including s.Àõ/ reduces variance while in-\n",
      "troducing bias.\n",
      "Fors.Àõ/Dc0kÀõkwe calculate\n",
      "Ps.Àõ/Dc0Àõ=kÀõkandRs.Àõ/Dc0\n",
      "kÀõk\u0012\n",
      "I\u0000ÀõÀõ0\n",
      "kÀõk2\u0013\n",
      "; (21.34)\n",
      "withIthep\u0002pidentity matrix. Adding the penalty s.Àõ/ in (21.31) pulls\n",
      "the MLE ofÀõtoward zero and the MLE of g.Àõ/toward a Ô¨Çat distribution\n",
      "overT. Looking at Var .OÀõ/in (21.33), a measure of the regularization effect\n",
      "is\n",
      "tr.Rs.OÀõ//= tr.I.OÀõ//; (21.35)\n",
      "which was never more than a few percent in our examples.\n",
      "Most often we will be more interested in the accuracy of OgDg.OÀõ/than\n",
      "in that ofOÀõitself. Letting\n",
      "D.OÀõ/Ddiag.g.OÀõ//\u0000g.OÀõ/g.OÀõ/0; (21.36)\n",
      "them\u0002pderivative matrix .@gj=@Àõh/is\n",
      "@g=@ÀõDD.Àõ/Q; (21.37)\n",
      "withQthe structure matrix in (21.11). The usual Ô¨Årst-order delta-method\n",
      "calculations then give the following theorem.\n",
      "Theorem 21.4 The penalized maximum likelihood estimate OgDg.OÀõ/\n",
      "has estimated bias vector and covariance matrix\n",
      "Bias.Og/DD.OÀõ/QBias.OÀõ/\n",
      "and Var.Og/DD.OÀõ/QVar.OÀõ/Q0D.OÀõ/(21.38)\n",
      "with Bias.OÀõ/andVar.OÀõ/as in (21.33) .3\n",
      "The many approximations going into Theorem 21.4 can be short-circuited\n",
      "by means of the parametric bootstrap, Section 10.4. Starting from OÀõand\n",
      "f.OÀõ/DPg.OÀõ/, we resample the count vector\n",
      "y\u0003\u0018Multn.N;f.OÀõ//; (21.39)\n",
      "and calculate4the penalized MLE OÀõ\u0003based ony\u0003, yieldingOg\u0003Dg.OÀõ\u0003/.\n",
      "3Note that the bias treats model (21.11) as the true prior, and arises as a result of the\n",
      "penalization.\n",
      "4Convergence of the nlm search process is speeded up by starting from OÀõ.430 Empirical Bayes Estimation Strategies\n",
      "BreplicationsOg\u00031;Og\u00032;:::;Og\u0003Bgives bias and covariance estimates\n",
      "dBiasDOg\u0003\u0001\u0000Og\n",
      "andcVarDBX\n",
      "bD1.Og\u0003b\u0000Og\u0003\u0001/.Og\u0003b\u0000Og\u0003\u0001/ƒ±\n",
      ".B\u00001/;(21.40)\n",
      "andOg\u0003\u0001DPB\n",
      "1Og\u0003b=B.\n",
      "Table 21.1 Comparison of delta method (21.38) and bootstrap (21.40)\n",
      "standard errors and biases for the nodes study estimate of gin\n",
      "Figure 6.4. All columns except the Ô¨Årst multiplied by 100.\n",
      "Standard Error Bias\n",
      "\u0012 g.\u0012/ Delta Boot Delta Boot\n",
      ".01 12.048 .887 .967 \u0000.518\u0000.592\n",
      ".12 1.045 .131 .139 .056 .071\n",
      ".23 .381 .058 .065 .025 .033\n",
      ".34 .779 .096 .095 \u0000.011\u0000.013\n",
      ".45 1.119 .121 .117 \u0000.040\u0000.049\n",
      ".56 .534 .102 .100 .019 .027\n",
      ".67 .264 .047 .051 .023 .027\n",
      ".78 .224 .056 .053 .018 .020\n",
      ".89 .321 .054 .048 .013 .009\n",
      ".99 .576 .164 .169 \u0000.008 .008\n",
      "Table 21.1 compares the delta method of Theorem 20.4 with the para-\n",
      "metric bootstrap ( BD1000 replications) for the surgical nodes example\n",
      "of Section 6.3. Both the standard errors‚Äîsquare roots of the diagonal el-\n",
      "ements of Var .Og/‚Äîand biases are well approximated by the delta method\n",
      "formulas (21.38). The delta method also performed reasonably well on the\n",
      "two examples of the next section.\n",
      "It did less well on the artiÔ¨Åcial example of Figure 21.1, where\n",
      "g.\u0012/D1\n",
      "8I≈í\u00003;3¬ç.\u0012/\n",
      "6C7\n",
      "81p\n",
      "2\u0019\u001b2e\u00001\n",
      "2\u00122\n",
      "\u001b2.\u001bD0:5/ (21.41)\n",
      "(1/8 uniform on ≈í\u00003;3¬ç and 7/8 N.0;0:52/). The vertical bars in Fig-\n",
      "ure 21.3 indicateÀôone standard error obtained from the parametric boot-\n",
      "strap, taking TDf\u00003;\u00002:8;:::;3gfor the sample space of ‚Äö, and as-\n",
      "suming a natural spline model in (21.11) with Ô¨Åve degrees of freedom,\n",
      "g.Àõ/DeQÀõ\u0000 .Àõ/;QDns(T,df=5): (21.42)21.3 Likelihood, Regularization, and Accuracy 431\n",
      "‚àí3 ‚àí2 ‚àí1 0 1 2 30.00 0.05 0.10 0.15\n",
      "Œ∏g(Œ∏)\n",
      "Figure 21.3 The red curve is g.\u0012/ for the artiÔ¨Åcial example of\n",
      "Figure 21.1. Vertical bars are Àôone standard error for g-model\n",
      "estimateg.OÀõ/; speciÔ¨Åcations (21.41)‚Äì(21.42), sample size\n",
      "ND1000 observationsXi\u0018N.‚Äöi;1/, using parametric\n",
      "bootstrap (21.40), BD500. The light dashed line follows\n",
      "bootstrap meansOg\u0003\n",
      "j. Some deÔ¨Ånitional bias is apparent.\n",
      "The sampling model was Xi\u0018N.‚Äöi;1/foriD1;2;:::;ND1000 . In\n",
      "this case the delta method standard errors were about 25% too small.\n",
      "The light dashed curve in Figure 21.3 traces Ng.\u0012/ , the average of the\n",
      "BD500bootstrap replications g\u0003b. There is noticeable bias, compared\n",
      "withg.\u0012/ . The reason is simple: the exponential family (21.42) for g.Àõ/\n",
      "does not include g.\u0012/ (21.41). In fact,Ng.\u0012/ is (nearly) the closest mem-\n",
      "ber of the exponential family to g.\u0012/ . This kind of deÔ¨Ånitional bias is a\n",
      "disadvantage of parametric g-modeling.\n",
      "\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001‚Äî‚Äî\u0001\u0001\n",
      "Ourg-modeling examples, and those of the next section, bring together\n",
      "a variety of themes from modern statistical practice: classical maximum\n",
      "likelihood theory, exponential family modeling, regularization, bootstrap\n",
      "methods, large data sets of parallel structure, indirect evidence, and a com-\n",
      "bination of Bayesian and frequentist thinking, all of this enabled by mas-\n",
      "sive computer power. Taken together they paint an attractive picture of the\n",
      "range of inferential methodology in the twenty-Ô¨Årst century.432 Empirical Bayes Estimation Strategies\n",
      "21.4 Two Examples\n",
      "We now reconsider two previous data sets from a g-modeling point of\n",
      "view. the Ô¨Årst is the artiÔ¨Åcial microarray-type example (20.24) comprising\n",
      "ND10,000 independent observations\n",
      "ziind\u0018N.\u0016i;1/; iD1;2;:::;ND10,000; (21.43)\n",
      "with\n",
      "\u0016i\u0018(\n",
      "0 foriD1;2;:::;9000\n",
      "N.\u00003;1/ foriD9001;:::; 10,000:(21.44)\n",
      "Figure 20.3 displays the points .zi;\u0016i/foriD9001;:::;10;000 , illus-\n",
      "trating the Bayes posterior 95% conditional intervals (20.26),\n",
      "\u0016i2.zi\u00003/=2Àô1:96ƒ±p\n",
      "2: (21.45)\n",
      "These required knowing the Bayes prior distribution \u0016i\u0018N.\u00003;1/. We\n",
      "would like to recover intervals (21.45) using just the observed data zi,iD\n",
      "1;2;:::;10;000 , without knowledge of the prior.\n",
      " \n",
      "z-valuesFrequency\n",
      "‚àí8 ‚àí6 ‚àí4 ‚àí2 0 2 40 200 400 600 800\n",
      "| |^ ^\n",
      "Figure 21.4 Histogram of observed sample of ND10,000\n",
      "valueszifrom simulations (21.43)‚Äì(21.44).\n",
      "A histogram of the 10,000 z-values is shown in Figure 21.4; g-modeling\n",
      "(21.9)‚Äì(21.11) was applied to them (now with \u0016playing the role of ‚Äú ‚Äö‚Äù21.4 Two Examples 433\n",
      "andzbeing ‚Äúx‚Äù), taking TD.\u00006;\u00005:75;:::;3/ .Qwas composed of a\n",
      "delta function at \u0016D0and a Ô¨Åfth-degree polynomial basis for the nonzero\n",
      "\u0016, again a family of spike-and-slab priors. The penalized MLE Og(21.31),\n",
      "(21.32),c0D1, estimated the probability of \u0016D0as\n",
      "Og.0/D0:891Àô0:006 (21.46)\n",
      "(using (21.38), which also provided bias estimate 0.001).\n",
      "‚àí8 ‚àí6 ‚àí4 ‚àí2 0‚àí10 ‚àí8 ‚àí6 ‚àí4 ‚àí2 0\n",
      "Observed z¬µ‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè\n",
      "‚óè\n",
      "‚óè\n",
      "‚óè‚óè‚óè‚óè\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "BY.loBY.up‚àí2.77\n",
      "‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè‚óè\n",
      "Bayes.loBayes.up\n",
      "EB.loEB.up\n",
      "Figure 21.5 Purple curves show g-modeling estimates of\n",
      "conditional 95% credible intervals for \u0016givenzin artiÔ¨Åcial\n",
      "microarray example (21.43)‚Äì(21.44). They are a close match to\n",
      "the actual Bayes intervals, dotted lines; cf. Figure 20.3.\n",
      "The estimated posterior density of \u0016givenzis\n",
      "Og.\u0016jz/DczOg.\u0016/\u001e.z\u0000\u0016/; (21.47)\n",
      "\u001e.\u0001/the standard normal density and czthe constant required for Og.\u0016jz/\n",
      "to integrate to 1. Let q.Àõ/.z/denote theÀõth quantile ofOg.\u0016jz/. The purple\n",
      "curves in Figure 21.5 trace the estimated 95% credible intervals\n",
      "\u0010\n",
      "q.:025/.z/;q.:975/.z/\u0011\n",
      ": (21.48)\n",
      "They are a close match to the actual credible intervals (21.45).\n",
      "The solid black curve in Figure 21.6 shows Og.\u0016/ for\u0016¬§0(the ‚Äúslab‚Äù\n",
      "portion of the estimated prior). As an estimate of the actual slab density434 Empirical Bayes Estimation Strategies\n",
      "‚àí6 ‚àí4 ‚àí2 0 2 40.00 0.02 0.04 0.06 0.08 0.10\n",
      "Œ∏Density\n",
      "‚óèatom\n",
      ".891N(‚àí3,1)\n",
      "Figure 21.6 The heavy black curve is the g-modeling estimate of\n",
      "g.\u0016/ for\u0016¬§0in the artiÔ¨Åcial microarray example, suppressing\n",
      "the atom at zero,Og.0/D0:891 . It is only a rough estimate of the\n",
      "actual nonzero density N.\u00003;1/.\n",
      "\u0016\u0018N.\u00003;1/ it is only roughly accurate, but apparently still accurate\n",
      "enough to yield the reasonably good posterior intervals seen in Figure 21.5.\n",
      "The fundamental impediment to deconvolution‚Äîthat large changes in g.\u0012/\n",
      "produce only small changes in f.x/ ‚Äîcan sometimes operate in the statis-\n",
      "tician‚Äôs favor, when only a rough knowledge of gsufÔ¨Åces for applied pur-\n",
      "poses.\n",
      "Our second example concerns the prostate study data, last seen in\n",
      "Figure 15.1:nD102men, 52 cancer patients and 50 normal controls, each\n",
      "have had their genetic activities measured on a microarray of ND6033\n",
      "genes; gene iyields a test statistic zicomparing patients with controls,\n",
      "zi\u0018N.\u0016i;\u001b2\n",
      "0/; (21.49)\n",
      "with\u0016ithe gene‚Äôs effect size. (Here we will take the variance \u001b2\n",
      "0as a\n",
      "parameter to be estimated, rather than assuming \u001b2\n",
      "0D1.) What is the prior\n",
      "densityg.\u0016/ for the effects?\n",
      "The local false-discovery rate program locfdr , Section 15.5, was ap-\n",
      "plied to the 6033 zivalues, as shown in Figure 21.7. Locfdr is an ‚Äúf-\n",
      "modeling‚Äù method, where probability models are proposed directly for21.4 Two Examples 435\n",
      "‚àí4 ‚àí2 0 2 40 100 200 300 400 \n",
      "z-valuesCounts\n",
      "Figure 21.7 The green curve is a six-parameter Poisson\n",
      "regression estimate Ô¨Åt to counts of the observed zivalues for the\n",
      "prostate data. The dashed curve is the empirical null (15.48),\n",
      "zi\u0018N.0:00;1:062/. Thef-modeling program locfdr\n",
      "estimated null probability Pr f\u0016D0gD0:984 . Genes with\n",
      "z-values lying beyond the red triangles have estimated fdr values\n",
      "less than 0.20.\n",
      "the marginal density f.\u0001/rather than for the prior density g.\u0001/; see Sec-\n",
      "tion (21.6). Here we can compare locfdr ‚Äôs results with those from g-\n",
      "modeling. The former gave5\n",
      "\u0010\n",
      "Oƒ±0;O\u001b0;O\u00190\u0011\n",
      "D.0:00;1:06;0:984/ (21.50)\n",
      "in the notation of (15.50); that is, it estimated the null distribution as \u0016\u0018\n",
      "N.0;1:062/, with probabilityO\u00190D0:984 of a gene being null ( \u0016D0).\n",
      "Only 22 genes were estimated to have local fdr values less than 0.20, the\n",
      "9 withzi\u0014\u00003:71 and the 12 with zi\u00153:81. (These are more pessimistic\n",
      "results than in Figure 15.5, where we used the theoretical null N.0;1/\n",
      "rather than the empirical null N.0;1:062/.)\n",
      "Theg-modeling approach (21.11) was applied to the prostate study\n",
      "data, assuming zi\u0018N.\u0016i;\u001b2\n",
      "0/,\u001b0D1:06 as suggested by (21.50). The\n",
      "5Using a six-parameter Poisson regression Ô¨Åt to the zivalues, of the type employed in\n",
      "Section 10.4.436 Empirical Bayes Estimation Strategies\n",
      "structure matrix Qin (21.11) had a delta function at \u0016D0and a Ô¨Åve-\n",
      "parameter natural spline basis for \u0016¬§0;TD.\u00003:6;\u00003:4;:::;3:6/ for\n",
      "the discretized ‚Äöspace (21.9). This gave a penalized MLE Oghaving null\n",
      "probability\n",
      "Og.0/D0:946Àô0:011: (21.51)\n",
      "‚àí4 ‚àí2 0 2 40.0000 0.0005 0.0010 0.0015 0.0020 0.0025 0.0030\n",
      "Œ∏g(Œ∏)\n",
      "| | ‚óènull atom\n",
      "0.946\n",
      "Figure 21.8 Theg-modeling estimate for the non-null density\n",
      "Og.\u0016/ ,\u0016¬§0, for the prostate study data, also indicating the\n",
      "null atomOg.0/D0:946 . About 2% of the genes are estimated to\n",
      "have effect sizesj\u0016ij\u00152. The red bars show Àôone standard\n",
      "error as computed from Theorem 21.4 (page 429).\n",
      "The non-null distribution, Og.\u0016/ for\u0016¬§0, appears in Figure 21.8, where\n",
      "it is seen to be modestly unimodal around \u0016D0. Dashed red bars indicate\n",
      "Àôone standard error for the Og.\u0012.j//estimates obtained from Theorem 21.4\n",
      "(page 429). The accuracy is not very good. It is better for larger regions of\n",
      "the‚Äöspace, for example\n",
      "bPrfj\u0012j\u00152gD0:020Àô0:0014: (21.52)\n",
      "Hereg-modeling estimated less prior null probability, 0.946 compared\n",
      "with 0.984 from f-modeling, but then attributed much of the non-null\n",
      "probability to small values of j\u0016ij.\n",
      "Taking (21.52) literally suggests 121 ( D0:020\u00016033 ) genes with true21.5 Generalized Linear Mixed Models 437\n",
      "‚àí4 ‚àí2 0 2 40.0 0.2 0.4 0.6 0.8 1.0\n",
      "z-valuefdr(z)\n",
      "Figure 21.9 The black curve is the empirical Bayes estimated\n",
      "false-discovery rate bPrf\u0016D0jzgfromg-modeling. For large\n",
      "values ofjzjit nearly matches the locfdrf-modeling estimate\n",
      "fdr.z/, red curve.\n",
      "effect sizesj\u0016ij\u00152. That doesn‚Äôt mean we can say with certainty which\n",
      "121. Figure 21.9 compares the g-modeling empirical Bayes false-discovery\n",
      "rate\n",
      "bPrf\u0016D0jzgDczOg.0/\u001e\u0012z\u0000\u0016\n",
      "O\u001b0\u0013\n",
      "; (21.53)\n",
      "as in (21.47), with the f-modeling estimate cfdr.z/produced by locfdr .\n",
      "Where it counts, in the tails, they are nearly the same.\n",
      "21.5 Generalized Linear Mixed Models\n",
      "Theg-modeling theory can be extended to the situation where each ob-\n",
      "servationXiis accompanied by an observed vector of covariates ci, say\n",
      "of dimension d. We return to the generalized linear model setup of Sec-\n",
      "tion 8.2, where each Xihas a one-parameter exponential family density\n",
      "indexed by its own natural parameter \u0015i,\n",
      ".\u0015i/gf0.Xi/ (21.54)\n",
      "in notation (8.20).438 Empirical Bayes Estimation Strategies\n",
      "Our key assumption is that each \u0015iis the sum of a deterministic compo-\n",
      "nent, depending on the covariates ci, and a random term ‚Äöi,\n",
      "\u0015iD‚ÄöiCc0\n",
      "iÀá: (21.55)\n",
      "Here‚Äöiis an unobserved realization from g.Àõ/DexpfQÀõ\u0000 .Àõ/g\n",
      "(21.11) andÀáis an unknown d-dimensional parameter. If ÀáD0then\n",
      "(21.55) is ag-model as before,6while if all the ‚ÄöiD0then it is a stan-\n",
      "dard GLM (8.20)‚Äì(8.22). Taken together, (21.55) represents a generalized\n",
      "linear mixed model (GLMM). The likelihood and accuracy calculations of\n",
      "Section 21.3 extend to GLMMs, as referenced in the endnotes, but here we\n",
      "will only discuss a GLMM analysis of the nodes study of Section 6.3.\n",
      "In addition to nithe number of nodes removed and Xithe number\n",
      "found positive (6.33), a vector of four covariates\n",
      "ciD.agei, sexi, smoke i, progi/ (21.56)\n",
      "was observed for each patient: a standardized version of age in years; sex\n",
      "being 0 for female or 1 for male; smoke being 0 for no or 1 for yes to long-\n",
      "term smoking; and prog being a post-operative prognosis score with large\n",
      "values more favorable.\n",
      "GLMM model (21.55) was applied to the nodes data. Now\u0015iwas the\n",
      "logit log≈í\u0019i=.1\u0000\u0019i/¬ç, where\n",
      "Xi\u0018Bi.ni;\u0019i/ (21.57)\n",
      "as in Table 8.4, i.e., \u0019iis the probability that any one node from patient\n",
      "iis positive. To make the correspondence with the analysis in Section 6.3\n",
      "exact, we used a variant of (21.55)\n",
      "\u0015iDlogit.‚Äöi/Cc0\n",
      "iÀá: (21.58)\n",
      "Now withÀáD0,‚Äöiis exactly the binomial probability \u0019ifor theith\n",
      "case. Maximum likelihood estimates were calculated for Àõin (21.11)‚Äî\n",
      "withTD.0:01;0:02;:::;0:99/ andQDpoly( T,5) (21.14)‚Äîand\n",
      "Àáin (21.58). The MLE prior g.OÀõ/was almost the same as that estimated\n",
      "without covariates in Figure 6.4.\n",
      "Table 21.2 shows the MLE values .OÀá1;OÀá2;OÀá3;OÀá4/, their standard errors\n",
      "(from a parametric bootstrap simulation), and the z-valuesOÀák=bsek.Sex\n",
      "looks like it has a signiÔ¨Åcant effect, with males tending toward larger values\n",
      "of\u0019i, that is, a greater number of positive nodes. The big effect though is\n",
      "prog , larger values of prog indicating smaller values of \u0019i.\n",
      "6Here the setup is more speciÔ¨Åc; fis exponential family, and ‚Äöiis on the\n",
      "natural-parameter scale.21.5 Generalized Linear Mixed Models 439\n",
      "Table 21.2 Maximum likelihood estimates .OÀá1;OÀá2;OÀá3;OÀá4/for GLMM\n",
      "analysis of the nodes data, and standard errors from a parametric\n",
      "bootstrap simulation; large values of progipredict low values of \u0019i.\n",
      "age sex smoke prog\n",
      "MLE\u0000.078 .192 .089 \u0000.698\n",
      "Boot st err .066 .070 .063 .077\n",
      "z-value\u00001.18 2.74 1.41 \u00009.07\n",
      "0.0 0.2 0.4 0.6 0.8 1.00.00 0.05 0.10 0.15\n",
      "Probability positive nodeDensity\n",
      "worst prognosisbest prognosis\n",
      "Figure 21.10 Distribution of \u0019i, individual probabilities of a\n",
      "positive node, for best and worst levels of factor prog ; from\n",
      "GLMM analysis of nodes data.\n",
      "Figure 21.10 displays the distribution of \u0019iD1=≈í1Cexp.\u0000\u0015i/¬çimplied\n",
      "by the GLMM model for the best and worst values of prog (setting age,\n",
      "sex, and smoke to their average values and letting ‚Äöhave distribution\n",
      "g.OÀõ/). The implied distribution is concentrated near \u0019D0for the best-\n",
      "levelprog , while it is roughly uniform over ≈í0;1¬ç for the worst level.\n",
      "The random effects we have called ‚Äöiare sometimes called frailties : a\n",
      "composite of unmeasured individual factors lumped together as an index\n",
      "of disease susceptibility. Taken together, Figures 6.4 and 21.10 show sub-\n",
      "stantial frailty and covariate effects both at work in the nodes data. In440 Empirical Bayes Estimation Strategies\n",
      "the language of Section 6.1, we have amassed ‚Äúindirect evidence‚Äù for each\n",
      "patient, using both Bayesian and frequentist methods.\n",
      "21.6 Deconvolution and f-Modeling\n",
      "Empirical Bayes applications have traditionally been dominated by f-\n",
      "modeling‚Äînot the g-modeling approach of the previous sections‚Äîwhere\n",
      "probability models for the marginal density f.x/ , usually exponential fam-\n",
      "ilies, are Ô¨Åt directly to the observed sample X1;X2;:::;XN. We have seen\n",
      "several examples: Robbins‚Äô estimator in Table 6.1 (particularly the bottom\n",
      "line), locfdr ‚Äôs Poisson regression estimates in Figures 15.6 and 21.7, and\n",
      "Tweedie‚Äôs estimate in Figure 20.7.\n",
      "Both the advantages and the disadvantages of f-modeling can be seen in\n",
      "the inferential diagram of Figure 21.2. For f-modeling the red curve now\n",
      "can represent an exponential family ff.Àõ/g, whose concave log likelihood\n",
      "function greatly simpliÔ¨Åes the calculation of f.OÀõ/fromy=N. This comes\n",
      "at a price: the deconvolution step, from f.OÀõ/to a prior distribution g.OÀõ/,\n",
      "is problematical, as discussed below.\n",
      "This is only a problem if we want to know g. The traditional applications\n",
      "off-modeling apply to problems where the desired answer can be phrased\n",
      "directly in terms of f. This was the case for Robbins‚Äô formula (6.5), the\n",
      "local false-discovery rate (15.38), and Tweedie‚Äôs formula (20.37).\n",
      "Nevertheless, f-modeling methodology for the estimation of the prior\n",
      "g.\u0012/ does exist, an elegant example being the Fourier method described\n",
      "next. A function f.x/ and its Fourier transform \u001e.t/ are related by\n",
      "\u001e.t/DZ1\n",
      "\u00001f.x/eitxdx andf.x/D1\n",
      "2\u0019Z1\n",
      "\u00001\u001e.t/e\u0000itxdt:\n",
      "(21.59)\n",
      "For the normal case whereXiD‚ÄöiCZiwithZi\u0018N.0;1/ , the Fourier\n",
      "transform off.x/ is a multiple of that for g.\u0012/ ,\n",
      "\u001ef.t/D\u001eg.t/e\u0000t2=2; (21.60)\n",
      "so, on the transform scale, estimating gfromfamounts to removing the\n",
      "factor exp.t2=2/.\n",
      "The Fourier method begins with the empirical density Nf.x/ that puts\n",
      "probability1=N on each observed value Xi, and then proceeds in three\n",
      "steps.\n",
      "1Nf.x/ is smoothed using the ‚Äúsinc‚Äù kernel,¬é ¬é321.6 Deconvolution and f-Modeling 441\n",
      "Qf.x/D1\n",
      "N\u0015NX\n",
      "iD1sinc\u0012Xi\u0000x\n",
      "\u0015\u0013\n",
      "; sinc.x/Dsin.x/\n",
      "x:(21.61)\n",
      "2 The Fourier transform of Qf, sayQ\u001e.t/, is calculated.\n",
      "3 Finally,Og.\u0012/ is taken to be the inverse Fourier transform of Q\u001e.t/et2=2,\n",
      "this last step eliminating the unwanted factor e\u0000t2=2in (21.60).\n",
      "A pleasantly surprising aspect of the Fourier method is that Og.\u0012/ can be\n",
      "expressed directly as a kernel estimate,\n",
      "Og.\u0012/D1\n",
      "NNX\n",
      "iD1k\u0015.Xi\u0000\u0012/DZ1\n",
      "\u00001k\u0015.x\u0000\u0012/Nf.x/dx; (21.62)\n",
      "where the kernel k\u0015.\u0001/is\n",
      "k\u0015.x/D1\n",
      "\u0019Z1=\u0015\n",
      "0et2=2cos.tx/dt: (21.63)\n",
      "Large values of \u0015smoothNf.x/ more in (21.61), reducing the variance of\n",
      "Og.\u0012/ at the expense of increased bias.\n",
      "Despite its compelling rationale, there are two drawbacks to the Fourier\n",
      "method. First of all, it applies only to situations XiD‚ÄöiCZiwhereXiis\n",
      "‚Äöiplus iid noise. More seriously, the bias/variance trade-off in the choice\n",
      "of\u0015can be quite unfavorable.\n",
      "This is illustrated in Figure 21.11 for the artiÔ¨Åcial example of Figure 21.1.\n",
      "The black curve is the standard deviation of the g-modeling estimate of\n",
      "g.\u0012/ for\u0012in≈í\u00003;3¬ç, under speciÔ¨Åcations (21.41)‚Äì(21.42). The red curve\n",
      "graphs the standard deviation of the f-modeling estimate (21.62), with\n",
      "\u0015D1=3, a value that produced roughly the same amount of bias as the g-\n",
      "modeling estimate (seen in Figure 21.3). The ratio of red to black standard\n",
      "deviations averages more than 20 over the range of \u0012.\n",
      "This comparison is at least partly unfair: g-modeling is parametric while\n",
      "the Fourier method is almost nonparametric in its assumptions about f.x/\n",
      "org.\u0012/ . It can be greatly improved by beginning the three-step algorithm\n",
      "with a parametric estimate Of.x/ rather thanNf.x/ . The blue dotted curve in\n",
      "Figure 21.11 does this with Of.x/ a Poisson regression on the data X1;X2;\n",
      ":::;XN‚Äîas in Figure 10.5 but here using a natural spline basis ns(df=5)\n",
      "‚Äîgiving the estimate\n",
      "Og.\u0012/DZ1\n",
      "\u00001k\u0015.x\u0000\u0012/Of.x/dx: (21.64)442 Empirical Bayes Estimation Strategies\n",
      "‚àí3 ‚àí2 ‚àí1 0 1 2 30.00 0.01 0.02 0.03 0.04 0.05\n",
      "Œ∏sd g^(Œ∏)\n",
      "g‚àímodelparametric\n",
      "f‚àímodelnon‚àíparametric\n",
      "f‚àímodel\n",
      "Figure 21.11 Standard deviations of estimated prior density Og.\u0012/\n",
      "for the artiÔ¨Åcial example of Figure 21.1, based on ND1000\n",
      "observationsXi\u0018N.‚Äöi;1/; black curve using g-modeling\n",
      "under speciÔ¨Åcations (21.41)‚Äì(21.42); red curve nonparametric\n",
      "f-modeling (21.62), \u0015D1=3; blue curve parametric f-modeling\n",
      "(21.64), withOf.x/ estimated from Poisson regression with a\n",
      "structure matrix having Ô¨Åve degrees of freedom.\n",
      "We see a substantial decrease in standard deviation, though still not attain-\n",
      "ingg-modeling rates.\n",
      "As commented before, the great majority of empirical Bayes applica-\n",
      "tions have been of the Robbins/fdr/Tweedie variety, where f-modeling\n",
      "is the natural choice. g-modeling comes into its own for situations like\n",
      "thenodes data analysis of Figures 6.4 and 6.5, where we really want\n",
      "an estimate of the prior g.\u0012/ . Twenty-Ô¨Årst-century science is producing\n",
      "more such data sets, an impetus for the further development of g-modeling\n",
      "strategies.\n",
      "Table 21.3 concerns the g-modeling estimation of ExDEf‚ÄöjXDxg,\n",
      "ExDZ\n",
      "T\u0012g.\u0012/f\u0012.x/d\u0012\u001eZ\n",
      "Tg.\u0012/f\u0012.x/d\u0012 (21.65)\n",
      "for the artiÔ¨Åcial example, under the same speciÔ¨Åcations as in Figure 21.11.\n",
      "Samples of size ND1000 ofXi\u0018N.‚Äöi;1/were drawn from model\n",
      "(21.41)‚Äì(21.42), yielding MLE Og.\u0012/ and estimatesOExforxbetween\u0000421.6 Deconvolution and f-Modeling 443\n",
      "Table 21.3 Standard deviation of OEf‚Äöjxgcomputed from parametric\n",
      "bootstrap simulations of Og.\u0012/ . Theg-modeling is as in Figure 21.11, with\n",
      "ND1000 observations Xi\u0018N.‚Äöi;1/from the artiÔ¨Åcial example for\n",
      "each simulation. The column ‚Äúinfo‚Äù is the implied empirical Bayes\n",
      "information for estimating Ef‚Äöjxgobtained from one ‚Äúother‚Äù\n",
      "observationXi.\n",
      "x Ef‚Äöjxgsd.OE/ info\n",
      "\u00003:5\u00002:00 .10 .11\n",
      "\u00002:5\u00001:06 .10 .11\n",
      "\u00001:5\u0000:44 .05 .47\n",
      "\u0000:5\u0000:13 .03 .89\n",
      ".5 .13 .04 .80\n",
      "1.5 .44 .05 .44\n",
      "2.5 1.06 .10 .10\n",
      "3.5 2.00 .16 .04\n",
      "and 4. One thousand such estimates OExwere generated, averaging almost\n",
      "exactlyEx, with standard deviations as shown. Accuracy is reasonably\n",
      "good, the coefÔ¨Åcient of variation sd .OEx/=Exbeing about 0.05 for large\n",
      "values ofjxj. (Estimate (21.65) is a favorable case: results are worse for\n",
      "other conditional estimates¬ésuch asEf‚Äö2jXDxg.) ¬é4\n",
      "Theorem 21.4 (page 429) implies that, for large values of the sample\n",
      "sizeN, the variance ofOExdecreases as1=N , say\n",
      "varn\n",
      "OExo:Dcx=N: (21.66)\n",
      "By analogy with the Fisher information bound (5.27), we can deÔ¨Åne the\n",
      "empirical Bayes information for estimating Exin one observation to be\n",
      "ixD1.\u0010\n",
      "N\u0001varn\n",
      "OExo\u0011\n",
      "; (21.67)\n",
      "so that varfOExg:Di\u00001\n",
      "x=N.\n",
      "Empirical Bayes inference leads us directly into the world of indirect\n",
      "evidence, learning from the experience of others as in Sections 6.4 and\n",
      "7.4. So, ifXiD2:5, each ‚Äú other‚Äù observation Xjprovides 0.10 units of\n",
      "information for learning Ef‚ÄöjXiD2:5g(compared with the usual Fisher\n",
      "information value ID1for the direct estimation of ‚ÄöifromXi). This\n",
      "is a favorable case, as mentioned, and ixis often much smaller. The main\n",
      "point, perhaps, is that assuming a Bayes prior is not a casual matter, and444 Empirical Bayes Estimation Strategies\n",
      "can amount to the assumption of an enormous amount of relevant other\n",
      "information.\n",
      "21.7 Notes and Details\n",
      "Empirical Bayes and James‚ÄìStein estimation, Chapters 6 and 7, exploded\n",
      "onto the statistics scene almost simultaneously in the 1950s. They repre-\n",
      "sented a genuinely new branch of statistical inference, unlike the computer-\n",
      "based extensions of classical methodology reviewed in previous chapters.\n",
      "Their development as practical tools has been comparatively slow. The\n",
      "pace has quickened in the twenty-Ô¨Årst century, with false-discovery rates,\n",
      "Chapter 15, as a major step forward. A practical empirical Bayes method-\n",
      "ology for use beyond traditional f-modeling venues such as fdr is the goal\n",
      "of theg-modeling approach.\n",
      "¬é1[p. 428] Lemmas 21.1 and 21.2. The derivations of Lemmas 21.1 and 21.2\n",
      "are straightforward but somewhat involved exercises in differential calcu-\n",
      "lus, carried out in Remark B of Efron (2016). Here we will present just\n",
      "a sample of the calculations. From (21.18), the gradient vector Pfk.Àõ/D\n",
      ".@fk.Àõ/=@Àõl/with respect to Àõis\n",
      "Pfk.Àõ/DPg.Àõ/0Pk; (21.68)\n",
      "wherePg.Àõ/is them\u0002pderivative matrix\n",
      "Pg.Àõ/D.@gj.Àõ/=@Àõl/DDQ; (21.69)\n",
      "withDas in (21.36), the last equality following, after some work, by dif-\n",
      "ferentiation of log g.Àõ/DQÀõ\u0000\u001e.Àõ/ .\n",
      "LetlkDlogfk(now suppressing Àõfrom the notation). The gradient\n",
      "with respect to Àõoflkis then\n",
      "PlkDPfk=fkDQ0DPk=fk: (21.70)\n",
      "The vectorDPk=fkhas components\n",
      ".gjpkj\u0000gjfk/=fkDwkj (21.71)\n",
      "(21.27), using g0PkDfk. This givesPlkDQ0Wk.Àõ/(21.28). Adding up\n",
      "the independent score functions Plkover the full sample yields the overall\n",
      "scorePly.Àõ/DQ0Pn\n",
      "1ykWk.Àõ/, which is Lemma 21.1.\n",
      "¬é2[p. 428] Lemma 21.3. The penalized MLE OÀõsatisÔ¨Åes\n",
      "OD Pm.OÀõ/:D Pm.Àõ0/CRm.Àõ0/.OÀõ\u0000Àõ0/; (21.72)21.7 Notes and Details 445\n",
      "whereÀõ0is the true value of Àõ, or\n",
      "OÀõ\u0000Àõ0:D.\u0000Rm.Àõ0//\u00001Pm.Àõ0/\u0010\n",
      "\u0000Rly.Àõ0/CRs.Àõ0/\u0011\u00001\u0010\n",
      "Ply.Àõ0/\u0000Ps.Àõ0/\u0011\n",
      ":\n",
      "(21.73)\n",
      "Standard MLE theory shows that the random variable Ply.Àõ0/has mean 0\n",
      "and covariance Fisher information matrix I.Àõ0/, while\u0000Rly.Àõ0/asymptot-\n",
      "ically approximates I.Àõ0/. Substituting in (21.73),\n",
      "OÀõ\u0000Àõ0:D.I.Àõ0/CRs.Àõ0//\u00001Z; (21.74)\n",
      "whereZhas mean\u0000Ps.Àõ0/and covariance I.Àõ0/. This gives Bias .OÀõ/and\n",
      "Var.OÀõ/as in Lemma 21.3. Note that the bias is with respect to a truepara-\n",
      "metric model (21.11), and is a consequence of the penalization.\n",
      "¬é3[p. 440] The sinc kernel. The Fourier transform \u001es.t/of the scaled sinc\n",
      "functions.x/Dsin.x=\u0015/=.\u0019x/ is the indicator of the interval ≈í\u00001=\u0015;1=\u0015¬ç ,\n",
      "while that ofNf.x/ is.1=N/PN\n",
      "1exp.itXj/. Formula (21.61) is the convo-\n",
      "lutionNf\u0003s, soQfhas the product transform\n",
      "\u001eQf.t/D2\n",
      "41\n",
      "NNX\n",
      "jD1eitXj3\n",
      "5I≈í\u00001=\u0015;1=\u0015¬ç.t/: (21.75)\n",
      "The effect of the sinc convolution is to censor the high-frequency (large t)\n",
      "components ofNfor\u001eNf. Larger\u0015yields more censoring. Formula (21.63)\n",
      "has upper limits 1=\u0015 because of\u001es.t/. All of this is due to Stefanski and\n",
      "Carroll (1990). Smoothers other than the sinc kernel have been suggested\n",
      "in the literature, but without substantial improvements on deconvolution\n",
      "performance.\n",
      "¬é4[p. 443] Conditional expectation (21.65) .Efron (2014b) considers estimat-\n",
      "ingEf‚Äö2jXDxgand other such conditional expectations, both for f-\n",
      "modeling and for g-modeling.Ef‚ÄöjXDxgis by far the easiest case, as\n",
      "might be expected from the simple form of Tweedie‚Äôs estimate (20.37).Epilogue\n",
      "Something important changed in the world of statistics in the new millen-\n",
      "nium. Twentieth-century statistics, even after the heated expansion of its\n",
      "late period, could still be contained within the classic Bayesian‚Äìfrequentist‚Äì\n",
      "Fisherian inferential triangle (Figure 14.1). This is not so in the twenty-Ô¨Årst\n",
      "century. Some of the topics discussed in Part III‚Äîfalse-discovery rates,\n",
      "post-selection inference, empirical Bayes modeling, the lasso‚ÄîÔ¨Åt within\n",
      "the triangle but others seem to have escaped, heading south from the fre-\n",
      "quentist corner, perhaps in the direction of computer science.\n",
      "The escapees were the large-scale prediction algorithms of Chapters 17‚Äì\n",
      "19: neural nets, deep learning, boosting, random forests, and support-vector\n",
      "machines. Notably missing from their development were parametric prob-\n",
      "ability models, the building blocks of classical inference. Prediction algo-\n",
      "rithms are the media stars of the big-data era. It is worth asking why they\n",
      "have taken center stage and what it means for the future of the statistics\n",
      "discipline.\n",
      "Thewhy is easy enough: prediction is commercially valuable. Modern\n",
      "equipment has enabled the collection of mountainous data troves, which\n",
      "the ‚Äúdata miners‚Äù can then burrow into, extracting valuable information.\n",
      "Moreover, prediction is the simplest use of regression theory (Section 8.4).\n",
      "It can be carried out successfully without probability models, perhaps with\n",
      "the assistance of nonparametric analysis tools such as cross-validation, per-\n",
      "mutations, and the bootstrap.\n",
      "A great amount of ingenuity and experimentation has gone into the\n",
      "development of modern prediction algorithms, with statisticians playing\n",
      "an important but not dominant role.1There is no shortage of impressive\n",
      "success stories. In the absence of optimality criteria, either frequentist or\n",
      "Bayesian, the prediction community grades algorithmic excellence on per-\n",
      "1All papers mentioned in this section have their complete references in the bibliography.\n",
      "Footnotes will identify papers not fully speciÔ¨Åed in the text.\n",
      "446Epilogue 447\n",
      "formance within a catalog of often-visited examples such as the spam and\n",
      "digits data sets of Chapters 17 and 18.2Meanwhile, ‚Äútraditional statistics‚Äù\n",
      "‚Äîprobability models, optimality criteria, Bayes priors, asymptotics‚Äîhas\n",
      "continued successfully along on a parallel track. Pessimistically or opti-\n",
      "mistically, one can consider this as a bipolar disorder of the Ô¨Åeld or as a\n",
      "healthy duality that is bound to improve both branches. There are histori-\n",
      "cal and intellectual arguments favoring the optimists‚Äô side of the story.\n",
      "The Ô¨Årst thing to say is that the current situation is not entirely unprece-\n",
      "dented. By the end of the nineteenth century there was available an im-\n",
      "pressive inventory of statistical methods‚ÄîBayes‚Äô theorem, least squares,\n",
      "correlation, regression, the multivariate normal distribution‚Äîbut these ex-\n",
      "isted more as individual algorithms than as a uniÔ¨Åed discipline. Statistics\n",
      "as a distinct intellectual enterprise was not yet well-formed.\n",
      "A small but crucial step forward was taken in 1914 when the astrophysi-\n",
      "cist Arthur Eddington3claimed that mean absolute deviation was superior\n",
      "to the familiar root mean square estimate for the standard deviation from a\n",
      "normal sample. Fisher in 1919 showed that this was wrong, and moreover,\n",
      "in a clear mathematical sense, the root mean square was the best possible\n",
      "estimate . Eddington conceded the point while Fisher went on to develop\n",
      "the theory of sufÔ¨Åciency and optimal estimation.4\n",
      "‚ÄúOptimal‚Äù is the key word here. Before Fisher, statisticians didn‚Äôt really\n",
      "understand estimation. The same can be said now about prediction. Despite\n",
      "their impressive performance on a raft of test problems, it might still be\n",
      "possible to do much better than neural nets, deep learning, random forests,\n",
      "and boosting‚Äîor perhaps they are coming close to some as-yet unknown\n",
      "theoretical minimum.\n",
      "It is the job of statistical inference to connect ‚Äúdangling algorithms‚Äù to\n",
      "the central core of well-understood methodology. The connection process\n",
      "is already underway. Section 17.4 showed how Adaboost , the original\n",
      "machine learning algorithm, could be restated as a close cousin of logis-\n",
      "tic regression. Purely empirical approaches like the Common Task Frame-\n",
      "work are ultimately unsatisfying without some form of principled justi-\n",
      "Ô¨Åcation. Our optimistic scenario has the big-data/data-science prediction\n",
      "world rejoining the mainstream of statistical inference, to the beneÔ¨Åt of\n",
      "both branches.\n",
      "2This empirical approach to optimality is sometimes codiÔ¨Åed as the Common Task\n",
      "Framework (Liberman, 2015 and Donoho, 2015).\n",
      "3Eddington became world-famous for his 1919 empirical veriÔ¨Åcation of Einstein‚Äôs\n",
      "relativity theory.\n",
      "4See Stigler (2006) for the full story.448 Epilogue\n",
      " \n",
      "‚óè ‚óè‚óè\n",
      "Mathematics ComputationApplications\n",
      "‚óè19th Century\n",
      "1900\n",
      "1908\n",
      "1925\n",
      "1933\n",
      "1937\n",
      "195019621972197919952000\n",
      "20012016b\n",
      "2016a\n",
      "Development of the statistics discipline since the end of the nine-\n",
      "teenth century, as discussed in the text.\n",
      "Whether or not we can predict the future of statistics, we can at least\n",
      "examine the past to see how we‚Äôve gotten where we are. The next Ô¨Ågure\n",
      "does so in terms of a new triangle diagram, this time with the poles la-\n",
      "beled Applications ,Mathematics , and Computation . ‚ÄúMathematics‚Äù here\n",
      "is shorthand for the mathematical/logical justiÔ¨Åcation of statistical meth-\n",
      "ods. ‚ÄúComputation‚Äù stands for the empirical/numerical approach.\n",
      "Statistics is a branch of applied mathematics, and is ultimately judged\n",
      "by how well it serves the world of applications. Mathematical logic, `a\n",
      "laFisher, has been the traditional vehicle for the development and under-\n",
      "standing of statistical methods. Computation, slow and difÔ¨Åcult before the\n",
      "1950s, was only a bottleneck, but now has emerged as a competitor to (or\n",
      "perhaps a seven-league boots enabler of) mathematical analysis. At any\n",
      "one time the discipline‚Äôs energy and excitement is directed unequally to-\n",
      "ward the three poles. The Ô¨Ågure attempts, in admittedly crude fashion, to\n",
      "track the changes in direction over the past 100Cyears.Epilogue 449\n",
      "The tour begins at the end of the nineteenth century. Mathematicians of\n",
      "the caliber of Gauss and Laplace had contributed to the available method-\n",
      "ology, but the subsequent development was almost entirely applications-\n",
      "driven. Quetelet5was especially inÔ¨Çuential, applying the Gauss‚ÄìLaplace\n",
      "formulation to census data and his ‚ÄúAverage Man.‚Äù A modern reader will\n",
      "search almost in vain for any mathematical symbology in nineteenth-century\n",
      "statistics journals.\n",
      "1900\n",
      "Karl Pearson‚Äôs chi-square paper was a bold step into the new century, ap-\n",
      "plying a new mathematical tool, matrix theory, in the service of statisti-\n",
      "cal methodology. He and Weldon went on to found Biometrika in 1901,\n",
      "the Ô¨Årst recognizably modern statistics journal. Pearson‚Äôs paper, and Bio-\n",
      "metrika , launched the statistics discipline on a Ô¨Åfty-year march toward the\n",
      "mathematics pole of the triangle.\n",
      "1908\n",
      "Student‚Äôststatistic was a crucial Ô¨Årst result in small-sample ‚Äúexact‚Äù infer-\n",
      "ence, and a major inÔ¨Çuence on Fisher‚Äôs thinking.\n",
      "1925\n",
      "Fisher‚Äôs great estimation paper‚Äîa more coherent version of its 1922 pre-\n",
      "decessor. It introduced a host of fundamental ideas, including sufÔ¨Åciency,\n",
      "efÔ¨Åciency, Fisher information, maximum likelihood theory, and the notion\n",
      "of optimal estimation. Optimality is a mark of maturity in mathematics,\n",
      "making 1925 the year statistical inference went from a collection of inge-\n",
      "nious techniques to a coherent discipline.\n",
      "1933\n",
      "This represents Neyman and Pearson‚Äôs paper on optimal hypothesis test-\n",
      "ing. A logical completion of Fisher‚Äôs program, it nevertheless aroused his\n",
      "strong antipathy. This was partly personal, but also reÔ¨Çected Fisher‚Äôs con-\n",
      "cern that mathematization was squeezing intuitive correctness out of statis-\n",
      "tical thinking (Section 4.2).\n",
      "1937\n",
      "Neyman‚Äôs seminal paper on conÔ¨Ådence intervals. His sophisticated mathe-\n",
      "matical treatment of statistical inference was a harbinger of decision theory.\n",
      "5Adolphe Quetelet was a tireless organizer, helping found the Royal Statistical Society in\n",
      "1834, with the American Statistical Association following in 1839.450 Epilogue\n",
      "1950\n",
      "The publication of Wald‚Äôs Statistical Decision Functions . Decision theory\n",
      "completed the full mathematization of statistical inference. This date can\n",
      "also stand for Savage‚Äôs and de Finetti‚Äôs decision-theoretic formulation of\n",
      "Bayesian inference. We are as far as possible from the Applications corner\n",
      "of the triangle now, and it is fair to describe the 1950s as a nadir of the\n",
      "inÔ¨Çuence of the statistics discipline on scientiÔ¨Åc applications.\n",
      "1962\n",
      "The arrival of electronic computation in the mid 1950s began the process\n",
      "of stirring statistics out of its inward-gazing preoccupation with mathe-\n",
      "matical structure. Tukey‚Äôs paper ‚ÄúThe future of data analysis‚Äù argued for\n",
      "a more application- and computation-oriented discipline. Mosteller and\n",
      "Tukey later suggested changing the Ô¨Åeld‚Äôs name to data analysis , a pre-\n",
      "scient hint of today‚Äôs data science .\n",
      "1972\n",
      "Cox‚Äôs proportional hazards paper. Immensely useful in its own right, it sig-\n",
      "naled a growing interest in biostatistical applications and particularly sur-\n",
      "vival analysis, which was to assert its scientiÔ¨Åc importance in the analysis\n",
      "of AIDS epidemic data.\n",
      "1979\n",
      "The bootstrap, and later the widespread use of MCMC: electronic compu-\n",
      "tation used for the extension of classic statistical inference.\n",
      "1995\n",
      "This stands for false-discovery rates and, a year later, the lasso.6Both are\n",
      "computer-intensive algorithms, Ô¨Årmly rooted in the ethos of statistical in-\n",
      "ference. They lead, however, in different directions, as indicated by the\n",
      "split in the diagram.\n",
      "2000\n",
      "Microarray technology inspires enormous interest in large-scale inference,\n",
      "both in theory and as applied to the analysis of microbiological data.\n",
      "6Benjamini and Hochberg (1995) and Tibshirani (1996).Epilogue 451\n",
      "2001\n",
      "Random forests; it joins boosting7and the resurgence of neural nets in the\n",
      "ranks of machine learning prediction algorithms.\n",
      "2016a\n",
      "Data science: a more popular successor to Tukey and Mosteller‚Äôs ‚Äúdata\n",
      "analysis,‚Äù at one extreme it seems to represent a statistics discipline with-\n",
      "out parametric probability models or formal inference. The Data Science\n",
      "Association deÔ¨Ånes a practitioner as one who ‚Äú. . . uses scientiÔ¨Åc methods\n",
      "to liberate and create meaning from raw data.‚Äù In practice the emphasis is\n",
      "on the algorithmic processing of large data sets for the extraction of useful\n",
      "information, with the prediction algorithms as exemplars.\n",
      "2016b\n",
      "This represents the traditional line of statistical thinking, of the kind that\n",
      "could be located within Figure 14.1, but now energized with a renewed\n",
      "focus on applications. Of particular applied interest are biology and ge-\n",
      "netics. Genome-wide association studies (GWAS) show a different face of\n",
      "big data. Prediction is important here,8but not sufÔ¨Åcient for the scientiÔ¨Åc\n",
      "understanding of disease.\n",
      "A cohesive inferential theory was forged in the Ô¨Årst half of the twenti-\n",
      "eth century, but unity came at the price of an inwardly focused discipline,\n",
      "of reduced practical utility. In the century‚Äôs second half, electronic com-\n",
      "putation unleashed a vast expansion of useful‚Äîand much used‚Äîstatistical\n",
      "methodology. Expansion accelerated at the turn of the millennium, further\n",
      "increasing the reach of statistical thinking, but now at the price of intellec-\n",
      "tual cohesion.\n",
      "It is tempting but risky to speculate on the future of statistics. What\n",
      "will the Mathematics‚ÄìApplications‚ÄìComputation diagram look like, say\n",
      "25 years from now? The appetite for statistical analysis seems to be always\n",
      "increasing, both from science and from society in general. Data science\n",
      "has blossomed in response, but so has the traditional wing of the Ô¨Åeld. The\n",
      "data-analytic initiatives represented in the diagram by 2016a and 2016b are\n",
      "in actuality not isolated points but the centers of overlapping distributions.\n",
      "7Breiman (1996) for random forests, Freund and Schapire (1997) for boosting.\n",
      "8‚ÄúPersonalized medicine‚Äù in which an individual‚Äôs genome predicts his or her optimal\n",
      "treatment has attracted grail-like attention.452 Epilogue\n",
      "A hopeful scenario for the future is one of an increasing overlap that puts\n",
      "data science on a solid footing while leading to a broader general formula-\n",
      "tion of statistical inference.References\n",
      "Abu-Mostafa, Y . 1995. Hints. Neural Computation ,7, 639‚Äì671.\n",
      "Achanta, R., and Hastie, T. 2015. Telugu OCR Framework using Deep Learning . Tech.\n",
      "rept. Statistics Department, Stanford University.\n",
      "Akaike, H. 1973. Information theory and an extension of the maximum likelihood prin-\n",
      "ciple. Pages 267‚Äì281 of: Second International Symposium on Information Theory\n",
      "(Tsahkadsor, 1971) . Akad ¬¥emiai Kiad ¬¥o, Budapest.\n",
      "Anderson, T. W. 2003. An Introduction to Multivariate Statistical Analysis . Third edn.\n",
      "Wiley Series in Probability and Statistics. Wiley-Interscience.\n",
      "Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A.,\n",
      "Bouchard, N., and Bengio, Y . 2012. Theano: new features and speed improvements .\n",
      "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.\n",
      "Becker, R., Chambers, J., and Wilks, A. 1988. The New S Language: A Programming\n",
      "Environment for Data Analysis and Graphics . PaciÔ¨Åc Grove, CA: Wadsworth and\n",
      "Brooks/Cole.\n",
      "Bellhouse, D. R. 2004. The Reverend Thomas Bayes, FRS: A biography to celebrate the\n",
      "tercentenary of his birth. Statist. Sci. ,19(1), 3‚Äì43. With comments and a rejoinder\n",
      "by the author.\n",
      "Bengio, Y ., Courville, A., and Vincent, P. 2013. Representation learning: a review and\n",
      "new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence ,\n",
      "35(8), 1798‚Äì1828.\n",
      "Benjamini, Y ., and Hochberg, Y . 1995. Controlling the false discovery rate: A practical\n",
      "and powerful approach to multiple testing. J. Roy. Statist. Soc. Ser. B ,57(1), 289‚Äì\n",
      "300.\n",
      "Benjamini, Y ., and Yekutieli, D. 2005. False discovery rate-adjusted multiple conÔ¨Å-\n",
      "dence intervals for selected parameters. J. Amer. Statist. Assoc. ,100(469), 71‚Äì93.\n",
      "Berger, J. O. 2006. The case for objective Bayesian analysis. Bayesian Anal. ,1(3),\n",
      "385‚Äì402 (electronic).\n",
      "Berger, J. O., and Pericchi, L. R. 1996. The intrinsic Bayes factor for model selection\n",
      "and prediction. J. Amer. Statist. Assoc. ,91(433), 109‚Äì122.\n",
      "Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian,\n",
      "J., Warde-Farley, D., and Bengio, Y . 2010 (June). Theano: a CPU and GPU math\n",
      "expression compiler. In: Proceedings of the Python for ScientiÔ¨Åc Computing Con-\n",
      "ference (SciPy) .\n",
      "Berk, R., Brown, L., Buja, A., Zhang, K., and Zhao, L. 2013. Valid post-selection\n",
      "inference. Ann. Statist. ,41(2), 802‚Äì837.\n",
      "453454 References\n",
      "Berkson, J. 1944. Application of the logistic function to bio-assay. J. Amer. Statist.\n",
      "Assoc. ,39(227), 357‚Äì365.\n",
      "Bernardo, J. M. 1979. Reference posterior distributions for Bayesian inference. J. Roy.\n",
      "Statist. Soc. Ser. B ,41(2), 113‚Äì147. With discussion.\n",
      "Birch, M. W. 1964. The detection of partial association. I. The 2\u00022case. J. Roy. Statist.\n",
      "Soc. Ser. B ,26(2), 313‚Äì324.\n",
      "Bishop, C. 1995. Neural Networks for Pattern Recognition . Clarendon Press, Oxford.\n",
      "Boos, D. D., and SerÔ¨Çing, R. J. 1980. A note on differentials and the CLT and LIL for\n",
      "statistical functions, with application to M-estimates. Ann. Statist. ,8(3), 618‚Äì624.\n",
      "Boser, B., Guyon, I., and Vapnik, V . 1992. A training algorithm for optimal margin\n",
      "classiÔ¨Åers. In: Proceedings of COLT II .\n",
      "Breiman, L. 1996. Bagging predictors. Mach. Learn. ,24(2), 123‚Äì140.\n",
      "Breiman, L. 1998. Arcing classiÔ¨Åers (with discussion). Annals of Statistics ,26, 801‚Äì\n",
      "849.\n",
      "Breiman, L. 2001. Random forests. Machine Learning ,45, 5‚Äì32.\n",
      "Breiman, L., Friedman, J., Olshen, R. A., and Stone, C. J. 1984. ClassiÔ¨Åcation and\n",
      "Regression Trees . Wadsworth Statistics/Probability Series. Wadsworth Advanced\n",
      "Books and Software.\n",
      "Carlin, B. P., and Louis, T. A. 1996. Bayes and Empirical Bayes Methods for Data\n",
      "Analysis . Monographs on Statistics and Applied Probability, vol. 69. Chapman &\n",
      "Hall.\n",
      "Carlin, B. P., and Louis, T. A. 2000. Bayes and Empirical Bayes Methods for Data\n",
      "Analysis . 2 edn. Texts in Statistical Science. Chapman & Hall/CRC.\n",
      "Chambers, J. M., and Hastie, T. J. (eds). 1993. Statistical Models in S . Chapman &\n",
      "Hall Computer Science Series. Chapman & Hall.\n",
      "Cleveland, W. S. 1981. LOWESS: A program for smoothing scatterplots by robust\n",
      "locally weighted regression. Amer. Statist. ,35(1), 54.\n",
      "Cox, D. R. 1958. The regression analysis of binary sequences. J. Roy. Statist. Soc. Ser.\n",
      "B,20, 215‚Äì242.\n",
      "Cox, D. R. 1970. The Analysis of Binary Data . Methuen‚Äôs Monographs on Applied\n",
      "Probability and Statistics. Methuen & Co.\n",
      "Cox, D. R. 1972. Regression models and life-tables. J. Roy. Statist. Soc. Ser. B ,34(2),\n",
      "187‚Äì220.\n",
      "Cox, D. R. 1975. Partial likelihood. Biometrika ,62(2), 269‚Äì276.\n",
      "Cox, D. R., and Hinkley, D. V . 1974. Theoretical Statistics . Chapman & Hall.\n",
      "Cox, D. R., and Reid, N. 1987. Parameter orthogonality and approximate conditional\n",
      "inference. J. Roy. Statist. Soc. Ser. B ,49(1), 1‚Äì39. With a discussion.\n",
      "Crowley, J. 1974. Asymptotic normality of a new nonparametric statistic for use in\n",
      "organ transplant studies. J. Amer. Statist. Assoc. ,69(348), 1006‚Äì1011.\n",
      "de Finetti, B. 1972. Probability, Induction and Statistics. The Art of Guessing . John\n",
      "Wiley & Sons, London-New York-Sydney.\n",
      "Dembo, A., Cover, T. M., and Thomas, J. A. 1991. Information-theoretic inequalities.\n",
      "IEEE Trans. Inform. Theory ,37(6), 1501‚Äì1518.\n",
      "Dempster, A. P., Laird, N. M., and Rubin, D. B. 1977. Maximum likelihood from\n",
      "incomplete data via the EM algorithm. J. Roy. Statist. Soc. Ser. B ,39(1), 1‚Äì38.\n",
      "Diaconis, P., and Ylvisaker, D. 1979. Conjugate priors for exponential families. Ann.\n",
      "Statist. ,7(2), 269‚Äì281.References 455\n",
      "DiCiccio, T., and Efron, B. 1992. More accurate conÔ¨Ådence intervals in exponential\n",
      "families. Biometrika ,79(2), 231‚Äì245.\n",
      "Donoho, D. L. 2015. 50 years of data science. R-bloggers .www.r-bloggers.\n",
      "com/50-years-of-data-science-by-david-donoho/ .\n",
      "Edwards, A. W. F. 1992. Likelihood . Expanded edn. Johns Hopkins University Press.\n",
      "Revised reprint of the 1972 original.\n",
      "Efron, B. 1967. The two sample problem with censored data. Pages 831‚Äì853 of: Proc.\n",
      "5th Berkeley Symp. Math. Statist. and Prob., Vol. 4 . University of California Press.\n",
      "Efron, B. 1975. DeÔ¨Åning the curvature of a statistical problem (with applications to\n",
      "second order efÔ¨Åciency). Ann. Statist. ,3(6), 1189‚Äì1242. With discussion and a\n",
      "reply by the author.\n",
      "Efron, B. 1977. The efÔ¨Åciency of Cox‚Äôs likelihood function for censored data. J. Amer.\n",
      "Statist. Assoc. ,72(359), 557‚Äì565.\n",
      "Efron, B. 1979. Bootstrap methods: Another look at the jackknife. Ann. Statist. ,7(1),\n",
      "1‚Äì26.\n",
      "Efron, B. 1982. The Jackknife, the Bootstrap and Other Resampling Plans . CBMS-NSF\n",
      "Regional Conference Series in Applied Mathematics, vol. 38. Society for Industrial\n",
      "and Applied Mathematics (SIAM).\n",
      "Efron, B. 1983. Estimating the error rate of a prediction rule: Improvement on cross-\n",
      "validation. J. Amer. Statist. Assoc. ,78(382), 316‚Äì331.\n",
      "Efron, B. 1985. Bootstrap conÔ¨Ådence intervals for a class of parametric problems.\n",
      "Biometrika ,72(1), 45‚Äì58.\n",
      "Efron, B. 1986. How biased is the apparent error rate of a prediction rule? J. Amer.\n",
      "Statist. Assoc. ,81(394), 461‚Äì470.\n",
      "Efron, B. 1987. Better bootstrap conÔ¨Ådence intervals. J. Amer. Statist. Assoc. ,82(397),\n",
      "171‚Äì200. With comments and a rejoinder by the author.\n",
      "Efron, B. 1988. Logistic regression, survival analysis, and the Kaplan‚ÄìMeier curve. J.\n",
      "Amer. Statist. Assoc. ,83(402), 414‚Äì425.\n",
      "Efron, B. 1993. Bayes and likelihood calculations from conÔ¨Ådence intervals.\n",
      "Biometrika ,80(1), 3‚Äì26.\n",
      "Efron, B. 1998. R. A. Fisher in the 21st Century (invited paper presented at the 1996\n",
      "R. A. Fisher Lecture). Statist. Sci. ,13(2), 95‚Äì122. With comments and a rejoinder\n",
      "by the author.\n",
      "Efron, B. 2004. The estimation of prediction error: Covariance penalties and cross-\n",
      "validation. J. Amer. Statist. Assoc. ,99(467), 619‚Äì642. With comments and a rejoin-\n",
      "der by the author.\n",
      "Efron, B. 2010. Large-Scale Inference: Empirical Bayes Methods for Estimation, Test-\n",
      "ing, and Prediction . Institute of Mathematical Statistics Monographs, vol. 1. Cam-\n",
      "bridge University Press.\n",
      "Efron, B. 2011. Tweedie‚Äôs formula and selection bias. J. Amer. Statist. Assoc. ,106(496),\n",
      "1602‚Äì1614.\n",
      "Efron, B. 2014a. Estimation and accuracy after model selection. J. Amer. Statist. Assoc. ,\n",
      "109(507), 991‚Äì1007.\n",
      "Efron, B. 2014b. Two modeling strategies for empirical Bayes estimation. Statist. Sci. ,\n",
      "29(2), 285‚Äì301.\n",
      "Efron, B. 2015. Frequentist accuracy of Bayesian estimates. J. Roy. Statist. Soc. Ser. B ,\n",
      "77(3), 617‚Äì646.456 References\n",
      "Efron, B. 2016. Empirical Bayes deconvolution estimates. Biometrika ,103(1), 1‚Äì20.\n",
      "Efron, B., and Feldman, D. 1991. Compliance as an explanatory variable in clinical\n",
      "trials. J. Amer. Statist. Assoc. ,86(413), 9‚Äì17.\n",
      "Efron, B., and Gous, A. 2001. Scales of evidence for model selection: Fisher versus\n",
      "Jeffreys. Pages 208‚Äì256 of: Model Selection . IMS Lecture Notes Monograph Series,\n",
      "vol. 38. Beachwood, OH: Institute of Mathematics and Statististics. With discussion\n",
      "and a rejoinder by the authors.\n",
      "Efron, B., and Hinkley, D. V . 1978. Assessing the accuracy of the maximum likelihood\n",
      "estimator: Observed versus expected Fisher information. Biometrika ,65(3), 457‚Äì\n",
      "487. With comments and a reply by the authors.\n",
      "Efron, B., and Morris, C. 1972. Limiting the risk of Bayes and empirical Bayes estima-\n",
      "tors. II. The empirical Bayes case. J. Amer. Statist. Assoc. ,67, 130‚Äì139.\n",
      "Efron, B., and Morris, C. 1977. Stein‚Äôs paradox in statistics. ScientiÔ¨Åc American ,\n",
      "236(5), 119‚Äì127.\n",
      "Efron, B., and Petrosian, V . 1992. A simple test of independence for truncated data\n",
      "with applications to redshift surveys. Astrophys. J. ,399(Nov), 345‚Äì352.\n",
      "Efron, B., and Stein, C. 1981. The jackknife estimate of variance. Ann. Statist. ,9(3),\n",
      "586‚Äì596.\n",
      "Efron, B., and Thisted, R. 1976. Estimating the number of unseen species: How many\n",
      "words did Shakespeare know? Biometrika ,63(3), 435‚Äì447.\n",
      "Efron, B., and Tibshirani, R. 1993. An Introduction to the Bootstrap . Monographs on\n",
      "Statistics and Applied Probability, vol. 57. Chapman & Hall.\n",
      "Efron, B., and Tibshirani, R. 1997. Improvements on cross-validation: The .632+ boot-\n",
      "strap method. J. Amer. Statist. Assoc. ,92(438), 548‚Äì560.\n",
      "Efron, B., Hastie, T., Johnstone, I., and Tibshirani, R. 2004. Least angle regression. An-\n",
      "nals of Statistics ,32(2), 407‚Äì499. (with discussion, and a rejoinder by the authors).\n",
      "Finney, D. J. 1947. The estimation from individual records of the relationship between\n",
      "dose and quantal response. Biometrika ,34(3/4), 320‚Äì334.\n",
      "Fisher, R. A. 1915. Frequency distribution of the values of the correlation coefÔ¨Åcient in\n",
      "samples from an indeÔ¨Ånitely large population. Biometrika ,10(4), 507‚Äì521.\n",
      "Fisher, R. A. 1925. Theory of statistical estimation. Math. Proc. Cambridge Phil. Soc. ,\n",
      "22(7), 700‚Äì725.\n",
      "Fisher, R. A. 1930. Inverse probability. Math. Proc. Cambridge Phil. Soc. ,26(10),\n",
      "528‚Äì535.\n",
      "Fisher, R. A., Corbet, A., and Williams, C. 1943. The relation between the number of\n",
      "species and the number of individuals in a random sample of an animal population.\n",
      "J. Anim. Ecol. ,12, 42‚Äì58.\n",
      "Fithian, W., Sun, D., and Taylor, J. 2014. Optimal inference after model selection.\n",
      "ArXiv e-prints , Oct.\n",
      "Freund, Y ., and Schapire, R. 1996. Experiments with a new boosting algorithm. Pages\n",
      "148‚Äì156 of: Machine Learning: Proceedings of the Thirteenth International Con-\n",
      "ference . Morgan Kauffman, San Francisco.\n",
      "Freund, Y ., and Schapire, R. 1997. A decision-theoretic generalization of online learn-\n",
      "ing and an application to boosting. Journal of Computer and System Sciences ,55,\n",
      "119‚Äì139.\n",
      "Friedman, J. 2001. Greedy function approximation: a gradient boosting machine. An-\n",
      "nals of Statistics ,29(5), 1189‚Äì1232.References 457\n",
      "Friedman, J., and Popescu, B. 2005. Predictive Learning via Rule Ensembles . Tech.\n",
      "rept. Stanford University.\n",
      "Friedman, J., Hastie, T., and Tibshirani, R. 2000. Additive logistic regression: a statis-\n",
      "tical view of boosting (with discussion). Annals of Statistics ,28, 337‚Äì307.\n",
      "Friedman, J., Hastie, T., and Tibshirani, R. 2009. glmnet: Lasso and elastic-net regu-\n",
      "larized generalized linear models . R package version 1.1-4.\n",
      "Friedman, J., Hastie, T., and Tibshirani, R. 2010. Regularization paths for generalized\n",
      "linear models via coordinate descent. Journal of Statistical Software ,33(1), 1‚Äì22.\n",
      "Geisser, S. 1974. A predictive approach to the random effect model. Biometrika ,61,\n",
      "101‚Äì107.\n",
      "Gerber, M., and Chopin, N. 2015. Sequential quasi Monte Carlo. J. Roy. Statist. Soc.\n",
      "B,77(3), 509‚Äì580. with discussion, doi: 10.1111/rssb.12104.\n",
      "Gholami, S., Janson, L., Worhunsky, D. J., Tran, T. B., Squires, Malcolm, I., Jin, L. X.,\n",
      "Spolverato, G., V otanopoulos, K. I., Schmidt, C., Weber, S. M., Bloomston, M., Cho,\n",
      "C. S., Levine, E. A., Fields, R. C., Pawlik, T. M., Maithel, S. K., Efron, B., Norton,\n",
      "J. A., and Poultsides, G. A. 2015. Number of lymph nodes removed and survival after\n",
      "gastric cancer resection: An analysis from the US Gastric Cancer Collaborative. J.\n",
      "Amer. Coll. Surg. ,221(2), 291‚Äì299.\n",
      "Good, I., and Toulmin, G. 1956. The number of new species, and the increase in popu-\n",
      "lation coverage, when a sample is increased. Biometrika ,43, 45‚Äì63.\n",
      "Hall, P. 1988. Theoretical comparison of bootstrap conÔ¨Ådence intervals. Ann. Statist. ,\n",
      "16(3), 927‚Äì985. with discussion and a reply by the author.\n",
      "Hampel, F. R. 1974. The inÔ¨Çuence curve and its role in robust estimation. J. Amer.\n",
      "Statist. Assoc. ,69, 383‚Äì393.\n",
      "Hampel, F. R., Ronchetti, E. M., Rousseeuw, P. J., and Stahel, W. A. 1986. Robust\n",
      "Statistics: The approach based on inÔ¨Çuence functions . Wiley Series in Probability\n",
      "and Mathematical Statistics. John Wiley & Sons.\n",
      "Harford, T. 2014. Big data: A big mistake? SigniÔ¨Åcance ,11(5), 14‚Äì19.\n",
      "Hastie, T., and Loader, C. 1993. Local regression: automatic kernel carpentry (with\n",
      "discussion). Statistical Science ,8, 120‚Äì143.\n",
      "Hastie, T., and Tibshirani, R. 1990. Generalized Additive Models . Chapman and Hall.\n",
      "Hastie, T., and Tibshirani, R. 2004. EfÔ¨Åcient quadratic regularization for expression\n",
      "arrays. Biostatistics ,5(3), 329‚Äì340.\n",
      "Hastie, T., Tibshirani, R., and Friedman, J. 2009. The Elements of Statistical Learning.\n",
      "Data mining, Inference, and Prediction . Second edn. Springer Series in Statistics.\n",
      "Springer.\n",
      "Hastie, T., Tibshirani, R., and Wainwright, M. 2015. Statistical Learning with Sparsity:\n",
      "the Lasso and Generalizations . Chapman and Hall, CRC Press.\n",
      "Hoeffding, W. 1952. The large-sample power of tests based on permutations of obser-\n",
      "vations. Ann. Math. Statist. ,23, 169‚Äì192.\n",
      "Hoeffding, W. 1965. Asymptotically optimal tests for multinomial distributions. Ann.\n",
      "Math. Statist. ,36(2), 369‚Äì408.\n",
      "Hoerl, A. E., and Kennard, R. W. 1970. Ridge regression: Biased estimation for nonor-\n",
      "thogonal problems. Technometrics ,12(1), 55‚Äì67.\n",
      "Huber, P. J. 1964. Robust estimation of a location parameter. Ann. Math. Statist. ,35,\n",
      "73‚Äì101.458 References\n",
      "Jaeckel, L. A. 1972. Estimating regression coefÔ¨Åcients by minimizing the dispersion of\n",
      "the residuals. Ann. Math. Statist. ,43, 1449‚Äì1458.\n",
      "James, W., and Stein, C. 1961. Estimation with quadratic loss. Pages 361‚Äì379 of:\n",
      "Proc. 4th Berkeley Symposium on Mathematical Statistics and Probability , vol. I.\n",
      "University of California Press.\n",
      "Jansen, L., Fithian, W., and Hastie, T. 2015. Effective degrees of freedom: a Ô¨Çawed\n",
      "metaphor. Biometrika ,102(2), 479‚Äì485.\n",
      "Javanmard, A., and Montanari, A. 2014. ConÔ¨Ådence intervals and hypothesis testing\n",
      "for high-dimensional regression. J. of Machine Learning Res. ,15, 2869‚Äì2909.\n",
      "Jaynes, E. 1968. Prior probabilities. IEEE Trans. Syst. Sci. Cybernet. ,4(3), 227‚Äì241.\n",
      "Jeffreys, H. 1961. Theory of Probability . Third ed. Clarendon Press.\n",
      "Johnson, N. L., and Kotz, S. 1969. Distributions in Statistics: Discrete Distributions .\n",
      "Houghton MifÔ¨Çin Co.\n",
      "Johnson, N. L., and Kotz, S. 1970a. Distributions in Statistics. Continuous Univariate\n",
      "Distributions. 1 . Houghton MifÔ¨Çin Co.\n",
      "Johnson, N. L., and Kotz, S. 1970b. Distributions in Statistics. Continuous Univariate\n",
      "Distributions. 2 . Houghton MifÔ¨Çin Co.\n",
      "Johnson, N. L., and Kotz, S. 1972. Distributions in Statistics: Continuous Multivariate\n",
      "Distributions . John Wiley & Sons.\n",
      "Kaplan, E. L., and Meier, P. 1958. Nonparametric estimation from incomplete obser-\n",
      "vations. J. Amer. Statist. Assoc. ,53(282), 457‚Äì481.\n",
      "Kass, R. E., and Raftery, A. E. 1995. Bayes factors. J. Amer. Statist. Assoc. ,90(430),\n",
      "773‚Äì795.\n",
      "Kass, R. E., and Wasserman, L. 1996. The selection of prior distributions by formal\n",
      "rules. J. Amer. Statist. Assoc. ,91(435), 1343‚Äì1370.\n",
      "Kuffner, R., Zach, N., Norel, R., Hawe, J., Schoenfeld, D., Wang, L., Li, G., Fang,\n",
      "L., Mackey, L., Hardiman, O., Cudkowicz, M., Sherman, A., Ertaylan, G., Grosse-\n",
      "Wentrup, M., Hothorn, T., van Ligtenberg, J., Macke, J. H., Meyer, T., Scholkopf,\n",
      "B., Tran, L., Vaughan, R., Stolovitzky, G., and Leitner, M. L. 2015. Crowdsourced\n",
      "analysis of clinical trial data to predict amyotrophic lateral sclerosis progression. Nat\n",
      "Biotech ,33(1), 51‚Äì57.\n",
      "LeCun, Y ., and Cortes, C. 2010. MNIST Handwritten Digit Database .\n",
      "http://yann.lecun.com/exdb/mnist/.\n",
      "LeCun, Y ., Bengio, Y ., and Hinton, G. 2015. Deep learning. Nature ,521(7553), 436‚Äì\n",
      "444.\n",
      "Lee, J., Sun, D., Sun, Y ., and Taylor, J. 2016. Exact post-selection inference, with\n",
      "application to the Lasso. Annals of Statistics ,44(3), 907‚Äì927.\n",
      "Lehmann, E. L. 1983. Theory of Point Estimation . Wiley Series in Probability and\n",
      "Mathematical Statistics. John Wiley & Sons.\n",
      "Leslie, C., Eskin, E., Cohen, A., Weston, J., and Noble, W. S. 2003. Mismatch string\n",
      "kernels for discriminative pretein classiÔ¨Åcation. Bioinformatics ,1, 1‚Äì10.\n",
      "Liaw, A., and Wiener, M. 2002. ClassiÔ¨Åcation and regression by randomForest. R\n",
      "News ,2(3), 18‚Äì22.\n",
      "Liberman, M. 2015 (April). ‚ÄúReproducible Research and the Common Task Method‚Äù .\n",
      "Simons Foundation Frontiers of Data Science Lecture, April 1, 2015; video avail-\n",
      "able.References 459\n",
      "Lockhart, R., Taylor, J., Tibshirani, R., and Tibshirani, R. 2014. A signiÔ¨Åcance test for\n",
      "the lasso. Annals of Statistics ,42(2), 413‚Äì468. With discussion and a rejoinder by\n",
      "the authors.\n",
      "Lynden-Bell, D. 1971. A method for allowing for known observational selection in\n",
      "small samples applied to 3CR quasars. Mon. Not. Roy. Astron. Soc. ,155(1), 95‚Äì18.\n",
      "Mallows, C. L. 1973. Some comments on Cp.Technometrics ,15(4), 661‚Äì675.\n",
      "Mantel, N., and Haenszel, W. 1959. Statistical aspects of the analysis of data from\n",
      "retrospective studies of disease. J. Natl. Cancer Inst. ,22(4), 719‚Äì748.\n",
      "Mardia, K. V ., Kent, J. T., and Bibby, J. M. 1979. Multivariate Analysis . Academic\n",
      "Press.\n",
      "McCullagh, P., and Nelder, J. 1983. Generalized Linear Models . Monographs on Statis-\n",
      "tics and Applied Probability. Chapman & Hall.\n",
      "McCullagh, P., and Nelder, J. 1989. Generalized Linear Models . Second edn. Mono-\n",
      "graphs on Statistics and Applied Probability. Chapman & Hall.\n",
      "Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E.\n",
      "1953. Equation of state calculations by fast computing machines. J. Chem. Phys. ,\n",
      "21(6), 1087‚Äì1092.\n",
      "Miller, Jr, R. G. 1964. A trustworthy jackknife. Ann. Math. Statist ,35, 1594‚Äì1605.\n",
      "Miller, Jr, R. G. 1981. Simultaneous Statistical Inference . Second edn. Springer Series\n",
      "in Statistics. New York: Springer-Verlag.\n",
      "Nesterov, Y . 2013. Gradient methods for minimizing composite functions. Mathemati-\n",
      "cal Programming ,140(1), 125‚Äì161.\n",
      "Neyman, J. 1937. Outline of a theory of statistical estimation based on the classical\n",
      "theory of probability. Phil. Trans. Roy. Soc. ,236(767), 333‚Äì380.\n",
      "Neyman, J. 1977. Frequentist probability and frequentist statistics. Synthese ,36(1),\n",
      "97‚Äì131.\n",
      "Neyman, J., and Pearson, E. S. 1933. On the problem of the most efÔ¨Åcient tests of\n",
      "statistical hypotheses. Phil. Trans. Roy. Soc. A ,231(694-706), 289‚Äì337.\n",
      "Ng, A. 2015. Neural Networks .http://deeplearning.stanford.edu/\n",
      "wiki/index.php/Neural_Networks . Lecture notes.\n",
      "Ngiam, J., Chen, Z., Chia, D., Koh, P. W., Le, Q. V ., and Ng, A. 2010. Tiled convo-\n",
      "lutional neural networks. Pages 1279‚Äì1287 of: Lafferty, J., Williams, C., Shawe-\n",
      "Taylor, J., Zemel, R., and Culotta, A. (eds), Advances in Neural Information Pro-\n",
      "cessing Systems 23 . Curran Associates, Inc.\n",
      "O‚ÄôHagan, A. 1995. Fractional Bayes factors for model comparison. J. Roy. Statist. Soc.\n",
      "Ser. B ,57(1), 99‚Äì138. With discussion and a reply by the author.\n",
      "Park, T., and Casella, G. 2008. The Bayesian lasso. J. Amer. Statist. Assoc. ,103(482),\n",
      "681‚Äì686.\n",
      "Pearson, K. 1900. On the criterion that a given system of deviations from the probable in\n",
      "the case of a correlated system of variables is such that it can be reasonably supposed\n",
      "to have arisen from random sampling. Phil. Mag. ,50(302), 157‚Äì175.\n",
      "Pritchard, J., Stephens, M., and Donnelly, P. 2000. Inference of Population Structure\n",
      "using Multilocus Genotype Data. Genetics ,155(June), 945‚Äì959.\n",
      "Quenouille, M. H. 1956. Notes on bias in estimation. Biometrika ,43, 353‚Äì360.\n",
      "R Core Team. 2015. R: A Language and Environment for Statistical Computing . R\n",
      "Foundation for Statistical Computing, Vienna, Austria.460 References\n",
      "Ridgeway, G. 2005. Generalized boosted models: A guide to the gbm package . Avail-\n",
      "able online.\n",
      "Ridgeway, G., and MacDonald, J. M. 2009. Doubly robust internal benchmarking and\n",
      "false discovery rates for detecting racial bias in police stops. J. Amer. Statist. Assoc. ,\n",
      "104(486), 661‚Äì668.\n",
      "Ripley, B. D. 1996. Pattern Recognition and Neural Networks . Cambridge University\n",
      "Press.\n",
      "Robbins, H. 1956. An empirical Bayes approach to statistics. Pages 157‚Äì163 of: Proc.\n",
      "3rd Berkeley Symposium on Mathematical Statistics and Probability , vol. I. Univer-\n",
      "sity of California Press.\n",
      "Rosset, S., Zhu, J., and Hastie, T. 2004. Margin maximizing loss functions. In: Thrun,\n",
      "S., Saul, L., and Sch ¬®olkopf, B. (eds), Advances in Neural Information Processing\n",
      "Systems 16 . MIT Press.\n",
      "Rubin, D. B. 1981. The Bayesian bootstrap. Ann. Statist. ,9(1), 130‚Äì134.\n",
      "Savage, L. J. 1954. The Foundations of Statistics . John Wiley & Sons; Chapman &\n",
      "Hill.\n",
      "Schapire, R. 1990. The strength of weak learnability. Machine Learning ,5(2), 197‚Äì\n",
      "227.\n",
      "Schapire, R., and Freund, Y . 2012. Boosting: Foundations and Algorithms . MIT Press.\n",
      "Scheff ¬¥e, H. 1953. A method for judging all contrasts in the analysis of variance.\n",
      "Biometrika ,40(1-2), 87‚Äì110.\n",
      "Sch¬®olkopf, B., and Smola, A. 2001. Learning with Kernels: Support Vector Ma-\n",
      "chines, Regularization, Optimization, and Beyond (Adaptive Computation and Ma-\n",
      "chine Learning) . MIT Press.\n",
      "Schwarz, G. 1978. Estimating the dimension of a model. Ann. Statist. ,6(2), 461‚Äì464.\n",
      "Senn, S. 2008. A note concerning a selection ‚Äúparadox‚Äù of Dawid‚Äôs. Amer. Statist. ,\n",
      "62(3), 206‚Äì210.\n",
      "Soric, B. 1989. Statistical ‚Äúdiscoveries‚Äù and effect-size estimation. J. Amer. Statist.\n",
      "Assoc. ,84(406), 608‚Äì610.\n",
      "Spevack, M. 1968. A Complete and Systematic Concordance to the Works of Shake-\n",
      "speare . V ol. 1‚Äì6. Georg Olms Verlag.\n",
      "Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. 2014.\n",
      "Dropout: a simple way to prevent neural networks from overÔ¨Åtting. J. of Machine\n",
      "Learning Res. ,15, 1929‚Äì1958.\n",
      "Stefanski, L., and Carroll, R. J. 1990. Deconvoluting kernel density estimators. Statis-\n",
      "tics,21(2), 169‚Äì184.\n",
      "Stein, C. 1956. Inadmissibility of the usual estimator for the mean of a multivariate nor-\n",
      "mal distribution. Pages 197‚Äì206 of: Proc. 3rd Berkeley Symposium on Mathematical\n",
      "Statististics and Probability , vol. I. University of California Press.\n",
      "Stein, C. 1981. Estimation of the mean of a multivariate normal distribution. Ann.\n",
      "Statist. ,9(6), 1135‚Äì1151.\n",
      "Stein, C. 1985. On the coverage probability of conÔ¨Ådence sets based on a prior distribu-\n",
      "tion. Pages 485‚Äì514 of: Sequential Methods in Statistics . Banach Center Publication,\n",
      "vol. 16. PWN, Warsaw.\n",
      "Stigler, S. M. 2006. How Ronald Fisher became a mathematical statistician. Math. Sci.\n",
      "Hum. Math. Soc. Sci. ,176(176), 23‚Äì30.References 461\n",
      "Stone, M. 1974. Cross-validatory choice and assessment of statistical predictions. J.\n",
      "Roy. Statist. Soc. B ,36, 111‚Äì147. With discussion and a reply by the author.\n",
      "Storey, J. D., Taylor, J., and Siegmund, D. 2004. Strong control, conservative point\n",
      "estimation and simultaneous conservative consistency of false discovery rates: A\n",
      "uniÔ¨Åed approach. J. Roy. Statist. Soc. B ,66(1), 187‚Äì205.\n",
      "Tanner, M. A., and Wong, W. H. 1987. The calculation of posterior distributions by\n",
      "data augmentation. J. Amer. Statist. Assoc. ,82(398), 528‚Äì550. With discussion and\n",
      "a reply by the authors.\n",
      "Taylor, J., Loftus, J., and Tibshirani, R. 2015. Tests in adaptive regression via the Kac-\n",
      "Rice formula. Annals of Statistics ,44(2), 743‚Äì770.\n",
      "Thisted, R., and Efron, B. 1987. Did Shakespeare write a newly-discovered poem?\n",
      "Biometrika ,74(3), 445‚Äì455.\n",
      "Tibshirani, R. 1989. Noninformative priors for one parameter of many. Biometrika ,\n",
      "76(3), 604‚Äì608.\n",
      "Tibshirani, R. 1996. Regression shrinkage and selection via the lasso. J. Roy. Statist.\n",
      "Soc. B ,58(1), 267‚Äì288.\n",
      "Tibshirani, R. 2006. A simple method for assessing sample sizes in microarray experi-\n",
      "ments. BMC Bioinformatics ,7(Mar), 106.\n",
      "Tibshirani, R., Bien, J., Friedman, J., Hastie, T., Simon, N., Taylor, J., and Tibshirani, R.\n",
      "2012. Strong rules for discarding predictors in lasso-type problems. J. Roy. Statist.\n",
      "Soc. B ,74.\n",
      "Tibshirani, R., Tibshirani, R., Taylor, J., Loftus, J., and Reid, S. 2016. selectiveInfer-\n",
      "ence: Tools for Post-Selection Inference . R package version 1.1.3.\n",
      "Tukey, J. W. 1958. ‚ÄúBias and conÔ¨Ådence in not-quite large samples‚Äù in Abstracts of\n",
      "Papers. Ann. Math. Statist. ,29(2), 614.\n",
      "Tukey, J. W. 1960. A survey of sampling from contaminated distributions. Pages\n",
      "448‚Äì485 of: Contributions to Probability and Statistics: Essays in Honor of Harold\n",
      "Hotelling (I. Olkin, et. al, ed.). Stanford University Press.\n",
      "Tukey, J. W. 1962. The future of data analysis. Ann. Math. Statist. ,33, 1‚Äì67.\n",
      "Tukey, J. W. 1977. Exploratory Data Analysis . Behavioral Science Series. Addison-\n",
      "Wesley.\n",
      "van de Geer, S., B ¬®uhlmann, P., Ritov, Y ., and Dezeure, R. 2014. On asymptotically op-\n",
      "timal conÔ¨Ådence regions and tests for high-dimensional models. Annals of Statistics ,\n",
      "42(3), 1166‚Äì1202.\n",
      "Vapnik, V . 1996. The Nature of Statistical Learning Theory . Springer.\n",
      "Wager, S., Wang, S., and Liang, P. S. 2013. Dropout training as adaptive regularization.\n",
      "Pages 351‚Äì359 of: Burges, C., Bottou, L., Welling, M., Ghahramani, Z., and Wein-\n",
      "berger, K. (eds), Advances in Neural Information Processing Systems 26 . Curran\n",
      "Associates, Inc.\n",
      "Wager, S., Hastie, T., and Efron, B. 2014. ConÔ¨Ådence intervals for random forests: the\n",
      "jacknife and the inÔ¨Åntesimal jacknife. J. of Machine Learning Res. ,15, 1625‚Äì1651.\n",
      "Wahba, G. 1990. Spline Models for Observational Data . SIAM.\n",
      "Wahba, G., Lin, Y ., and Zhang, H. 2000. GACV for support vector machines. Pages\n",
      "297‚Äì311 of: Smola, A., Bartlett, P., Sch ¬®olkopf, B., and Schuurmans, D. (eds), Ad-\n",
      "vances in Large Margin ClassiÔ¨Åers . MIT Press.\n",
      "Wald, A. 1950. Statistical Decision Functions . John Wiley & Sons; Chapman & Hall.462 References\n",
      "Wedderburn, R. W. M. 1974. Quasi-likelihood functions, generalized linear models,\n",
      "and the Gauss‚ÄìNewton method. Biometrika ,61(3), 439‚Äì447.\n",
      "Welch, B. L., and Peers, H. W. 1963. On formulae for conÔ¨Ådence points based on\n",
      "integrals of weighted likelihoods. J. Roy. Statist. Soc. B ,25, 318‚Äì329.\n",
      "Westfall, P., and Young, S. 1993. Resampling-based Multiple Testing: Examples and\n",
      "Methods forp-Value Adjustment . Wiley Series in Probability and Statistics. Wiley-\n",
      "Interscience.\n",
      "Xie, M., and Singh, K. 2013. ConÔ¨Ådence distribution, the frequentist distribution esti-\n",
      "mator of a parameter: A review. Int. Statist. Rev. ,81(1), 3‚Äì39. with discussion.\n",
      "Ye, J. 1998. On measuring and correcting the effects of data mining and model selec-\n",
      "tion. J. Amer. Statist. Assoc. ,93(441), 120‚Äì131.\n",
      "Zhang, C.-H., and Zhang, S. 2014. ConÔ¨Ådence intervals for low-dimensional parame-\n",
      "ters with high-dimensional data. J. Roy. Statist. Soc. B ,76(1), 217‚Äì242.\n",
      "Zou, H., Hastie, T., and Tibshirani, R. 2007. On the ‚Äúdegrees of freedom‚Äù of the lasso.\n",
      "Ann. Statist. ,35(5), 2173‚Äì2192.Author Index\n",
      "Abu-Mostafa, Y . 372, 453\n",
      "Achanta, R. 372, 453\n",
      "Akaike, H. 231, 453\n",
      "Anderson, T. W. 69, 453\n",
      "Bastien, F. 374, 453\n",
      "Becker, R. 128, 453\n",
      "Bellhouse, D. R. 36, 453\n",
      "Bengio, Y . 372, 374, 453, 458\n",
      "Benjamini, Y . 294, 418, 450, 453\n",
      "Berger, J. O. 36, 261, 453\n",
      "Bergeron, A. 374, 453\n",
      "Bergstra, J. 374, 453\n",
      "Berk, R. 323, 419, 453\n",
      "Berkson, J. 128, 454\n",
      "Bernardo, J. M. 261, 454\n",
      "Bibby, J. M. 37, 69, 459\n",
      "Bien, J. 322, 461\n",
      "Birch, M. W. 128, 454\n",
      "Bishop, C. 371, 454\n",
      "Bloomston, M. 89, 457\n",
      "Boos, D. D. 179, 454\n",
      "Boser, B. 390, 454\n",
      "Bouchard, N. 374, 453\n",
      "Breiman, L. 128, 348, 451, 454\n",
      "Breuleux, O. 374, 453\n",
      "Brown, L. 323, 419, 453\n",
      "B¬®uhlmann, P. 323, 461\n",
      "Buja, A. 323, 419, 453\n",
      "Carlin, B. P. 89, 261, 454\n",
      "Carroll, R. J. 445, 460\n",
      "Casella, G. 420, 459\n",
      "Chambers, J. 128, 453\n",
      "Chen, Z. 372, 459\n",
      "Chia, D. 372, 459\n",
      "Cho, C. S. 89, 457\n",
      "Chopin, N. 261, 457\n",
      "Cleveland, W. S. 11, 454\n",
      "Cohen, A. 393, 458Corbet, A. 456\n",
      "Cortes, C. 372, 458\n",
      "Courville, A. 372, 453\n",
      "Cover, T. M. 52, 454\n",
      "Cox, D. R. 52, 128, 152, 153, 262, 454\n",
      "Crowley, J. 153, 454\n",
      "Cudkowicz, M. 349, 458\n",
      "de Finetti, B. 261, 454\n",
      "Dembo, A. 52, 454\n",
      "Dempster, A. P. 152, 454\n",
      "Desjardins, G. 374, 453\n",
      "Dezeure, R. 323, 461\n",
      "Diaconis, P. 262, 454\n",
      "DiCiccio, T. 204, 455\n",
      "Donnelly, P. 261, 459\n",
      "Donoho, D. L. 447, 455\n",
      "Edwards, A. W. F. 37, 455\n",
      "Efron, B. 11, 20, 37, 51, 52, 69, 89, 90,\n",
      "105, 106, 130, 152, 154, 177‚Äì179,\n",
      "204, 206, 207, 231, 232, 262, 267,\n",
      "294‚Äì297, 321, 323, 348, 417, 419,\n",
      "420, 444, 445, 455‚Äì457, 461\n",
      "Ertaylan, G. 349, 458\n",
      "Eskin, E. 393, 458\n",
      "Fang, L. 349, 458\n",
      "Feldman, D. 417, 456\n",
      "Fields, R. C. 89, 457\n",
      "Finney, D. J. 262, 456\n",
      "Fisher, R. A. 184, 204, 449, 456\n",
      "Fithian, W. 323, 456, 458\n",
      "Freund, Y . 348, 451, 456, 460\n",
      "Friedman, J. 128, 231, 321, 322, 348,\n",
      "349, 371, 454, 456, 457, 461\n",
      "Geisser, S. 231, 457\n",
      "Gerber, M. 261, 457\n",
      "Gholami, S. 89, 457\n",
      "Good, I. 88, 457\n",
      "463464 Author Index\n",
      "Goodfellow, I. J. 374, 453\n",
      "Gous, A. 262, 456\n",
      "Grosse-Wentrup, M. 349, 458\n",
      "Guyon, I. 390, 454\n",
      "Haenszel, W. 152, 459\n",
      "Hall, P. 204, 457\n",
      "Hampel, F. R. 179, 457\n",
      "Hardiman, O. 349, 458\n",
      "Harford, T. 232, 457\n",
      "Hastie, T. 128, 231, 321‚Äì323, 348, 349,\n",
      "371, 372, 392, 393, 453, 456‚Äì458,\n",
      "460‚Äì462\n",
      "Hawe, J. 349, 458\n",
      "Hinkley, D. V . 52, 69, 454, 456\n",
      "Hinton, G. 372, 458, 460\n",
      "Hochberg, Y . 294, 418, 450, 453\n",
      "Hoeffding, W. 129, 296, 457\n",
      "Hoerl, A. E. 105, 457\n",
      "Hothorn, T. 349, 458\n",
      "Huber, P. J. 179, 457\n",
      "Jaeckel, L. A. 178, 458\n",
      "James, W. 104, 458\n",
      "Jansen, L. 323, 458\n",
      "Janson, L. 89, 457\n",
      "Javanmard, A. 323, 458\n",
      "Jaynes, E. 261, 458\n",
      "Jeffreys, H. 261, 458\n",
      "Jin, L. X. 89, 457\n",
      "Johnson, N. L. 36, 458\n",
      "Johnstone, I. 231, 321, 323, 456\n",
      "Kaplan, E. L. 152, 458\n",
      "Kass, R. E. 261, 262, 458\n",
      "Kennard, R. W. 105, 457\n",
      "Kent, J. T. 37, 69, 459\n",
      "Koh, P. W. 372, 459\n",
      "Kotz, S. 36, 458\n",
      "Krizhevsky, A. 372, 460\n",
      "Kuffner, R. 349, 458\n",
      "Laird, N. M. 152, 454\n",
      "Lamblin, P. 374, 453\n",
      "Le, Q. V . 372, 459\n",
      "LeCun, Y . 372, 458\n",
      "Lee, J. 323, 458\n",
      "Lehmann, E. L. 52, 458\n",
      "Leitner, M. L. 349, 458\n",
      "Leslie, C. 393, 458\n",
      "Levine, E. A. 89, 457\n",
      "Li, G. 349, 458\n",
      "Liang, P. S. 372, 373, 461\n",
      "Liaw, A. 348, 458Liberman, M. 447, 458\n",
      "Lin, Y . 391, 461\n",
      "Loader, C. 393, 457\n",
      "Lockhart, R. 323, 459\n",
      "Loftus, J. 323, 461\n",
      "Louis, T. A. 89, 261, 454\n",
      "Lynden-Bell, D. 150, 459\n",
      "MacDonald, J. M. 294, 460\n",
      "Macke, J. H. 349, 458\n",
      "Mackey, L. 349, 458\n",
      "Maithel, S. K. 89, 457\n",
      "Mallows, C. L. 231, 459\n",
      "Mantel, N. 152, 459\n",
      "Mardia, K. V . 37, 69, 459\n",
      "McCullagh, P. 128, 322, 459\n",
      "Meier, P. 152, 458\n",
      "Metropolis, N. 261, 459\n",
      "Meyer, T. 349, 458\n",
      "Miller, R. G., Jr 177, 294, 418, 459\n",
      "Montanari, A. 323, 458\n",
      "Morris, C. 105, 456\n",
      "Nelder, J. 128, 322, 459\n",
      "Nesterov, Y . 372, 459\n",
      "Neyman, J. 20, 204, 449, 459\n",
      "Ng, A. 372, 459\n",
      "Ngiam, J. 372, 459\n",
      "Noble, W. S. 393, 458\n",
      "Norel, R. 349, 458\n",
      "Norton, J. A. 89, 457\n",
      "O‚ÄôHagan, A. 261, 459\n",
      "Olshen, R. A. 128, 348, 454\n",
      "Park, T. 420, 459\n",
      "Pascanu, R. 374, 453\n",
      "Pawlik, T. M. 89, 457\n",
      "Pearson, E. S. 449, 459\n",
      "Pearson, K. 449, 459\n",
      "Peers, H. W. 37, 207, 261, 462\n",
      "Pericchi, L. R. 261, 453\n",
      "Petrosian, V . 130, 456\n",
      "Popescu, B. 348, 457\n",
      "Poultsides, G. A. 89, 457\n",
      "Pritchard, J. 261, 459\n",
      "Quenouille, M. H. 177, 459\n",
      "R Core Team 128, 459\n",
      "Raftery, A. E. 262, 458\n",
      "Reid, N. 262, 454\n",
      "Reid, S. 323, 461\n",
      "Ridgeway, G. 294, 348, 460\n",
      "Ripley, B. D. 371, 460\n",
      "Ritov, Y . 323, 461Author Index 465\n",
      "Robbins, H. 88, 104, 419, 460\n",
      "Ronchetti, E. M. 179, 457\n",
      "Rosenbluth, A. W. 261, 459\n",
      "Rosenbluth, M. N. 261, 459\n",
      "Rosset, S. 392, 460\n",
      "Rousseeuw, P. J. 179, 457\n",
      "Rubin, D. B. 152, 179, 454, 460\n",
      "Salakhutdinov, R. 372, 460\n",
      "Savage, L. J. 261, 460\n",
      "Schapire, R. 348, 451, 456, 460\n",
      "Scheff ¬¥e, H. 417, 460\n",
      "Schmidt, C. 89, 457\n",
      "Schoenfeld, D. 349, 458\n",
      "Sch¬®olkopf, B. 390, 460\n",
      "Schwarz, G. 262, 460\n",
      "Senn, S. 37, 460\n",
      "SerÔ¨Çing, R. J. 179, 454\n",
      "Sherman, A. 349, 458\n",
      "Siegmund, D. 294, 461\n",
      "Simon, N. 322, 461\n",
      "Singh, K. 51, 207, 462\n",
      "Smola, A. 390, 460\n",
      "Soric, B. 294, 460\n",
      "Spevack, M. 89, 460\n",
      "Spolverato, G. 89, 457\n",
      "Squires, I., Malcolm 89, 457\n",
      "Srivastava, N. 372, 460\n",
      "Stahel, W. A. 179, 457\n",
      "Stefanski, L. 445, 460\n",
      "Stein, C. 104, 106, 178, 232, 261, 456,\n",
      "458, 460\n",
      "Stephens, M. 261, 459\n",
      "Stigler, S. M. 447, 460\n",
      "Stolovitzky, G. 349, 458\n",
      "Stone, C. J. 128, 348, 454\n",
      "Stone, M. 231, 461\n",
      "Storey, J. D. 294, 461\n",
      "Sun, D. 323, 456, 458\n",
      "Sun, Y . 323, 458\n",
      "Sutskever, I. 372, 460\n",
      "Tanner, M. A. 263, 461\n",
      "Taylor, J. 294, 322, 323, 456, 458, 459,\n",
      "461\n",
      "Teller, A. H. 261, 459\n",
      "Teller, E. 261, 459\n",
      "Thisted, R. 89, 456, 461\n",
      "Thomas, J. A. 52, 454Tibshirani, R. 128, 179, 207, 231, 232,\n",
      "261, 321‚Äì323, 348, 349, 371, 392,\n",
      "420, 450, 456, 457, 459, 461, 462\n",
      "Toulmin, G. 88, 457\n",
      "Tran, L. 349, 458\n",
      "Tran, T. B. 89, 457\n",
      "Tukey, J. W. 11, 177, 179, 450, 461\n",
      "Turian, J. 374, 453\n",
      "van de Geer, S. 323, 461\n",
      "van Ligtenberg, J. 349, 458\n",
      "Vapnik, V . 390, 454, 461\n",
      "Vaughan, R. 349, 458\n",
      "Vincent, P. 372, 453\n",
      "V otanopoulos, K. I. 89, 457\n",
      "Wager, S. 348, 372, 373, 461\n",
      "Wahba, G. 391, 392, 461\n",
      "Wainwright, M. 321‚Äì323, 457\n",
      "Wald, A. 450, 461\n",
      "Wang, L. 349, 458\n",
      "Wang, S. 372, 373, 461\n",
      "Warde-Farley, D. 374, 453\n",
      "Wasserman, L. 261, 262, 458\n",
      "Weber, S. M. 89, 457\n",
      "Wedderburn, R. W. M. 128, 462\n",
      "Welch, B. L. 37, 207, 261, 462\n",
      "Westfall, P. 294, 418, 462\n",
      "Weston, J. 393, 458\n",
      "Wiener, M. 348, 458\n",
      "Wilks, A. 128, 453\n",
      "Williams, C. 456\n",
      "Wong, W. H. 263, 461\n",
      "Worhunsky, D. J. 89, 457\n",
      "Xie, M. 51, 207, 462\n",
      "Ye, J. 231, 462\n",
      "Yekutieli, D. 418, 453\n",
      "Ylvisaker, D. 262, 454\n",
      "Young, S. 294, 418, 462\n",
      "Zach, N. 349, 458\n",
      "Zhang, C.-H. 323, 462\n",
      "Zhang, H. 391, 461\n",
      "Zhang, K. 323, 419, 453\n",
      "Zhang, S. 323, 462\n",
      "Zhao, L. 323, 419, 453\n",
      "Zhu, J. 392, 460\n",
      "Zou, H. 231, 322, 462Subject Index\n",
      "abc method, 194, 204\n",
      "Accelerated gradient descent, 359\n",
      "Acceleration, 192, 206\n",
      "Accuracy, 14\n",
      "after model selection, 402‚Äì408\n",
      "Accurate but not correct, 402\n",
      "Activation function, 355, 361\n",
      "leaky rectiÔ¨Åed linear, 362\n",
      "rectiÔ¨Åed linear, 362\n",
      "ReLU, 362\n",
      "tanh, 362\n",
      "Active set, 301, 308\n",
      "adaboost algorithm, 341‚Äì345, 447\n",
      "Adaboost.M1, 342\n",
      "Adaptation, 404\n",
      "Adaptive estimator, 404\n",
      "Adaptive rate control, 359\n",
      "Additive model, 324\n",
      "adaptive, 346\n",
      "Adjusted compliance, 404\n",
      "Admixture modeling, 256‚Äì260\n",
      "AIC, seeAkaike information criterion\n",
      "Akaike information criterion, 208, 218,\n",
      "226, 231, 246, 267\n",
      "Allele frequency, 257\n",
      "American Statistical Association, 449\n",
      "Ancillary, 44, 46, 139\n",
      "Apparent error, 211, 213, 219\n",
      "arcsin transformation, 95\n",
      "Arthur Eddington, 447\n",
      "Asymptotics, xvi, 119, 120\n",
      "Autoencoder, 362‚Äì364\n",
      "BackÔ¨Åtting, 346\n",
      "Backpropagation, 356‚Äì358\n",
      "Bagged estimate, 404, 406\n",
      "Bagging, 226, 327, 406, 408, 419\n",
      "Balance equations, 256\n",
      "Barycentric plot, 259Basis expansion, 375\n",
      "Bayes\n",
      "deconvolution, 421‚Äì424\n",
      "factor, 244, 285\n",
      "false-discovery rate, 279\n",
      "posterior distribution, 254\n",
      "posterior probability, 280\n",
      "shrinkage, 212\n",
      "t-statistic, 255\n",
      "theorem, 22\n",
      "Bayes‚Äìfrequentist estimation, 412‚Äì417\n",
      "Bayesian\n",
      "inference, 22‚Äì37\n",
      "information criterion, 246\n",
      "lasso, 420\n",
      "lasso prior, 415\n",
      "model selection, 244\n",
      "trees, 349\n",
      "Bayesian information criterion, 267\n",
      "Bayesianism, 3\n",
      "BCa\n",
      "accuracy and correctness, 205\n",
      "conÔ¨Ådence density, 202, 207, 237, 242,\n",
      "243\n",
      "interval, 202\n",
      "method, 192\n",
      "Benjamini and Hochberg, 276\n",
      "Benjamini‚ÄìYekutieli, 400\n",
      "Bernoulli, 338\n",
      "Best-approximating linear subspace, 363\n",
      "Best-subset selection, 299\n",
      "Beta\n",
      "distribution, 54, 239\n",
      "BHq, 276\n",
      "Bias, 14, 352\n",
      "Bias-corrected, 330\n",
      "and accelerated, seeBCa method\n",
      "conÔ¨Ådence intervals, 190‚Äì191\n",
      "percentile method, 190\n",
      "467468 Subject Index\n",
      "Bias-correction value, 191\n",
      "Biased estimation, 321\n",
      "BIC, seeBayesian information criterion\n",
      "Big-data era, xv, 446\n",
      "Binomial, 109, 117\n",
      "distribution, 54, 117, 239\n",
      "log-likelihood, 380\n",
      "standard deviation, 111\n",
      "Bioassay, 109\n",
      "Biometrika , 449\n",
      "Bivariate normal, 182\n",
      "Bonferroni bound, 273\n",
      "Boole‚Äôs inequality, 274\n",
      "Boosting, 320, 324, 333‚Äì350\n",
      "Bootstrap, 7, 155‚Äì180, 266, 327\n",
      "Baron Munchausen, 177\n",
      "Bayesian, 168, 179\n",
      "cdf, 187\n",
      "conÔ¨Ådence intervals, 181‚Äì207\n",
      "ideal estimate, 161, 179\n",
      "jackknife after, 179\n",
      "moving blocks, 168\n",
      "multisample, 167\n",
      "nonparametric, 159‚Äì163, 217\n",
      "out of bootstrap, 232\n",
      "packages, 178\n",
      "parametric, 169‚Äì173, 223, 312, 429\n",
      "probabilities, 164\n",
      "replication, 159\n",
      "sample, 159\n",
      "sample size, 179, 205\n",
      "smoothing, 226, 404, 406\n",
      "t, 196\n",
      "tintervals, 195‚Äì198\n",
      "Bound form, 305\n",
      "Bounding hyperplane, 398\n",
      "Burn-in, 260\n",
      "BYqalgorithm, 400\n",
      "Causal inference, xvi\n",
      "Censored\n",
      "data, 134‚Äì139\n",
      "not truncated, 150\n",
      "Centering, 107\n",
      "Central limit theorem, 119\n",
      "Chain rule for differentiation, 356\n",
      "Classic statistical inference, 3‚Äì73\n",
      "ClassiÔ¨Åcation, 124, 209\n",
      "ClassiÔ¨Åcation accuracy, 375\n",
      "ClassiÔ¨Åcation error, 209\n",
      "ClassiÔ¨Åcation tree, 348\n",
      "Cochran‚ÄìMantel‚ÄìHaenszel test, 131Coherent behavior, 261\n",
      "Common task framework, 447\n",
      "Compliance, 394\n",
      "Computational bottleneck, 128\n",
      "Computer age, xv\n",
      "Computer-intensive, 127\n",
      "inference, 189, 267\n",
      "statistics, 159\n",
      "Conditional, 58\n",
      "Conditional distribution\n",
      "full, 253\n",
      "Conditional inference, 45‚Äì48, 139, 142\n",
      "lasso, 318\n",
      "Conditionality, 44\n",
      "ConÔ¨Ådence\n",
      "density, 200, 201, 235\n",
      "distribution, 198‚Äì203\n",
      "interval, 17\n",
      "region, 397\n",
      "Conjugate, 253, 259\n",
      "prior, 238\n",
      "priors, 237\n",
      "Convex optimization, 304, 308, 321, 323,\n",
      "377\n",
      "Convolution, 422, 445\n",
      "Ô¨Ålters, 368\n",
      "layer, 367\n",
      "Corrected differences, 411\n",
      "Correlation effects, 295\n",
      "Covariance\n",
      "formula, 312\n",
      "penalty, 218‚Äì226\n",
      "Coverage, 181\n",
      "Coverage level, 274\n",
      "Coverage matching prior, 236‚Äì237\n",
      "Cox model, seeproportional hazards\n",
      "model\n",
      "Cp, 217, 218, 221, 231, 267, 300, 394,\n",
      "395, 403\n",
      "Cram ¬¥er‚ÄìRao lower bound, 44\n",
      "Credible interval, 198, 417\n",
      "Cross-validation, 208‚Äì232, 267, 335\n",
      "10-fold, 326\n",
      "estimate, 214\n",
      "K-fold, 300\n",
      "leave one out, 214, 231\n",
      "Cumulant generating function, 67\n",
      "Curse of dimensionality, 387\n",
      "Dark energy, 210, 231\n",
      "Data analysis, 450\n",
      "Data science, xvii, 450, 451Subject Index 469\n",
      "Data sets\n",
      "ALS, 334\n",
      "AML,seeleukemia\n",
      "baseball , 94\n",
      "butterfly , 78\n",
      "cell infusion , 112\n",
      "cholesterol , 395, 402, 403\n",
      "CIFAR-100 , 365\n",
      "diabetes , 98, 209, 396, 414, 416\n",
      "dose-response , 109\n",
      "galaxy , 120\n",
      "handwritten digits\n",
      "(MNIST) , 353\n",
      "head/neck cancer , 135\n",
      "human ancestry , 257\n",
      "insurance , 131\n",
      "kidney function , 157, 222\n",
      "leukemia , 176, 196, 377\n",
      "NCOG , 134\n",
      "nodes , 424, 427, 430, 438, 439, 442\n",
      "pediatric cancer , 143\n",
      "police , 287\n",
      "prostate , 249, 272, 289, 408, 410,\n",
      "423, 434‚Äì436\n",
      "protein classification , 385\n",
      "shakespear , 81\n",
      "spam , 113, 127, 209, 215, 300‚Äì302,\n",
      "325\n",
      "student score , 173, 181, 186,\n",
      "202, 203\n",
      "supernova , 210, 212, 217, 221, 224\n",
      "vasoconstriction , 240, 241,\n",
      "246, 252\n",
      "Data snooping, 398\n",
      "De Finetti, B., 35, 36, 251, 450\n",
      "De Finetti‚ÄìSavage school, 251\n",
      "Debias, 318\n",
      "Decision rule, 275\n",
      "Decision theory, xvi\n",
      "Deconvolution, 422\n",
      "Deep learning, 351‚Äì374\n",
      "DeÔ¨Ånitional bias, 431\n",
      "Degrees of freedom, 221, 231, 312‚Äì313\n",
      "Delta method, 15, 414, 420\n",
      "Deviance, 112, 118, 119, 301\n",
      "Deviance residual, 123\n",
      "Diffusion tensor imaging, 291\n",
      "Direct evidence, 105, 109, 421\n",
      "Directional derivatives, 158\n",
      "Distribution\n",
      "beta, 54, 239binomial, 54, 117, 239\n",
      "gamma, 54, 117, 239\n",
      "Gaussian, 54\n",
      "normal, 54, 117, 239\n",
      "Poisson, 54, 117, 239\n",
      "Divide-and-conquer algorithm, 325\n",
      "Document retrieval, 298\n",
      "Dose‚Äìresponse, 109\n",
      "Dropout learning, 368, 372\n",
      "DTI, seediffusion tensor imaging\n",
      "Early computer-age, xvi, 75‚Äì268\n",
      "Early stopping, 362\n",
      "Effect size, 272, 288, 399, 408\n",
      "EfÔ¨Åciency, 44, 120\n",
      "Eigenratio, 162, 173, 194\n",
      "Elastic net, 316, 356\n",
      "Ellipsoid, 398\n",
      "EM algorithm, 146‚Äì150\n",
      "missing data, 266\n",
      "Empirical Bayes, 75‚Äì90, 93, 264\n",
      "estimation strategies, 421‚Äì445\n",
      "information, 443\n",
      "large-scale testing, 278‚Äì282\n",
      "Empirical null, 286\n",
      "estimation, 289‚Äì290\n",
      "maximum-likelihood estimation, 296\n",
      "Empirical probability distribution, 160\n",
      "Ensemble, 324, 334\n",
      "Ephemeral predictors, 227\n",
      "Epoch, 359\n",
      "Equilibrium distribution, 256\n",
      "Equivariant, 106\n",
      "Exact inferences, 119\n",
      "Expectation parameter, 118\n",
      "Experimental design, xvi\n",
      "Exponential family, 53‚Äì72, 225\n",
      "p-parameter, 117, 413, 424\n",
      "curved, 69\n",
      "one-parameter, 116\n",
      "Fdistribution, 397\n",
      "Ftests, 394\n",
      "f-modeling, 424, 434, 440‚Äì444\n",
      "Fake-data principle, 148, 154, 266\n",
      "False coverage\n",
      "control, 399\n",
      "False discovery, 275\n",
      "control, 399\n",
      "control theorem, 294\n",
      "proportion, 275\n",
      "rate, 271‚Äì297\n",
      "False-discovery470 Subject Index\n",
      "rate, 9\n",
      "Family of probability densities, 64\n",
      "Family-wise error rate, 274\n",
      "FDR, seefalse-discovery rate\n",
      "Feed-forward, 351\n",
      "Fiducial, 267\n",
      "constructions, 199\n",
      "density, 200\n",
      "inference, 51\n",
      "Fisher, 79\n",
      "Fisher information, 29, 41, 59\n",
      "bound, 41\n",
      "matrix, 236, 427\n",
      "Fisherian correctness, 205\n",
      "Fisherian inference, 38‚Äì52, 235\n",
      "Fixed-knot regression splines, 345\n",
      "Flat prior, 235\n",
      "Forward pass, 357\n",
      "Forward-stagewise, 346\n",
      "Ô¨Åtting, 320\n",
      "Forward-stepwise, 298‚Äì303\n",
      "computations, 322\n",
      "logistic regression, 322\n",
      "regression, 300\n",
      "Fourier\n",
      "method, 440\n",
      "transform, 440\n",
      "Frailties, 439\n",
      "Frequentism, 3, 12‚Äì22, 30, 35, 51, 146,\n",
      "267\n",
      "Frequentist, 413\n",
      "inference, 12‚Äì21\n",
      "strongly, 218\n",
      "Fully connected layer, 368\n",
      "Functional gradient descent, 340\n",
      "FWER, seefamily-wise error rate\n",
      "g-modeling, 423\n",
      "Gamma, 117\n",
      "distribution, 54, 117, 239\n",
      "General estimating equations, xvi\n",
      "General information criterion, 248\n",
      "Generalized\n",
      "linear mixed model, 437‚Äì440\n",
      "linear model, 108‚Äì123, 266\n",
      "ridge problem, 384\n",
      "Genome, 257\n",
      "Genome-wide association studies, 451\n",
      "Gibbs sampling, 251‚Äì260, 267, 414\n",
      "GLM, seegeneralized linear model\n",
      "GLMM, seegeneralized linear mixed\n",
      "modelGoogle Ô¨Çu trends, 230, 232\n",
      "Gradient boosting, 338‚Äì341\n",
      "Gradient descent, 354, 356\n",
      "Gram matrix, 381\n",
      "Gram-Schmidt orthogonalization, 322\n",
      "Graphical lasso, 321\n",
      "Graphical models, xvi\n",
      "Greenwood‚Äôs formula, 137, 151\n",
      "Group lasso, 321\n",
      "Hadamard product, 358\n",
      "Handwritten digits, 353\n",
      "Haplotype estimation, 261\n",
      "Hazard rate, 131‚Äì134\n",
      "parametric estimate, 138\n",
      "Hidden layer, 351, 352, 354\n",
      "High-order interaction, 325\n",
      "Hinge loss, 380\n",
      "Hints\n",
      "learning with, 369\n",
      "Hoeffding‚Äôs lemma, 118\n",
      "Holm‚Äôs procedure, 274, 294\n",
      "Homotopy path, 306\n",
      "Hypergeometric distribution, 141, 152\n",
      "Imputation, 149\n",
      "Inadmissible, 93\n",
      "Indirect evidence, 102, 109, 266, 290,\n",
      "421, 440, 443\n",
      "Inductive inference, 120\n",
      "Inference, 3\n",
      "Inference after model selection, 394‚Äì420\n",
      "Inferential triangle, 446\n",
      "InÔ¨Ånitesimal forward stagewise, 320\n",
      "InÔ¨Ånitesimal jackknife, 167\n",
      "estimate, 406\n",
      "standard deviations, 407\n",
      "InÔ¨Çuence function, 174‚Äì177\n",
      "empirical, 175\n",
      "InÔ¨Çuenza outbreaks, 230\n",
      "Input distortion, 369, 373\n",
      "Input layer, 355\n",
      "Insample error, 219\n",
      "Inverse chi-squared, 262\n",
      "Inverse gamma, 239, 262\n",
      "IRLS, seeiteratively reweighted least\n",
      "squares\n",
      "Iteratively reweighted least squares, 301,\n",
      "322\n",
      "Jackknife, 155‚Äì180, 266, 330\n",
      "estimate of standard error, 156\n",
      "standard error, 178Subject Index 471\n",
      "James‚ÄìStein\n",
      "estimation, 91‚Äì107, 282, 305, 410\n",
      "ridge regression, 265\n",
      "Jeffreys\n",
      "prior, 237\n",
      "Jeffreys‚Äô\n",
      "prior, 28‚Äì30, 36, 198, 203, 236\n",
      "prior, multiparameter, 242\n",
      "scale, 285\n",
      "Jumpiness of estimator, 405\n",
      "Kaplan‚ÄìMeier, 131, 134, 136, 137\n",
      "estimate, 134‚Äì139, 266\n",
      "Karush‚ÄìKuhn‚ÄìTucker optimality\n",
      "conditions, 308\n",
      "Kernel\n",
      "function, 382\n",
      "logistic regression, 386\n",
      "method, 375‚Äì393\n",
      "smoothing, 375, 387‚Äì390\n",
      "SVM, 386\n",
      "trick, 375, 381‚Äì383, 392\n",
      "Knots, 309\n",
      "Kullback‚ÄìLeibler distance, 112\n",
      "`1regularization, 321\n",
      "Lagrange\n",
      "dual, 381\n",
      "form, 305, 308\n",
      "multiplier, 391\n",
      "primal, 391\n",
      "Large-scale\n",
      "hypothesis testing, 271‚Äì297\n",
      "testing, 272‚Äì275\n",
      "Large-scale prediction algorithms, 446\n",
      "Lasso, 101, 210, 217, 222, 231, 298‚Äì323\n",
      "modiÔ¨Åcation, 312\n",
      "path, 312\n",
      "penalty, 356\n",
      "Learning from the experience of others,\n",
      "104, 280, 290, 421, 443\n",
      "Learning rate, 358\n",
      "Least squares, 98, 112, 299\n",
      "Least-angle regression, 309‚Äì313, 321\n",
      "Least-favorable family, 262\n",
      "Left-truncated, 150\n",
      "Lehmann alternative, 294\n",
      "Life table, 131‚Äì134\n",
      "Likelihood function, 38\n",
      "concavity, 118\n",
      "Limited-translation rule, 293\n",
      "Lindsey‚Äôs method, 68, 171\n",
      "Linearly separable, 375Link function, 237, 340\n",
      "Local false-discovery rate, 280, 282‚Äì286\n",
      "Local regression, 387‚Äì390, 393\n",
      "Local translation invariance, 368\n",
      "Log polynomial regression, 410\n",
      "Log-rank statistic, 152\n",
      "Log-rank test, 131, 139‚Äì142, 152, 266\n",
      "Logic of inductive inference, 185, 205\n",
      "Logistic regression, 109‚Äì115, 139, 214,\n",
      "299, 375\n",
      "multiclass, 355\n",
      "Logit, 109\n",
      "Loss plus penalty, 385\n",
      "Machine learning, 208, 267, 375\n",
      "Mallows‚ÄôCp,seeCp\n",
      "Mantel‚ÄìHaenzel test, 131\n",
      "MAP, 101\n",
      "MAP estimate, 420\n",
      "Margin, 376\n",
      "Marginal density, 409, 422\n",
      "Markov chain Monte Carlo, seeMCMC\n",
      "Markov chain theory, 256\n",
      "Martingale theory, 294\n",
      "Matching prior, 198, 200\n",
      "Matlab, 271\n",
      "Matrix completion, 321\n",
      "Max pool layer, 366\n",
      "Maximized a-posteriori probability, see\n",
      "MAP\n",
      "Maximum likelihood, 299\n",
      "Maximum likelihood estimation, 38‚Äì52\n",
      "MCMC, 234, 251‚Äì260, 267, 414\n",
      "McNemar test, 341\n",
      "Mean absolute deviation, 447\n",
      "Median unbiased, 190\n",
      "Memory-based methods, 390\n",
      "Meter reader, 30\n",
      "Meter-reader, 37\n",
      "Microarrays, 227, 271\n",
      "Minitab, 271\n",
      "MisclassiÔ¨Åcation error, 302\n",
      "Missing data, 146‚Äì150, 325\n",
      "EM algorithm, 266\n",
      "Missing-species problem, 78‚Äì84\n",
      "Mixed features, 325\n",
      "Mixture density, 279\n",
      "Model averaging, 408\n",
      "Model selection, 243‚Äì250, 398\n",
      "criteria, 250\n",
      "Monotone lasso, 320\n",
      "Monotonic increasing function, 184472 Subject Index\n",
      "Multinomial\n",
      "distribution, 61‚Äì64, 425\n",
      "from Poisson, 63\n",
      "Multiple testing, 272\n",
      "Multivariate\n",
      "analysis, 119\n",
      "normal, 55‚Äì59\n",
      "n-gram, 385\n",
      "N-P complete, 299\n",
      "Nadaraya‚ÄìWatson estimator, 388\n",
      "Natural parameter, 116\n",
      "Natural spline model, 430\n",
      "NCOG, seeNorthern California\n",
      "Oncology Group\n",
      "Nested models, 299\n",
      "Neural Information Processing Systems,\n",
      "372\n",
      "Neural network, 351‚Äì374\n",
      "adaptive tuning, 360\n",
      "number of hidden layers, 361\n",
      "Neurons, 351\n",
      "Neyman‚Äôs construction, 181, 183, 193,\n",
      "204\n",
      "Neyman‚ÄìPearson, 18, 19, 293\n",
      "Non-null, 272\n",
      "Noncentral chi-square variable, 207\n",
      "Nonlinear transformations, 375\n",
      "Nonlinearity, 361\n",
      "Nonparameteric\n",
      "regression, 375\n",
      "Nonparametric, 53, 127\n",
      "MLE, 150, 160\n",
      "percentile interval, 187\n",
      "Normal\n",
      "correlation coefÔ¨Åcient, 182\n",
      "distribution, 54, 117, 239\n",
      "multivariate, 55‚Äì59\n",
      "regression model, 414\n",
      "theory, 119\n",
      "Northern California Oncology Group,\n",
      "134\n",
      "Nuclear norm, 321\n",
      "Nuisance parameters, 142, 199\n",
      "Objective Bayes, 36, 267\n",
      "inference, 233‚Äì263\n",
      "intervals, 198‚Äì203\n",
      "prior distribution, 234‚Äì237\n",
      "OCR, seeoptical character recognition\n",
      "Offset, 349\n",
      "OLS\n",
      "algorithm, 403estimation, 395\n",
      "predictor, 221\n",
      "One-sample nonparametric bootstrap,\n",
      "161\n",
      "One-sample problems, 156\n",
      "OOB, seeout-of-bag error\n",
      "Optical character recognition, 353\n",
      "Optimal separating hyperplane, 375‚Äì377\n",
      "Optimal-margin classiÔ¨Åer, 376\n",
      "Optimality, 18\n",
      "Oracle, 275\n",
      "Orthogonal parameters, 262\n",
      "Out-of-bag error, 232, 327, 329‚Äì330\n",
      "Out-the-box learning algorithm, 324\n",
      "Output layer, 352\n",
      "Outsample error, 219\n",
      "Over parametrized, 298\n",
      "OverÔ¨Åtting, 304\n",
      "Overshrinks, 97\n",
      "p-value, 9, 282\n",
      "Package/program\n",
      "gbm, 335, 348\n",
      "glmnet , 214, 315, 322, 348\n",
      "h2o, 372\n",
      "lars , 312, 320\n",
      "liblineaR , 381\n",
      "locfdr , 289‚Äì291, 296, 437\n",
      "lowess , 6, 222, 388\n",
      "nlm, 428\n",
      "randomForest , 327, 348\n",
      "selectiveInference , 323\n",
      "Pairwise inner products, 381\n",
      "Parameter space, 22, 29, 54, 62, 66\n",
      "Parametric bootstrap, 242\n",
      "Parametric family, 169\n",
      "Parametric models, 53‚Äì72\n",
      "Partial likelihood, 142, 145, 151, 153,\n",
      "266, 341\n",
      "Partial logistic regression, 152\n",
      "Partial residual, 346\n",
      "Path-wise coordinate descent, 314\n",
      "Penalized\n",
      "least squares, 101\n",
      "likelihood, 101, 428\n",
      "logistic regression, 356\n",
      "maximum likelihood, 226, 307\n",
      "Percentile method, 185‚Äì190\n",
      "central interval, 187\n",
      "Permutation null, 289, 296\n",
      "Permutation test, 49‚Äì51\n",
      "Phylogenetic tree, 261Subject Index 473\n",
      "Piecewise\n",
      "linear, 313\n",
      "nonlinear, 314\n",
      "Pivotal\n",
      "argument, 183\n",
      "quantity, 196, 198\n",
      "statistic, 16\n",
      ".632 rule, 232\n",
      "Poisson, 117, 193\n",
      "distribution, 54, 117, 239\n",
      "regression, 120‚Äì123, 249, 284, 295,\n",
      "435\n",
      "Poisson regression, 171\n",
      "Polynomial kernel, 382, 392\n",
      "Positive-deÔ¨Ånite function, 382\n",
      "Post-selection inference, 317, 394‚Äì420\n",
      "Posterior density, 235, 238\n",
      "Posterior distribution, 416\n",
      "Postwar era, 264\n",
      "Prediction\n",
      "errors, 216\n",
      "rule, 208‚Äì213\n",
      "Predictors, 124, 208\n",
      "Principal components, 362\n",
      "Prior distribution, 234‚Äì243\n",
      "beta, 239\n",
      "conjugate, 237‚Äì243\n",
      "coverage matching, 236‚Äì237\n",
      "gamma, 239\n",
      "normal, 239\n",
      "objective Bayes, 234\n",
      "proper, 239\n",
      "Probit analysis, 112, 120, 128\n",
      "Propagation of errors, 420\n",
      "Proper prior, 239\n",
      "Proportional hazards model, 131,\n",
      "142‚Äì146, 266\n",
      "Proximal-Newton, 315\n",
      "q-value, 280\n",
      "QQ plot, 287\n",
      "QR decomposition, 311, 322\n",
      "Quadratic program, 377\n",
      "Quasilikelihood, 266\n",
      "Quetelet, Adolphe, 449\n",
      "R, 178, 271\n",
      "Random forest, 209, 229, 324‚Äì332,\n",
      "347‚Äì350\n",
      "adaptive nearest-neighbor estimator,\n",
      "328\n",
      "leave-one-out cross-validated error,\n",
      "329Monte Carlo variance, 330\n",
      "sampling variance, 330\n",
      "standard error, 330‚Äì331\n",
      "Randomization, 49‚Äì51\n",
      "Rao‚ÄìBlackwell, 227, 231\n",
      "Rate annealing, 360\n",
      "RectiÔ¨Åed linear, 359\n",
      "Regression, 109\n",
      "Regression rule, 219\n",
      "Regression to the mean, 33\n",
      "Regression tree, 124‚Äì128, 266, 348\n",
      "Regularization, 101, 173, 298, 379, 428\n",
      "path, 306\n",
      "Relevance, 290‚Äì293\n",
      "Relevance function, 293\n",
      "Relevance theory, 297\n",
      "Reproducing kernel Hilbert space, 375,\n",
      "384, 392\n",
      "Resampling, 163\n",
      "plans, 163‚Äì169\n",
      "simplex, 164, 169\n",
      "vector, 163\n",
      "Residual deviance, 283\n",
      "Response, 124, 208\n",
      "Ridge regression, 97‚Äì102, 209, 304, 327,\n",
      "332, 372, 381\n",
      "James‚ÄìStein, 265\n",
      "Ridge regularization, 368\n",
      "logistic regression, 392\n",
      "Right-censored, 150\n",
      "Risk set, 144\n",
      "RKHS, seereproducing-kernel Hilbert\n",
      "space\n",
      "Robbins‚Äô formula, 75, 77, 422, 440\n",
      "Robust estimation, 174‚Äì177\n",
      "Royal Statistical Society, 449\n",
      "S language, 271\n",
      "Sample correlation coefÔ¨Åcient, 182\n",
      "Sample size coherency, 248\n",
      "Sampling distribution, 312\n",
      "SAS, 271\n",
      "Savage, L. J., 35, 36, 51, 199, 233, 251,\n",
      "450\n",
      "Scale of evidence\n",
      "Fisher, 245\n",
      "Jeffreys, 245\n",
      "Scheff ¬¥e\n",
      "interval, 396, 397, 417\n",
      "theorem, 398\n",
      "Score function, 42\n",
      "Score tests, 301474 Subject Index\n",
      "Second-order accuracy, 192‚Äì195\n",
      "Selection bias, 33, 408‚Äì411\n",
      "Self-consistent, 149\n",
      "Separating hyperplane, 375\n",
      "geometry, 390\n",
      "Seven-league boots, 448\n",
      "Shrinkage, 115, 316, 338\n",
      "estimator, 59, 91, 94, 96, 410\n",
      "Sigmoid function, 352\n",
      "SigniÔ¨Åcance level, 274\n",
      "Simulation, 155‚Äì207\n",
      "Simultaneous conÔ¨Ådence intervals,\n",
      "395‚Äì399\n",
      "Simultaneous inference, 294, 418\n",
      "Sinc kernel, 440, 445\n",
      "Single-nucleotide polymorphism, see\n",
      "SNP\n",
      "Smoothing operator, 346\n",
      "SNP, 257\n",
      "Soft-margin classiÔ¨Åer, 378‚Äì379\n",
      "Soft-threshold, 315\n",
      "Softmax, 355\n",
      "Spam Ô¨Ålter, 115\n",
      "Sparse\n",
      "models, 298‚Äì323\n",
      "principal components, 321\n",
      "Sparse matrix, 316\n",
      "Sparsity, 321\n",
      "Split-variable randomization, 327, 332\n",
      "SPSS, 271\n",
      "Squared error, 209\n",
      "Standard candles, 210, 231\n",
      "Standard error, 155\n",
      "external, 408\n",
      "internal, 408\n",
      "Standard interval, 181\n",
      "Stein‚Äôs\n",
      "paradox, 105\n",
      "unbiased risk estimate, 218, 231\n",
      "Stepwise selection, 299\n",
      "Stochastic gradient descent, 358\n",
      "Stopping rule, 32, 413\n",
      "Stopping rules, 243\n",
      "String kernel, 385, 386\n",
      "Strong rules, 316, 322\n",
      "Structure, 261\n",
      "Structure matrix, 97, 424\n",
      "Studentt\n",
      "conÔ¨Ådence interval, 396\n",
      "distribution, 196, 272\n",
      "statistic, 449two-sample, 8, 272\n",
      "Studentized range, 418\n",
      "Subgradient\n",
      "condition, 308\n",
      "equation, 312, 315\n",
      "Subjective prior distribution, 233\n",
      "Subjective probability, 233\n",
      "Subjectivism, 35, 233, 243, 261\n",
      "SufÔ¨Åciency, 44\n",
      "SufÔ¨Åcient\n",
      "statistic, 66, 112, 116\n",
      "vector, 66\n",
      "Supervised learning, 352\n",
      "Support\n",
      "set, 377, 378\n",
      "vector, 377\n",
      "vector classiÔ¨Åers, 381\n",
      "vector machine, 319, 375‚Äì393\n",
      "SURE, seeStein‚Äôs unbiased risk estimate\n",
      "Survival analysis, 131‚Äì154, 266\n",
      "Survival curve, 137, 279\n",
      "SVM\n",
      "Lagrange dual, 391\n",
      "Lagrange primal, 391\n",
      "loss function, 391\n",
      "Taylor series, 157, 420\n",
      "Theoretical null, 286\n",
      "Tied weights, 368\n",
      "Time series, xvi\n",
      "Training set, 208\n",
      "Transformation invariance, 183‚Äì185, 236\n",
      "Transient episodes, 228\n",
      "Trees\n",
      "averaging, 348\n",
      "best-Ô¨Årst, 333\n",
      "depth, 335\n",
      "terminal node, 126\n",
      "Tricube kernel, 388, 389\n",
      "Trimmed mean, 175\n",
      "Triple-point, xv\n",
      "True error rate, 210\n",
      "True-discovery rates, 286\n",
      "Tukey, J. W., 418, 450\n",
      "Tukey, J. W., 418\n",
      "Tweedie‚Äôs formula, 409, 419, 440\n",
      "Twenty-Ô¨Årst-century methods, xvi,\n",
      "271‚Äì446\n",
      "Two-groups model, 278\n",
      "Uncorrected differences, 411\n",
      "Uninformative prior, 28, 169, 233, 261\n",
      "Universal approximator, 351Subject Index 475\n",
      "Unlabeled images, 365\n",
      "Unobserved covariates, 288\n",
      "Validation set, 213\n",
      "Variable-importance plot, 331‚Äì332, 336\n",
      "Variance, 14\n",
      "Variance reduction, 324\n",
      "Velocity vector, 360\n",
      "V oting, 333\n",
      "Warm starts, 314, 363\n",
      "Weak learner, 333, 342\n",
      "Weight\n",
      "decay, 356\n",
      "regularization, 361, 362\n",
      "sharing, 352, 367Weighted exponential loss, 345\n",
      "Weighted least squares, 315\n",
      "Weighted majority vote, 341\n",
      "Weights, 352\n",
      "Wide data, 298, 321\n",
      "Wilks‚Äô likelihood ratio statistic, 246\n",
      "Winner‚Äôs curse, 33, 408\n",
      "Winsorized mean, 175\n",
      "Working response, 315, 322\n",
      "z.Àõ/, 188\n",
      "Zero set, 296\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../files/Computer Age Statistical Inference Book.pdf\" # this would be your plug and play pdf file \n",
    "with open(file_path, 'rb') as file:\n",
    "    pdf_reader = pypdf.PdfReader(file)\n",
    "    \n",
    "    all_text = []\n",
    "\n",
    "    for page in pdf_reader.pages:\n",
    "        all_text.append(page.extract_text())\n",
    "\n",
    "print(''.join(filter(None, all_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking Context for LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_text_splitters/base.py:93\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     91\u001b[0m texts, metadatas \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 93\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[1;32m     94\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "text_splitter.split_documents(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
